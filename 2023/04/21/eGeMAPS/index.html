<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8" />
    <meta name="naver-site-verification" content="760dcf2c928601f50f3941df3b6b4629fd244c7c" />
    <!-- Google Ads -->
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2182912223281192"
    crossorigin="anonymous"></script>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-90VXCLXLJT"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-90VXCLXLJT');
    </script>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-245679127-1"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-245679127-1');
    </script>

    

    
    <title>The Geneva Minimalistic Acoustic Parameter Set (GeMAPS) for Voice Research andAffective Computing | Jang Minjee</title>
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="keywords" content="Speech Emotion Recognition,Acoustic Parameter Set" />
    
    <meta name="description" content="Journal&#x2F;Conference : IEEEYear(published year): 2016Author: Florian Eyben, Klaus R. Scherer, Bjorn W. Schuller, Johan Sundberg, Elisabeth Andre, Carlos Busso, Laurence Y. Devillers, Julien Epps, P">
<meta property="og:type" content="article">
<meta property="og:title" content="The Geneva Minimalistic Acoustic Parameter Set (GeMAPS) for Voice Research andAffective Computing">
<meta property="og:url" content="https://jmj3047.github.io/2023/04/21/eGeMAPS/index.html">
<meta property="og:site_name" content="Jang Minjee">
<meta property="og:description" content="Journal&#x2F;Conference : IEEEYear(published year): 2016Author: Florian Eyben, Klaus R. Scherer, Bjorn W. Schuller, Johan Sundberg, Elisabeth Andre, Carlos Busso, Laurence Y. Devillers, Julien Epps, P">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://jmj3047.github.io/images/eGeMAPS/Untitled.png">
<meta property="article:published_time" content="2023-04-20T15:00:00.000Z">
<meta property="article:modified_time" content="2023-04-20T21:06:58.521Z">
<meta property="article:author" content="Jang Minjee">
<meta property="article:tag" content="Speech Emotion Recognition">
<meta property="article:tag" content="Acoustic Parameter Set">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://jmj3047.github.io/images/eGeMAPS/Untitled.png">
    

    

    
        <link rel="icon" href="/images/favicon.png" />
    

    
<link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">

    
<link rel="stylesheet" href="/libs/titillium-web/styles.css">

    
<link rel="stylesheet" href="/libs/source-code-pro/styles.css">


    
<link rel="stylesheet" href="/css/style.css">


    
<script src="/libs/jquery/3.5.0/jquery.min.js"></script>

    
    
        
<link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css">

    
    
        
<link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css">

    
    
        <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=[object Object]"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', '[object Object]');
</script>
<!-- End Google Analytics -->

    
    
    


    
<link rel="stylesheet" href="https://cdn.rawgit.com/innks/NanumSquareRound/master/nanumsquareround.css">

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head>

<body>
    <div id="wrap">
        <header id="header">
    <div id="header-outer" class="outer">
        <div class="container">
            <div class="container-inner">
                <div id="header-title">
                    <h1 class="logo-wrap">
                        <a href="/" class="logo"></a>
                    </h1>
                    
                </div>
                <div id="header-inner" class="nav-container">
                    <a id="main-nav-toggle" class="nav-icon fa fa-bars"></a>
                    <div class="nav-container-inner">
                        <ul id="main-nav">
                            
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" target="_blank" rel="noopener" href="https://github.com/jmj3047/mj_portfolio">About Me</a>
                                </li>
                            
                                    <ul class="main-nav-list"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Data-Analysis/">Data Analysis</a><ul class="main-nav-list-child"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Data-Analysis/Basic/">Basic</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Data-Analysis/Model/">Model</a></li></ul></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Data-Platform-Base/">Data Platform/Base</a><ul class="main-nav-list-child"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Data-Platform-Base/GCP/">GCP</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Data-Platform-Base/MongoDB/">MongoDB</a></li></ul></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Paper/">Paper</a><ul class="main-nav-list-child"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Paper/Computer-Vision/">Computer Vision</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Paper/Generative-Model/">Generative Model</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Paper/Multi-Task-Learning/">Multi-Task Learning</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Paper/NLP/">NLP</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Paper/Recommendation-System/">Recommendation System</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Paper/Speaker-Verification/">Speaker Verification</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Paper/Speech-Emotion-Recognition/">Speech Emotion Recognition</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Paper/Speech-Representations/">Speech Representations</a></li></ul></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Python/">Python</a><ul class="main-nav-list-child"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Python/Django/">Django</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Python/HTML/">HTML</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Python/Pyspark/">Pyspark</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Python/Python/">Python</a></li></ul></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Setting/">Setting</a></li></ul>
                                
                        </ul>
                        <nav id="sub-nav">
                            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/js/insight.js"></script>


</div>
                        </nav>
                    </div>
                </div>
            </div>
        </div>
    </div>
</header>

        <div class="container">
            <div class="main-body container-inner">
                <div class="main-body-inner">
                    <section id="main">
                        <div class="main-body-header">
    <h1 class="header">
    
    <a class="page-title-link" href="/categories/Paper/">Paper</a><i class="icon fa fa-angle-right"></i><a class="page-title-link" href="/categories/Paper/Speech-Emotion-Recognition/">Speech Emotion Recognition</a>
    </h1>
</div>

                        <div class="main-body-content">
                            <article id="post-eGeMAPS" class="article article-single article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
            <header class="article-header">
                
    
        <h1 class="article-title" itemprop="name">
        The Geneva Minimalistic Acoustic Parameter Set (GeMAPS) for Voice Research andAffective Computing
        </h1>
    

                
            </header>
        
        
            <div class="article-meta">
                
  <div class="article-date">
    <i class="fa fa-calendar"></i>
    <a href="/2023/04/21/eGeMAPS/" class="article-date">
       <time datetime="2023-04-20T15:00:00.000Z" itemprop="datePublished">2023-04-21</time>
    </a>
  </div>


<div class="article-date">
  <i class="fa fa-calendar-plus-o"></i>
  <a href="/2023/04/21/eGeMAPS/" class="article-date">
     <time datetime="2023-04-20T21:06:58.521Z" itemprop="dateModified">2023-04-21</time>
  </a>
</div>


                

                
    <div class="article-tag">
        <i class="fa fa-tag"></i>
        <a class="tag-link-link" href="/tags/Acoustic-Parameter-Set/" rel="tag">Acoustic Parameter Set</a>, <a class="tag-link-link" href="/tags/Speech-Emotion-Recognition/" rel="tag">Speech Emotion Recognition</a>
    </div>

                

                

            </div>
        
        
        <div class="article-entry" itemprop="articleBody">
            

            

            

            <p>Journal&#x2F;Conference : IEEE<br>Year(published year): 2016<br>Author: Florian Eyben, Klaus R. Scherer, Bjorn W. Schuller, Johan Sundberg, Elisabeth Andre, Carlos Busso, Laurence Y. Devillers, Julien Epps, Petri Laukka, Shrikanth S. Narayanan, and Khiet P. Truong<br>Subject: Acoustic Parameter Set, eGeMAPS</p>
<blockquote>
<p>Summary</p>
</blockquote>
<ul>
<li>이 논문은 음성 특징 추출을 위한 최소한의 매개 변수 세트를 제안합니다.</li>
<li>제안된 매개 변수 세트는 다양한 감정 분류 작업에서 높은 성능을 보입니다.</li>
<li>binary arousal 및 binary valence 분류에 대한 요약 결과가 Table 2에 나와 있습니다.</li>
<li>FAU AIBO를 제외한 모든 데이터베이스와 9개의 최고 SVM 복잡도(C&#x3D;0.0025)에서 UAR(Unweighted Average Recall)을 평균화합니다.</li>
<li>각 화자에 대한 표준화와 균형 잡힌 훈련 세트를 위해 인스턴스 업샘플링이 수행됩니다.</li>
</ul>
<h4 id="88개-파라미터-정리"><a href="#88개-파라미터-정리" class="headerlink" title="88개 파라미터 정리"></a>88개 파라미터 정리</h4><ul>
<li><p><strong>기본 GeMAPS, 62개 파라미터</strong></p>
<ol>
<li>Pitch</li>
<li>Jitter</li>
<li>Formant 1 frequency</li>
<li>Formant 2 frequency</li>
<li>Formant 3 frequency</li>
<li>Formant 1</li>
<li>Shimmer</li>
<li>Loudness</li>
<li>Harmonics-to-noise ratio (HNR)</li>
<li>Alpha Ratio</li>
<li>Hammarberg Index</li>
<li>Spectral Slope 0-500 Hz</li>
<li>Spectral Slope 500-1500 Hz</li>
<li>Formant 1 relative energy</li>
<li>Formant 2 relative energy</li>
<li>Formant 3 relative energy</li>
<li>Harmonic difference H1-H2</li>
<li>Harmonic difference H1-A3</li>
</ol>
<ul>
<li>위 18개의 산술평균과 변동계수를 적용 -&gt; 36개 + loudness 8가지 함수 추가 적용 파라미터+ pitch 8가지 함수 추가 적용 파라미터 -&gt; 52개</li>
</ul>
<ol start="53">
<li>Alpha Ratio의 산술평균</li>
<li>Hammarberg Index의 산술평균</li>
<li>0-500 Hz 스펙트럼 기울기의 산술평균 (무성음 구간 전체)</li>
<li>500-1500 Hz 스펙트럼 기울기의 산술평균 (무성음 구간 전체)</li>
<li>the rate of loudness peaks</li>
<li>continuously voiced regions의 평균 길이(F0 &gt; 0)</li>
<li>continuously voiced regions의 표준편차(F0 &gt; 0)</li>
<li>unvoiced regions (approximating pauses)의 평균 길이(F0 &#x3D; 0)</li>
<li>unvoiced regions (approximating pauses)의 표준편차(F0 &#x3D; 0)</li>
<li>continuous voiced regions의 초당 개수 (pseudo syllable rate)</li>
</ol>
</li>
<li><p><strong>extended GeMAPS(eGeMAPS), 추가 26개 파라미터</strong></p>
<ul>
<li>7개의 LLD에 대해 산술평균과 변동계수 적용 -&gt; 14개의 discriptor 추가<ol>
<li>MFCC 1 (MFCC의 첫번째 계수)</li>
<li>MFCC 2</li>
<li>MFCC 3</li>
<li>MFCC 4</li>
<li>Spectral flux</li>
<li>Formant 2 bandwidth</li>
<li>Formant 2 bandwidth</li>
</ol>
</li>
<li>11개의 discriptor가 추가 <ol>
<li>무성영역에서만 spectral flux의 산술 평균</li>
<li>유성영역에서만 spectral flux의 산술 평균</li>
<li>유성영역에서만 spectral flux의 변동 계수</li>
<li>유성 영역에서만 MFCC 1의 변동 계수</li>
<li>유성 영역에서만 MFCC 2의 변동 계수</li>
<li>유성 영역에서만 MFCC 3의 변동 계수</li>
<li>유성 영역에서만 MFCC 4의 변동 계수</li>
<li>유성 영역에서만 MFCC 1의 산술 평균</li>
<li>유성 영역에서만 MFCC 2의 산술 평균</li>
<li>유성 영역에서만 MFCC 3의 산술 평균</li>
<li>유성 영역에서만 MFCC 4의 산술 평균</li>
</ol>
</li>
</ul>
<ul>
<li>equivalent soud level</li>
</ul>
<ul>
<li>이렇게 하면 확장된 파라미터에는 14+11+1 26개의 파라미터가 추가 됨</li>
</ul>
</li>
</ul>
<h2 id="0-abstract"><a href="#0-abstract" class="headerlink" title="0.abstract"></a>0.abstract</h2><ul>
<li>GeMAPS는 음성과 감정 컴퓨팅 분야에서 사용되는 기본 표준 음향 파라미터 세트입니다.</li>
<li>이 세트는 다양한 자동 음성 분석 영역에 적용될 수 있습니다.</li>
<li>GeMAPS는 감정적 생리적 변화를 색인화하는 데 잠재력이 있습니다.</li>
<li>이전 연구에서 입증된 가치와 이론적 중요성을 가지며, 자동 추출 가능합니다.</li>
<li>GeMAPS는 미래 연구 평가의 공통 기준을 제공하고, 다른 파라미터 세트 또는 동일한 파라미터의 다른 구현으로 인한 차이를 제거하기 위해 만들어졌습니다.</li>
</ul>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><ul>
<li>이 단락에서는 다양한 감정 상태의 음성 표현에 대한 관심이 오랫동안 지속되어 왔으며, 다양한 분야의 연구자들이 이 주제에 대해 연구해 왔다고 설명합니다.</li>
<li>정신과 의사는 감정 상태를 진단하기 위해 노력해 왔으며, 심리학자와 커뮤니케이션 연구자들은 감정의 신호를 전달하는 목소리의 능력을 탐구해 왔습니다.</li>
<li>언어학자와 음성학자들은 언어 생산과 지각에서 정서적 실용적 정보의 역할을 발견해 왔습니다.</li>
<li>최근에는 컴퓨터 과학자와 엔지니어들이 화자의 태도와 감정을 자동으로 인식하고 조작하여 인간 사용자가 정보 기술에 더 쉽게 접근하고 신뢰할 수 있도록 하려는 시도를 하고 있습니다.</li>
<li>이러한 연구의 대부분은 음성 신호에서 음향 매개변수를 추출하여 다양한 감정과 기타 정서적 성향이 발성 패턴을 통해 어떻게 표현되는지 이해하는 방법을 사용합니다.</li>
<li>기본 이론적 가정은 정서적 과정으로 인한 자율신경 흥분과 근육 긴장의 변화를 음향 파형의 다양한 파라미터로 추정할 수 있다는 것입니다.</li>
<li>GeMAPS는 음성과 감정 컴퓨팅 분야에서 사용되는 기본 표준 음향 파라미터 세트로, 이전 연구에서는 다양한 파라미터들이 사용되었지만 일관된 방식으로 추출되지 않았습니다. 따라서 GeMAPS는 다양한 자동 음성 분석 영역에 적용될 수 있는 기본 표준 세트로서, 감정적 생리적 변화를 색인화하는 데 잠재력이 있으며, 이전 연구에서 입증된 가치와 이론적 중요성을 가지고 있습니다.</li>
</ul>
<h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2.Related Work"></a>2.Related Work</h2><ul>
<li>CEICES는 더 엔지니어링 중심적인 “수집기(collector)” 접근 방식으로, 분류 실험에서 성공적인 파라미터들을 모두 포함합니다. 반면, GeMAPS는 다양한 출처의 교차학제적 증거와 이론적 중요성 또는 몇 가지 파라미터를 기반으로 하는 <strong>최소한의 파라미터 세트를 합의하는 보다 교차학제적인 시도입니다.</strong></li>
<li>초기 조사[15]와 최근 개요[17]는 정서적 음성 연구에 관한 수십 년간의 심리학 문헌을 잘 요약하고 있으며, 제시된 경험적 데이터를 통해<ul>
<li><strong>강도(음량)</strong></li>
<li><strong>F0(기본 주파수) 평균</strong></li>
<li><strong>변동성 및 범위</strong></li>
<li><strong>음성 신호의 고주파 콘텐츠&#x2F;에너지가 스트레스(강도, F0 평균)</strong></li>
<li><strong>분노 및 슬픔(모든 매개변수)</strong></li>
<li><strong>지루함(F0 변동성 및 범위)과 같은 전형적인 음성 정서 표현과 상관관계를 보인다는 결론</strong>을 내렸습니다.</li>
</ul>
</li>
<li><strong>speech와 articulation rate은 감정표현에 영향을 미치지 않는다</strong>는 것으로 나타났습니다.</li>
<li>[16]은 <strong>F0 및 스펙트럼 분포와 관련된 매개변수가 감정적 발화 내용에 대한 중요한 단서</strong>임을 확인함</li>
<li>[17]과 같이 대부분의 연구는 청각적 감정분석을 다루며 accoustic arousal에 대한 단서인 상당히 일관된 매개변수를 보고함</li>
<li>[24]에 보면, 그냥 음량을 측정하는거보다 다양한 주파수 대역의 신호에너지에 human-hearing’s frequency sensitivity에 따라 가중치를 부과하면 vocal affect dimension과 더 많은 상관관계가 있는 것으로 나타났음. 또한 <strong>spectral flux가 단일 피처에 대해 전반적으로 가장 높은 상관관계</strong>를 보였음.</li>
<li>[17],[25] : particular valence(감정의 긍정성 또는 부정성을 나타내는 지표)</li>
<li>[25]에서는 LOI(Level of Interest, 대화 참여자의 관심 수준을 나타내는 지표입니다. <strong>LOI는 boredom(지루함), overneutral(중립), joyful interaction(즐거운 상호작용)으로 구분) 같은 경우 MFCC의 중요성이 크다는 것을 보여줌</strong>, lower order MFCC는 스펙트럼 전체범위(first coefficient, 첫번째 계수) 또는 다양한 작은 sub-bands(second and higher coefficient, 두번째 및 상위계수)에서 어느정도 spectral tilt(slope)를 측정합니다.<ul>
<li>spectral slope(Spectral slope는 주파수 대역에서 파워 스펙트럼의 기울기를 나타내는 지표입니다. 이 논문에서는 0-500 Hz 및 500-1500 Hz 대역에서의 스펙트럼 기울기를 계산하는 방법에 대해 설명하고 있습니다. 이러한 스펙트럼 기울기는 음성 특징 추출 및 음성 감정 인식과 같은 다양한 연구 분야에서 사용됩니다.)</li>
</ul>
</li>
<li><strong>이 연구의 목표는 이전의 관련 연구 결과에 따라 관련 파라미터를 선택하는 것</strong></li>
<li>많은 automatically extracted brute-force 파라미터 세트는 formant 파라미터를 안정적으로 추출하기 어렵기 때문에 이를 무시하지만, voice research and automatic classification를 위해서는 formant 파라미터가 매우 중요.<ul>
<li>formant는 다양한 형태의 감정과 정신상태에 민감한 것으로 나타났으며 거의 sota 수준의 cognitive load classification result[27]와 우울증 인식 및 평가 결과[31],[38]를 제공하며 다른 시스템의 feature dimension에 비해 훨씬 적은수의 formant로도 경쟁력 있는 감정인식 성능을 제공할수 있기 때문</li>
<li>cognitive load: 인간의 인지 능력이 처리해야 하는 정보의 양과 복잡성에 따라 발생하는 정신적인 부담을 의미</li>
</ul>
</li>
<li>그래서 이 논문에 <strong>제안된 파라미터세트에는 formant도 포함</strong>되어 있음.</li>
<li>fundamental frequency(기본 주파수[6])와 진폭&#x2F;강도의 중요성이 입증되었기 때문에, 강력한 fundamental frequency 측정과 pseudo-auditory loudness(유사 청각적 음량) 측정이 제안된 세트에 포함되어 있습니다.</li>
<li>분포 변화를 포착하기 위해 시간에 따른 다양한 통계가 두 매개변수에 적용됩니다.</li>
<li>고주파 콘텐츠와 스펙트럼 밸런스를 강력하게 표현하기 위해 alpha 비율, Hammerberg 지수, 스펙트럼 기울기를 설명자로 고려합니다.<ul>
<li>Alpha Ratio는 음성 신호의 저주파 대역과 고주파 대역 간의 에너지 비율을 나타내는 지표입니다. 이 논문에서는 Alpha Ratio가 50-1000 Hz와 1-5 kHz 대역에서의 에너지 합의 비율로 정의되며, Hammarberg index와 유사한 방식으로 계산됩니다.</li>
<li>Hammarberg index는 음성 신호의 저주파 대역과 고주파 대역 간의 에너지 비율을 나타내는 지표입니다. 이 논문에서는 Hammarberg index가 0-2 kHz와 2-5 kHz 대역에서 가장 큰 에너지 피크 간의 비율로 정의되며, 고정된 pivot point인 2 kHz를 기준으로 계산</li>
</ul>
</li>
<li>vocal timbre(보컬 음색)은 MFCC로 인코딩되고, F0의 period-to-period jitter와 shimmer를 사용하여 음성 발화 신호의 질을 평가.<ul>
<li>Vocal timbre는 음성의 톤 색깔이나 음색을 나타내는 용어</li>
<li>F0의 period-to-period jitter와 shimmer는 음성 발화 신호의 안정성과 규칙성을 나타내는 지표입니다. Jitter는 연속된 F0 주기 간 최고점의 차이를 나타내며, shimmer는 연속된 F0 주기 간 최고점의 크기 차이를 나타냅니다. 이러한 지표들은 음성 발화 신호의 질을 평가하는 데 사용</li>
</ul>
</li>
<li>모음 기반 음성 연구를 허용하고 특정 작업에 대한 관련성이 입증된 formant 파라미터도 세트에 포함되어 있습니다.</li>
</ul>
<h2 id="3-Acoustic-Parameter-Recommendation"><a href="#3-Acoustic-Parameter-Recommendation" class="headerlink" title="3.Acoustic Parameter Recommendation"></a>3.Acoustic Parameter Recommendation</h2><ul>
<li>여기에 제시된 권장 사항은 제네바에서 열린 음성 및 언어 과학자들의 학제 간 회의에서 고안되었으며 뮌헨공과대학교(TUM)에서 더욱 발전시켰습니다.</li>
<li>세가지 기준에 따라 매개변수의 선택이 이루어졌음<ol>
<li>정서적 과정 중 음성 생성의 생리적 변화를 지표화 할수 있는 음향 매개변수의 잠재력<ul>
<li>voice production 과정에서 감정적 변화가 생기면서 생리학적 변화를 나타내는 음향 파라미터의 potential.</li>
</ul>
</li>
<li>과거 문헌에서 해당 매개변수가 사용된 빈도 및 성공 여부(related works 참조)</li>
<li>이론적 중요성([1],[2])</li>
</ol>
</li>
<li>두가지 버전의 acoustic parameter set가 권장됨<ul>
<li>저자의 이전 연구에서 가장 중요한 것으로 밝혀진 prosodic, excitation, vocal tract, and spectral descriptors를 구현하는 최소한의 파라미터 세트</li>
<li>문학([40])에서 pure prosodic 및 스펙트럼 마라미터 세트보다 automatic affect recognition의 정확도를 높이는 것으로 일관되게 알려진 소량의 cepstral descriptors를 포함하는 최소한의 세트에 대한 확장</li>
<li>제안하는 음향 파라미터 세트: 여기서는 두 가지 버전의 음향 파라미터 세트를 제안하고 있습니다. 첫 번째는 저자들의 이전 연구에서 가장 중요하다고 판단된 prosodic, excitation, vocal tract 및 spectral descriptors를 구현한 최소한의 파라미터 세트입니다. 두 번째는 최소한의 파라미터 세트에 cepstral descriptors를 추가한 확장된 버전입니다. 이러한 cepstral descriptors는 [40]과 같은 literature에서 자주 언급되며, 순수한 prosodic 및 spectral parameter set보다 자동 감정 인식의 정확도를 높이는 것으로 알려져 있습니다.</li>
</ul>
</li>
<li>자동 파라미터 선택에 대한 연구 결과와 MFCCs의 기본 함수에 대한 설명,  [23], [24]와 같은 자동 파라미터 선택에 관한 연구들은, 감정 및 언어적 음성 분석 작업에서 낮은 차수의 MFCCs가 더 중요하다는 것을 시사합니다. MFCCs를 계산할 때 사용되는 이산 코사인 변환(DCT-II) 기본 함수를 살펴보면, 낮은 차수의 MFCCs가 스펙트럼 기울기와 스펙트럼 에너지의 전체 분포와 관련이 있다는 것이 분명합니다. 높은 차수의 MFCCs는 보다 정교한 에너지 분포를 반영하며, 음성 속성(non-verbal voice attribute)보다는 음운 내용(phonetic content)을 식별하는 데 더 중요할 것으로 추정됩니다.</li>
</ul>
<h3 id="3-1-Minimalistic-Parameter-Set"><a href="#3-1-Minimalistic-Parameter-Set" class="headerlink" title="3.1 Minimalistic Parameter Set"></a>3.1 Minimalistic Parameter Set</h3><ul>
<li>18개의 Low-Level descriptors<ul>
<li><strong>Frequency related parameters<code>(1~6)</code>:</strong><ol>
<li><strong>Pitch</strong>, logarithmic F0 on a semitone frequency scale, starting at 27.5 Hz (semitone 0). <ul>
<li>피치, 27.5Hz(반음 0)에서 시작하는 반음 주파수 스케일에서 로그 F0입니다.</li>
<li>이 문장은 F0 값에 대한 설명입니다. F0 값은 음성 신호에서 기본 주파수를 나타내며, 이 값은 세미톤 주파수 스케일에서 로그함수로 변환됩니다. 이 스케일은 27.5 Hz (세미톤 0)에서 시작하며, 각 세미톤 간의 주파수 차이는 2^(1&#x2F;12)로 계산됩니다. 따라서, 예를 들어, 세미톤 1은 29.136 Hz이고, 세미톤 2는 30.868 Hz입니다. 이러한 변환을 통해 F0 값을 보다 직관적으로 이해할 수 있습니다.</li>
</ul>
</li>
<li><strong>Jitter</strong>, deviations in individual consecutive F0 period lengths.<ul>
<li>지터는 개별 연속 F0 주기 길이의 편차입니다.</li>
<li>이 문장은 음성 파라미터 중 하나인 Jitter에 대한 설명입니다. Jitter는 연속적인 F0 주기의 길이에서 발생하는 변동을 나타내는 지표입니다. 즉, F0 주기의 길이가 일정하지 않고 변동이 크면 Jitter 값이 높아집니다. 이러한 변동은 음성 신호에서 발생하는 국소적인 성대 운동 불규칙성으로 인해 발생할 수 있습니다. Jitter 값은 음성 장애 진단 및 감정 분석 등에 사용됩니다.</li>
</ul>
</li>
<li><strong>Formant 1 frequency</strong>, </li>
<li><strong>Formant 2 frequency</strong>, </li>
<li><strong>Formant 3 frequency,</strong> centre frequency of first, second, and third formant<ul>
<li>1,2,3 번째 formant의 중심 주파수</li>
</ul>
</li>
<li><strong>Formant 1</strong>, bandwidth of first formant.<ul>
<li>첫번째 formant의 중심 파라미터</li>
</ul>
</li>
</ol>
</li>
<li><strong>Energy&#x2F;Amplitude related parameters<code>(7~9)</code>:</strong><ol start="7">
<li><strong>Shimmer</strong>, difference of the peak amplitudes of consecutive F0 periods.<ul>
<li>쉬머는 연속된 F0 기간의 피크 진폭의 차이입니다.</li>
<li>이 문장은 음성 파라미터 중 하나인 Shimmer에 대한 설명입니다. Shimmer는 연속적인 F0 주기에서 peak amplitude의 차이를 나타내는 지표입니다. 즉, F0 주기에서 peak amplitude의 차이가 크면 Shimmer 값이 높아집니다. 이러한 변동은 음성 신호에서 발생하는 국소적인 성대 운동 불규칙성으로 인해 발생할 수 있습니다. Shimmer 값은 음성 장애 진단 및 감정 분석 등에 사용됩니다.</li>
</ul>
</li>
<li><strong>Loudness</strong>, estimate of perceived signal intensity from an auditory spectrum.<ul>
<li>청각 스펙트럼에서 감지된 신호강도의 estimate</li>
</ul>
</li>
<li><strong>Harmonics-to-noise ratio (HNR)</strong>, relation of energy in harmonic components to energy in noise-like components.<ul>
<li>Harmonics 대 잡음비(HNR)는 Harmonics 성분의 에너지와 잡음 성분의 에너지를 고조파 성분의 에너지와 잡음 성분의 에너지의 관계입니다.</li>
<li>이 문장은 음성 파라미터 중 하나인 Harmonics-to-noise ratio (HNR)에 대한 설명입니다. HNR은 음성 신호에서 harmonic components와 noise-like components 간의 에너지 비율을 나타내는 지표입니다. 즉, HNR 값이 높을수록 음성 신호에서 harmonic components의 비중이 높아지고, HNR 값이 낮을수록 noise-like components의 비중이 높아집니다. 이러한 지표는 음성 신호에서 발생하는 잡음과 왜곡 등을 분석하고, 음성 장애 진단 및 감정 분석 등에 사용됩니다.</li>
</ul>
</li>
</ol>
</li>
<li><strong>Spectral (balance) parameters<code>(10~18)</code>:</strong><ol start="10">
<li><strong>Alpha Ratio</strong>, ratio of the summed energy from 50-1000 Hz and 1-5 kHz<ul>
<li>50-1000Hz, 1-5kHz에서 합산된 에너지의 비율</li>
<li>음성 신호의 저주파 대역과 고주파 대역 간의 에너지 비율을 나타내는 지표</li>
<li>Alpha Ratio는 저주파 영역과 고주파 영역의 에너지 비율을 나타내는 파라미터입니다. 구체적으로, 50-1000 Hz와 1-5 kHz에서의 총 에너지 비율을 나타냅니다.</li>
<li>Alpha Ratio는 음성 신호의 성질을 분석하는 데 사용됩니다. 예를 들어, Alpha Ratio가 높은 경우 저주파 성분이 많은 음성 신호일 가능성이 높습니다. 이러한 정보는 음성 질환 진단 및 치료에 유용하게 사용될 수 있습니다.</li>
</ul>
</li>
<li><strong>Hammarberg Index</strong>, ratio of the strongest energy peak in the 0-2 kHz region to the strongest peak in the 2–5kHz region.<ul>
<li>0-2kHz 영역에서 가장 강한 에너지 피크와 2-5kHz 영역에서 가장 강한 피크의 비율</li>
<li>이 문장은 음성 파라미터 중 하나인 Hammarberg Index에 대한 설명입니다. Hammarberg Index는 음성 신호에서 0-2 kHz 영역과 2-5 kHz 영역에서 가장 강한 에너지 peak 간의 비율을 나타내는 지표입니다. 즉, Hammarberg Index 값이 높을수록 0-2 kHz 영역에서의 에너지 peak가 높아지고, Hammarberg Index 값이 낮을수록 2-5 kHz 영역에서의 에너지 peak가 높아집니다. 이러한 지표는 음성 신호에서 발생하는 고음과 저음 등을 분석하고, 음성 장애 진단 및 감정 분석 등에 사용됩니다.</li>
</ul>
</li>
<li><strong>Spectral Slope 0-500 Hz</strong> </li>
<li><strong>Spectral Slope 500-1500 Hz</strong>, linear regression slope of the logarithmic power spectrum within the two given bands.<ul>
<li>0-500 Hz 과 500-1500 Hz, 주어진 두 대역 내에서 로그 파워 스펙트럼의 선형 회귀 기울기</li>
<li>이 문장은 음성 파라미터 중 하나인 Spectral Slope 0-500 Hz and 500-1500 Hz에 대한 설명입니다. Spectral Slope는 주어진 두 개의 주파수 범위 (0-500 Hz 및 500-1500 Hz) 내에서 로그 스케일의 파워 스펙트럼의 선형 회귀 기울기를 나타내는 지표입니다. 즉, Spectral Slope 값이 높을수록 파워 스펙트럼이 빠르게 감소하고, Spectral Slope 값이 낮을수록 파워 스펙트럼이 천천히 감소합니다. 이러한 지표는 음성 신호에서 발생하는 저역대와 고역대의 에너지 분포를 분석하고, 음성 장애 진단 및 감정 분석 등에 사용됩니다.</li>
<li>파워 스펙트럼은 시간 도메인의 신호를 주파수 도메인으로 변환한 것입니다. 즉, 파워 스펙트럼은 주파수별로 신호의 에너지를 나타내는 그래프입니다. 파워 스펙트럼을 계산하면 주파수 영역에서 신호의 성분을 분석할 수 있습니다. 이러한 분석은 음성 인식, 음성 합성, 음성 변조 및 음성 감정 분석 등에 사용됩니다.</li>
</ul>
</li>
<li><strong>Formant 1 relative energy</strong></li>
<li><strong>Formant 2 relative energy</strong></li>
<li><strong>Formant 3 relative energy</strong>, as well as the ratio of the energy of the spectral harmonic peak at the first, second, third formant’s centre frequency to the energy of the spectral peak at F0.<ul>
<li>Formant 1, 2, 3 상대 에너지는 첫 번째, 두 번째, 세 번째 포먼트의 중심 주파수에서 spectral harmonic peak의 에너지와 F0에서 spectral peak의 에너지의 비율입니다.</li>
</ul>
</li>
<li><strong>Harmonic difference H1-H2</strong>, ratio of energy of the first F0 harmonic (H1) to the energy of the second F0<br>  harmonic (H2).<ul>
<li>첫 번째 F0 Harmonic(H1)의 에너지와 두 번째 F0 Harmonic(H2)의 에너지 둘 사이의 비율입니다.</li>
</ul>
</li>
<li><strong>Harmonic difference H1-A3</strong>, ratio of energy of the first F0 harmonic (H1) to the energy of the highest<br>  harmonic in the third formant range (A3).<ul>
<li>Harmonic 차이 H1-A3은 첫 번째 F0 harmonic(H1)의 에너지와 세 번째 formant 범위(A3)에서 가장 높은 harmonic의 에너지의 비율입니다.</li>
<li>첫 번째 F0 harmonic(H1)와 세 번째 formant 에너지(A3) 비율을 계산</li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
<li>LLD(Low-Level Descriptors)는 음성 신호에서 추출된 저수준 특징을 나타내는 지표입니다.</li>
<li>이러한 지표들은 시간에 따라 변화하므로, 3 프레임 길이의 대칭 이동 평균 필터(symmetric moving average filter*)*를 사용하여 시간적으로 smoothing(평활화)됩니다.</li>
<li>smoothing은 발성 영역 내에서만 수행되기 때문에 pitch, jitter 및 shimmer와 같은 경우, 0(무성음)에서 0이 아님으로의 전환은 smoothing하지 않습니다.<ul>
<li>Smoothing은 시계열 데이터에서 잡음이나 불규칙성을 제거하고, 데이터의 전반적인 추세를 부드럽게 만드는 기술입니다. 이를 위해 대표적으로 이동 평균 필터와 같은 필터링 기법이 사용됩니다. 이동 평균 필터는 일정한 길이의 윈도우를 설정하고, 해당 윈도우 내의 데이터들의 평균값을 구하여 각각의 데이터에 대해 적용하는 방식으로 smoothing을 수행합니다.</li>
</ul>
</li>
<li>이러한 LLD에 대해 산술평균(arithmetic mean)과 변동 계수 (coefficient of variation, 표준 편차를 산술 평균으로 정규화 한 것)가 모든 18개의 LLD에 대해 적용되어 <code>36개의 파라미터</code>가 생성됩니다.<ul>
<li>음성 파라미터 추출 과정에서 LLD에 대한 통계적인 지표 계산 방법<ul>
<li>산술평균은 각 LLD의 값들을 모두 더한 후, LLD의 개수로 나누어 평균값을 구하는 것입니다.</li>
<li>변동 계수는 표준 편차를 산술 평균으로 정규화하여, 데이터의 변동성을 상대적으로 비교할 수 있는 지표입니다.</li>
</ul>
</li>
<li>이러한 산술평균과 변동 계수를 각각 18개의 LLD에 대해 적용하여, 36개의 파라미터가 생성됩니다.</li>
</ul>
</li>
<li>음량과 피치에는 20번째, 50번째, 80번째 백분위수, 20~80번째 백분위수 범위, 상승&#x2F;하강 신호 부분의 기울기 평균과 표준편차 등 <strong><code>8가지 함수가 추가</code>로</strong> 적용됩니다.<ul>
<li>이 문장은 음성 파라미터 추출 과정에서 음량과 피치에 대해 적용되는 추가적인 함수들에 대한 설명입니다. 이러한 함수들은 20번째, 50번째, 80번째 백분위수, 20~80번째 백분위수 범위, 상승&#x2F;하강 신호 부분의 기울기 평균과 표준편차 등 총 8가지 함수로 구성됩니다. 이러한 함수들은 음성 신호에서 추출된 저수준 특징을 더욱 상세하게 분석하기 위해 사용됩니다. 예를 들어, 백분위수는 데이터의 분포를 파악하는 데 사용되며, 상승&#x2F;하강 신호 부분의 기울기 평균과 표준편차는 음성의 강도나 높낮이 변화를 분석하는 데 사용됩니다. 이러한 파라미터들은 음성 인식 및 감정 분석 등 다양한 분야에서 활용됩니다.</li>
<li>20,50,80 번째 백분위수: 각각 전체 데이터의 20%, 50%, 80%에 해당하는 값을 의미합니다. 예를 들어, 만약 어떤 데이터 집합에서 50번째 백분위수가 10이라면, 이는 해당 데이터 집합에서 중간값(median)이 10이라는 것을 의미합니다. 이러한 백분위수들은 데이터의 분포를 파악하고, 대표값을 추정하는 데 사용됩니다. 따라서 음성 신호에서 추출된 파라미터들에 대해 적용되는 백분위수들은 해당 파라미터들의 분포를 파악하고, 대표값을 추정하는 데 사용</li>
<li>상승&#x2F;하강 신호 부분의 기울기 평균과 표준편차: 음성 신호에서 추출된 파라미터들 중 하나로, 음성의 강도나 높낮이 변화를 분석하는 데 사용됩니다. 이러한 파라미터는 상승&#x2F;하강 신호 부분에서의 기울기를 계산하여, 해당 구간에서의 음성 변화 정도를 나타내는 지표입니다. 기울기 평균은 해당 구간에서의 기울기 값들을 모두 더한 후, 구간 길이로 나누어 평균값을 구하는 것이며, 표준편차는 해당 구간에서의 기울기 값들의 흩어진 정도를 나타내는 지표입니다. 이러한 파라미터들은 음성 인식 및 감정 분석 등 다양한 분야에서 활용</li>
<li>상승&#x2F;하강 신호 부분의 기울기 구하는 방법: 상승&#x2F;하강 신호 부분의 기울기는 일반적으로 미분을 통해 구합니다. 음성 신호에서는 일반적으로 Short-Time Energy(STE) 또는 Zero Crossing Rate(ZCR)와 같은 저수준 특징을 사용하여, 상승&#x2F;하강 신호 부분을 검출한 후, 해당 구간에서의 기울기를 계산합니다. 예를 들어, STE를 사용하여 음성 신호에서 상승&#x2F;하강 신호 부분을 검출한 후, 해당 구간에서의 기울기를 계산하기 위해서는 해당 구간 내 STE 값들의 차이를 계산하여, 이를 구간 길이로 나누어 기울기 값을 얻을 수 있습니다.</li>
</ul>
</li>
<li>loudness에 적용되는 함수들을 제외한 모든 함수는 음성영역(F0가 0이 아닌것)에서만 적용됨.</li>
<li>이렇게 <code>36개 파라미터 + loudness 8가지 함수 추가 적용 파라미터 + pitch 8가지 함수 추가 적용 파라미터 = 52개의 파라미터가 최종</code></li>
<li>또한 모든 무성음 세그먼트에 대한 알파 비율의 산술 평균, 하마버그 지수, 0-500Hz 및 500-1500Hz의 스펙트럼 슬로프가 포함되어 <code>총 56개의 파라미터</code>가 있습니다. 또한 6개의 시간적 특징이 포함되어 있습니다.<ol start="53">
<li>Alpha Ratio의 산술평균</li>
<li>Hammarberg Index의 산술평균</li>
<li>0-500 Hz 스펙트럼 기울기의 산술평균 (무성음 구간 전체)</li>
<li>500-1500 Hz 스펙트럼 기울기의 산술평균 (무성음 구간 전체)</li>
</ol>
</li>
<li>음성 신호에서 voiced 또는 unvoiced 구간의 최소 길이가 정해져 있지 않으므로 이러한 구간이 한 프레임만큼 짧을 수도 있다. 그러나 F0 contour의 Viterbi-based smoothing은 에러로 인해 단일 voiced frame이 누락되는 것을 효과적으로 방지합니다.<ol start="57">
<li>the rate of loudness peaks: 음성 신호에서 초당 loudness peak의 수를 나타내는 파라미터</li>
<li>continuously voiced regions의 평균 길이(F0 &gt; 0)<em>,</em></li>
<li>continuously voiced regions의 표준편차(F0 &gt; 0)<em>,</em></li>
<li>unvoiced regions (approximating pauses)의 평균 길이(F0 &#x3D; 0)<em>,</em></li>
<li>unvoiced regions (approximating pauses)의 표준편차(F0 &#x3D; 0)<em>,</em></li>
<li>continuous voiced regions의 초당 개수 (pseudo syllable rate)<ul>
<li>음성 신호에서 연속적으로 발생하는 voiced 구간의 개수를 초당 단위로 나타내는 파라미터입니다. 이러한 파라미터는 음성 신호에서 발생하는 언어적인 정보를 추출하는 데 사용</li>
</ul>
</li>
</ol>
<ul>
<li>파라미터들은 음성 신호에서 발생하는 pause나 강세 등과 같은 다양한 정보를 추출하는 데 사용</li>
</ul>
</li>
</ul>
<h3 id="3-2-Extended-Parameter-Set"><a href="#3-2-Extended-Parameter-Set" class="headerlink" title="3.2 Extended Parameter Set"></a>3.2 Extended Parameter Set</h3><ul>
<li>3.1의 파라미터에는 cepstral parameter나 dynamic parameter가 거의 들어가지 않음<ul>
<li>Cepstral parameter는 음성 신호의 주파수 특성을 분석하는 데 사용되는 파라미터입니다. 이러한 파라미터는 Mel-Frequency Cepstral Coefficients (MFCCs)와 같은 변환을 통해 추출됩니다. MFCCs는 음성 신호를 일련의 주파수 대역으로 분할하고, 각 대역에서 적절한 계수를 추출하여 구성됩니다. 이러한 계수들은 음성 신호의 주파수 특성을 잘 나타내며, 음성 인식 및 감정 인식과 같은 다양한 어플리케이션에서 유용하게 사용됩니다.</li>
<li>Dynamic parameter는 시간적인 변화를 나타내는 파라미터입니다. 이러한 파라미터는 음성 신호의 동적인 특징을 분석하는 데 사용됩니다. 예를 들어, Delta 및 Delta-Delta 계수와 같은 동적 파라미터는 MFCCs와 같은 정적 파라미터에 추가하여 사용됩니다. 이러한 동적 파라미터는 음성 신호의 빠른 변화를 잘 나타내며, 음성 인식 및 감정 인식과 같은 다양한 어플리케이션에서 유용하게 사용됩니다.</li>
</ul>
</li>
<li>즉, 이 minimal Parameter Set은 Delta 회귀 계수 및 차이 특징과 같은 동적 파라미터를 포함하지 않습니다. 대신, 상승 및 하강하는 F0와 음량 세그먼트의 기울기만이 일부 동적 정보를 포함합니다.<ul>
<li>즉, 이 Parameter Set에서는 두 개의 연속된 프레임 간 차이를 나타내는 파라미터가 포함되어 있지 않으며, 상승 및 하강하는 F0와 음량 세그먼트의 기울기만이 일부 동적 정보를 추출할 수 있는 파라미터로 사용됩니다.</li>
</ul>
</li>
<li>[23],[40],[41]에서는 affective state를 모델링하는데 cepstral 파라미터가 매우 성공적인 것으로 입증되었다.</li>
<li>따라서 확장된 set에서는 minimalistic set에 추가로 <code>7개의 LLD를 포함</code>하는 것으로 제안함<ul>
<li><strong>Spectral (balance&#x2F;shape&#x2F;dynamics) parameters:</strong><ol>
<li><strong>MFCC 1 (MFCC의 첫번째 계수)</strong></li>
<li><strong>MFCC 2</strong></li>
<li><strong>MFCC 3</strong></li>
<li><strong>MFCC 4</strong>, Mel-Frequency Cepstral Coefficients 1-4.<ul>
<li>MFCC는 여러 개의 계수로 구성됩니다. 일반적으로 MFCC는 12개에서 40개까지의 계수를 사용합니다. 이러한 계수는 음성 신호에서 추출된 특징을 나타내며, 일반적으로 음성 인식, 감정 인식 및 화자 인식과 같은 작업에 사용됩니다. 더 낮은 순서의 MFCC는 주파수 스펙트럼의 기울기와 전체 스펙트럼 에너지 분포와 관련이 있으며, 더 높은 순서의 MFCC는 보다 세부적인 에너지 분포를 나타냅니다. 이러한 다양한 계수를 조합하여 음성 처리 작업에서 보다 정확한 결과를 얻을 수 있습니다.</li>
</ul>
</li>
<li><strong>Spectral flux</strong>, difference of the spectra of two consecutive frames.<ul>
<li>Spectral flux는 음성 신호의 스펙트럼 변화를 측정하는 파라미터 중 하나입니다. 이 파라미터는 연속된 프레임 간 스펙트럼 차이의 제곱합을 계산하여 구합니다. 이러한 계산은 음성 신호의 주파수 성분이 얼마나 빠르게 변화하는지를 나타내며, 음성 신호의 에너지 분포가 어떻게 변화하는지를 추적할 수 있습니다. Spectral flux는 음성 인식 및 감정 인식과 같은 다양한 어플리케이션에서 유용하게 사용</li>
</ul>
</li>
</ol>
</li>
<li><strong>Frequency related parameters:</strong><ol start="6">
<li><strong>Formant 2 bandwidth</strong></li>
<li><strong>Formant 3 bandwidth,</strong> added for completeness of<br>  Formant 1-3 parameters.<ul>
<li>Formant 1-3 파라미터의 완성도를 위해 Formant2,3 대역폭이 추가</li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
<li>함수로써 산술평균과 변동계수는 이 7가지 추가 LLD에 모두 적용됨. 세그먼트(유,무성 모두)를 포함하여 유성영역에만 함수가 적용되는 대역폭은 제외됨. → 최종적으로 <code>14개의 discriptor가 추가</code>가 됨</li>
<li>또한 무성 영역에서만 spectral flux의 산술 평균, 유성영역에서만 spectral flux의 산술 평균과 변동 계수 및 MFCC 1-4가 포함됨. → 이 결과 <code>11개의 discriptor가 추가</code>됨.</li>
<li>추가적으로 equivalent soud level이 포함됨.<ul>
<li>Equivalent sound level (LEq)은 프레임당 RMS 에너지의 평균값을 로그 스케일로 변환하여 계산됩니다. 이를 통해 음성 신호의 평균적인 에너지 레벨을 측정할 수 있습니다. LEq는 주로 소음 및 음향 관련 어플리케이션에서 사용되며, 음성 감정 인식과 같은 분야에서도 유용하게 사용됩니다.</li>
<li>RMS (Root Mean Square) 에너지 평균값은 음성 신호의 에너지를 측정하는 방법 중 하나입니다. 이 값은 각 프레임에서 음성 신호의 진폭 값을 제곱한 후, 평균을 구한 값입니다. RMS 에너지는 음성 신호의 전체적인 에너지 레벨</li>
</ul>
</li>
<li>이렇게 하면 총 <code>26개의 추가 파라미터</code>가 생성</li>
<li><strong>결과적으로 extended Geneva Minimalistic Acoustic Parameter Set(eGeMAPS)는 88개의 파라미터가 포함되어 있음.</strong></li>
</ul>
<h2 id="4-Baseline-Evaluation"><a href="#4-Baseline-Evaluation" class="headerlink" title="4.Baseline Evaluation"></a>4.Baseline Evaluation</h2><ul>
<li>위에서 제안된 두가지 파라미터세트는 각각 binary arousal와 binary valence dimensions에서 자동 인식 작업을 수행하기 위해 평가되었습니다. 이를 위해, 여러 가지 감정적인 음성 데이터베이스에서 제공된 원래 라벨을 binary dimensional labels (Arousal&#x2F;Valence)로 매핑하여 사용하였습니다.<ul>
<li>Binary arousal와 binary valence dimensions는 감정 인식 분야에서 사용되는 두 가지 이진 차원입니다. Arousal 차원은 감정의 활성화 수준을 나타내며, 낮은 수준의 활성화는 침착하고 집중력이 높은 상태를 나타내고, 높은 수준의 활성화는 흥분하거나 공포와 같은 강한 감정을 나타냅니다. Valence 차원은 감정의 긍정성&#x2F;부정성 정도를 나타내며, 긍정적인 감정은 높은 valence 값을 가지고 부정적인 감정은 낮은 valence 값을 가집니다. 이러한 이진 차원들을 사용하여 음성 신호에서 특정한 감정을 자동으로 인식하는 작업이 가능합니다.</li>
</ul>
</li>
<li>원래 데이터 세트에 라벨링 된 감정<ul>
<li>TUM AVIC 데이터베이스의 Levels of Interest</li>
<li>Geneva Multimodal Emotion Portrayals (GEMEP) corpus 및 German Berlin Emotional Speech database (EMO-DB)</li>
<li>professional opera singers의 노래에서 나타난 감정 (GeSiE)</li>
<li>FAU AIBO corpus의 어린이들의 발화에서 나타난 valence</li>
<li>German talk-show recordings (Vera-am-Mittag corpus)에서 나타난 실제 감정</li>
</ul>
</li>
<li>제안된 minimal set은 INTERSPEECH 2009 Emotion Challenge, INTERSPEECH 2010 Paralinguistic Challenge, INTERSPEECH 2011 Speaker State Challenge, INTERSPEECH 2012 Speaker Trait Challenge 및 INTERSPEECH 2013 Computational Paralingusitics ChallengE (ComParE)와 같은 대규모 brute-forced baseline acoustic feature set과 비교되었습니다.</li>
</ul>
<h3 id="4-1-Database"><a href="#4-1-Database" class="headerlink" title="4.1 Database"></a>4.1 Database</h3><ul>
<li>4.1.1 FAU AIBO: FAU AIBO는 세계 최초의 국제 감정 챌린지의 공식 말뭉치로 사용되었습니다. 이 데이터베이스는 Sony 애완동물 로봇 Aibo와 상호작용하는 어린이들의 발화 녹음을 포함하고 있습니다. 따라서 spontaneous 하며, 감정적으로 색칠된 독일어 발화를 포함합니다. 어린이들은 Aibo 로봇이 방향에 대한 목소리 명령에 반응한다고 알려졌지만, 실제로 로봇은 때로는 불복종적으로 행동하여 어린이들로부터 강한 감정 반응을 유도하기 위해 인간 조작자에 의해 제어되었습니다. 이 녹음은 MONT와 OHM이라는 두 개의 학교에서 <code>총 51명의 어린이 (10-13세, 21명 남성, 30명 여성)으로부터 수집되었으며, 쉬는 시간을 제외하고 약 9.2시간의 발화</code>가 포함되어 있습니다.</li>
<li>4.1.2 TUM Audiovisual Interest Corpus (TUM-AVIC): TUM Audiovisual Interest Corpus (TUM-AVIC)는 spontaneous한 affective interactions을 포함하는 audiovisual 녹음을 담고 있는 데이터베이스입니다. 이 데이터베이스는 INTERSPEECH 2010 Paralinguistics Challenge를 위한 데이터셋으로 사용되었습니다. 이 데이터셋은 제품 프레젠터가 대상자를 상품 프레젠테이션을 통해 안내하는 과정에서 수집되었습니다. <code>사용된 언어는 영어이지만, 대부분의 제품 프레젠터는 독일어 원어민입니다. 대상자들은 주로 유럽 및 아시아 국적입니다. 이 데이터베이스에는 21명의 대상자 (여성 10명)의 녹음이 포함</code>되어 있습니다.<ul>
<li>LOI는 각 sub-turn마다 (화자 turn의 수동적인 pause 기반 sub-division을 통해 찾아낸) 세 가지 라벨로 표시됩니다. 이 세 가지 라벨은 <code>boredom (대상자가 대화나 주제에 지루함을 느끼며 매우 수동적이며 대화를 따르지 않음; loi1로도 알려짐), over neutral (대상자가 대화를 따르고 참여하지만 주제에 관심이 있는지 아니면 무관심한지 판단할 수 없음; loi2로도 알려짐), joyful interaction (대상자가 대화하고 주제에 대해 더 배우고 싶어하는 강한 욕구를 보여주며, 즉, 그/그녀는 토론에 큰 관심을 가짐; loi3으로도 알려짐)</code>입니다. 이 평가에서는 [47]의 <code>3,002개의 구문(sub-turns)</code>이 사용되었습니다. 이는 예를 들어 [46]에서 사용된 high interlabeller agreement를 가진 996개의 구문보다 더 많습니다.<ul>
<li>high interlabeller agreement는 서로 다른 라벨러들이 동일한 라벨을 부여하는 정도를 나타내는 지표입니다. 즉, 서로 다른 라벨러들이 동일한 구문에 대해 동일한 라벨을 부여하는 경우, 그 구문은 high interlabeller agreement를 가진다고 할 수 있습니다. 이것은 데이터셋의 신뢰성과 일관성을 보장하기 위해 중요한 지표 중 하나입니다.</li>
</ul>
</li>
</ul>
</li>
<li>4.1.3 Berlin Emotional Speech Database: Berlin Emotional Speech Database 또는 EMO-DB는 자동 감정 분류의 효과를 테스트하기 위해 매우 잘 알려져 있고 널리 사용되는 데이터베이스입니다. 이 데이터베이스는 Levels of Emotion Annotation Space (LEAS)와 함께 개발되었습니다. <code>EMO-DB에는 10명의 배우가 7가지 감정(분노, 경멸, 두려움, 기쁨, 슬픔, 수치심, 중립)을 나타내는 독일어 단어 및 문장을 발화한 녹음</code>이 포함되어 있습니다. 이 데이터베이스에는 대략 <code>5시간의 오디오 녹음</code>이 포함되어 있습니다.</li>
<li>4.1.4 The Geneva Multimodal Emotion Portrayals: The Geneva Multimodal Emotion Portrayals(GEMEP)은 <code>10명의 프랑스어 배우가 연기한 1,260개의 멀티모달 감정 표현을 수집한 데이터베이스</code>입니다. 이 데이터베이스는 얼굴 표정, 음성, 제스처 등 다양한 모달리티를 사용하여 감정을 나타냅니다. GEMEP 데이터베이스는 많은 연구에서 사용되며, 이 연구에서도 사용되었습니다. 구체적으로는 12가지 감정이 있으며 총 24개러 분류 된다.</li>
<li>4.1.5 Geneva Singing Voice Emotion Database(GeSiE): 3명의 가수가 녹음한 감정적인 노래를 수집한 데이터베이스입니다. 이 데이터베이스는 5명의 전문 오페라 가수가 추가로 녹음한 것으로 확장되었습니다. <code>총 8명의 가수들이 10가지 감정 범주에서 세 가지 다른 구절과 음계를 부르며 녹음</code>되었습니다. 이 데이터베이스는 많은 연구에서 사용되고 있습니다.</li>
<li>Vera-Am-Mittag: 독일 TV 쇼 “Vera am Mittag”에서 추출한 <code>947개의 감정적인 발화로 구성</code>되어 있습니다. 이 쇼에서 주인공인 Vera는 게스트들 간의 토론을 주관합니다. 이 데이터베이스는 매우 자발적이고 감정적으로 매우 다양한 상태를 포함하고 있습니다. 이 데이터베이스의 감정은 활성화, 가치 및 지배력&#x2F;권력 세 가지 차원으로 설명됩니다.비디오로 구성된 데이터베이스입니다. 이 쇼에서 주인공인 Vera는 게스트들 간의 토론을 주관합니다. 이 데이터베이스는 많은 연구에서 사용되고 있습니다.<ul>
<li>이 부분에서는 EmoReact 데이터베이스의 주석 방법에 대한 정보가 제공됩니다. 주석자들은 각각의 감정 차원에 대해 다섯 개의 이미지 중 하나를 선택할 수 있는 아이콘 기반 방법을 사용했습니다. 주석자들은 먼저 수동으로 분할된 발화를 듣고, 그 발화에서 가장 잘 설명하는 감정 차원에 대한 아이콘을 선택해야 했습니다. 이 아이콘의 선택은 후에 각 차원마다 [-1;1] 범위 내에서 균등하게 분포된 다섯 가지 범주로 매핑되었으며, 주석자의 확신도를 고려하는 가중치 함수를 적용하여 평균값을 계산했습니다. 비교적 평가를 가능하게 하기 위해 연속적인 가치와 활성화 라벨은 네 개의 클래스로 이산화되었으며, 이는 activation-valence 공간의 네 사분면(q1:positive-active, q2:positive-passive, q3:negative-passive, and q4:negative-active)을 나타냅니다.</li>
</ul>
</li>
</ul>
<h3 id="4-2-Common-Mapping-of-Emotions"><a href="#4-2-Common-Mapping-of-Emotions" class="headerlink" title="4.2 Common Mapping of Emotions"></a>4.2 Common Mapping of Emotions</h3><p><img src="/images/eGeMAPS/Untitled.png" alt=" "></p>
<ul>
<li>이 부분에서는 모든 데이터셋에서 결과와 특징 집합의 성능을 비교할 수 있도록 하기 위해, 각각의 데이터셋에 대한 특정한 감정 라벨을 공통적 binary arousal and valence representation으로 매핑했다는 것을 설명합니다. 이 매핑은 [43], [47], [49] (GEMEP의 경우)에서 제안된 대로 [53]를 참고하여 수행되었습니다. GeSiE의 경우 GEMEP에서 사용된 절차와 유사하게 매핑이 수행되었습니다. Table 1은 감정 범주를 바이너리 활성화 및 가치 라벨로 매핑한 결과를 보여줍니다.</li>
<li>Note, that for FAU AIBO: 원래 5개의 클래스 레이블 특성상 binary valence에 대한 매핑만 가능</li>
</ul>
<h3 id="4-3-Experimental-Protocol"><a href="#4-3-Experimental-Protocol" class="headerlink" title="4.3 Experimental Protocol"></a>4.3 Experimental Protocol</h3><ul>
<li>AIBO에 대한 실험을 제외한 모든 실험은 LOSO(Leave-One-Speaker(Group)-Out) 교차 검증을 사용하여 수행됩니다.</li>
<li>LOSO는 “Leave-One-Speaker-Out”의 약어로, 각 실험에서 하나의 화자를 제외한 모든 화자 데이터를 사용하여 모델을 학습하고, 나머지 화자의 데이터를 사용하여 모델을 평가하는 교차 검증 방법입니다. 이 방법은 모델이 다른 화자들에 대해서도 일반화될 수 있는지 확인하기 위해 사용됩니다. 즉, 각각의 테스트 세트는 한 명의 화자에 대한 데이터만 포함하며, 이러한 방식으로 모든 화자에 대한 평가가 수행됩니다. (교차검증의 일종)</li>
<li>GeSiE 데이터셋처럼 화자 수가 8명 이하인 경우, 각 화자의 데이터를 하나의 교차 검증 폴드로 처리합니다. 그러나 8명 이상인 경우, 화자 ID가 무작위로 8개의 그룹으로 나누어지고, 이에 따라 데이터가 8개의 폴드로 분할됩니다. 그런 다음 교차 검증은 각각 7개 폴드에서 데이터를 사용하여 8개의 다른 모델을 훈련하고, 첫 번째 모델에 대한 테스트를 위해 첫 번째 폴드를 제외하고 두 번째 모델에 대한 테스트를 위해 두 번째 폴드를 제외하는 식으로 수행됩니다. 이렇게 하면 전체 데이터셋에 대한 예측이 훈련 및 테스트 데이터 중복 없이 생성됩니다. FAU AIBO의 경우, OHM에서 학습하고 MONT에서 평가하는 방식과 MONT에서 학습하고 OHM에서 평가하는 방식으로 두 개의 교차 검증 폴드를 사용합니다.<ul>
<li>교차 검증 폴드는 교차 검증에서 사용되는 데이터 세트의 하위 집합입니다. 전체 데이터 세트를 여러 개의 폴드로 나누어 각각을 테스트 세트와 학습 세트로 사용하여 모델을 반복적으로 학습하고 평가하는 방법에서, 각각의 폴드는 서로 다른 데이터를 포함하며, 전체 데이터 세트를 대표할 수 있는 크기와 분포를 가지도록 구성됩니다. 예를 들어, 10개의 폴드로 나누어 교차 검증을 수행하는 경우, 10개의 서로 다른 하위 집합으로 데이터가 분할되며, 각각의 폴드는 10%씩의 데이터를 포함합니다. 이러한 방식으로 모든 데이터가 테스트 및 학습에 사용되며, 모델이 일반화될 수 있는지 확인할 수 있습니다.</li>
</ul>
</li>
<li>paralinguistics 분야에서 가장 널리 사용되는 정적 분류기로서, support-vector machines(SVMs)가 선택되었습니다. SVMs는 WEKA [54]에서 구현된 순차 최소 최적화 알고리즘으로 학습됩니다. 모델 복잡도 C의 값 범위가 평가되며, 결과는 매개 변수 세트의 성능과 관련하여 더 안정적인 결과를 얻기 위해 전체 범위에서 평균화됩니다. c값의 범위는 C1&#x3D;0.000025, C2&#x3D;0.00005, C3&#x3D;0.000075, C4&#x3D;0.0001, …, C15&#x3D;0.075, C16&#x3D;0.1, C17&#x3D;0.25으로 총 17개의 값이 평가 대상입니다. 이 범위에서 모델 복잡도를 평가하고 결과를 평균화하여 매개 변수 세트의 성능을 더 안정적으로 평가합니다.</li>
<li>SVMs를 구현하는 데 필요한 데이터 균형 문제에 대해 설명하고 있습니다. SVMs를 사용할 때, 다수 클래스에 대한 사전 편향을 피하기 위해 각 클래스에 대해 동일한 수의 인스턴스가 필요합니다. 이를 위해 각각의 훈련 파티션은 균형을 맞추어야 합니다. Up-sampling은 소수 클래스의 데이터를 복제하여 다수 클래스와 동일한 수의 데이터를 생성하는 방식으로 수행됩니다. 이렇게 함으로써, 모델이 다수 클래스에 대해 사전 편향을 학습하지 않도록 하고, 소수 클래스의 정보를 더 잘 반영할 수 있도록 합니다. Up-sampling은 일반적으로 무작위로 선택된 소수 클래스 샘플을 복제하여 데이터 세트의 크기를 증가시키는 방식으로 수행됩니다.</li>
<li>VMs를 수치적으로 효율적으로 만들기 위해 모든 음향 매개 변수가 공통 값 범위로 정규화되어야 함을 설명하고 있습니다. 이를 위해 z-정규화, 즉 평균이 0이고 분산이 1인 정규화가 수행됩니다. 이 논문에서는 정규화 매개 변수를 계산하고 적용하는 세 가지 다른 방법을 조사합니다. 첫째, 전체 훈련 파티션에서 평균과 분산을 계산하는 방법(std)입니다. 둘째, 각 화자에 대해 개별적으로 평균과 분산을 계산하는 방법(spkstd)입니다. 이 방법은 [55]와 유사합니다. 셋째, 훈련 및 테스트 파티션 각각에 대해 개별적으로 평균과 분산을 계산하는 방법(stdI)입니다.</li>
</ul>
<h3 id="4-4-Result"><a href="#4-4-Result" class="headerlink" title="4.4 Result"></a>4.4 Result</h3><ul>
<li>제안된 최소한의 매개 변수 세트와 Interspeech Challenges 시리즈에서 사용된 대규모 brute-forced 매개 변수 세트 간의 결과를 비교합니다. brute-forced 매개 변수 세트는 Interspeech Challenges는 2009년 [43] (InterSp09)의 감정, 2010년 [36] (InterSp10)의 연령 및 성별, 관심도 수준, 2011년 [44] (InterSp11)의 화자 상태, 2012년 [45] (InterSp12)의 화자 특성 및 2013 및 2014년 [12], [37] (ComParE)의 전산언어학에 대한 시리즈로 구성이 되어있습니다.</li>
<li>binary arousal and binary valence classification의 결과</li>
<li>매개 변수 세트를 제외한 모든 변수를 제거하기 위해 결과는 다섯 개의 데이터베이스(all, FAU AIBO 제외)와 C&#x3D;0.0025부터 시작하는 가장 높은 아홉 개의 SVM 복잡도 설정에서 평균화됩니다. 더 높은 복잡도 설정에서만 평균을 계산하는 결정은 작은 특징 집합의 경우 이 임계값보다 낮은 복잡도에서 성능이 크게 저하되기 때문입니다. 이러한 저하가 평균화를 편향시키기 때문입니다.</li>
</ul>
<p><img src="/images/eGeMAPS/Untitled%201.png" alt=" "></p>
<ul>
<li>UAR은 Unweighted Average Recall의 약자로, 각 클래스에 대한 재현율을 평균화한 값입니다. 이 부분에서는 FAU AIBO를 제외한 모든 데이터베이스와 9개의 최고 SVM 복잡도(C&gt;&#x3D;0.0025)에서 UAR을 평균화합니다. 이는 가중치가 적용되지 않은 평균입니다. 또한, 각 화자에 대한 표준화와 균형 잡힌 훈련 세트를 위해 인스턴스 업샘플링이 수행됩니다.<ul>
<li>각 클래스에 대한 재현율은 분류기가 실제 양성인 샘플을 얼마나 잘 찾아냈는지를 나타내는 지표입니다. 즉, 분류기가 참 양성(True Positive)으로 분류한 샘플 수를 실제 양성(True Positive + False Negative)인 전체 샘플 수로 나눈 값입니다. 이 값은 해당 클래스의 분류 성능을 평가하는 데 사용됩니다.</li>
</ul>
</li>
<li>평균 결과에서 GeMAPS 세트의 높은 효율성을 확인할수 있습니다. eGeMAPS 세트는 arousal에서 거의 80%의 UAR에 도달하여 가장 우수한 성능을 보였습니다.</li>
</ul>
<p><img src="/images/eGeMAPS/Untitled%202.png" alt=" "></p>
<ul>
<li>eGeMAPS 세트는 GEMEP 데이터베이스의 이진 각성 분류와 GeSiE 데이터베이스의 이진 가치 분류에서 최상의 결과를 보입니다. eGeMAPS 세트는 항상 GeMAPS 세트보다 우수하거나 동일하며, 이는 추가 파라미터(MFCC 및 스펙트럼 플럭스 특히)의 중요성을 시사합니다. 이는 특히 가치에서 GeMAPS와 eGeMAPS 간의 평균 차이가 더 큰 경우에 그러하며, 이로써 음향 가치에 대한 그러한 파라미터의 중요성을 제안합니다. 큰 규모의 파라미터 세트들에 비해 전반적으로 GeMAPS 세트는 최소한의 크기임에도 불구하고 놀랍게도 비슷한 성능을 보입니다. 향후 연구에서는 제안된 최소한의 세트가 교차 데이터베이스 분류 실험에서 더 나은 일반화를 얻을 수 있는지 조사해야 합니다.</li>
</ul>
<h2 id="5-Discussion-and-Conclusion"><a href="#5-Discussion-and-Conclusion" class="headerlink" title="5.Discussion and Conclusion"></a>5.Discussion and Conclusion</h2><ul>
<li>GeMAPS는 수동 상호작용이나 보정 없이 오디오 파형에서 음향 파라미터 세트를 추출하는 자동 추출 시스템을 기반으로 합니다.</li>
<li>하지만 특정 현상과 관련성이 있거나 상관관계가 있는 것으로 밝혀진 모든 파라미터를 자동으로 안정적으로 추출할 수 있는 것은 아닙니다. 예를 들어 모음기반 포먼트 분석에는 신뢰할수 있는 자동모음감지및 분류 시스템이 필요합니다.</li>
<li>따라서 GeMAPS에서는 깨끗한 음향 조건에서 감독없이 안정적으로 추출할 수 있는 파라미터만 포함했습니다. 검증 실험은 데이터 베이스간 최상의 비교 가능성을 위해 binary classification 실험으로 제한되었습니다.</li>
<li>자동 추출을 통한 표준 파라미터 세트의 잠재적 위험 중 하나는 발성 현상과의 연결이 무시될 수 있다는 것입니다. 파라미터 세트를 선택할 때, 이러한 연결을 강조하고 수집 기준 중 하나로 기본 발성 메커니즘을 사용했습니다. 향후 연구를 통해 이러한 기초를 강화하고 새로운 통찰력을 제공할 것으로 기대됩니다. 예를 들어, 각성과 빠른 발성 및&#x2F;또는 조음 제스처와 관련이 있다고 예상할 수 있으며, 평화로운 성격은 느린 제스처로부터 비롯된다고 합니다. 따라서 앞으로는 발성의 음향 출력을 소리 크기, 피치 등 기본 파라미터를 넘어 생리학적으로 관련된 파라미터로 이해를 확장하는 것이 가치가 있습니다.</li>
<li>이러한 맥락에서 성대 접착은 특히 관련성이 높은 파라미터입니다. 접착 증가는 닫힌 단계를 길게 하고 횡성대 공기 흐름 펄스의 진폭을 줄입니다. 이로 인해 음성 원천 기본값의 감쇄 또는 더 구체적으로는 음성 원천 부분 음 두 개 사이의 레벨 차이를 줄일 것으로 예상됩니다. 이러한 방사음에서 이 레벨 차이는 주로 첫 번째 포먼트의 주파수에 영향을 받으며, 이는 발성의 감정 색채에 이차적인 중요성을 가집니다. GeMAPS의 향후 발전은 음성 원천 파라미터를 직접 측정하기 위해 음향 출력 신호를 역 필터링하는 기술의 추가를 포함할 수 있습니다. 이러한 분석은 감정 표현의 다양한 음향 출력 특성의 생리적 상관 관계를 확인할 수 있게 하여 감정적 각성이 음성 생산에 미치는 메커니즘에 대한 지식을 강화합니다.</li>
</ul>

        </div>
        <footer class="article-footer">
            



    <a data-url="https://jmj3047.github.io/2023/04/21/eGeMAPS/" data-id="clgw0znc300hd4du62fdu1xp3" class="article-share-link"><i class="fa fa-share"></i>Share</a>
<script>
    (function ($) {
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
</script>

        </footer>
    </div>
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "BlogPosting",
        "author": {
            "@type": "Person",
            "name": "Jang Minjee"
        },
        "headline": "The Geneva Minimalistic Acoustic Parameter Set (GeMAPS) for Voice Research andAffective Computing",
        "image": "https://jmj3047.github.io/images/eGeMAPS/Untitled.png",
        "keywords": "Speech Emotion Recognition Acoustic Parameter Set",
        "genre": "Paper Speech Emotion Recognition",
        "datePublished": "2023-04-21",
        "dateCreated": "2023-04-21",
        "dateModified": "2023-04-21",
        "url": "https://jmj3047.github.io/2023/04/21/eGeMAPS/",
        "description": "Journal&#x2F;Conference : IEEEYear(published year): 2016Author: Florian Eyben, Klaus R. Scherer, Bjorn W. Schuller, Johan Sundberg, Elisabeth Andre, Carlos Busso, Laurence Y. Devillers, Julien Epps, P",
        "wordCount": 5044
    }
</script>

</article>

    <section id="comments">
    
    </section>



                        </div>
                    </section>
                    
                    <aside id="sidebar">
    <a class="sidebar-toggle" title="Expand Sidebar"><i class="toggle icon"></i></a>
    <div class="sidebar-top">
        <p>follow:</p>
        <ul class="social-links">
            
                
                <li>
                    <a class="social-tooltip" title="github" href="https://github.com/jmj3047" target="_blank" rel="noopener">
                        <i class="icon fa fa-github"></i>
                    </a>
                </li>
                
            
                
                <li>
                    <a class="social-tooltip" title="dropbox" href="https://www.dropbox.com/home" target="_blank" rel="noopener">
                        <i class="icon fa fa-dropbox"></i>
                    </a>
                </li>
                
            
                
                <li>
                    <a class="social-tooltip" title="google" href="https://www.google.com" target="_blank" rel="noopener">
                        <i class="icon fa fa-google"></i>
                    </a>
                </li>
                
            
                
                <li>
                    <a class="social-tooltip" title="youtube" href="https://www.youtube.com/" target="_blank" rel="noopener">
                        <i class="icon fa fa-youtube"></i>
                    </a>
                </li>
                
            
                
                <li>
                    <a class="social-tooltip" title="discord" href="https://discord.com/channels/@me" target="_blank" rel="noopener">
                        <i class="icon fa fa-discord"></i>
                    </a>
                </li>
                
            
        </ul>
    </div>
<!-- 
    <div class="github-card" data-github="jmj3047" data-width="400" data-height="" data-theme="default">
    </div>
    <script src="//cdn.jsdelivr.net/github-cards/latest/widget.js">    </script> -->

    
        
<nav id="article-nav">
    
        <a href="/2023/04/21/Wav2Vec/" id="article-nav-newer" class="article-nav-link-wrap">
        <strong class="article-nav-caption">newer</strong>
        <p class="article-nav-title">
        
            wav2vec 2.0, A Framework for Self-Supervised Learning of Speech Representations
        
        </p>
        <i class="icon fa fa-chevron-right" id="icon-chevron-right"></i>
    </a>
    
    
        <a href="/2023/04/21/DL1_Quiz/" id="article-nav-older" class="article-nav-link-wrap">
        <strong class="article-nav-caption">older</strong>
        <p class="article-nav-title">Neural Networks and Deep Learning_Quiz</p>
        <i class="icon fa fa-chevron-left" id="icon-chevron-left"></i>
        </a>
    
</nav>

    


    <div class="widgets-container">
        
        
        
            
                

            
                
    <div class="widget-wrap">
        <h3 class="widget-title">recents</h3>
        <div class="widget">
            <ul id="recent-post" class="no-thumbnail">
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/Paper/">Paper</a><i class="icon fa fa-angle-right"></i><a class="article-category-link" href="/categories/Paper/Speech-Emotion-Recognition/">Speech Emotion Recognition</a></p>
                            <p class="item-title"><a href="/2023/08/04/wav2vec_hubert_for_SER_SV_SLU/" class="title">A fine-tuned wav2vec2.0/Hubert benchmark for SER, Speaker verification and spoken language understanding</a></p>
                            <p class="item-date"><time datetime="2023-08-03T15:00:00.000Z" itemprop="datePublished">2023-08-04</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/Paper/">Paper</a><i class="icon fa fa-angle-right"></i><a class="article-category-link" href="/categories/Paper/Speech-Representations/">Speech Representations</a></p>
                            <p class="item-title"><a href="/2023/08/02/SUPERB/" class="title">SUPERB, Speech processing Universal PERformance Benchmark</a></p>
                            <p class="item-date"><time datetime="2023-08-01T15:00:00.000Z" itemprop="datePublished">2023-08-02</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/Paper/">Paper</a><i class="icon fa fa-angle-right"></i><a class="article-category-link" href="/categories/Paper/Speech-Emotion-Recognition/">Speech Emotion Recognition</a></p>
                            <p class="item-title"><a href="/2023/07/17/Speaker_Normalization_for_SER/" class="title">Speaker Normalization for Self-Supervised Speech Emotion Recognition</a></p>
                            <p class="item-date"><time datetime="2023-07-16T15:00:00.000Z" itemprop="datePublished">2023-07-17</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/Paper/">Paper</a><i class="icon fa fa-angle-right"></i><a class="article-category-link" href="/categories/Paper/Speech-Emotion-Recognition/">Speech Emotion Recognition</a></p>
                            <p class="item-title"><a href="/2023/07/15/SER_for_self_supervised/" class="title">Speech Emotion Recognition Using Self-Supervised Features</a></p>
                            <p class="item-date"><time datetime="2023-07-14T15:00:00.000Z" itemprop="datePublished">2023-07-15</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/Data-Analysis/">Data Analysis</a><i class="icon fa fa-angle-right"></i><a class="article-category-link" href="/categories/Data-Analysis/Model/">Model</a></p>
                            <p class="item-title"><a href="/2023/07/11/MLOps_Part2/" class="title">Machine Learning Data Lifecycle in Production</a></p>
                            <p class="item-date"><time datetime="2023-07-10T15:00:00.000Z" itemprop="datePublished">2023-07-11</time></p>
                        </div>
                    </li>
                
            </ul>
        </div>
    </div>

            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">categories</h3>
        <div class="widget">
            <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Data-Analysis/">Data Analysis</a><span class="category-list-count">38</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Data-Analysis/Basic/">Basic</a><span class="category-list-count">22</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Data-Analysis/Model/">Model</a><span class="category-list-count">16</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Data-Platform-Base/">Data Platform/Base</a><span class="category-list-count">16</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Data-Platform-Base/GCP/">GCP</a><span class="category-list-count">13</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Data-Platform-Base/MongoDB/">MongoDB</a><span class="category-list-count">3</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Paper/">Paper</a><span class="category-list-count">23</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Paper/Computer-Vision/">Computer Vision</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Paper/Generative-Model/">Generative Model</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Paper/Multi-Task-Learning/">Multi-Task Learning</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Paper/NLP/">NLP</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Paper/Recommendation-System/">Recommendation System</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Paper/Speaker-Verification/">Speaker Verification</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Paper/Speech-Emotion-Recognition/">Speech Emotion Recognition</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Paper/Speech-Representations/">Speech Representations</a><span class="category-list-count">2</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a><span class="category-list-count">15</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Python/Django/">Django</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/HTML/">HTML</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/Pyspark/">Pyspark</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/Python/">Python</a><span class="category-list-count">5</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Setting/">Setting</a><span class="category-list-count">5</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-float">
        <h3 class="widget-title">tag cloud</h3>
        <div class="widget tagcloud">
            <a href="/tags/ACF/" style="font-size: 10px;">ACF</a> <a href="/tags/AI-Studies/" style="font-size: 16.92px;">AI Studies</a> <a href="/tags/AI-tool/" style="font-size: 10px;">AI tool</a> <a href="/tags/ARIMA/" style="font-size: 10px;">ARIMA</a> <a href="/tags/Acoustic-Parameter-Set/" style="font-size: 10px;">Acoustic Parameter Set</a> <a href="/tags/Adversarial-Domain-Adaptation/" style="font-size: 10px;">Adversarial Domain Adaptation</a> <a href="/tags/Adversarial-Speaker-Verification/" style="font-size: 10px;">Adversarial Speaker Verification</a> <a href="/tags/Attention/" style="font-size: 10.77px;">Attention</a> <a href="/tags/Auth/" style="font-size: 10px;">Auth</a> <a href="/tags/Automation/" style="font-size: 10px;">Automation</a> <a href="/tags/Bash/" style="font-size: 10px;">Bash</a> <a href="/tags/BeautifulSoup/" style="font-size: 11.54px;">BeautifulSoup</a> <a href="/tags/Benchmark/" style="font-size: 10px;">Benchmark</a> <a href="/tags/Big-Query/" style="font-size: 16.15px;">Big Query</a> <a href="/tags/BigQueryML/" style="font-size: 13.85px;">BigQueryML</a> <a href="/tags/Brython/" style="font-size: 10px;">Brython</a> <a href="/tags/CGI/" style="font-size: 10px;">CGI</a> <a href="/tags/Center-Loss/" style="font-size: 10px;">Center Loss</a> <a href="/tags/Chatbot/" style="font-size: 13.08px;">Chatbot</a> <a href="/tags/Clustering/" style="font-size: 13.85px;">Clustering</a> <a href="/tags/Confusion-Matrix/" style="font-size: 10px;">Confusion Matrix</a> <a href="/tags/Convolutional-Neural-Networks/" style="font-size: 10.77px;">Convolutional Neural Networks</a> <a href="/tags/Coursera/" style="font-size: 18.46px;">Coursera</a> <a href="/tags/Cross-domain/" style="font-size: 10px;">Cross-domain</a> <a href="/tags/DBSCAN/" style="font-size: 10px;">DBSCAN</a> <a href="/tags/DCGAN/" style="font-size: 10px;">DCGAN</a> <a href="/tags/DataFlow/" style="font-size: 10px;">DataFlow</a> <a href="/tags/Decision-Tree-Classifier/" style="font-size: 10px;">Decision Tree Classifier</a> <a href="/tags/Deep-Nueral-Networks/" style="font-size: 10.77px;">Deep Nueral Networks</a> <a href="/tags/Deep-Machine-Learning-Paper-Study/" style="font-size: 17.69px;">Deep/Machine Learning Paper Study</a> <a href="/tags/Doc2vec/" style="font-size: 12.31px;">Doc2vec</a> <a href="/tags/Domain-Adaptation/" style="font-size: 10px;">Domain Adaptation</a> <a href="/tags/Domain-Adversarial-Layer/" style="font-size: 10px;">Domain Adversarial Layer</a> <a href="/tags/Domain-Invariant-Feature-Learning/" style="font-size: 10.77px;">Domain Invariant Feature Learning</a> <a href="/tags/English/" style="font-size: 19.23px;">English</a> <a href="/tags/Ensemble-Model/" style="font-size: 10px;">Ensemble Model</a> <a href="/tags/Error/" style="font-size: 10px;">Error</a> <a href="/tags/Evaluation/" style="font-size: 10px;">Evaluation</a> <a href="/tags/F1-measure/" style="font-size: 10px;">F1 measure</a> <a href="/tags/Flask/" style="font-size: 10px;">Flask</a> <a href="/tags/GAN/" style="font-size: 10.77px;">GAN</a> <a href="/tags/GCP/" style="font-size: 10px;">GCP</a> <a href="/tags/Gaussian-Mixture-Model/" style="font-size: 10.77px;">Gaussian Mixture Model</a> <a href="/tags/Gen-App-Builder/" style="font-size: 10px;">Gen App Builder</a> <a href="/tags/Generative-Adversarial-Network/" style="font-size: 10px;">Generative Adversarial Network</a> <a href="/tags/Generative-Model/" style="font-size: 10px;">Generative Model</a> <a href="/tags/Git/" style="font-size: 10.77px;">Git</a> <a href="/tags/Grid-Search-CV/" style="font-size: 10px;">Grid Search CV</a> <a href="/tags/HTML/" style="font-size: 11.54px;">HTML</a> <a href="/tags/Hexo/" style="font-size: 12.31px;">Hexo</a> <a href="/tags/HuBERT/" style="font-size: 10px;">HuBERT</a> <a href="/tags/Hueman/" style="font-size: 10px;">Hueman</a> <a href="/tags/Image-Classification/" style="font-size: 11.54px;">Image Classification</a> <a href="/tags/K-Means-Clustering/" style="font-size: 11.54px;">K-Means Clustering</a> <a href="/tags/Kaggle/" style="font-size: 10px;">Kaggle</a> <a href="/tags/Looker-Studio/" style="font-size: 11.54px;">Looker Studio</a> <a href="/tags/ML-Analysis/" style="font-size: 20px;">ML Analysis</a> <a href="/tags/ML-Operations/" style="font-size: 10.77px;">ML Operations</a> <a href="/tags/ML-Process/" style="font-size: 10.77px;">ML Process</a> <a href="/tags/Model-Generalization/" style="font-size: 10px;">Model Generalization</a> <a href="/tags/MongoDB/" style="font-size: 11.54px;">MongoDB</a> <a href="/tags/Multi-Task-Learning/" style="font-size: 10.77px;">Multi-Task Learning</a> <a href="/tags/NLP/" style="font-size: 13.08px;">NLP</a> <a href="/tags/Nonprobability-sampling/" style="font-size: 10px;">Nonprobability sampling</a> <a href="/tags/Normal-Distribution/" style="font-size: 10px;">Normal Distribution</a> <a href="/tags/PACF/" style="font-size: 10px;">PACF</a> <a href="/tags/Pandas-Dataframe/" style="font-size: 10px;">Pandas Dataframe</a> <a href="/tags/Precision-Recall/" style="font-size: 10px;">Precision-Recall</a> <a href="/tags/Probabilistic-sampling/" style="font-size: 10px;">Probabilistic sampling</a> <a href="/tags/Probability-Density-Function/" style="font-size: 10px;">Probability Density Function</a> <a href="/tags/Probability-Distribution-Function/" style="font-size: 10px;">Probability Distribution Function</a> <a href="/tags/Python/" style="font-size: 16.92px;">Python</a> <a href="/tags/Quiz/" style="font-size: 14.62px;">Quiz</a> <a href="/tags/ROC-curve/" style="font-size: 10px;">ROC curve</a> <a href="/tags/Recommendation-System/" style="font-size: 10.77px;">Recommendation System</a> <a href="/tags/Representation-Learning/" style="font-size: 10px;">Representation Learning</a> <a href="/tags/Self-Supervised-Features/" style="font-size: 10px;">Self-Supervised Features</a> <a href="/tags/Self-Supervised-Learning/" style="font-size: 11.54px;">Self-Supervised Learning</a> <a href="/tags/Speaker-GAN/" style="font-size: 10px;">Speaker GAN</a> <a href="/tags/Speaker-Identification/" style="font-size: 10px;">Speaker Identification</a> <a href="/tags/Speaker-Independent/" style="font-size: 10px;">Speaker Independent</a> <a href="/tags/Speaker-Normalization/" style="font-size: 10px;">Speaker Normalization</a> <a href="/tags/Speaker-Verification/" style="font-size: 10px;">Speaker Verification</a> <a href="/tags/Speaker-Verification/" style="font-size: 10px;">Speaker Verification,</a> <a href="/tags/Speech-Emotion-Recognition/" style="font-size: 15.38px;">Speech Emotion Recognition</a> <a href="/tags/Speech-Feature-Extraction/" style="font-size: 10px;">Speech Feature Extraction</a> <a href="/tags/Speech-Representations/" style="font-size: 10px;">Speech Representations</a> <a href="/tags/Spoken-Language-Understanding/" style="font-size: 10px;">Spoken Language Understanding</a> <a href="/tags/Standard-Normal-Distribution/" style="font-size: 10px;">Standard Normal Distribution</a> <a href="/tags/TD-SV/" style="font-size: 10px;">TD-SV</a> <a href="/tags/Threshold-Adjustment/" style="font-size: 10px;">Threshold Adjustment</a> <a href="/tags/Time-Series/" style="font-size: 10px;">Time Series</a> <a href="/tags/Transfer-Learning/" style="font-size: 10px;">Transfer Learning</a> <a href="/tags/Transformer/" style="font-size: 11.54px;">Transformer</a> <a href="/tags/Vertex-AI/" style="font-size: 10px;">Vertex AI</a> <a href="/tags/Virtualenv/" style="font-size: 10px;">Virtualenv</a> <a href="/tags/Voice-Trigger-Detection/" style="font-size: 10.77px;">Voice Trigger Detection</a> <a href="/tags/WGAN/" style="font-size: 10px;">WGAN</a> <a href="/tags/WGAN-GP/" style="font-size: 10px;">WGAN-GP</a> <a href="/tags/Wake-Up-Words/" style="font-size: 10px;">Wake-Up Words</a> <a href="/tags/Wav2vec-2-0/" style="font-size: 10px;">Wav2vec 2.0</a> <a href="/tags/Web-Crawling/" style="font-size: 11.54px;">Web Crawling</a> <a href="/tags/Web-Server/" style="font-size: 10.77px;">Web Server</a> <a href="/tags/kaggle/" style="font-size: 10px;">kaggle</a> <a href="/tags/pyspark/" style="font-size: 12.31px;">pyspark</a>
        </div>
    </div>


            
        

    </div>
</aside>


                </div>
            </div>
        </div>
        <footer id="footer">
    <div class="container">
        <div class="container-inner">
            <a id="back-to-top" href="javascript:;"><i class="icon fa fa-angle-up"></i></a>
            <div class="credit">
                <h1 class="logo-wrap">
                    <a href="/" class="logo"></a>
                </h1>
                <p>&copy; 2023 Jang Minjee</p>
                
                <p>Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>. Theme by <a href="https://github.com/ppoffice" target="_blank">PPOffice</a></p>
                
            </div>
            <div class="footer-plugins">
              
    


            </div>
        </div>
    </div>
</footer>

    </div>
    


    
        
<script src="/libs/lightgallery/js/lightgallery.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-pager.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-autoplay.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-zoom.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-hash.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-share.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-video.min.js"></script>

    
    
        
<script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>

    
    
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
        </script>
        
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML.js"></script>

    

    
      <script data-ad-client="ca-pub-2182912223281192" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

    
    
    



<!-- Custom Scripts -->

<script src="/js/main.js"></script>


</body>
</html>
