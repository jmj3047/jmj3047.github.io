<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8" />
    <meta name="naver-site-verification" content="760dcf2c928601f50f3941df3b6b4629fd244c7c" />
    <!-- Google Ads -->
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2182912223281192"
    crossorigin="anonymous"></script>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-90VXCLXLJT"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-90VXCLXLJT');
    </script>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-245679127-1"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-245679127-1');
    </script>

    

    
    <title>Speaker to Emotion, Domain Adaptation for Speech Emotion Recognition with Residual Adapters | Jang Minjee</title>
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="keywords" content="Speech Emotion Recognition,Domain Adaptation" />
    
    <meta name="description" content="Journal&#x2F;Conference : Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)Year(published year): 2019Author: Yuxuan Xi, Pengcheng Li, Yan Song, Yihen">
<meta property="og:type" content="article">
<meta property="og:title" content="Speaker to Emotion, Domain Adaptation for Speech Emotion Recognition with Residual Adapters">
<meta property="og:url" content="https://jmj3047.github.io/2023/11/07/Speaker_to_Emotion/index.html">
<meta property="og:site_name" content="Jang Minjee">
<meta property="og:description" content="Journal&#x2F;Conference : Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)Year(published year): 2019Author: Yuxuan Xi, Pengcheng Li, Yan Song, Yihen">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://jmj3047.github.io/images/Speaker_to_Emotion/Untitled.png">
<meta property="article:published_time" content="2023-11-06T15:00:00.000Z">
<meta property="article:modified_time" content="2023-11-12T14:07:28.067Z">
<meta property="article:author" content="Jang Minjee">
<meta property="article:tag" content="Speech Emotion Recognition">
<meta property="article:tag" content="Domain Adaptation">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://jmj3047.github.io/images/Speaker_to_Emotion/Untitled.png">
    

    

    
        <link rel="icon" href="/images/favicon.png" />
    

    
<link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">

    
<link rel="stylesheet" href="/libs/titillium-web/styles.css">

    
<link rel="stylesheet" href="/libs/source-code-pro/styles.css">


    
<link rel="stylesheet" href="/css/style.css">


    
<script src="/libs/jquery/3.5.0/jquery.min.js"></script>

    
    
        
<link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css">

    
    
        
<link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css">

    
    
        <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=[object Object]"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', '[object Object]');
</script>
<!-- End Google Analytics -->

    
    
    


    
<link rel="stylesheet" href="https://cdn.rawgit.com/innks/NanumSquareRound/master/nanumsquareround.css">

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head>

<body>
    <div id="wrap">
        <header id="header">
    <div id="header-outer" class="outer">
        <div class="container">
            <div class="container-inner">
                <div id="header-title">
                    <h1 class="logo-wrap">
                        <a href="/" class="logo"></a>
                    </h1>
                    
                </div>
                <div id="header-inner" class="nav-container">
                    <a id="main-nav-toggle" class="nav-icon fa fa-bars"></a>
                    <div class="nav-container-inner">
                        <ul id="main-nav">
                            
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" target="_blank" rel="noopener" href="https://github.com/jmj3047/mj_portfolio">About Me</a>
                                </li>
                            
                                    <ul class="main-nav-list"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Data-Analysis/">Data Analysis</a><ul class="main-nav-list-child"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Data-Analysis/Basic/">Basic</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Data-Analysis/Model/">Model</a></li></ul></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Data-Platform-Base/">Data Platform/Base</a><ul class="main-nav-list-child"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Data-Platform-Base/GCP/">GCP</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Data-Platform-Base/MongoDB/">MongoDB</a></li></ul></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Paper/">Paper</a><ul class="main-nav-list-child"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Paper/Computer-Vision/">Computer Vision</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Paper/Generative-Model/">Generative Model</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Paper/Multi-Task-Learning/">Multi-Task Learning</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Paper/NLP/">NLP</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Paper/Recommendation-System/">Recommendation System</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Paper/Speaker-Verification/">Speaker Verification</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Paper/Speech-Emotion-Recognition/">Speech Emotion Recognition</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Paper/Speech-Representations/">Speech Representations</a></li></ul></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Python/">Python</a><ul class="main-nav-list-child"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Python/Django/">Django</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Python/HTML/">HTML</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Python/Pyspark/">Pyspark</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Python/Python/">Python</a></li></ul></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Setting/">Setting</a></li></ul>
                                
                        </ul>
                        <nav id="sub-nav">
                            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/js/insight.js"></script>


</div>
                        </nav>
                    </div>
                </div>
            </div>
        </div>
    </div>
</header>

        <div class="container">
            <div class="main-body container-inner">
                <div class="main-body-inner">
                    <section id="main">
                        <div class="main-body-header">
    <h1 class="header">
    
    <a class="page-title-link" href="/categories/Paper/">Paper</a><i class="icon fa fa-angle-right"></i><a class="page-title-link" href="/categories/Paper/Speech-Emotion-Recognition/">Speech Emotion Recognition</a>
    </h1>
</div>

                        <div class="main-body-content">
                            <article id="post-Speaker_to_Emotion" class="article article-single article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
            <header class="article-header">
                
    
        <h1 class="article-title" itemprop="name">
        Speaker to Emotion, Domain Adaptation for Speech Emotion Recognition with Residual Adapters
        </h1>
    

                
            </header>
        
        
            <div class="article-meta">
                
  <div class="article-date">
    <i class="fa fa-calendar"></i>
    <a href="/2023/11/07/Speaker_to_Emotion/" class="article-date">
       <time datetime="2023-11-06T15:00:00.000Z" itemprop="datePublished">2023-11-07</time>
    </a>
  </div>


<div class="article-date">
  <i class="fa fa-calendar-plus-o"></i>
  <a href="/2023/11/07/Speaker_to_Emotion/" class="article-date">
     <time datetime="2023-11-12T14:07:28.067Z" itemprop="dateModified">2023-11-12</time>
  </a>
</div>


                

                
    <div class="article-tag">
        <i class="fa fa-tag"></i>
        <a class="tag-link-link" href="/tags/Domain-Adaptation/" rel="tag">Domain Adaptation</a>, <a class="tag-link-link" href="/tags/Speech-Emotion-Recognition/" rel="tag">Speech Emotion Recognition</a>
    </div>

                

                

            </div>
        
        
        <div class="article-entry" itemprop="articleBody">
            

            

            

            <p>Journal&#x2F;Conference : Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)<br>Year(published year): 2019<br>Author: Yuxuan Xi, Pengcheng Li, Yan Song, Yiheng Jiang, Lirong Dai<br>Subject: Domain Adaptation, Speech Emotion Recognition</p>
<h1 id="Speaker-to-Emotion-Domain-Adaptation-for-Speech-Emotion-Recognition-with-Residual-Adapters"><a href="#Speaker-to-Emotion-Domain-Adaptation-for-Speech-Emotion-Recognition-with-Residual-Adapters" class="headerlink" title="Speaker to Emotion: Domain Adaptation for Speech Emotion Recognition with Residual Adapters"></a>Speaker to Emotion: Domain Adaptation for Speech Emotion Recognition with Residual Adapters</h1><blockquote>
<p>Summary</p>
</blockquote>
<ul>
<li>The paper proposes a new method for domain adaptation in speech emotion recognition using residual adapters.</li>
<li>The proposed method transfers information from a speaker corpus to an emotion corpus, resulting in significant improvements in SER performance.</li>
<li>The paper demonstrates the effectiveness of the proposed method through experiments and shows that domain-agnostic parameters learned by VoxCeleb2 are necessary for effective domain adaptation in SER.</li>
</ul>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Recently, deep learning based systems have achieved significant progress for SER, but to be successful, sufficient labeled data is needed, particularly due to the complexity of emotional information. However, existing corpora, such as IEMOCAP [15], CHEAVD [14], FAU-AIBO [29], and EMODB [30], are generally size-limited, in part due to annotation cost, and also suffer label ambiguity.</p>
<p>One possible solution is to utilize emotion information from multiple corpora. Based on this approach, several transfer learning and multi-task learning (MTL) based methods have been proposed [1], [2], [3], [4].</p>
<p>Transfer learning focuses on adapting knowledge from available auxiliary resources to the target domain.</p>
<p>However, due to the limited size of emotion corpora, SER performance is far from satisfactory and it is still difficult to apply successful deep learning architectures like ResNet and DenseNet to further improve performance.</p>
<p>Based on this view, we propose a domain adaptive model which can utilize a common representation between emotion and speaker identity to further improve SER accuracy, using ResNet as a backbone architecture.</p>
<p>Specifically, the proposed method aims to tackle the lack of labeled corpus by employing a residual adapter model [12] to transfer the information from VoxCeleb to a specific SER target dataset. The residual adapter resembles ResNet [10], with the major difference that all convolutional layers are replaced by adapter modules. </p>
<p>In this paper, the residual adapter model is trained using VoxCeleb2 data with speaker labels, then emotion corpora are used to train the domain-specific parameters, and different fully-connected layers are used to predict the classification score, as shown in Fig.1.</p>
<p>The main difference lies in that the proposed residual adapter utilizes supervised learning to exploit the relationship between speaker and emotion data. To prove the effectiveness of our method, we first use ResNet that is trained with emotion data only as a baseline system, and then conduct a series of experiments as shown in Fig.1, including: (1) A ResNet trained by VoxCeleb2 data as the feature extractor, then the classifier trained for SER. (2) The same ResNet fine-tuned with emotion data – the common practice in transfer learning. (3) The proposed residual adapter method, furthermore testing the adapter module alone, aiming to demonstrate that features learned from the speaker classification task can be beneficial to SER.</p>
<h2 id="Overview-of-Speaker-To-Emotion-Domain-Adaptation-Framework"><a href="#Overview-of-Speaker-To-Emotion-Domain-Adaptation-Framework" class="headerlink" title="Overview of Speaker-To-Emotion Domain Adaptation Framework"></a>Overview of Speaker-To-Emotion Domain Adaptation Framework</h2><p>SER encompasses some existing problems. (1) Deep learning based methods have become prevalent in recent years [20], [21], [22], owing to the powerful representation learning ability of neural networks.  In general, increasing network depth benefits performance, but the limited scale of emotion corpora greatly restricts the network complexity in practice.</p>
<p>(2) Existing methods mainly focus on cross-corpus learning among emotion corpora, but due to the difficulty and cost of labeling emotion data, cross-corpus methods still have limitations.</p>
<p>Speaker-labeled corpora are potential choices, as described in Section I, where speaker characteristics such as age and gender can influence SER results.</p>
<p>This fact indicates that there is some shared representation between speaker characteristics and emotion.</p>
<p>On the other hand, the scale of speaker corpora are much larger than those for emotion, a fact that aids in training a deep neural network. Based on those motivations, the VoxCeleb2 [13] corpus is selected in this paper for initial model training.</p>
<p>In order to utilize speaker labels, a complex network is first trained by speaker corpora, and then adapted to the target emotion corpora.</p>
<p><img src="/images/Speaker_to_Emotion/Untitled.png" alt=" "></p>
<p>The first method is a feature extractor, which constructs an emotion classifier by retraining the topmost FC layer. The second method is finetuning, which takes the same structure as the feature extractor, then all network parameters as well as the FC classifier, are fine-tuned using the emotion corpora. Although these two<br>methods can exploit the information from both speaker and emotion data, they have some obvious problems.</p>
<p>Firstly, the network parameters are pre-trained for speaker verification, which may be quite different from SER, therefore directly utilizing the model may not be a appropriate choice. Secondly, due to the limited scale of the target emotion corpora, training the whole network may be difficult and may cause an overfitting<br>problem. On the other hand, the fine-tuning stage may result in forgetting the source corpora, reducing the benefit of the auxiliary information.</p>
<p>첫 번째 방법은 특징 추출기입니다. 이 방법은 화자 코퍼스 대해 훈련된 사전 훈련된 복잡한 네트워크를 가져와 최상위 완전 연결(FC) 계층만 재훈련하여 감정 분류기를 구성하는 것입니다. FC 레이어는 추출된 특징을 출력 클래스에 매핑하는 네트워크의 마지막 레이어입니다. 이 레이어만 재학습하면 화자 코퍼스의 정보를 그대로 활용하면서 네트워크를 목표 감정 코퍼스에 맞게 조정할 수 있습니다.</p>
<p>이 방법은 특징 추출기와 동일한 복잡한 네트워크 구조를 취하고 감정 코퍼스를 사용하여 FC 분류기를 포함한 모든 네트워크 파라미터를 미세 조정하는 것입니다. 미세 조정을 통해 네트워크가 특징 추출기 방식보다 더 광범위하게 대상 감정 코퍼스에 적응할 수 있지만, 대상 감정 코퍼스의 제한된 규모에 과적합하고 화자 코퍼스의 정보를 잊어버릴 위험이 있습니다.</p>
<p>To address these issues, this paper attempts to establish the third method, a new domain adaptation. In this method, the deep learning model is first trained using VoxCeleb2 data with speaker labels as usual. But during the adaptation stage, some extra emotion-specific parameters are added to the original model, then the emotion corpora are utilized to only fine-tune the additional parameters which coexist alongside the previously trained parameters. Through the proposed framework, the information forgetting problem is avoided, and because the emotion corpora is only utilized to fine-tune a part of the network, the over-fitting problem may be mitigated.</p>
<p>이러한 문제를 해결하기 위해 본 논문에서는 세 번째 방법인 새로운 도메인 적응 방식을 시도 합니다. 이 방법에서는 먼저 평소와 같이 화자 레이블이 있는 VoxCeleb2 데이터를 사용하여 딥러닝 모델을 훈련합니다. 그러나 적응단계 에서는 원래 모델에 몇 가지 추가 감정 관련 파라미터를 더한다음, 감정 코퍼스를 활용하여 이전에 학습된 파라미터를 함께 공존하는 추가 파라미터만 미세조정 합니다. 제안된 프레임워크를 통해 정보 망각 문제를 피할 수 있으며 감정 코퍼스 네트워크의 일부분만 미세조정 하는데 활용하기 때문에 과적합 문제를 완화할 수 있습니다.</p>
<h2 id="Residual-Adapter-Model"><a href="#Residual-Adapter-Model" class="headerlink" title="Residual Adapter Model"></a>Residual Adapter Model</h2><h3 id="Model-Design"><a href="#Model-Design" class="headerlink" title="Model Design"></a>Model Design</h3><p>The basic idea of constructing an adapter module is to linearly parameterize the convolutional filter group, which is the same as introducing an intermediate convolutional layer.</p>
<p>기본 아이디어는 컨볼루션 필터 그룹을 선형적으로 파라미터화하는 것으로 이는 중간 컨볼루션 계층을 도입하는 것과 같습니다. </p>
<p>For the training progress, firstly the model is trained on the initial task, using a large corpus. Next, the parameters are fixed and other domain adapters are trained using the target domain corpus.</p>
<p>이 문장은 잔여 어댑터를 사용한 도메인 적응을 위한 훈련 과정을 의미합니다. 첫 번째 단계는 초기 작업(이 경우 화자 검증)을 위해 대규모 코퍼스에서 딥러닝 모델을 훈련하는 것입니다. 모델이 학습되면 파라미터가 고정되고 목표 도메인 말뭉치(이 경우 감정 코퍼스)를 사용하여 추가 도메인 어댑터를 학습합니다. 도메인 어댑터를 원래 모델에 추가하고 대상 도메인 코퍼스를 사용하여 미세 조정하여 모델을 새 도메인에 맞게 조정합니다. 제안한 방법은 원래 모델의 파라미터를 고정하고 추가 도메인 어댑터만 미세 조정함으로써 전체 네트워크를 미세 조정할 때 발생할 수 있는 과적합 및 정보 망각 문제를 방지하는 것을 목표로 합니다.</p>
<h3 id="Adapter-module-and-network-structure"><a href="#Adapter-module-and-network-structure" class="headerlink" title="Adapter module and network structure"></a>Adapter module and network structure</h3><p><img src="/images/Speaker_to_Emotion/Untitled%201.png" alt=" "></p>
<p>The residual adapter model is constructed on the ResNet20 model with a network structure outlined in Table I.</p>
<p><img src="/images/Speaker_to_Emotion/Untitled%202.png" alt=" "></p>
<h2 id="EXPERIMENTS-AND-ANALYSIS"><a href="#EXPERIMENTS-AND-ANALYSIS" class="headerlink" title="EXPERIMENTS AND ANALYSIS"></a>EXPERIMENTS AND ANALYSIS</h2><h3 id="Data-description-and-pre-processing"><a href="#Data-description-and-pre-processing" class="headerlink" title="Data description and pre-processing"></a>Data description and pre-processing</h3><p>In this study, both speaker data and emotion data are utilized. For speaker-labeled data, we choose the VoxCeleb2 corpus [13]. VoxCeleb2 is a large-scale speaker-labeled database, prevalent for SV tasks, that was collected from more than 6000 celebrities on YouTube. VoxCeleb2 consists of 2442 hours, with more than a million speech utterances, covering different ages, genders, accents and scenes.</p>
<p>For the emotion part, we select Interactive Emotional Dyadic Motion Capture (IEMOCAP) [15] and Chinese Natural Audio-Visual Emotion Database (CHEAVD) [14] 2.0 databases. IEMOCAP has scripted and improvised parts, depending on the recording scenarios. We choose the improvised data part in order to exclude undesired contextual information. Labels of neutral, angry, happy and sad are used.</p>
<p>CHEAVD 2.0 is a Chinese emotion corpus, the official data of the Multimodal Emotion Recognition Challenge (MEC) 2017. CHEAVD contains data selected from Chinese movies, soap operas and TV shows. It contains 8 emotion labels (angry, happy, sad, worried, anxious, surprise, disgust, neutral).</p>
<p>The corpus is divided into training, validation and testing sets. We use the training&#x2F;validation split for performance evaluation, the hyper-parameter tuning is based on validation set, keeping the evaluation that same as in [25].</p>
<p>Magnitude spectrograms are utilized as input features, with the spectrograms extracted over 40 ms Hamming windows with a 10 ms window shift and 1600 FFT points. Then 0-4000 Hz spectrogram are utilized since human vocal expression is mainly located in this frequency range. The speech utterances are cut into 2 s portions with 1 s overlap, and zero-padding applied for utterances shorter than 2 s. Thus the input spectrograms have a size of 400 200. For each spectrogram, we then apply a $\mu$-law expansion, as used and described in our previous paper [18].</p>
<p>윈도우 시프트가 10밀리초인 40밀리초 해밍 윈도우와 1600개의 FFT 포인트로 추출한 스펙트로그램을 입력 피쳐로 활용합니다. 그런 다음 사람의 음성 표현이 주로 이 주파수 범위에 위치하기 때문에 0-4000Hz 스펙트로그램을 활용합니다. 음성 발화는 1초가 겹치는 2초 부분으로 잘리고, 2초보다 짧은 발화에 대해서는 제로 패딩을 적용하여 입력 스펙트로그램의 크기는 400×200입니다. 그런 다음 각 스펙트로그램에 대해 이전 논문 [18]에서 사용 및 설명한 대로 μ 법칙 확장을 적용합니다.</p>
<p>진폭 스펙트로그램은 시간에 따라 변화하는 신호 주파수의 진폭을 시각적으로 표현한 것입니다. 이는 시간에 따른 신호의 주파수 내용을 분석하는 방법인 신호의 단시간 푸리에 변환(STFT)의 크기를 구하여 얻을 수 있습니다. STFT는 신호를 짧게 겹치는 윈도우로 나누고 각 윈도우에 윈도우 함수를 적용한 다음 각 윈도우의 푸리에 변환을 취하여 계산합니다. 그런 다음 결과 복소수 값의 STFT 계수의 크기를 취하여 크기 스펙트로그램을 얻습니다. 크기 스펙트로그램은 시간 경과에 따른 신호의 주파수 내용을 간결하고 유익하게 표현하기 때문에 음성 처리 및 기타 신호 처리 애플리케이션에서 일반적으로 사용되는 특징 표현입니다.</p>
<p>For VoxCeleb2 data, we randomly choose 50 speakers to train the ResNet20 with adapters. For IEMOCAP improvised data, we conduct a 5-fold cross-validation, where 4 sections are used to train the network and the remaining 2 speakers are used as validation and test data.</p>
<h3 id="Experiment-setup"><a href="#Experiment-setup" class="headerlink" title="Experiment setup"></a>Experiment setup</h3><p><img src="/images/Speaker_to_Emotion/Untitled%203.png" alt=" "></p>
<p><strong>Baseline</strong>: We use IEMOCAP and CHEAVD to train a plain ResNet20 with results in the top row of Table II. Obviously, emotion data is insufficient to train a ResNet, so UA, WA are unsatisfactory for IEMOCAP and CHEAVD, in line with our expectations.</p>
<p><strong>Fine-tuning</strong>: We use VoxCeleb2 data to pre-train a plain ResNet20 then, after training, the FC layer of the network is replaced and the whole network fine-tuned by the target emotion corpus. The result is not significantly better than baseline, likely to be because the number of parameters is too large for the smaller extent of emotion data to train. On the other hand, forgetting the learned speaker information may be another problem which would reduce the accuracy.</p>
<p>VoxCeleb2 데이터를 사용하여 일반 ResNet20을 사전 훈련한 다음, 훈련 후 네트워크의 FC 계층을 교체하고 전체 네트워크를 목표 감정 코퍼스로 미세 조정합니다. 결과는 기준선보다 크게 나아지지 않는데, 이는 훈련할 감정 데이터의 범위가 작아 매개변수 수가 너무 많기 때문일 가능성이 높습니다. 반면에 학습된 화자 정보를 잊어버리는 것도 정확도를 떨어뜨리는 또 다른 문제일 수 있습니다.</p>
<p><strong>Feature extractor</strong>: When fixing the parameters learned by the primary domain, the network becomes a feature extractor. In this experiment we fix all ResNet20 parameters and train the FC layer with emotion corpora. The performance is worse than the fine-tuning method, this indicates utilizing only speaker information is not appropriate for SER.</p>
<p>기본 도메인에서 학습한 파라미터를 수정하면 네트워크는 특징 추출기가 됩니다. 이 실험에서는 모든 ResNet20 파라미터를 수정하고 감정 말뭉치로 FC 레이어를 훈련합니다. 미세 조정 방식보다 성능이 떨어지는데, 이는 화자 정보만 활용하는 것이 SER에 적합하지 않다는 것을 나타냅니다.</p>
<p><strong>Residual Adapter</strong>: We next evaluate the residual adapter model. We use VoxCeleb2 data to train the same ResNet20 with adapter modules. During the adapting process, all of the parameters of the 3   3 filters are fixed, then the adapters are trained using emotion data. The result significantly outperforms the baseline system, especially for IEMOCAP, where the UA and WA achieve 67.58% and 72.73%. On CHEAVD they achieve 34.08% and 43.96%. We attempted to increase the number of speakers during residual adapter training, but the performance did not benefit from this, perhaps because a more complex model is needed.</p>
<p>다음으로 잔여 어댑터 모델을 평가합니다. VoxCeleb2 데이터를 사용하여 어댑터 모듈로 동일한 ResNet20을 훈련합니다. 적응 과정에서 3개의 필터의 모든 파라미터가 고정된 다음 감정 데이터를 사용하여 어댑터를 훈련합니다. 그 결과 기준 시스템보다 훨씬 뛰어난 성능을 발휘하며, 특히 IEMOCAP의 경우 UA와 WA가 67.58%와 72.73%를 달성합니다. CHEAVD에서는 34.08%와 43.96%를 달성했습니다. 잔류 어댑터 훈련 중에 화자 수를 늘리려고 시도했지만 더 복잡한 모델이 필요하기 때문에 성능에 도움이 되지 않았습니다.</p>
<p><strong>Evaluation of adapters</strong>: Finally, we want to clarify if the improvement in SER performance has benefited from domain-agnostic parameters learned by VoxCeleb2, or simply because adapters have fewer parameters so the model can be trained by emotion corpora. To answer the question, we keep the same experiment configuration with the residual adapters, retain the model but set all 3X3 convolutional filter weights to 0, so the domain-agnostic parameters will offer no information. As a result, the accuracy significantly drops, which proves the necessity of domain-agnostic parameters.</p>
<p>마지막으로 SER 성능의 개선이 VoxCeleb2가 학습한 도메인에 구애받지 않는 파라미터의 덕분인지, 아니면 단순히 감정 코퍼스로 모델을 학습할 수 있도록 어댑터의 파라미터 수가 적기 때문인지 명확히 하고자 합니다. 이 질문에 답하기 위해 잔여 어댑터와 동일한 실험 구성을 유지하고 모델을 유지하되 3개의 컨볼루션 필터 가중치를 모두 0으로 설정하여 도메인에 구애받지 않는 파라미터가 아무런 정보도 제공하지 않도록 했습니다. 결과적으로 정확도가 크게 떨어지며 도메인에 구애받지 않는 파라미터의 필요성이 입증됩니다.</p>
<h3 id="Comparison-to-state-of-the-art-systems"><a href="#Comparison-to-state-of-the-art-systems" class="headerlink" title="Comparison to state-of-the-art systems"></a>Comparison to state-of-the-art systems</h3><p><img src="/images/Speaker_to_Emotion/Untitled%204.png" alt=" "></p>
<p>Compared to [25], our method does not exceed their WA but has better UA, indicating that the performance of data-limited small classes is improved. In fact these results show that the proposed residual adapter model can effectively utilize speaker characteristic information from the VoxCeleb2 training data, yet also provide discrimination ability for the SER task. In future we believe there is potential to exploit a deeper network for SER to further improve performance.</p>

        </div>
        <footer class="article-footer">
            



    <a data-url="https://jmj3047.github.io/2023/11/07/Speaker_to_Emotion/" data-id="clovjm9qp0000j3u6c82m45n9" class="article-share-link"><i class="fa fa-share"></i>Share</a>
<script>
    (function ($) {
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
</script>

        </footer>
    </div>
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "BlogPosting",
        "author": {
            "@type": "Person",
            "name": "Jang Minjee"
        },
        "headline": "Speaker to Emotion, Domain Adaptation for Speech Emotion Recognition with Residual Adapters",
        "image": "https://jmj3047.github.io/images/Speaker_to_Emotion/Untitled.png",
        "keywords": "Speech Emotion Recognition Domain Adaptation",
        "genre": "Paper Speech Emotion Recognition",
        "datePublished": "2023-11-07",
        "dateCreated": "2023-11-07",
        "dateModified": "2023-11-12",
        "url": "https://jmj3047.github.io/2023/11/07/Speaker_to_Emotion/",
        "description": "Journal&#x2F;Conference : Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)Year(published year): 2019Author: Yuxuan Xi, Pengcheng Li, Yan Song, Yihen",
        "wordCount": 2425
    }
</script>

</article>

    <section id="comments">
    
    </section>



                        </div>
                    </section>
                    
                    <aside id="sidebar">
    <a class="sidebar-toggle" title="Expand Sidebar"><i class="toggle icon"></i></a>
    <div class="sidebar-top">
        <p>follow:</p>
        <ul class="social-links">
            
                
                <li>
                    <a class="social-tooltip" title="github" href="https://github.com/jmj3047" target="_blank" rel="noopener">
                        <i class="icon fa fa-github"></i>
                    </a>
                </li>
                
            
                
                <li>
                    <a class="social-tooltip" title="dropbox" href="https://www.dropbox.com/home" target="_blank" rel="noopener">
                        <i class="icon fa fa-dropbox"></i>
                    </a>
                </li>
                
            
                
                <li>
                    <a class="social-tooltip" title="google" href="https://www.google.com" target="_blank" rel="noopener">
                        <i class="icon fa fa-google"></i>
                    </a>
                </li>
                
            
                
                <li>
                    <a class="social-tooltip" title="youtube" href="https://www.youtube.com/" target="_blank" rel="noopener">
                        <i class="icon fa fa-youtube"></i>
                    </a>
                </li>
                
            
                
                <li>
                    <a class="social-tooltip" title="discord" href="https://discord.com/channels/@me" target="_blank" rel="noopener">
                        <i class="icon fa fa-discord"></i>
                    </a>
                </li>
                
            
        </ul>
    </div>
<!-- 
    <div class="github-card" data-github="jmj3047" data-width="400" data-height="" data-theme="default">
    </div>
    <script src="//cdn.jsdelivr.net/github-cards/latest/widget.js">    </script> -->

    
        
<nav id="article-nav">
    
    
        <a href="/2023/09/11/Sentence_Bert/" id="article-nav-older" class="article-nav-link-wrap">
        <strong class="article-nav-caption">older</strong>
        <p class="article-nav-title">Sentence Bert</p>
        <i class="icon fa fa-chevron-left" id="icon-chevron-left"></i>
        </a>
    
</nav>

    


    <div class="widgets-container">
        
        
        
            
                

            
                
    <div class="widget-wrap">
        <h3 class="widget-title">recents</h3>
        <div class="widget">
            <ul id="recent-post" class="no-thumbnail">
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/Paper/">Paper</a><i class="icon fa fa-angle-right"></i><a class="article-category-link" href="/categories/Paper/Speech-Emotion-Recognition/">Speech Emotion Recognition</a></p>
                            <p class="item-title"><a href="/2023/11/07/Speaker_to_Emotion/" class="title">Speaker to Emotion, Domain Adaptation for Speech Emotion Recognition with Residual Adapters</a></p>
                            <p class="item-date"><time datetime="2023-11-06T15:00:00.000Z" itemprop="datePublished">2023-11-07</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/Paper/">Paper</a><i class="icon fa fa-angle-right"></i><a class="article-category-link" href="/categories/Paper/NLP/">NLP</a></p>
                            <p class="item-title"><a href="/2023/09/11/Sentence_Bert/" class="title">Sentence Bert</a></p>
                            <p class="item-date"><time datetime="2023-09-10T15:00:00.000Z" itemprop="datePublished">2023-09-11</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/Data-Analysis/">Data Analysis</a><i class="icon fa fa-angle-right"></i><a class="article-category-link" href="/categories/Data-Analysis/Basic/">Basic</a></p>
                            <p class="item-title"><a href="/2023/08/17/MLOps4_Quiz/" class="title">Deploying Machine Learning Models in Production_Quiz</a></p>
                            <p class="item-date"><time datetime="2023-08-16T15:00:00.000Z" itemprop="datePublished">2023-08-17</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/Data-Analysis/">Data Analysis</a><i class="icon fa fa-angle-right"></i><a class="article-category-link" href="/categories/Data-Analysis/Basic/">Basic</a></p>
                            <p class="item-title"><a href="/2023/08/16/MLOps3_Quiz/" class="title">Machine Learning Modeling Pipelines in Production_Quiz</a></p>
                            <p class="item-date"><time datetime="2023-08-15T15:00:00.000Z" itemprop="datePublished">2023-08-16</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/Paper/">Paper</a><i class="icon fa fa-angle-right"></i><a class="article-category-link" href="/categories/Paper/Speech-Emotion-Recognition/">Speech Emotion Recognition</a></p>
                            <p class="item-title"><a href="/2023/08/04/wav2vec_hubert_for_SER_SV_SLU/" class="title">A fine-tuned wav2vec2.0/Hubert benchmark for SER, Speaker verification and spoken language understanding</a></p>
                            <p class="item-date"><time datetime="2023-08-03T15:00:00.000Z" itemprop="datePublished">2023-08-04</time></p>
                        </div>
                    </li>
                
            </ul>
        </div>
    </div>

            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">categories</h3>
        <div class="widget">
            <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Data-Analysis/">Data Analysis</a><span class="category-list-count">40</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Data-Analysis/Basic/">Basic</a><span class="category-list-count">24</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Data-Analysis/Model/">Model</a><span class="category-list-count">16</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Data-Platform-Base/">Data Platform/Base</a><span class="category-list-count">16</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Data-Platform-Base/GCP/">GCP</a><span class="category-list-count">13</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Data-Platform-Base/MongoDB/">MongoDB</a><span class="category-list-count">3</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Paper/">Paper</a><span class="category-list-count">25</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Paper/Computer-Vision/">Computer Vision</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Paper/Generative-Model/">Generative Model</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Paper/Multi-Task-Learning/">Multi-Task Learning</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Paper/NLP/">NLP</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Paper/Recommendation-System/">Recommendation System</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Paper/Speaker-Verification/">Speaker Verification</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Paper/Speech-Emotion-Recognition/">Speech Emotion Recognition</a><span class="category-list-count">10</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Paper/Speech-Representations/">Speech Representations</a><span class="category-list-count">2</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a><span class="category-list-count">15</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Python/Django/">Django</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/HTML/">HTML</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/Pyspark/">Pyspark</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/Python/">Python</a><span class="category-list-count">5</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Setting/">Setting</a><span class="category-list-count">5</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-float">
        <h3 class="widget-title">tag cloud</h3>
        <div class="widget tagcloud">
            <a href="/tags/ACF/" style="font-size: 10px;">ACF</a> <a href="/tags/AI-Studies/" style="font-size: 16.36px;">AI Studies</a> <a href="/tags/AI-tool/" style="font-size: 10px;">AI tool</a> <a href="/tags/ARIMA/" style="font-size: 10px;">ARIMA</a> <a href="/tags/Acoustic-Parameter-Set/" style="font-size: 10px;">Acoustic Parameter Set</a> <a href="/tags/Adversarial-Domain-Adaptation/" style="font-size: 10px;">Adversarial Domain Adaptation</a> <a href="/tags/Adversarial-Speaker-Verification/" style="font-size: 10px;">Adversarial Speaker Verification</a> <a href="/tags/Attention/" style="font-size: 10.91px;">Attention</a> <a href="/tags/Auth/" style="font-size: 10px;">Auth</a> <a href="/tags/Automation/" style="font-size: 10px;">Automation</a> <a href="/tags/Bash/" style="font-size: 10px;">Bash</a> <a href="/tags/BeautifulSoup/" style="font-size: 11.82px;">BeautifulSoup</a> <a href="/tags/Benchmark/" style="font-size: 10px;">Benchmark</a> <a href="/tags/Bi-Encoder/" style="font-size: 10px;">Bi Encoder</a> <a href="/tags/Big-Query/" style="font-size: 15.45px;">Big Query</a> <a href="/tags/BigQueryML/" style="font-size: 14.55px;">BigQueryML</a> <a href="/tags/Brython/" style="font-size: 10px;">Brython</a> <a href="/tags/CGI/" style="font-size: 10px;">CGI</a> <a href="/tags/Center-Loss/" style="font-size: 10px;">Center Loss</a> <a href="/tags/Chatbot/" style="font-size: 13.64px;">Chatbot</a> <a href="/tags/Clustering/" style="font-size: 14.55px;">Clustering</a> <a href="/tags/Confusion-Matrix/" style="font-size: 10px;">Confusion Matrix</a> <a href="/tags/Convolutional-Neural-Networks/" style="font-size: 10.91px;">Convolutional Neural Networks</a> <a href="/tags/Coursera/" style="font-size: 18.18px;">Coursera</a> <a href="/tags/Cross-Encoder/" style="font-size: 10px;">Cross Encoder</a> <a href="/tags/Cross-domain/" style="font-size: 10px;">Cross-domain</a> <a href="/tags/DBSCAN/" style="font-size: 10px;">DBSCAN</a> <a href="/tags/DCGAN/" style="font-size: 10px;">DCGAN</a> <a href="/tags/DataFlow/" style="font-size: 10px;">DataFlow</a> <a href="/tags/Decision-Tree-Classifier/" style="font-size: 10px;">Decision Tree Classifier</a> <a href="/tags/Deep-Nueral-Networks/" style="font-size: 10.91px;">Deep Nueral Networks</a> <a href="/tags/Deep-Machine-Learning-Paper-Study/" style="font-size: 17.27px;">Deep/Machine Learning Paper Study</a> <a href="/tags/Doc2vec/" style="font-size: 12.73px;">Doc2vec</a> <a href="/tags/Domain-Adaptation/" style="font-size: 10.91px;">Domain Adaptation</a> <a href="/tags/Domain-Adversarial-Layer/" style="font-size: 10px;">Domain Adversarial Layer</a> <a href="/tags/Domain-Invariant-Feature-Learning/" style="font-size: 10.91px;">Domain Invariant Feature Learning</a> <a href="/tags/English/" style="font-size: 19.09px;">English</a> <a href="/tags/Ensemble-Model/" style="font-size: 10px;">Ensemble Model</a> <a href="/tags/Error/" style="font-size: 10px;">Error</a> <a href="/tags/Evaluation/" style="font-size: 10px;">Evaluation</a> <a href="/tags/F1-measure/" style="font-size: 10px;">F1 measure</a> <a href="/tags/Flask/" style="font-size: 10px;">Flask</a> <a href="/tags/GAN/" style="font-size: 10.91px;">GAN</a> <a href="/tags/GCP/" style="font-size: 10px;">GCP</a> <a href="/tags/Gaussian-Mixture-Model/" style="font-size: 10.91px;">Gaussian Mixture Model</a> <a href="/tags/Gen-App-Builder/" style="font-size: 10px;">Gen App Builder</a> <a href="/tags/Generative-Adversarial-Network/" style="font-size: 10px;">Generative Adversarial Network</a> <a href="/tags/Generative-Model/" style="font-size: 10px;">Generative Model</a> <a href="/tags/Git/" style="font-size: 10.91px;">Git</a> <a href="/tags/Grid-Search-CV/" style="font-size: 10px;">Grid Search CV</a> <a href="/tags/HTML/" style="font-size: 11.82px;">HTML</a> <a href="/tags/Hexo/" style="font-size: 12.73px;">Hexo</a> <a href="/tags/HuBERT/" style="font-size: 10px;">HuBERT</a> <a href="/tags/Hueman/" style="font-size: 10px;">Hueman</a> <a href="/tags/Image-Classification/" style="font-size: 11.82px;">Image Classification</a> <a href="/tags/K-Means-Clustering/" style="font-size: 11.82px;">K-Means Clustering</a> <a href="/tags/Kaggle/" style="font-size: 10px;">Kaggle</a> <a href="/tags/Looker-Studio/" style="font-size: 11.82px;">Looker Studio</a> <a href="/tags/ML-Analysis/" style="font-size: 20px;">ML Analysis</a> <a href="/tags/ML-Operations/" style="font-size: 10.91px;">ML Operations</a> <a href="/tags/ML-Process/" style="font-size: 10.91px;">ML Process</a> <a href="/tags/Model-Generalization/" style="font-size: 10px;">Model Generalization</a> <a href="/tags/MongoDB/" style="font-size: 11.82px;">MongoDB</a> <a href="/tags/Multi-Task-Learning/" style="font-size: 10.91px;">Multi-Task Learning</a> <a href="/tags/NLP/" style="font-size: 13.64px;">NLP</a> <a href="/tags/Nonprobability-sampling/" style="font-size: 10px;">Nonprobability sampling</a> <a href="/tags/Normal-Distribution/" style="font-size: 10px;">Normal Distribution</a> <a href="/tags/PACF/" style="font-size: 10px;">PACF</a> <a href="/tags/Pandas-Dataframe/" style="font-size: 10px;">Pandas Dataframe</a> <a href="/tags/Precision-Recall/" style="font-size: 10px;">Precision-Recall</a> <a href="/tags/Probabilistic-sampling/" style="font-size: 10px;">Probabilistic sampling</a> <a href="/tags/Probability-Density-Function/" style="font-size: 10px;">Probability Density Function</a> <a href="/tags/Probability-Distribution-Function/" style="font-size: 10px;">Probability Distribution Function</a> <a href="/tags/Python/" style="font-size: 16.36px;">Python</a> <a href="/tags/Quiz/" style="font-size: 15.45px;">Quiz</a> <a href="/tags/ROC-curve/" style="font-size: 10px;">ROC curve</a> <a href="/tags/Recommendation-System/" style="font-size: 10.91px;">Recommendation System</a> <a href="/tags/Representation-Learning/" style="font-size: 10px;">Representation Learning</a> <a href="/tags/Self-Supervised-Features/" style="font-size: 10px;">Self-Supervised Features</a> <a href="/tags/Self-Supervised-Learning/" style="font-size: 11.82px;">Self-Supervised Learning</a> <a href="/tags/Sentence-Bert/" style="font-size: 10px;">Sentence Bert</a> <a href="/tags/Speaker-GAN/" style="font-size: 10px;">Speaker GAN</a> <a href="/tags/Speaker-Identification/" style="font-size: 10px;">Speaker Identification</a> <a href="/tags/Speaker-Independent/" style="font-size: 10px;">Speaker Independent</a> <a href="/tags/Speaker-Normalization/" style="font-size: 10px;">Speaker Normalization</a> <a href="/tags/Speaker-Verification/" style="font-size: 10px;">Speaker Verification</a> <a href="/tags/Speaker-Verification/" style="font-size: 10px;">Speaker Verification,</a> <a href="/tags/Speech-Emotion-Recognition/" style="font-size: 15.45px;">Speech Emotion Recognition</a> <a href="/tags/Speech-Feature-Extraction/" style="font-size: 10px;">Speech Feature Extraction</a> <a href="/tags/Speech-Representations/" style="font-size: 10px;">Speech Representations</a> <a href="/tags/Spoken-Language-Understanding/" style="font-size: 10px;">Spoken Language Understanding</a> <a href="/tags/Standard-Normal-Distribution/" style="font-size: 10px;">Standard Normal Distribution</a> <a href="/tags/TD-SV/" style="font-size: 10px;">TD-SV</a> <a href="/tags/Threshold-Adjustment/" style="font-size: 10px;">Threshold Adjustment</a> <a href="/tags/Time-Series/" style="font-size: 10px;">Time Series</a> <a href="/tags/Transfer-Learning/" style="font-size: 10px;">Transfer Learning</a> <a href="/tags/Transformer/" style="font-size: 11.82px;">Transformer</a> <a href="/tags/Vertex-AI/" style="font-size: 10px;">Vertex AI</a> <a href="/tags/Virtualenv/" style="font-size: 10px;">Virtualenv</a> <a href="/tags/Voice-Trigger-Detection/" style="font-size: 10.91px;">Voice Trigger Detection</a> <a href="/tags/WGAN/" style="font-size: 10px;">WGAN</a> <a href="/tags/WGAN-GP/" style="font-size: 10px;">WGAN-GP</a> <a href="/tags/Wake-Up-Words/" style="font-size: 10px;">Wake-Up Words</a> <a href="/tags/Wav2vec-2-0/" style="font-size: 10px;">Wav2vec 2.0</a> <a href="/tags/Web-Crawling/" style="font-size: 11.82px;">Web Crawling</a> <a href="/tags/Web-Server/" style="font-size: 10.91px;">Web Server</a> <a href="/tags/kaggle/" style="font-size: 10px;">kaggle</a> <a href="/tags/pyspark/" style="font-size: 12.73px;">pyspark</a>
        </div>
    </div>


            
        

    </div>
</aside>


                </div>
            </div>
        </div>
        <footer id="footer">
    <div class="container">
        <div class="container-inner">
            <a id="back-to-top" href="javascript:;"><i class="icon fa fa-angle-up"></i></a>
            <div class="credit">
                <h1 class="logo-wrap">
                    <a href="/" class="logo"></a>
                </h1>
                <p>&copy; 2023 Jang Minjee</p>
                
                <p>Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>. Theme by <a href="https://github.com/ppoffice" target="_blank">PPOffice</a></p>
                
            </div>
            <div class="footer-plugins">
              
    


            </div>
        </div>
    </div>
</footer>

    </div>
    


    
        
<script src="/libs/lightgallery/js/lightgallery.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-pager.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-autoplay.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-zoom.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-hash.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-share.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-video.min.js"></script>

    
    
        
<script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>

    
    
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
        </script>
        
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML.js"></script>

    

    
      <script data-ad-client="ca-pub-2182912223281192" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

    
    
    



<!-- Custom Scripts -->

<script src="/js/main.js"></script>


</body>
</html>
