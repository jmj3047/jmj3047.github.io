{"meta":{"title":"Jang Minjee","subtitle":"","description":"","author":"Jang Minjee","url":"https://jmj3047.github.io"},"pages":[],"posts":[{"title":"Machine Learning Modeling Pipelines in Production_Quiz","slug":"MLOps3_Quiz","date":"2023-08-15T15:00:00.000Z","updated":"2023-08-17T14:37:02.694Z","comments":true,"path":"2023/08/16/MLOps3_Quiz/","link":"","permalink":"https://jmj3047.github.io/2023/08/16/MLOps3_Quiz/","excerpt":"","text":"개요Coursera ML Ops Course 3 Quiz 1. Hyperparameter Tuning and Neural Architecture Search Link: https://www.coursera.org/learn/machine-learning-modeling-pipelines-in-production/home/week/1 2. AutoML 3. Dimensionality Reduction Link: https://www.coursera.org/learn/machine-learning-modeling-pipelines-in-production/home/week/2 4. Quantization and Pruning 5. High-Performance Modeling Link: https://www.coursera.org/learn/machine-learning-modeling-pipelines-in-production/home/week/3 6. Knowledge Distillation 7. Model Analysis Link: https://www.coursera.org/learn/machine-learning-modeling-pipelines-in-production/home/week/4 8. Model Analysis and Debugging 9. Continuous Evaluation and Monitoring 10. Explainable AI Link: https://www.coursera.org/learn/machine-learning-modeling-pipelines-in-production/home/week/5 11. Interpretability 12. Understanding Model Predictions","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"Coursera","slug":"Coursera","permalink":"https://jmj3047.github.io/tags/Coursera/"},{"name":"Quiz","slug":"Quiz","permalink":"https://jmj3047.github.io/tags/Quiz/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}]},{"title":"A fine-tuned wav2vec2.0/Hubert benchmark for SER, Speaker verification and spoken language understanding","slug":"wav2vec_hubert_for_SER_SV_SLU","date":"2023-08-03T15:00:00.000Z","updated":"2023-08-08T13:50:44.993Z","comments":true,"path":"2023/08/04/wav2vec_hubert_for_SER_SV_SLU/","link":"","permalink":"https://jmj3047.github.io/2023/08/04/wav2vec_hubert_for_SER_SV_SLU/","excerpt":"","text":"Journal&#x2F;Conference: arXiv preprint arXiv:2111.02735Year(published year): 2022Author: Yingzhi Wang, Abdelmoumene Boumadane, Abdelwahab HebaSubject: wav2vec 2.0, HuBERT, speech emotion recognition, speaker verification, spoken language understanding A fine-tuned wav2vec2.0&#x2F;Hubert benchmark for SER, Speaker verification and spoken language understandingIntroduction The wav2vec 2.0 model architecture contains mainly three modules. A convolutional neural network (CNN) feature encoder encodes the raw waveform inputs into latent speech representations. Mask operations are applied before they are fed to the Transformer based contextualized encoder. A quantization module is used to quantize the latent speech representations from the CNN encoder into a discretized embedding which is then used as the target. HuBERT shares the same architecture as wav2vec 2.0. Specifically, HuBERT consumes masked continuous speech features to predict predetermined cluster assignments. The predictive loss is applied over the masked regions, forcing the model to learn good high-level representations of unmasked inputs in order to infer the targets of masked ones correctly. In the field of Speech Emotion Recognition (SER), Speaker Verification (SV) and Spoken Language Understanding (SLU), it is still vague whether self-supervised models can produce better performance compared with traditional supervised models (spectral features + CNN-based feature extraction + RNN&#x2F;Transformer based time series modeling) [12, 13, 14, 15, 16]. For SER, [22] combined the features from frozen wav2vec2.0 with other hand-crafted prosodic features and then fed them into a 1d-CNN for a deeper extraction. [23] explored wav2vec fine-tuning strategies and 65.4% WA on IEMOCAP was achieved. Taking inspiration from [10] and [11], we added another fine-tuning method by splitting a pre-trained wav2vec 2.0&#x2F;HuBERT model into two parts: the CNN feature encoder and the Transformer contextualized encoder. We froze the CNN feature encoder and only fine-tuned the Transformer contextualized encoder. We then tested partially fine-tuned wav2vec2.0&#x2F;HuBERT pre-trained models together with the entirely fine-tuned ones with the following tasks below: Speech Emotion Recognition on IEMOCAP Speaker Verification on VoxCeleb1 Spoken Language Understanding on SLURP [26] The code and fine-tuned models for SER and SLU have been open-sourced on SpeechBrain [27]. METHOD In this section, we will first introduce the pre-training of wav2vec 2.0&#x2F;HuBERT model, then we will show our fine-tuning methods and downstream models for each task. Pretrained wav2vec 2.0wav2vec 2.0 사전 훈련은 BERT[28]의 마스크 언어 모델링과 유사하며 자체 감독 설정에서 수행됩니다. CNN 인코더 표현의 연속적인 시간 단계는 무작위로 마스킹되며, 모델은 컨텍스트화된 인코더의 출력에서 마스킹된 프레임에 대해 양자화된 로컬 인코더 표현을 재현하도록 훈련됩니다. Training Objective sim($c_t$, $q_t$): cosine similarity between the contextualized encoder outputs $c_t$ and the quantized CNN encoder representations $q_t$. t is the masked time step $Q_t$: the union of candidate representations $\\tilde{q}$ which includes $q_t$ and K &#x3D; 100 distractors $\\mathcal{K}$ is the temperature which is set to 0.1. The distractors are outputs of the local encoder sampled from masked frames belonging to the same utterance as $q_t$. The contrastive loss is then given by $L_m$ summed over all masked frames. At the end, an L2 regularization is added to the contrastive loss, as well as a diversity loss to increase the use of the quantized codebook representations. In this work, we compare four released wav2vec 2.0 pre-trained models the wav2vec 2.0 base model (12 transformer blocks and 768 embedding dimension) its ASR fine-tuned version the wav2vec 2.0 large model (24 transformer blocks and 1024 embedding dimension) its ASR fine-tuned version. Both base and large models are pre-trained on 960h LibriSpeech [31] data, which is also used for their ASR fine-tuning. ASR fine-tuned models for both wav2vec 2.0 and HuBERT are taken into consideration because we assume that some tasks may benefit from the ASR fine-tuning. Pretrained HuBERTwav2vec 2.0과 동일한 방식으로, CNN으로 인코딩된 오디오 피처는 HuBERT에서 무작위로 마스킹됩니다. HuBERT 사전 훈련의 first iteration을 위한 레이블을 생성하기 위해 39차원 MFCC 특징에 K-평균 클러스터링이 적용됩니다. 이후 반복을 위한 더 나은 타깃을 생성하기 위해 k-평균 클러스터링은 이전 반복에서 사전 학습된 HuBERT 모델에서 추출한 latent features에 대해 작동합니다. 클러스터 레이블을 예측하기 위해 트랜스포머 블록 위에 projection layer가 추가됩니다. Cross-entropy loss is computed over masked timestamps, which can be defined as: $M \\subset [T]$ denotes the set of indices to be masked for a length- $T$ sequence $X$ $\\tilde{X} &#x3D; r(X;M)$ denotes a corrupted version of $X$ where $x_t$ is replaced with a mask embedding $\\tilde{x}$ if $t \\in M$. A masked prediction model $f$ takes as input $\\tilde{X}$and predicts a distribution over the target indicies at each timestep $p_f(\\cdot | \\tilde{X} ; t)$. To improve target quality, cluster ensembles are utillized in case that an individual clustering model performs badly, $Z^(k)$ then denotes the target sequences generated by the $k$-th clustering model. HuBERT pre-training uses the same optimizer and learning rate scheduler as wav2vec 2.0. For ASR fine-tuning, the projection layer is removed and replaced by a randomly initialized softmax layer, then the CTC loss is optimized. For more details of the pre-training of HuBERT, please refer to [11]. Like wav2vec 2.0, we compare three released HuBERT pretrained models the HuBERT base model (12 transformer blocks and 768 embedding dimension, of which no ASR fine tuned version is released) the HuBERT large model (24 transformer blocks and 1024 embedding dimension) its ASR fine-tuned version. The HuBERT base model is pre-trained on 960h LibriSpeech data, while the large model is pre-trained on 60k hours Libri-Light [32] data. The ASR fine-tuning is also based on 960h LibriSpeech data. Fine-tuning Partial fine-tuning: the CNN based feature encoder and the transformer-based contextualized encoder. We froze the CNN-based feature encoder, fixing all the parameters of these CNN blocks, and only fine-tuned the parameters of the transformer blocks. Partial fine-tuning can be understood as a domain adaptation training for the top level, which aims to prevent interference and damage to the bottom CNN layers that already have an expressive ability. Entire fine-tuning: the CNN and Transformer modules are both fine-tuned during the downstream training process. By training general features at the bottom level, entire fine-tuning allows higher-level expressions to be more complete and more targeted. Then we directly added simple downstream adaptors (classifier&#x2F;decoder) to wav2vec 2.0&#x2F;Hu-BERT without adding another heavy and redundant encoder. The downstream adaptors for each task are presented as below. For SER, an average time pooling and one linear layer are added as a simple downstream classifier (Fig.2). The average time pooling compresses variant time lengths into one, then the linear layer effectuates an utterance-level classification minimizing the cross-entropy loss. For SV, a Speaker Identification (SID) task is first implemented using the same downstream framework as SER. Pairwise cosine-similarity scores are then produced for SV on the pre-trainedSID embeddings before the linear classification layer. ExperimentsDatasetsThe three most widely used and most representative datasets were chosen in our experiments, which are IEMOCAP for SER, VoxCeleb1 for SV and SLURP for SLU. IEMOCAP: The Interactive Emotional Dyadic Motion Capture (IEMOCAP) dataset has approximately 12 hours of data and consists of scripted and improvised dialogues by 10 speakers. In order to form a contrast in this work, we used 4 emotional classes as in SUPERB: anger, happiness, sadness and neutral, following the work of [34]. The evaluation metric is weighted accuracy (WA) and the experiments were carried out on two different split settings: Speaker-Dependent (SD) setting and Speaker-Independent (SI) setting. For SD, the results were averaged on 5 different random seeds for train-validation-test split. For SI, a 10-fold cross-validation was performed with a leave-two- speaker-out strategy (one for validation and one for test). 약 12시간 분량의 데이터로 구성되어 있으며, 10명의 화자가 대본에 따라 즉흥적으로 연기한 대화로 구성되어 있습니다. 이 작업에서 대비를 형성하기 위해 [34]의 연구에 따라 분노, 행복, 슬픔, 중립의 4가지 감정 클래스를 SUPERB에서와 같이 사용했습니다. 평가 지표는 가중 정확도(WA)이며 실험은 두 가지 다른 분할 설정에서 수행되었습니다: 화자 의존적(SD) 설정과 화자 독립적(SI) 설정입니다. SD의 경우, 훈련-검증-테스트 분할을 위해 5개의 서로 다른 무작위 시드에 대한 결과를 평균화했습니다. SI의 경우, 2명의 스피커를 제외하는 전략(하나는 검증용, 하나는 테스트용)을 사용하여 10배 교차 검증을 수행했습니다. VoxCeleb1: 1,251명의 화자로부터 나온 10만 개 이상의 발화, 총 351시간 분량의 오디오가 포함되어 있습니다. 먼저 speaker identification 작업을 구현하여 모델이 1211개의 서로 다른 보이스 프린트를 구별하는 방법을 학습하도록 했습니다. 그런 다음 사전 학습된 speaker identification 모델의 임베딩에서 코사인 유사도를 계산하여 40명의 화자로 구성된 vox1-o 테스트 세트에 대한 검증을 수행했습니다. 실험에서는 VoxCeleb2와 노이즈 증강을 사용하지 않았습니다. 평가 지표로 동일 오류율(EER)을 사용했으며, 훈련-검증 분할을 위해 5개의 서로 다른 시드에서 결과를 평균했습니다. Fine-tuning settingsWe rename the models we compare with a method as below. EF&#x2F;PF&#x2F;Frozen: Entirely Fine-tuned&#x2F;Partially Fine-tuned&#x2F;Not fine-tuned w2v&#x2F;hbt: wav2vec 2.0&#x2F;HuBERT based model base&#x2F;large: base&#x2F;large pre-trained model -&#x2F;960h: with&#x2F;without ASR fine-tuning using 960h LibriSpeech data EF-w2v-base : an entirely fine-tuned wav2vec 2.0 base model PF-hbt-large-960h : a partially fine-tuned HuBERT large model with an ASR fine-tuning. For more detailed parameters of released pre-trained wav2vec 2.0&#x2F;Hu-BERT models, please refer to [10] and [11]. During the fine-tuning process, we applied two different schedulers to respectively adjust the fine-tuning learning rate of the wav2vec 2.0&#x2F;HuBERT encoder and the learning rate of the downstream model. Both the schedulers use an Adam Optimizer and linearly anneal the learning rates according to the performance of validation stage. For SER and SV, the initialized fine-tuning learning rate and the downstream learning rate are set to $10^{-5}$ and $10^{-4}$. Results and discussionSpeech Emotion Recognition &amp; Speaker Verification [17]: SUPERB’s results as a non-fine-tuned baseline state-of-the-art baselines Head-Fusion ACNN [35] for SER-SD (Speaker-Dependent setting) Attention Pooling based representation [36] for SER-SI (Speaker-Independent setting) and Siamese Capsule network [37] for SV SER: 전체 미세 조정보다 부분 미세 조정이 더 나은 미세 조정 방법인 것으로 나타났습니다. IEMOCAP은 데이터가 12시간밖에 되지 않는 작은 데이터 세트이므로 너무 많은 파라미터를 학습시키면 과적합이 쉽게 발생할 수 있습니다. 또한 ASR 미세 조정이 다운스트림 SER 작업에 도움이 되지 않는 것으로 나타났는데, 이는 ASR 미세 조정 중에 prosodic information가 손실되었음을 시사합니다. CONCLUSIONSIn this work we explored different fine-tuning methods on two of the most powerful self-supervised models (wav2vec 2.0 and HuBERT), then benchmarked their performance on Speech Emotion Recognition, Speaker Verification and Spoken Language Understanding tasks. State-of-the-art results were achieved for all the three tasks, proving the excellent generalizability of wav2vec 2.0&#x2F;HuBERT on learning prosodic, voice-print and semantic representations. We hope to show the broad prospects of self-supervised learning and also provide some useful insights for its industrial applications.","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speech Emotion Recognition","slug":"Paper/Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/categories/Paper/Speech-Emotion-Recognition/"}],"tags":[{"name":"Wav2vec 2.0","slug":"Wav2vec-2-0","permalink":"https://jmj3047.github.io/tags/Wav2vec-2-0/"},{"name":"HuBERT","slug":"HuBERT","permalink":"https://jmj3047.github.io/tags/HuBERT/"},{"name":"Speaker Verification,","slug":"Speaker-Verification","permalink":"https://jmj3047.github.io/tags/Speaker-Verification/"},{"name":"Spoken Language Understanding","slug":"Spoken-Language-Understanding","permalink":"https://jmj3047.github.io/tags/Spoken-Language-Understanding/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speech Emotion Recognition","slug":"Paper/Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/categories/Paper/Speech-Emotion-Recognition/"}]},{"title":"SUPERB, Speech processing Universal PERformance Benchmark","slug":"SUPERB","date":"2023-08-01T15:00:00.000Z","updated":"2023-08-08T10:47:10.707Z","comments":true,"path":"2023/08/02/SUPERB/","link":"","permalink":"https://jmj3047.github.io/2023/08/02/SUPERB/","excerpt":"","text":"Journal&#x2F;Conference: arXiv preprint arXiv:2105.01051Year(published year): 2021Author: Shu-wen Yang, Po-Han Chi, Yung-Sung Chuang, Cheng-I Jeff Lai, Kushal Lakhotia,Yist Y. Lin, Andy T. Liu, Jiatong Shi, Xuankai Chang6, Guan-Ting Lin,Tzu-Hsien Huang, Wei-Cheng Tseng, Ko-tik Lee, Da-Rong Liu, Zili Huang, Shuyan Dong,Shang-Wen Li, Shinji Watanabe6, Abdelrahman Mohamed, Hung-yi LeeSubject: Speech, Self-Supervised Learning, Representation Learning, Model Generalization, Benchmark, Evaluation SUPERB: Speech processing Universal PERformance Benchmark Summary The paper introduces SUPERB, a standardized benchmark for evaluating the generalizability of pretrained models on various speech processing tasks. The framework uses a universal representation encoder that is pretrained on self-supervised learning (SSL) tasks and then fine-tuned on downstream tasks with lightweight prediction heads. The results show that the SUPERB framework yields competitive performance compared to traditional supervised pipelines and outperforms log mel filterbank (FBANK) by a large margin, demonstrating the potential of developing powerful, generalizable, and reusable pretrained models for speech processing. Introduction SSL has been explored in speech, including pretraining with generative loss [7, 8, 9, 10], discriminative loss [11, 12, 13, 14], or multi-task [15, 16]. While these works showed promising results of SSL on various speech processing tasks, unlike CV or NLP areas, they were investigated with different datasets and experimental setups. Absence of a shared benchmark makes it hard to compare and draw insights across the techniques. Furthermore, existing works explored a limited number of tasks or require heavyweight downstream training [9, 12, 14], blurring the generalizability and re-usability of SSL models across tasks. Both factors limit the impact of SSL on speech processing in research and industry. SUPERB aims to 360-degree examine models’ capability and collects various tasks with limited labeled data from speech communities to align with common research interests. Compared to existing efforts, SUPERB targets at the direct usability of pretrained models on various popular tasks through any usage3. As finetuning pretrained models typically requires huge resources and hinders the re-usability, in this paper, we focus on investigating a simple framework solving all SUPERB tasks with a frozen, shared pretrained model, and lightweight prediction heads finetuned for each task. Speech processing Universal PERformance Benchmark Tasks are designed with the following principles: (1) conventional evaluation protocols from speech communities, (2) publicly available datasets for everyone to participate, (3) limited labeled data to effectively benchmark the generalizability of models. Content Four tasks are collected from ASR and Spoken Term Detection communities. The former aims to transcribe speech into text content; the latter is to detect the spoken content with minimal effort even without transcribing. Phoneme Recognition(PR)PR transcribes an utterance into the smallest content units. We include alignment modeling in the PR task to avoid the potential inaccurate forced alignment. LibriSpeech [23] train-clean-100&#x2F;dev-clean&#x2F;test-clean subsets are adopted in SUPERB for training&#x2F;validation&#x2F;testing. Phoneme transcriptions are obtained from the LibriSpeech official g2p-model-5 and the conversion script in Kaldi librispeech s5 recipe. The evaluation metric is phone error rate (PER). Automatic Speech Recognition(ASR)ASR transcribes utterances into words. While PR analyzes the improvement in modeling phonetics, ASR reflects the significance of the improvement in a real-world scenario. LibriSpeech train-clean-100&#x2F;devclean&#x2F; test-clean subsets are used for training&#x2F;validation&#x2F;testing. The evaluation metric is word error rate (WER). Keyword Spotting(KS)KS detects preregistered keywords by classifying utterances into a predefined set of words. The task is usually performed on-device for the fast response time. Thus, accuracy, model size, and inference time are all crucial. We choose the widely used Speech Commands dataset v1.0 [24] for the task. The dataset consists of ten classes of keywords, a class for silence, and an unknown class to include the false positive. The evaluation metric is accuracy (ACC). Query by Example Spoken Term Detection(QbE)QbE detects a spoken term (query) in an audio database (documents) by binary discriminating a given pair of query and document into a match or not. The English subset in QUESST 2014 [25] challenge is adopted since we focus on investigating English as the first step. The evaluation metric is maximum term weighted value (MTWV) which balances misses and false alarms. SpeakerSpeaker Identification(SI)SID classifies each utterance for its speaker identity as a multi-class classification, where speakers are in the same predefined set for both training and testing. The widely used VoxCeleb1 [26] is adopted, and the evaluation metric is accuracy (ACC). Automatic Speaker Verification(ASV)ASV verifies whether the speakers of a pair of utterances match as a binary classification, and speakers in the testing set may not appear in the training set. Thus, ASV is more challenging than SID. Vox- Celeb1 [26] is used without VoxCeleb2 training data and noise augmentation. The evaluation metric is equal error rate (EER). Speaker Diarization(SD)SD predicts who is speaking when for each timestamp, and multiple speakers can speak simultaneously.The model has to encode rich speaker characteristics for each frame and should be able to represent mixtures of signals. LibriMix [27] is adopted where LibriSpeech train-clean-100&#x2F;dev-clean&#x2F;test-clean are used to generate mixtures for training&#x2F;validation&#x2F;testing. We focus on the two-speaker scenario as the first step. The time-coded speaker labels were generated using alignments from Kaldi LibriSpeech ASR model. The evaluation metric is diarization error rate (DER). SemanticsTwo tasks are collected from Spoken Language Understanding (SLU) community. While most works for these tasks are done in two stages: transcribing speech into text and predicting semantics on transcribed text, we focus on inferring high-level semantics directly from raw audio in an end-to-end fashion. Intent Classification(IC)IC classifies utterances into predefined classes to determine the intent of speakers. We use the Fluent Speech Commands [28] dataset, where each utterance is tagged with three intent labels: action, object, and location. The evaluation metric is accuracy (ACC). Slot Filling(SF)SF predicts a sequence of semantic slot-types from an utterance, like a slot-type FromLocation for a spokenword Taipei, which is known as a slot-value. Both slot-types and slot-values are essential for an SLU system to function [18]. The evaluation metrics thus include slot-type F1 score and slotvalue CER [29]. Audio SNIPS [18] is adopted, which synthesized multi-speaker utterances for SNIPS [30]. Following the standard split in SNIPS,US-accent speakers are further selected for training, and others are for validation&#x2F;testing. ParalinguisticsEmotion Recognition(ER)ER predicts an emotion class for each utterance. The most widely used ER dataset IEMOCAP [31] is adopted, and we follow the conventional evaluation protocol: we drop the unbalance emotion classes to leave the final four classes (neutral, happy, sad, angry) with a similar amount of data points and cross-validates on five folds of the standard splits. The evaluation metric is accuracy (ACC). Framework: Universal Representation Our framework aims to explore how simple and general the solution can be. Thus, we freeze the parameters of pretrained models across tasks and extract fixed representations to be fed into each task-specialized prediction head (small downstream model). Compared to previous setups in speech representation learning [9, 12, 13], the framework puts an explicit constraint on downstream models to be as lightweight as possible for all tasks, as their parameter size and required training resources are also crucial for the framework to be simple and re-usable in various use cases. With the above principles, the pretrained model solving all SUPERB tasks in this framework would be a universal representation encoder. Self-supervised pretrained modelsSSL models explored in this paper are summarized in Table 1 and categorized into three learning approaches: generative modeling, discriminative modeling, and multi-task learning. 음성 처리의 맥락에서 판별 모델링은 자동 음성 인식(ASR), 화자 식별, 구어 감지 등 다양한 종류의 음성을 구별하는 것이 목표인 작업에 자주 사용됩니다. 반면 생성 모델링은 원본 음성과 유사한 새로운 음성 샘플을 생성하는 것이 목표인 TTS(텍스트 음성 변환), 음성 변환, 소스 분리와 같은 작업에 자주 사용됩니다. Generative modeling 생성 모델링은 입력 데이터의 확률 분포를 모델링하고 이 모델을 사용하여 원본 데이터와 유사한 새로운 데이터 샘플을 생성하는 머신 러닝의 한 유형입니다. 생성 모델은 이미지 합성, 텍스트 생성, 음성 합성과 같은 작업에 자주 사용됩니다. APC adopts the language model-like pretraining scheme on a sequence of acoustic features (FBANK) with unidirectional RNN and generates future frames conditioning on past frames. VQ-APC further applies vector-quantization (VQ) layers onto APC’s representation to make it compact and low bit-rate. Mockingjay adopts the BERT-like pretraining on Transformer encoders by masking the input acoustic features in time axis and re-generating the masked parts. TERA extends Mockingjay to further mask the frequency bins. NPC improves the inference speed upon APC by replacing RNN with CNN and changing the future generation to masked reconstruction as Mockingjay. De-CoAR 2.0 improves upon Mockingjay by inserting a VQ layer right before the final prediction like VQ-APC, and is trained by larger input mask, larger batch size, and more unlabeled data. Discriminative modeling 판별 모델링은 입력 데이터가 주어졌을 때 출력의 조건부 확률을 모델링하는 머신 러닝의 한 유형입니다. 판별 모델은 분류, 회귀, 시퀀스 라벨링과 같은 작업에 자주 사용됩니다. CPC discriminates the correlated positive samples from negative samples with contrastive InfoNCE loss, which maximizes the mutual information between raw data and representations. Modified CPC [34] and wav2vec [12] proposed several architecture changes to improve CPC. vq-wav2vec introduces a VQ module to wav2vec. The module discretizes speech into a sequence of tokens after InfoNCE pretraining. Tokens are used as pseudo-text to train a BERT as did in NLP for contextualized representations. wav2vec 2.0 merges the pipeline of vq-wav2vec into one end-to-end training scheme by applying time masking in the latent space and replacing BERT’s token prediction with InfoNCE’s negative sampling to handle the intractable normalization on continuous speech. Motivated by DeepCluster [36], Hu-BERT [35] enables BERT’s token prediction via off-line clustering on representations. The clustered labels at the masked locations are then predicted. Multi-task learningMLT is applied in PASE+ [16], where lots of pretraining objectives are adopted: waveform generation, prosody features regression, contrastive InfoMax objectives, and more. Multiple contaminations are also applied to input speech like reverberation and additive noise. Downstream models and policiesWe design our framework to keep the downstream models and their finetuning simple, while ensuring the performance across pretrained models is comparable and the best model in each task is competitive. Since the last-layer representation is not always the best, the framework collects multiple hidden states from the pretrained model and weighted-sum them as the final representation. For a fair comparison, we also limit the space for downstream hyper-parameters tuning5. Downstream models and algorithms are summarized in the following and will be released in detail as a part of the challenge policy. PR,KS, SID, IC, ER are simple tasks that are solvable with linear downstream models. Hence, we use a frame-wise linear transformation for PR with CTC loss; mean-pooling followed by a linear transformation with cross-entropy loss for utterance-level tasks (KS, SID, IC, and ER). These five tasks also serve as the direct indication of representations’ quality following the conventional linear evaluation protocol. For ASR, a vanilla 2-layer 1024-unit BLSTM is adopted and optimized by CTC loss on characters. The trained model is decoded with LibriSpeech official 4-gram LM powered by KenLM [37] and flashlight [38] toolkit. We mostly follow the system proposed by GTTS-EHU for QUESST at MediaEval 2014 [39] for QbE but replace the conventional supervised phoneme posteriorgram (PPG) with SSL representations. We run Dynamic Time Warping[40] on all hidden states separately with standard distance functions and obtain a score for each query-document pair. The best distance function &#x2F; hidden state pair is reported. Regarding SF, slot-type labels are represented as special tokens to wrap the slot-values in transcriptions. SF is then re-formulated as an ASR problem. The finetuning scheme is the same as in our ASR task, except for the pre-processing to encode slot-types into transcriptions and post-processing to decode slot-types and slot-values from hypotheses. As for ASV, we adopt the well-known x-vector [41] as the downstream model and change Softmax loss to AMSoftmax loss with the same hyper-parameters as [26]. The simple cosine-similarity backend is used to produce pairwise matching scores. We employ the end-to-end training scheme with permutation-invariant training (PIT) loss [42] to SD, instead of using clustering-based methods. We leverage a single-layer 512-unit LSTM for the downstream model. Experiment For the tasks using linear models, FBANK cannot work on any task, while SSL representations all perform well to some degree with different specializations. It is a surprise that wav2vec 2.0 and HuBERT conquers PR and IC with just linear models and outperforms others by a large margin. Their results on SID and ER are also highly competitive. FBANK achieves competitive performance when allowing non-linear downstream models in ASR, SF, ASV, and SD, and yields better performance than some SSL representations. ConclusionWe present SUPERB, a challenge to generally benchmark the capability of SSL pretrained models on speech processing. We demonstrate a simple framework to solve all SUPERB tasks which leverages a frozen, shared pretrained model and achieves competitive performance with minimal architecture changes and downstream finetuning. We have open-sourced the evaluation toolkit2 and will release the detailed challenge policy on the leaderboard website1. We welcome the community to participate and drive the research frontier.","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speech Representations","slug":"Paper/Speech-Representations","permalink":"https://jmj3047.github.io/categories/Paper/Speech-Representations/"}],"tags":[{"name":"Self-Supervised Learning","slug":"Self-Supervised-Learning","permalink":"https://jmj3047.github.io/tags/Self-Supervised-Learning/"},{"name":"Representation Learning","slug":"Representation-Learning","permalink":"https://jmj3047.github.io/tags/Representation-Learning/"},{"name":"Model Generalization","slug":"Model-Generalization","permalink":"https://jmj3047.github.io/tags/Model-Generalization/"},{"name":"Benchmark","slug":"Benchmark","permalink":"https://jmj3047.github.io/tags/Benchmark/"},{"name":"Evaluation","slug":"Evaluation","permalink":"https://jmj3047.github.io/tags/Evaluation/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speech Representations","slug":"Paper/Speech-Representations","permalink":"https://jmj3047.github.io/categories/Paper/Speech-Representations/"}]},{"title":"Speaker Normalization for Self-Supervised Speech Emotion Recognition","slug":"Speaker_Normalization_for_SER","date":"2023-07-16T15:00:00.000Z","updated":"2023-07-18T16:52:28.193Z","comments":true,"path":"2023/07/17/Speaker_Normalization_for_SER/","link":"","permalink":"https://jmj3047.github.io/2023/07/17/Speaker_Normalization_for_SER/","excerpt":"","text":"Journal&#x2F;Conference : ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)Year(published year): 2022Author: Itai Gat, Hagai Aronowitz, Weizhong Zhu, Edmilson Morais, Ron HoorySubject: Speech emotion recognition, speaker normalization, self-supervised learning Speaker Normalization for Self-Supervised Speech Emotion Recognition Summary The paper proposes a method for speech emotion recognition that normalizes speaker characteristics to improve generalization capabilities of the model. The proposed method uses a pre-trained deep neural network for speech representation learning, called HuBERT, as the upstream model. The authors proposed two training strategies for their method: speaker normalization projector and train all parameters. They showed that the latter approach outperforms the former and achieves state-of-the-art results in speech emotion recognition. The authors evaluated their proposed method on both speaker-independent and speaker-dependent setups using various training set sizes and showed that their method outperforms the current state-of-the-art results for both setups. The proposed method has potential applications in various fields, such as human-robot interaction, virtual assistants, and mental health monitoring. IntroductionThe classic self-supervised learning process relies on a representation trained on a large unlabeled dataset, and a downstream task trained on a relatively small labeled dataset. Generally, our method enhances a downstream task performance by using a third dataset with labels different from the downstream task labels. For example, in this work, for speaker emotion recognition, our method normalizes undesired characteristics from the self-supervised representation to improve performance on the speech emotion recognition task. We carry this out by learning a feature representation that excels at speech emotion recognition while being robust enough for speaker characteristics (see Fig. 1). Our proposed method outperforms the current state-of-the-art results for both speaker-dependent and speaker-independent settings. In summary, we propose a general framework for speaker characteristics normalization from a self-supervised representation. We address the small dataset settings issue and propose a framework for it on the IEMOCAP benchmark. Through extensive experiments, we show that our method outperforms the current speech emotion recognition state-of-the-art results on several setups. 기존의 self-supervised learning process는 레이블이 지정되지 않은 대규모 데이터 세트에서 학습된 표현과 상대적으로 작은 레이블이 지정된 데이터 세트에서 학습된 downstream task에 의존합니다. 일반적으로 이 방법은 downstream task label과 다른 레이블을 가진 세 번째 데이터 세트를 사용하여 다운스트림 작업의 성능을 향상시킵니다. 예를 들어, 이 연구에서는 화자 감정 인식의 경우, self supervised representation에서 원하지 않는 특성을 정규화하여 음성 감정 인식 작업의 성능을 개선합니다. 이를 위해 화자 특성에 대해 충분히 robust하면서도 음성 감정 인식에 탁월한 feature representation을 학습하여 이를 수행합니다(그림 1 참조). 우리가 제안한 방법은 speaker-dependent and speaker-independent settings에서 현재의 최신 결과보다 성능이 뛰어납니다. 요약하면, 우리는 self supervised representation에서 speaker characterisitcs normalization를 위한 일반적인 프레임워크를 제안합니다. 작은 데이터 세트 설정 문제를 해결하고 이를 위한 프레임워크를 IEMOCAP benchmark에서 제안합니다. extensive experiment을 통해 이 방법이 여러 설정에서 현재 음성 감정 인식의 최신 결과보다 성능이 우수하다는 것을 보여줍니다. Related WorkSelf-supervised trained models음성 처리에서 사용되는 대부분의 self-supervised techniques은 세 가지 범주로 나뉩니다. 첫 번째 범주에서는 constuctive InfoNCE loss과 결합된 다양한 아키텍처가 사용됩니다. 두 번째 범주는 마스킹 된 토큰 분류를 기반으로 합니다. 세 번째 범주는 future frame 생성 및 입력의 마스크 된 부분을 재구성하는 인코더-디코더 접근 방식과 같은 다양한 기법을 사용하여 재구성 손실을 사용합니다. Feature normalizationNagrani et al. [17] suggest using a ”confusion loss,” which is a cross-entropy loss computed by comparing the prediction to a uniform distribution. Ganin et al. [18] use extra knowledge regarding the data-domain to tackle a domain adaptation problem. They propose to normalize domain features by negating gradients of a loss that predict the domain label. In contrast to those methods, we normalize cues based on a task rather than a domain. Additionally, our method focuses on the self-supervised representation framework. Nagrani 등[17]은 다음과 같이 계산된 교차 엔트로피 손실인 “혼동 손실”을 사용할 것을 제안합니다.예측을 균일 분포와 비교하여 계산되는 교차 엔트로피 손실입니다. Ganin 등[18]은 데이터 도메인에 관한 추가 지식을 사용하여 도메인 적응 문제를 해결합니다. 이들은 도메인 레이블을 예측하는 손실의 기울기를 음수화 하여 도메인 특징을 정규화 할 것을 제안합니다. 이러한 방법과 달리, 우리는 도메인이 아닌 작업을 기반으로 단서를 정규화 합니다. 또한, 우리의 방법은 자기 지도 표현 프레임 워크에 중점을 둡니다. Emotion Recognition음성 감정 인식은 발화를 기반으로 감정을 예측합니다. 음성 감정 인식에서 가장 널리 사용되는 벤치마크는 대화형 감정 다이나믹 모션 캡쳐 데이터베이스(IEMOCAP). 음성 감정 인식을 위해 초기 E2E 방식은 CNN과 LSTM을 결합합니다. 이후 attention based model은 다음과 같은 이유로 CNN과 LSTM 조합보다 성능이 뛰어났습니다. 최근 몇 년 동안 self-supervised learning model은 레이블이 지정되지 않은 데이터로부터 high quality representations을 학습할 수 있는 능력으로 인해 음성 처리 연구에서 큰 관심을 불러일으키고 있습니다. 이를 반영하여 Yang 등[25]은 벤치마크에서 self supervised model이 감정 인식에서 최첨단 결과를 생성한다는 것을 입증했습니다. 본 논문에서는 self supervised model과 normalization of speaker characteristics를 결합하여 음성 감정 인식을 향상시키는 방법을 제시합니다. MethodIn the following, we propose an approach for learning a task while normalizing cues from adifferent task (possibly from another dataset) in an upstream downstream architecture. Speaker NormalizationUpstream - Downstream 접근 방식을 사용하면 단일 Upstream model보다 하나의 task 이상을 할수 있습니다. 예를 들어 화자 식별과 감정 인식 과제를 모두 해결할 수 있습니다. 이 방법에는 세가지 discriminative learners를 고려합니다. 첫번째는 upstream model $h_w$, 두번째는 emotion recognition learner $g_{w_{er}}$, 그리고 세번째는 speaker identification classifier $g_{w_{id}}$. 감정 Downstream 작업에서 원치 않는 화자 특성을 활용하지 못하도록 Upstream 표현에서 이를 정규화하여 감정 분류기가 이러한 단서를 활용할 수 없도록 할 것을 제안합니다. 이를 위해 speaker identification task 과 관련하여 Upstream 모델의 gradients를 음수화(negating)할 것을 제안합니다. 간단하게 설명하기 위해 stochastic gradient descent(SGD)을 사용하는 방법을 설명하지만, 모든 최적화 알고리즘에 쉽게 적용할 수 있습니다. 접근 방식은 두 단계로 구성됩니다. 첫번째 단계는 standard gradient-based optimization. SGD 알고리즘을 사용하는 standard gradient-based learning에서 Upstream 및 Downstream 가중치 업데이트 단계는 다음과 같습니다. 여기서 $\\eta$는 학습률이고 $l_{er}$은 감정인식 손실(예: 교차 엔트로피 손실)입니다. 두번째 단계 에서는 Upstream 모델의 화자 ID 특징을 다음과 같이 정규화 합니다. 여기서 $\\lambda$는 화자 ID 손실에 대해 Upstream 모델에 대해서 gradient ascent 단계를 수행하여 Upstream 모델의 화자 특징을 어느정도 정규화할지 설정하는 파라미터 입니다. 그림1에 방법을 설명해 두었습니다. 이 단계는 independent 하기 때문에 감정인식에 사용되는 데이터에 화자 식별 레이블을 지정할 필요가 없으며 그 반대의 경우도 마찬가지입니다. Training StrategiesSelf-supervised upstream model은 많은 파라미터가 있습니다. HuBERT Large 모델에는 3억 1700만개의 파라미터가 있으며 HuBERT X-Large에는 거의 10억개의 파라미터가 있습니다. 따라서 이러한 네트워크를 fine tuning하는 것은 어려울수 있습니다. 그래서 두 가지 training procedure를 제안합니다: Speaker Normalization projector: 매개변수 $\\hat{w}$가 있는 새로운 비선형 레이어를 도입합니다. $\\hat{w}$은 Upstream과 Down Stream 모델 사이의 게이트 입니다. 감정인식 단계에서 upstream model을 최적화 하기 위해 $\\hat{w}$을 추가합니다. 반면 speaker identification 작업에는 수식(3)을 수정하여 $\\hat{w}$을 단독으로 최적화 합니다. 이렇게 하면 upstream의 최적화 단계를 건너뛸 수 있으므로 speaker ID 단계에서 gradient computation overhead를 줄일 수 있습니다. 모든 파라미터를 훈련합니다: 이 접근방식에서는 위에서 설명한 내용에 따라 up stream, down stream 파라미터 모두를 훈련합니다. 다음 섹션에셔는 다양한 훈련 세트 크기를 사용하여 화자 independent 설정과 dependent 설정에 대한 두가지 훈련 전략에 대해서 설명합니다. Experiments이 섹션에서 우리는 우리의 접근 방식을 소개하고 이전 연구와 비교합니다. Experimental setupIEMOCAP 데이터셋을 사용했습니다. 감정 클래스는 중립, 행복, 슬픔, 분노 네 가지를 사용합니다. Upstream 모델에는 HuBERT 기본 모델과 large 모델을 모두 사용했습니다. Downstream 모델의 경우 HuBERT의 temporal dimension에서 비선영 projection을 사용했습니다. $\\lambda$의 하이퍼 파라미터 범위는 [0.01, 0.0001]로 잡았지만 결국 가장 좋은 결과를 내는 것은 $\\lambda$ &#x3D; 0.001일때 였습니다. 따라서 우리는 모든 실험에 이 $\\lambda$ 값을 사용했습니다. Emotion Recognitionspeaker-independent, 두 화자의 발화를 테스트에 사용하고 다른 8명의 발화를 각 화자의 훈련 및 검증에 사용하는 5 fold cross validationㅇㄹ 수행했습니다. stopping criteria는 speaker out of distribution evaluation에서도 중요한 역할을 합니다. validation 세트에서 가장 좋은 epoch를 기준으로 테스트 세트의 정확도를 보고합니다. 이 작업에서는 unknown speaker에 대한 generalization capabilities를 조사합니다. 따라서 speaker independent setup에 초점을 맞춥니다. 그럼에도 불구하고 우리의 방법은 speaker dependent setup도 개선합니다. 그 설정은 train test split이 랜덤이고 train과 test 세트에 모든 화자가 포함되어 있습니다. 표1에는 speaker dependent(SD) 설정과 independent(five-fold) 설정의 결과가 나와있습니다. speaker independent 경우 두번째 훈련 절차가 SOTA 결과를 달성했습니다. 이러한 개선은 HuBERT Large and Base 모두에서 일관되게 나타났습니다. 또한 이 방법은 speaker dependent 설정에서 SOTA결과에서 0.5% 개선했습니다. Upstream model에서 speaker information을 정규화하는 방법의 ability를 평가했습니다. speaker identification을 위해 고정된 업스트림(즉, 업스트림 모델을 미세 조정하지 않고) HuBERT Large에 대해 분류기를 두 번 훈련시켰습니다. 먼저 speaker normalization method 이전에 HuBERT에 대해 down stream 모델을 훈련시켰습니다. 60.7%의 정확도를 얻었습니다. 그런 다음 제안한 방법으로 훈련된 고정된 HuBERT에 대해 additional speaker ID down stream model을 훈련했습니다. 그 결과 45.9%의 정확도를 얻었습니다. 따라서 원하는 대로 우리의 방법은 Upstream 모델의 화자 ID 특징에 해를 끼쳤습니다. 표 1: 5 fold cross validation을 위해 오디오 기능만 사용한 speaker independent 설정과 랜덤 train test split을 사용한 speaker dependent 설정에 대한 IEMOCAP의 최신 결과. 5-fold 및 SD 설정의 경우 weighted accuracy(WA) metric을 보고합니다. speaker noramalization projector(SNP)와 Train All Parameters(TAP)는 Training strategies 에 설명된 훈련 전략 입니다. 우리의 방법을 사용하면 현재 speaker independent SOTA 보다 2.3% 개선되었고 speaker dependent SOTA 보다는 0.5%가 개선되었습니다. 추가적으로 HuBERT Base and Large Models에서 둘다 개선된 우리의 접근 방법인 low resource 설정의 AUC를 제시합니다. Small data settings적은 리소스로 음성 감정 인식을 테스트 하기 위해 훈련 세트의 클래스당 샘풀 수를 늘릴것을 제안합니다. 결과를 stabilize 하기 위해 우리는 각 단계를 서로 다른 random split으로 다섯번 실행하고 각 단계의 mean을 계산합니다. 마지막으로 주어진 방법의 전반적 성능을 정량화 하기 위해 Fig2의 AUC를 계산합니다. 직관적으로 AUC 점수는 평가하는 각 설정에 대한 점수의 평균을 반영합니다. 그림 2는 우리가 제안한 low-source 설정에 대한 결과를 보여줍니다. 각 단계에서 우리는 HuBERT Large model을 우리의 방법을 사용한것과 사용하지 않은것으로 훈련했습니다. 우리는 각 method에 대한 AUC를 표1에 보고했습니다. 우리의 방법을 사용하여 기본과 대형 HuBERT 모델을 모두 개선할 수 있었습니다. 그림 2에서 우리의 방법은 모들 설정에서 HuBERT 정확도를 향상 시킨다는 것을 알 수 있습니다. Conclusion이 논문에서는 self-supervised feature representation에서 speaker characteristics normalization를 위한 프레임워크를 제시했습니다. 우리의 접근 방식은 한 과제에 대한 판별 학습과 다른 과제에 대한 적대적 학습을 결합합니다. 또한, 이 방법은 각 과제마다 다른 데이터 세트를 사용할 수 있습니다. 다양한 모델을 대상으로 테스트한 결과 음성 감정 인식에서 강력한 최첨단 결과를 얻었습니다. 또한 수정된 버전의 IEMOCAP을 사용하여 리소스가 적은 환경에서 연구할 것을 제안하고 그 결과 우리 방법이 성공적임을 보여주었습니다.","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speech Emotion Recognition","slug":"Paper/Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/categories/Paper/Speech-Emotion-Recognition/"}],"tags":[{"name":"Speech Emotion Recognition","slug":"Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/tags/Speech-Emotion-Recognition/"},{"name":"Self-Supervised Learning","slug":"Self-Supervised-Learning","permalink":"https://jmj3047.github.io/tags/Self-Supervised-Learning/"},{"name":"Speaker Normalization","slug":"Speaker-Normalization","permalink":"https://jmj3047.github.io/tags/Speaker-Normalization/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speech Emotion Recognition","slug":"Paper/Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/categories/Paper/Speech-Emotion-Recognition/"}]},{"title":"Speech Emotion Recognition Using Self-Supervised Features","slug":"SER_for_self_supervised","date":"2023-07-14T15:00:00.000Z","updated":"2023-07-18T00:21:15.630Z","comments":true,"path":"2023/07/15/SER_for_self_supervised/","link":"","permalink":"https://jmj3047.github.io/2023/07/15/SER_for_self_supervised/","excerpt":"","text":"Journal&#x2F;Conference : ICASSP IEEEYear(published year): 2022Author: Edmilson Morais, Ron Hoory, Weizhong Zhu, Itai Gat, Matheus Damasceno and Hagai AronowitzSubject: Speech Emotion Recognition, self-supervised features, end-to-end systems Speech Emotion Recognition Using Self-Supervised Features Summary The paper proposes a modular End-to-End (E2E) Speech Emotion Recognition (SER) system based on an Upstream + Downstream architecture model paradigm. The proposed system uses self-supervised features extracted from speech signals and script transcriptions of the speech signals. The authors compare the performance of different Upstream models for speech-based feature extraction, including Wav2vec 2.0 and huBERT. The authors fine-tune the features extracted from these models and combine them using an aggregator to create multimodal feature vectors. The authors achieve state-of-the-art performance on the IEMOCAP dataset, demonstrating the effectiveness of their multimodal approach to SER. Introduction인간의 감정은 본질적으로 복잡하고 여전히 어려운 연구 문제 입니다. 인간은 종종 음성 특성, 언어적 내용, 표정, 신체 동작과 같은 여러가지 단서를 동시에 사용하여 감정을 표현하기 때문에 SER은 본질적으로 복잡한 multi modal 작업입니다. 또한 데이터 수집의 어려움으로 인해 공개적으로 사용 가능한 데이터 세트에는 감정 표현의 개인적 차이를 제대로 커버할 수 있는 화자가 충분하지 않은 경우가 많습니다. 그 결과 SER에 통합된 가장 일반적인 딥러닝 기술 중 일부는 transfer learning, multitask learning , multimodal system 분야와 관련 있습니다. 본 논문의 주요 목표는 다음과 같습니다: (1)다양한 self supervised feature을 쉽게 사용&#x2F;통합할 수 있는 upstream + downstream 아키텍처 모델 패러다임에 기반한 모듈형 엔드투엔드(E2E) SER 시스템을 소개하고, (2)다양한 구성에서 제안된 E2E 시스템의 성능을 비교&#x2F;분석하는 일련의 실험을 제시하며, (3)음성 모달리티만 사용함에도 불구하고 제안된 E2E 시스템이 음성 및 텍스트 모달리티를 모두 사용하는 멀티모달 시스템이 달성하는 SOTA 결과와 비교하여 SOTA 결과를 얻을 수 있음을 보여주는 것입니다. Proposed Model이 논문에서 SER의 문제점을 연속적인 음성을 불연속적인 감정 레이블로 범주화하는 것이라고 여기고 이를 공식화 합니다. 여기서 사용되는 모델은 Upstream + Downstream 모델입니다. Upstream model: task-independent model, pretrained in self-supervised fashion and it works as Encoder or Front-End Model responsible for feature extraction. 일반적으로 Front End 모델은 입력 데이터를 처리하고 추가 처리 또는 분류를 위해 Downstream 모델에서 사용할 수 있는 관련 특징을 추출하는 역할을 담당하는 모델 유형입니다. 음성 처리의 맥락에서 Front End 모델은 일반적으로 원시 음성 신호에서 MFCC 또는 필터뱅크 에너지와 같은 음향적 특징을 추출하는 데 사용됩니다. 그런 다음 이러한 특징은 분류기나 음성 인식 시스템과 같은 Downstream 모델에 입력으로 사용됩니다. 반면에 인코더는 입력 데이터를 입력의 가장 중요한 특징을 포착하는 저차원 표현으로 변환하는 데 사용되는 일종의 신경망 계층입니다. 인코더는 일반적으로 입력 데이터가 고차원적이고 복잡한 이미지 또는 음성 인식과 같은 작업을 위한 딥 러닝에 사용됩니다. 이 논문에서 설명하는 SER 모델의 맥락에서 업스트림 모델은 프론트엔드 모델이자 인코더입니다. 입력 음성 신호를 처리하고 관련 특징을 추출한 다음 분류를 위해 다운스트림 모델에 입력으로 사용하는 역할을 합니다. 또한 업스트림 모델은 음성의 일반적인 특징을 학습하기 위해 자가 지도 방식으로 사전 학습되므로 SER 작업에 강력한 특징 추출기가 됩니다. Downstream model: task-dependent model, responsible for final task of classifying the features generated by the Upstream model into categorical labels of emotion. DatasetIEMOCAP은 12시간의 multimodal 데이터로 구성되어 있습니다. 총 5개의 세션과 10명의 화자로 구성되어 있으며, 한 세션은 두 명의 독점 화자의 대화로 구성됩니다. 이전 연구와 비교할 수 있도록 ‘화난’, ‘행복’, ‘흥분’, ‘슬픔’, ‘중립’ 에 속하는 레이블만 사용했습니다. 각 감정 클래스의 크기 균형을 맞추기 위해 ‘행복’과 ‘흥분’ 클래스를 병합하여 총 5,531개의 발화(행복 1636, 화남 1103, 슬픔 1084, 중립 1708)를 산출했습니다. leave-one-session-out 5-fold cross validation (CV) is used and the average result reported. At each fold of the 5-fold CV setup, 2 speakers are used for testing and the samples from the 8 speakers remaining are randomly split into 80% for training and 20% for validation. The splitting done here is exactly the same as the one done by SUPERB [18], which splits each of the 5 IEMOCAP folds into three subsets: a training-set, a validation-set and a test-set. The fine-tuning of our Upstream model is performed by training it jointly with a simple Mean-Average Pooling Network followed by a Linear Classifier, as described in Figure 2. Fine-tuning of the upstream modelsSER 시스템을 향상 시키기 위해 섹션에서 설명된 IEMOCAP 데이터 세트의 범주형 감정레이블을 사용하여 Upstream model을 미세 조정 하였습니다. 이 미세조정은 5 fold의 IEMOCAP dataset 각각에 대해 수행됩니다. 따라서 미세조정 프로세스가 끝나면 5개의 서로 다른 fune tuned 된 upstream model이 세션별로 생성됩니다. 2.1 Upstream model의 미세조정은 그림2에 설명된 대로 간단한 average pooling network와 Linear Classifier를 함께 훈련하여 수행합니다. Average of checkpoints 딥 러닝의 맥락에서 체크포인트는 학습 중 특정 시점의 모델 매개변수(wieght and bias)에 대한 스냅샷입니다. 체크포인트는 일반적으로 훈련 중에 주기적으로 저장되며, 특정 시점부터 훈련을 재개하거나 유효성 검사 세트에서 모델의 성능을 평가하는 데 사용할 수 있습니다. 논문에 설명된 SER 시스템에서 저자는 체크포인트를 사용하여 훈련 중에 미세 조정된 업스트림 모델의 성능을 추적합니다. 구체적으로는 미세 조정된 업스트림 모델의 정확도를 기준으로 IEMOCAP 데이터 세트의 5배수 각각에 대해 최고의 체크포인트 5개를 유효성 검사 등 각종 세트에 저장합니다. 이러한 체크포인트는 모델 파라미터의 평균을 계산하는 데 사용되며, 이는 업스트림 모델의 출력 분산을 줄이고 SER 시스템의 전반적인 성능을 개선하는 데 도움이 됩니다. why should output variance be minimized? SER 시스템의 맥락에서 업스트림 모델의 출력 분산을 최소화하는 것은 시스템의 전반적인 성능을 개선하는 데 도움이 되기 때문에 중요합니다. 출력 분산은 동일한 입력 음성 신호가 주어졌을 때 업스트림 모델에서 생성되는 출력 특징의 변동성을 나타냅니다. 출력 분산이 높으면 다운스트림 모델이 수신하는 특징이 일관되지 않거나 노이즈가 있을 수 있으므로 입력 음성 신호를 정확하게 분류하기가 더 어려워질 수 있습니다. 미세 조정된 업스트림 모델의 체크포인트를 평균화함으로써 작성자는 모델의 출력 분산을 줄이고 다운스트림 모델에서 작업할 수 있는 보다 일관되고 신뢰할 수 있는 특징을 생성할 수 있습니다. 이를 통해 SER 시스템의 전반적인 정확도와 견고성을 개선하고 다양한 음성 신호와 다양한 맥락에서 우수한 성능을 발휘할 수 있습니다. why did they use W2V2 and huBert both in Upstream model? 이 논문의 저자들은 SER 시스템의 업스트림 모델에 Wav2vec 2.0(W2V2)과 huBERT를 모두 사용했는데, 이는 사전 학습된 다양한 모델을 결합하여 시스템의 성능을 향상시킬 수 있는 방법을 모색하기 위해서였습니다. W2V2와 huBERT는 모두 음성 처리를 위해 사전 학습된 최첨단 모델이며 다양한 음성 작업에서 우수한 성능을 발휘하는 것으로 나타났습니다. 업스트림 모델에 두 모델을 모두 사용함으로써 저자들은 각 모델의 강점을 활용하고 다운스트림 모델에서 작업할 수 있는 더욱 강력하고 정확한 기능을 생성할 수 있었습니다. 특히 W2V2는 음성 신호의 문맥화된 표현을 추출하도록 설계된 반면, huBERT는 화자별 표현을 추출하도록 설계되었습니다. 이 두 모델을 결합함으로써 저자들은 입력 음성 신호의 문맥 정보와 화자별 정보를 모두 포착하는 특징을 생성할 수 있었고, 이는 SER 시스템의 전반적인 성능을 개선하는 데 도움이 되었습니다. Experimental SetupExperiment for evaluation SER 모델을 사용하여 수행할 수 있는 여러 실험 중에서 그림3과 표 1의 (1.A), (1.B) 세트에 설명된 실험을 선택했습니다. 이 실험의 목표는 다음과 같습니다: (1) the importance of fine-tuning the Upstream model; (2) the importance of averaging the Upstream and Downstream Model Checkpoints; (3) how Wav2vec 2.0 and huBERT can be combined to boost SER performance and (4) the performance of the two aggregators used: Mean Pooling and ECAPA-TDNN. what is aggregator? 이 논문에서 설명하는 SER 시스템의 맥락에서 aggregator는 다운스트림 모델의 구성 요소로, 업스트림 모델의 출력 특징을 감정 인식을 위한 최종 분류기에 입력할 수 있는 단일 특징 벡터로 결합하는 역할을 담당합니다. 업스트림 모델은 입력 음성 신호에서 feature를 추출하는 사전 학습된 모델입니다. 그런 다음 이러한 feature는 aggregator로 전달되어 감정 인식 작업을 위해 입력 음성 신호에 대한 가장 중요한 정보를 캡처하는 single feature vector로 결합됩니다. SER 시스템에서 사용할 수 있는 aggregator에는 평균 풀링과 ECAPA-TDNN 등 다양한 유형이 있습니다. 서로 다른 유형의 입력 특징 또는 서로 다른 유형의 음성 신호에 더 적합한 aggregator가 있을 수 있으므로 aggregator 선택이 시스템 성능에 영향을 미칠 수 있습니다. 전반적으로 aggregator는 다운스트림 모델이 작업할 수 있는 강력하고 정확한 특징 벡터를 생성하는 데 도움을 줌으로써 SER 시스템에서 중요한 역할을 합니다. Fig3에 의하면, 실험(1-4)에서 사용된 Upstream model used is either Wav2vec 2.0 or huBERT and the Downstream model is composed by a Mean Average Aggregator followed by a linear classifier. (1-2)의 실험에서는 업스트림 모델과 다운스트림 모델 모두 평균화 하지 않았습니다. (3-4)의 실험에서는 업스트림 모델과 다운스트림 모델 모두 평균을 냈습니다. 실험(5-6)과 (3-4)는 유사하며 유일한 차이점은 사용된 aggregator인 ECAPA-TDNN이 사용된 것입니다. Experiment 7 and 8 in the paper describe different types of feature fusion that were used to combine the output features of the Wav2vec 2.0 and huBERT models in the Upstream component of the SER system. 실험(7)에서는 ECAPA-TDNN aggregator를 통과 하기 직전에 W2V2과 huBERT 기능 간의 early fusion을 수행합니다. In Experiment 7, the authors used early fusion to combine the output features of the Wav2vec 2.0 and huBERT models. Early fusion involves combining the input features from two different modalities (in this case, speech and text) before they are processed by the Upstream models. Specifically, the authors concatenated the output features of the two models before passing them through the ECAPA-TDNN aggregator. 실험(8)에서는 두개의 ECAPA-TDNN이 생성한 utterance embedding을 나중에 융합했는데, 첫번째는 W2V2 가능에서 작동하고 두번째는 huBERT기능에서 작동합니다. In Experiment 8, the authors used later fusion to combine the output features of the Wav2vec 2.0 and huBERT models. Later fusion involves combining the output features of the two models after they have been processed by the Upstream models. Specifically, the authors used two separate ECAPA-TDNN aggregators to process the output features of the two models, and then concatenated the resulting feature vectors before passing them through the final classifier. Fusion refers to the process of combining information from multiple sources to produce a single output. In the context of the SER system described in the paper, fusion is used to combine the output features of the Wav2vec 2.0 and huBERT models in order to produce a more robust and accurate feature vector for the Downstream model to work with. By combining the strengths of the two models, the authors were able to improve the overall performance of the SER system. Experiments to be used as baselines Fig 3 실험과 비교하기 위한 base line으로 음성 피처는 Fbank, 텍스트는 BERT를 사용하였습니다. 실험(9)에서는 표준 필터 뱅크가 업스트림모델로 사용되고 실험(10)에서는 BERT 모델이 업스트림 모델로 사용됩니다. 실험 (11)에서는 음성에서는 Fbank, 텍스트 feature는 BERT를 사용하여 later fusion fashion 방식으로 사용됩니다. It is important to emphasize that the Fbank used here does not have explicit pitch information attached to it and that the fine-tuning optimization process of the BERT model may not follow the most advanced SOTA techniques available nowadays. However, despite not being as carefully prepared as it could be, these baseline models can help us to obtain insight on how powerful these fine-tuned and averaged Wav2vec 2.0 and huBERT features are. RESULTS 표 설명: In column 2 of Table 1, under the term (#), we indicate the number of the 11 experiments evaluated. In column 3 we indicate the input modality used in each experiment. In column 4 under the term Upstream model we can find the indication of the Input feature; if the Upstream model has been fine-tuned (FT); and if the Upstream model has been Averaged (AVG). The symbol “+” in experiment 7 (huBERT + W2V2) indicates early fusion of the features and the symbol “&amp;” in the experiments 8 and 11 indicates later fusion of the features. In column 5 under the term Downstream model we can find the indication of the Aggregator Model used (AGG); the Classification Model (Classifier) used; and if the full Downstream Model has been averaged (AVG). Since the test sets of IEMOCAP are slightly imbalanced between different emotion categories, in column 6 of Table 1 under the term Accuracy we report both Weighted Accuracy (WACC) and Unweighted Accuracy (UACC). Finally, in column 1 of Table 1 under the term SET we have: in (1.A) the subset of experiments from Figure 3 that use Mean Pooling as Aggregator; in (1.B) the subset of experiments from Figure 3 that use ECAPA-TDNN as Aggregator and in (2) the baseline experiments described in Figure 4.","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speech Emotion Recognition","slug":"Paper/Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/categories/Paper/Speech-Emotion-Recognition/"}],"tags":[{"name":"Speech Emotion Recognition","slug":"Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/tags/Speech-Emotion-Recognition/"},{"name":"Self-Supervised Features","slug":"Self-Supervised-Features","permalink":"https://jmj3047.github.io/tags/Self-Supervised-Features/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speech Emotion Recognition","slug":"Paper/Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/categories/Paper/Speech-Emotion-Recognition/"}]},{"title":"Machine Learning Data Lifecycle in Production","slug":"MLOps_Part2","date":"2023-07-10T15:00:00.000Z","updated":"2023-07-17T10:30:48.185Z","comments":true,"path":"2023/07/11/MLOps_Part2/","link":"","permalink":"https://jmj3047.github.io/2023/07/11/MLOps_Part2/","excerpt":"","text":"Course Link Lecture 2 in MLOps Data Label Collecting Data You need to make sure that your data covers the same region of your feature space as the prediction request that you’ll get your training data and you want to make sure that you’ve really maximize the predictive signal in that data. And you need to worry about the data quality not just at the beginning but throughout the life of the application. So part of that is making sure that your sourcing data responsibly and you’re thinking about things like bias and fairness. One of the key aspects of collecting data is to make sure that you’re collecting it responsibly and paying attention to things like security, privacy and fairness. key points, first of all, always account for fair Raters and fair representation in your data set to avoid potential biases. And take into account who those labelers are and what their incentives are, because if you design the incentives incorrectly, you could get a lot of garbage in your data. The cost is certainly always going to be an important consideration. So if you can find a way to do it with a high level of quality but at less cost, that’s great. But you need enough data. You need to find a way to do that. It’s one of the challenges of production applications and finally data freshness too. You’re going to be working with data and depending on how the world changes around the application and the data that you have, you’re going to need to refresh that data on some regular basis and detect when you need to do that. So those are all issues you need to think about to really manage collection of data and to do it in a responsible way. Labeling Data The key points of what we’re talking about here, model performance decays over time. It may decay slowly over time, in things like cats and dogs, that doesn’t change very quickly, or it may change very fast, things like markets. Model retraining will help you improve or maintain your performance. Certainly as your model performance decays, it’ll help you do that. Data labeling, assuming you’re doing supervised learning, which is pretty common, data labeling is a key part of that. You really need to think about how you’re going to approach that in your particular problem, in your particular domain and with the systems that you have available to you. Validating Data Just to wrap up, this week, you saw differences between ML modeling in academic or research environments and production ML systems. We discussed responsible data collection and how to really approach building a fair production ML system. We learned about process feedback and direct labeling and also human labeling. We looked at some of the issues that you can have with data and how to identify and detect those issues. Feature Preprocessing The art of feature engineering tries to improve your model’s ability to learn while reducing if possible, the compute resources it requires, it does this by transforming and projecting, eliminating and or combining the features in your raw data to form a new version of your data set. So typically across the ML pipeline, you incorporate the original features often transformed or projected to a new space and or combinations of your features. Objective function must be properly tuned to make sure your model is heading in the right direction and that is consistent with your feature engineering. You can also update your model by adding new features from the set of data that is available to you unlike many things in ML, this tends to be an iterative process that gradually improves your results as you iterate or you hope it does. You have to monitor that and if it’s not improving, maybe back up and take another approach. To review some key points, as the quote from Andrew Ng demonstrates, feature engineering can be very difficult and time consuming but it is also very important to success. You want to squeeze the most out of your data and you do that using feature engineering, by doing that, you enable your models to learn better. You also want to make sure that you concentrate predictive information, your data into as few features as possible to make the best and least expensive use of your compute resources. And you need to make sure that you apply the same feature engineering during serving as you applied during training. Here’s some of the main Preprocessing Operation. One of the most important Preprocessing Operations is Data cleansing, which in broad terms consists in eliminating or correcting erroneous data. You’ll often need to perform transformations on your data, so scaling or normalizing your numeric values, for example. Since models, especially neural networks, are sensitive to the amplitude or range of numerical features, data preprocessing helps Machine Learning build better predictive Models. Dimensionality reduction involves reducing the number of features by creating lower dimension and more robust data represents. Feature construction can be used to create new features by using several different techniques, which we’ll talk about some of them. Key points. Data preprocessing is a technique that’s used to transform raw data into useful data for training the Model. Feature Engineering consists in mapping raw input data and creating a feature vector from it using different techniques with different kinds of data. It can also include things like mapping data from one space into a different space, which depending on the characteristics of a Model, say a linear Model versus a neural network can have a big difference in how well the Model can learn from it. feature engineering Key points for this particular section, feature engineering. It’s going to prepare and tune and transform and extract and construct features where we’re going to work with features and change them starting with our raw data through to the data that we’re going to give to our model. Feature engineering is very important for model refinement. Really, it can make the difference between successfully modeling something and not. Feature engineering really helps with ML Analysis and really developing that intuitive understanding of our data. feature crosses What are Feature crosses? Well, they combine multiple features together into a new feature. That’s fundamentally what a feature across. It encodes non-linearity in the feature space, or encodes the same information and fewer features. We can create many different kinds of feature crosses and it really depends on our data. Feature Crossing as a way to create synthetic features, often encoding non-linearity in the features space. We’re going to transform both categorical and numerical. We could do that in, into either continuous variables or the other way around. Feature selection feature space: a feature space is defined by the n dimensional space that your features defined. So if you have two features, a feature space is two dimensional. If you have three features, its three dimensional and so forth, it does not include the target label. filter methods for filter methods, we’re primarily using correlation to look for the features that contain the information that we’re going to use to predict our target. we are going to start with all of the features and we’re going to select the best subset and we’re going to give those to our model and that’s going to give us our performance for the model with this subset of our features. wrapper method It stores supervised method, but we’re going to use this with a model and there’s different ways to do it. But basically you’re iterating through, it’s a search method against the features that you have using a model as the measure of their effectiveness. We can do it through forward elimination and we’ll talk about this in a second. Forward elimination, backward elimination or recurrent feature elimination. We start with all of our feature, regenerate a subset of those features, and we’ll talk about how that gets generated, that gets given to our model. The results that is generated from that model is then used to generate the next subset. That becomes this feedback loop to select the best subset of our features using our model as a measure. That gives us the performance of the final best subset that is selected. Embedded method(feature importance) L1 or L2 regularization is essentially an embedded method for doing feature selection. Feature importance is another method. Both of these are highly connected to the model that you’re using. So these both L1 regularization and feature importance really sort of an intrinsic characteristic of the model that you’re working with.\\ Data Storage MLMD(ML meta data) ML metadata stores a wide range of information about the results of the components and execution runs of a pipeline. You learned a lot about the architecture and nomenclature of ML metadata or MLMD and the artifacts and entities which it contains. This should give you some idea of how you can leverage MLMD to track metadata and the results flowing through your pipeline to better understand your training process, both now and in previous training runs of your pipeline. Data Warehouse Data warehouses are meant for analyzing data, whereas databases are often used for transaction purposes. Inside a data warehouse, there may be a delay between storing the data and the data getting reflected in the system. But in a database, data is usually available immediately after it’s stored. Data warehouses store data as a function of time, and therefore, historical data is also available. Data warehouses are typically capable of storing a larger amount of data compared to databases. Queries in data warehouses are complex in nature and tend to run for a long time. Whereas queries in database are simple and tend to run in real time. Normalization is not necessary for data warehouses, but it should be used with databases. Data Lake The primary difference between them is that in a data warehouse, data is stored in a consistent format which follows a schema, whereas in data lakes, the data is usually in its raw format. In data lakes, the reason for storing the data is often not determined ahead of time. This is usually not the case for a data warehouse, where it’s usually stored for a particular purpose. Data warehouses are often used by business professionals as well, whereas data lakes are typically used only by data professionals such as data scientists. Since the data in data warehouses is stored in a consistent format, changes to the data can be complex and costly. Data lakes however are more flexible, and make it easier to make changes to the data.","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}],"tags":[{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Coursera","slug":"Coursera","permalink":"https://jmj3047.github.io/tags/Coursera/"},{"name":"AI Studies","slug":"AI-Studies","permalink":"https://jmj3047.github.io/tags/AI-Studies/"},{"name":"ML Operations","slug":"ML-Operations","permalink":"https://jmj3047.github.io/tags/ML-Operations/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}]},{"title":"Machine Learning Data Lifecycle in Production_Quiz","slug":"MLOps2_Quiz","date":"2023-07-10T15:00:00.000Z","updated":"2023-07-17T11:06:33.517Z","comments":true,"path":"2023/07/11/MLOps2_Quiz/","link":"","permalink":"https://jmj3047.github.io/2023/07/11/MLOps2_Quiz/","excerpt":"","text":"개요Coursera ML Ops Course 2 Quiz 1. Intro to MLEP Link: https://www.coursera.org/learn/machine-learning-data-lifecycle-in-production/home/week/1 2. Data Collection 3. Data Labeling 4.Issues in Training Data 5.Feature Engineering and Preprocessing Link: https://www.coursera.org/learn/machine-learning-data-lifecycle-in-production/home/week/2 6. Feature Transformation 7. Feature Selection 8. Data Journey Link: https://www.coursera.org/learn/machine-learning-data-lifecycle-in-production/home/week/3 9.Schema Environments 10. Enterprise Data Storage 11. Advanced Labelling Link: https://www.coursera.org/learn/machine-learning-data-lifecycle-in-production/home/week/4 12. Data Augmentation 13. Different Data Types","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"Coursera","slug":"Coursera","permalink":"https://jmj3047.github.io/tags/Coursera/"},{"name":"Quiz","slug":"Quiz","permalink":"https://jmj3047.github.io/tags/Quiz/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}]},{"title":"Domain Invariant Feature Learning for Speaker-Independent Speech Emotion Recognition","slug":"DIFL_SI_SER","date":"2023-07-02T15:00:00.000Z","updated":"2023-07-05T07:14:22.920Z","comments":true,"path":"2023/07/03/DIFL_SI_SER/","link":"","permalink":"https://jmj3047.github.io/2023/07/03/DIFL_SI_SER/","excerpt":"","text":"Journal&#x2F;Conference : IEEE&#x2F;ACM Transactions on Audio, Speech, and Language ProcessingYear(published year): 2022Author: Cheng Lu , Yuan Zong , Member, IEEE, Wenming Zheng , Senior Member, IEEE, Yang Li , Member, IEEE, Chuangao Tang , and Björn W. Schuller , Fellow, IEEESubject: Domain Invariant Feature Learning(DIFL), Speech Emotion Recognition(SER) Domain Invariant Feature Learning for Speaker-Independent Speech Emotion Recognition Summary 멀티소스 UDA를 수행하기 위한 간단하면서도 효과적인 도메인 불변 학습 프레임워크를 제안합니다. 우리가 아는 한, 멀티 소스 UDA의 관점에서 화자 독립적 SER을 다룬 연구는 이번이 처음입니다. We propose a simple, yet effective domain-invariant learning framework to carry out multi-source UDA. To the best of our knowledge, this is the first study dealing with speaker-independent SER from the perspective of multi-source UDA. 화자의 정체성과 감정의 혼동을 제거하기 위해 특징 추출기 블록에 계층적 분포 정렬 방법을 제안하고, 계층적 정렬 계층에 포함된 강-약 정렬 전략을 활용하여 로컬 및 글로벌 특징에 대해 각각 강-약 정렬을 구현합니다. We propose a hierarchical distribution alignment method in the feature extractor block to remove the confusion of speakers’ identity and emotion, in which a strong-weak alignment strategy embedded in hierarchical alignment layers is utilized to realize the strong and weak alignment for local and global features, respectively. 화자 독립적 SER에 대한 도메인 불변 특징을 추가로 학습하기 위해 다중 판별자를 도입하여 도메인 판별자와 화자 판별자를 사용하여 화자 불변 특징을 얻고 레이블 판별자를 사용하여 감정과 관련된 판별 특징을 얻습니다. We introduce multiple discriminators to further learn domain invariant features for the speaker-independent SER, in which the domain discriminator and speaker discriminator are used to obtain speaker-invariant features while the label discriminator is used to obtain discriminative features that are emotion-related. I. IntroductionDIFL의 정의 도메인 불변 특징 학습(DIFL)은 화자 독립적인 음성 감정 인식을 다루기 위해 “화자 독립적인 음성 감정 인식을 위한 도메인 불변 특징 학습” 논문에서 제안된 방법입니다. DIFL의 기본 아이디어는 다중 소스 비지도 도메인 적응(multi-source unsupervised domain adaptation, UDA)의 관점에서 화자가 달라서 발생하는 훈련 데이터와 테스트 데이터 간의 도메인 이동을 제거하여 화자 불변 감정 특징을 학습하는 것(learn speaker invariant emotion feature)입니다. multi-source UDA의 정의 다중 소스 비지도 도메인 적응(UDA)은 하나 이상의 소스 도메인에서 학습된 모델을 레이블이 지정된 데이터가 부족하거나 사용할 수 없는 대상 도메인에 적응시키기 위해 머신 러닝에 사용되는 기법입니다. 다중 소스 UDA에서는 모델이 여러 소스 도메인에서 동시에 학습되며, 목표는 대상 도메인에 잘 일반화할 수 있는 도메인 불변형 표현을 학습하는 것입니다. 이 기법은 여러 소스의 데이터가 서로 다른 분포와 특성을 가질 수 있는 복잡한 실제 시나리오를 다룰 때 특히 유용합니다. 다중 소스 비지도 도메인 적응은 전이 학습의 한 유형입니다. 전이 학습은 하나 이상의 소스 도메인에서 학습한 지식을 목표 도메인으로 옮기는 과정을 말합니다. 다중 소스 UDA에서는 여러 소스 도메인에서 학습한 지식을 목표 도메인으로 전송하여 목표 작업에서 모델의 성능을 개선합니다. 관련 도메인의 지식을 활용함으로써 모델이 보이지 않는 새로운 데이터에 더 잘 적응할 수 있는 보다 일반화 가능한 표현을 학습할 수 있다는 것이 이 아이디어의 핵심입니다.→ 그러니까 이 논문에서 소스 도메인에서 타겟 도메인으로 넘어가는 과정의 화자와 관련된 특성을 제거시켜서 감정 인식이 더 잘되게 하는 방법을 제안하였음. UDA의 정의 다중 소스 UDA는 비지도 도메인 적응 모델의 한 유형입니다. 비지도 도메인 적응은 대상 도메인의 레이블이 지정된 데이터를 사용하지 않고 소스 도메인에서 학습된 모델을 레이블이 지정된 데이터가 부족하거나 사용할 수 없는 대상 도메인에 적응시키는 프로세스를 말합니다. 목표는 공유된 특징 공간에서 소스 도메인과 대상 도메인의 분포를 정렬하여 대상 도메인에 잘 일반화할 수 있는 도메인 불변 표현을 학습하는 것입니다. 이는 일반적으로 공유 특징 공간에서 소스 도메인과 대상 도메인 간의 거리 또는 불일치를 최소화하면서 당면한 작업에 대한 판별 정보를 보존함으로써 달성할 수 있습니다. 다중 소스 UDA는 이 아이디어를 여러 소스 도메인으로 확장하여 모델이 대상 도메인에 잘 일반화하면서 동시에 여러 다른 소스 도메인에 적응하는 방법을 학습합니다. 비지도 도메인 적응은 머신 러닝에서 한 도메인에서 학습된 모델을 레이블이 지정된 데이터가 부족하거나 사용할 수 없는 다른 도메인에 적응시키는 데 사용되는 기법입니다. 목표는 공유된 특징 공간에서 소스 도메인과 대상 도메인의 분포를 정렬하여 대상 도메인에 잘 일반화할 수 있는 표현을 학습하는 것입니다. 다중 소스 UDA는 이 아이디어를 여러 소스 도메인으로 확장하여 모델이 대상 도메인에 잘 일반화하면서 동시에 여러 다른 소스 도메인에 적응하는 방법을 학습합니다. → UDA는 source 도메인에서 학습이 완료된 모델을 target 도메인(데이터가 부족하거나 사용할 수 없는)에 적응시키는데 사용되는 방법. 목표는 source 도메인 모델을 target 도메인이 잘 일반화 시켜서 적응 시키는 것임. multi-source UDA는 이러한 점을 보완하기 위해서 source domain의 개수를 다중으로 만들어 일반화가 더 잘되게 적응 시킴. II. Related Work Overview of the Domain Invariant Feature Learning (DIFL) framework for speaker-independent SER, including the feature extractor block with a hierarchical alignment layer (HAL), and the discriminator block with a domain adversarial layer (DAL) consisting of a label discriminator $G_l$, source-target domain discriminator $G_{st}$, and speaker discriminator $G_{sp}$, where the GRL represents the gradient reversal layer. A. Speaker-Independent SER 기존의 연구 방향: (1) feature fusion; (2) classifier enhancement; and (3) speaker normalization. feature fusion: The feature fusion category aims to fuse multiple hand-crafted features to obtain a discriminative feature set, which is mostly used in early works. classifier enhancement: to enhance the robustness of the classifier to deal with speaker-independent SER, which are usually combined with suited feature fusion based methods. speaker normalization: The speaker normalization category mainly utilizes speaker normalization (e. g., mean normalization, cumulative distribution mapping, factor analysis) to reduce the specific variability of speakers during the feature extraction. B. Multi source UDA It mainly focuses on the situation that the source data are collected from multiple different domains [36]. Therefore, multi-source UDA not only considers the domain shift between the source and target domains, but also handles the discrepancy across multiple domains in the source data [36], [37]. latent space transformation The latent space transformation based methods attempt to convert different domain features to a specific latent space, and then use the divergence-based or adversarial-based loss to align the domain shift. It is worth noting that the divergence-based methods are non-parametric, only depending on the selection of divergence functions and embedding position in feature layers, while the adversarial-based methods need to learn new parameters of the discriminator. In this paper, we combine these two methods in the DIFL framework to find the optimal balance among their advantages and disadvantages. intermediate domain generation intermediate domain generation strategy tries to generate the new adapted domain for each domain in the source data, where these new domains are indistinguishable from the target domain. These methods are all based on GANs [44] or auto-encoders [45], e. g., Coupled GAN [46], CycleGAN in MADAN [47], or variational auto-encoder [45], as the generator to generate the intermediate domain from the latent space. III. Proposed MethodA. Hierarchial Representation for Emotional Speech감정적 음성을 위한 계층적 표현은 음성 데이터를 계층적 또는 계층적 구조로 표현하는 방식을 말하며, 각 계층은 음성 데이터의 점점 더 추상적이거나 복잡한 특징을 포착합니다. 이러한 계층적 표현은 여러 개의 컨볼루션 블록과 완전 연결(FC) 블록으로 구성된 심층 컨볼루션 신경망(DCNN)을 사용하여 얻을 수 있습니다. DCNN에서 각 컨볼루션 블록에는 컨볼루션, 일괄 정규화, ReLU 및 MaxPooling 연산이 포함됩니다. 이러한 연산은 음성 데이터에서 로컬 피처를 추출하는 데 사용되며, 이 피처는 fc 블록을 통과하여 특정 작업과 관련성이 높고 변별력이 강한 글로벌 피처를 추출합니다. 소스 데이터와 타깃 데이터의 특징 맵은 각각 소스 데이터의 경우 Cm(Xs)와 Ln(Xs), 타깃 데이터의 경우 Cm(Xt)와 Ln(Xt)로 표현됩니다. 여기서 Xs와 Xt는 각각 소스 도메인과 타깃 도메인의 원시 음성 데이터를 나타내고, Cm과 Ln은 각각 컨볼루션과 fc 블록에서 얻은 특징 맵을 나타냅니다. 계층적 표현을 사용하면 DCNN은 음성 데이터에서 보다 유익하고 차별적인 특징을 추출할 수 있으며, 이를 통해 음성 감정 인식(SER) 모델의 성능을 개선할 수 있습니다. 계층적 표현을 통해 모델은 기존의 수작업 피처로는 포착하기 어려운 복잡하고 미묘한 음성 감정의 변화를 포착할 수 있습니다. local feature and global feature 딥러닝의 맥락에서 로컬 피처는 처음 몇 개의 컨볼루션 레이어와 같은 신경망의 얕은 레이어에서 입력 데이터에서 추출되는 낮은 수준의 피처를 의미합니다. 이러한 로컬 피처는 가장자리, 모서리, 텍스처와 같은 입력 데이터의 로컬 패턴과 구조를 포착하며 다양한 작업과 도메인에 걸쳐 비교적 강력한 일반화 기능을 제공합니다. 반면에 글로벌 피처는 완전 연결(FC) 레이어와 같은 신경망의 심층 레이어에서 입력 데이터에서 추출되는 높은 수준의 피처를 말합니다. 이러한 글로벌 특징은 화자의 신원이나 음성에 표현된 감정 등 입력 데이터의 고차원적인 의미 정보를 포착하며, 특정 작업과 연관성이 높고 변별력이 강합니다. 음성 감정 인식(SER)의 맥락에서 감정적 음성의 로컬 특징은 일반적으로 포먼트의 모양과 기본 주파수의 위치와 같은 음성의 저수준 음향 특성을 포착합니다. 이러한 로컬 피처는 음성 데이터의 스펙트로그램에서 추출되며 음성 신호의 로컬 패턴과 구조를 캡처하는 데 사용됩니다. 반면에 감정적 음성의 글로벌 특징에는 화자의 신원이나 음성에 표현된 감정과 같은 작업별 의미 정보가 포함됩니다. 이러한 글로벌 특징은 DCNN의 FC 레이어에서 추출되며 음성 신호 2의 높은 수준의 의미 정보를 캡처하는 데 사용됩니다. 저수준 특징과 고수준 특징의 정렬을 결합함으로써 DCNN은 화자 독립적 SER에서 서로 다른 화자와 감정 표현의 혼동 문제를 처리하고 SER 모델 의 성능을 향상시킬 수 있습니다. 이 논문의 저자는 음성의 감정적 표현을 포착하기 위해 로컬 및 글로벌 특징을 모두 사용합니다. 로컬 피처는 음성 데이터의 스펙트로그램에서 추출되며 포먼트의 모양과 기본 주파수의 위치 등 음성의 저수준 음향 특성을 포착합니다. 이러한 로컬 특징은 일반적이며 다양한 작업과 도메인에 걸쳐 비교적 강력한 일반화 특성을 갖습니다. 반면에 글로벌 특징에는 화자의 신원이나 음성에 표현된 감정과 같은 작업별 의미 정보가 포함되어 있습니다. 이러한 글로벌 특징은 DCNN의 FC 계층에서 추출되며 음성 신호의 높은 수준의 의미 정보를 캡처합니다. 글로벌 특징은 특정 작업과 관련이 높고 변별력이 강합니다. 저수준 특징과 고수준 특징의 정렬을 결합함으로써 DCNN은 화자 독립적 SER에서 다양한 화자와 감정 표현의 혼동 문제를 처리하고 SER 모델의 성능을 향상시킬 수 있습니다. 로컬 및 글로벌 피처를 모두 사용하여 얻은 감정적 음성의 계층적 표현을 통해 모델은 기존의 수작업 피처로는 포착하기 어려운 복잡하고 미묘한 음성 감정의 변화를 포착할 수 있습니다. B. Hierarchical Alignment Layer (HAL) hierarchical representation을 얻을때 먼저 훈련 데이터 셋과 테스트 데이터 셋 간에 서로 다른 화자로 인해 발생하는 도메인 이동을 제거하는걸 고려해야 합니다. 이를 위해 hierarchical alignment layer(HAL)을 제안합니다. HAL은 소스 데이터와 대상 데이터의 특징 맵을 계층적 방식으로 정렬하는 방식으로 작동합니다. 특징 맵은 각각 음성 데이터에서 로컬 및 글로벌 특징을 추출하는 DCNN의 컨볼루션 및 완전 연결(FC) 블록에서 얻습니다. 피처 맵의 계층적 정렬을 통해 모델은 기존의 수작업 피처로는 포착하기 어려운 복잡하고 미묘한 음성 감정의 변화를 포착할 수 있습니다. HAL은 화자 독립적 SER을 위한 도메인 불변 특징 학습을 위해 제안된 방법의 핵심 구성 요소 중 하나입니다. 계층적 표현과 도메인 적대적 학습을 사용하여 제안된 방법은 소스 도메인과 타겟 도메인 간의 도메인 격차를 효과적으로 줄이고 SER 모델의 성능을 향상시킬 수 있습니다. HAL은 SWAS(강약 정렬 전략)를 사용하여 로컬 및 글로벌 특징의 정렬을 강화하고 음성 특징의 감정 변별력을 보장합니다. SWAS는 서로 다른 가중치를 사용하여 로컬 피처는 강하게 정렬하고 글로벌 피처는 약하게 정렬합니다. 특히 강력한 특징 분포 정렬에서 스펙트로그램은 컨볼루션 레이어를 통해 로컬 특징을 얻는데, 이는 대부분 포먼트와 에너지 분포(즉, 이미지의 가장자리, 모양, 색상)의 에지 또는 모양 설명입니다. 이러한 로컬 특징은 훈련 데이터 세트와 테스트 데이터 세트 간에 서로 다른 화자로 인해 발생하는 도메인 이동을 줄이기 위해 강력하게 정렬됩니다. 약한 특징 분포 정렬에서 DCNN의 FC 레이어는 특정 작업과 관련이 높고 변별력이 강한 전역 특징을 얻습니다. 이러한 글로벌 특징은 작업별 정보를 보존하고 작업별 정보의 손실로 이어질 수 있는 과도한 정렬을 피하기 위해 약하게 정렬됩니다. 로컬 특징과 글로벌 특징의 강-약 정렬을 결합함으로써 SWAS는 HAL의 정렬을 강화하고 SER 모델의 성능을 향상시킬 수 있습니다. 또한 제안된 강-약 정렬 전략을 사용하여 화자 독립적 SER의 성능을 향상시킬 수 있습니다. In the proposed Strong-Weak Alignment Strategy (SWAS), the strong alignment is applied to the local features obtained from the convolutional layers, while the weak alignment is applied to the global features obtained from the fully connected (fc) layers. The strong alignment is achieved by aligning the domain distributions of local features generated by convolutional layers using a larger weight λl. The specific calculation process of strong alignment in each convolutional layer is shown in Fig. 2(b) of the paper. The strong alignment process involves applying global max pooling (GMP) and global average pooling (GAP) to the local features and then using a weighted sum of the two pooling results to obtain the aligned features. On the other hand, the weak alignment is achieved by aligning the domain distributions of global features obtained by fc layers using a smaller weight λg. The specific calculation process of weak alignment in each fc layer is shown in Fig. 3(c) of the paper. The weak alignment process involves applying a domain adversarial loss to the global features to encourage the model to learn domain-invariant representations. By using different weights for strong and weak alignment, SWAS can effectively align the local and global features and improve the performance of the SER model. The proposed SWAS can also reduce the domain shift caused by different speakers between the training and testing datasets and improve the generalization ability of the SER model. C. Domain Adversarial Layer(DAL) What is divergence? 발산은 두 확률 분포 간의 차이를 정량화하는 통계적 척도입니다. 비지도 도메인 적응(UDA)의 맥락에서 발산 측정값은 소스 도메인과 대상 도메인 간의 분포 불일치를 평가하고 두 도메인의 특징 공간 정렬을 안내하는 데 사용할 수 있습니다. page 3에서 언급한 것처럼 최대 평균 불일치(MMD), 레니-발산, L2 거리, 모멘트 거리 등 UDA에서 사용할 수 있는 다양한 유형의 발산 측정값이 있습니다. 이러한 발산 측정값은 소스 도메인과 대상 도메인의 특징 분포 간의 불일치를 측정하고 두 도메인의 특징 공간 정렬을 유도하는 데 사용할 수 있습니다. 그러나 page 4에서 언급했듯이 발산 함수의 선택은 민감할 수 있으며 복잡하고 미묘한 음성 감정의 변화를 처리하기에 충분히 강력하지 않을 수 있습니다. 따라서 화자 독립적 음성 감정 인식(SER)에서 UDA를 위한 보다 효과적이고 강력한 솔루션으로 도메인 적대적 학습(DAL)과 같은 적대적 기반 UDA 방법이 제안되고 있습니다. DAL은 도메인 분류기를 훈련시켜 소스 도메인과 대상 도메인을 구분하는 동시에 특징 추출기를 훈련시켜 도메인 분류기를 속이는 도메인 불변 표현을 생성하는 과정을 포함합니다. 본 논문에서 제안한 방법은 DAL을 사용하여 소스 도메인과 타겟 도메인 간의 도메인 갭을 효과적으로 줄이고 SER 모델의 성능을 향상시킬 수 있습니다. What is divergency based UDA? 2에 따르면 발산 기반 UDA는 발산 측정값을 활용하여 소스 도메인과 타겟 도메인 간의 불일치를 평가하는 비지도 도메인 적응(UDA) 방법의 한 유형을 말합니다. 발산 측정값은 두 확률 분포 간의 차이를 정량화하는 통계적 측정값입니다. UDA의 맥락에서 발산 측정값은 소스 도메인과 대상 도메인 간의 분포 불일치를 측정하고 두 도메인의 특징 공간의 정렬을 안내하는 데 사용할 수 있습니다. 화자 독립적 음성 감정 인식(SER)의 경우, 발산 기반 UDA 방법을 사용하여 소스 도메인과 목표 도메인의 특징 공간을 정렬하고 훈련 데이터 세트와 테스트 데이터 세트 간에 서로 다른 화자로 인해 발생하는 도메인 이동을 줄였습니다. 그러나 5에서 언급했듯이 발산 기반 UDA 방법은 발산 함수의 선택에 민감할 수 있으며 복잡하고 미묘한 음성 감정의 변화를 처리하기에 충분히 강력하지 않을 수 있습니다. 이 문제를 해결하기 위해 이 논문에서 제안한 방법에서는 도메인 적대적 학습(DAL)과 같은 적대적 기반 UDA 방법을 사용하여 화자 독립적 SER을 위한 보다 강력하고 효과적인 솔루션을 제공합니다. DAL은 도메인 분류기를 훈련시켜 소스 도메인과 대상 도메인을 구분하는 동시에 특징 추출기를 훈련시켜 도메인 분류기를 속이는 도메인 불변 표현을 생성하는 적대적 훈련의 일종입니다. 제안한 방법은 DAL을 사용하여 소스 도메인과 타겟 도메인 간의 도메인 갭을 효과적으로 줄이고 SER 모델의 성능을 향상시킬 수 있습니다. Why HAL is problem using in divergency based UDA? 5에 따르면, HAL은 발산 기반 UDA를 기반으로 하며 다양한 메트릭 학습 전략을 사용하여 소스 도메인과 대상 도메인 간의 특징 분포 불일치를 측정하려고 시도합니다. 그러나 서로 다른 도메인에서 공통 정렬 공간을 설정하는 것은 선택된 발산 함수에 따라 달라지기 때문에 이 특징 공간은 많은 제약이 있고 견고성이 부족합니다. 즉, 발산 기반 UDA에서 HAL을 사용할 때의 문제점은 소스 도메인과 대상 도메인 간의 특징 분포 불일치를 측정하기 위해 통계적 가설과 발산 함수에 의존한다는 것입니다. 이 접근 방식은 발산 함수의 선택에 민감할 수 있으며 복잡하고 미묘한 음성 감정의 변화를 처리하기에 충분히 강력하지 않을 수 있습니다. 이 문제를 해결하기 위해 이 논문에서 제안한 방법은 도메인 적대적 학습(DAL)과 같은 적대적 기반 UDA 방법을 사용하여 유망한 솔루션을 제공합니다. DAL은 도메인 분류기를 훈련시켜 소스 도메인과 대상 도메인을 구분하는 동시에 특징 추출기를 훈련시켜 도메인 분류기를 속이는 도메인 불변 표현을 생성하는 적대적 훈련의 일종입니다. 제안한 방법은 DAL을 사용하여 소스 도메인과 타겟 도메인 간의 도메인 갭을 효과적으로 줄이고 SER 모델의 성능을 향상시킬 수 있습니다. DAL과 DANN의 차이 both DANN and DAL are adversarial-based UDA methods that aim to reduce the domain gap between the source and target domains in SER. However, DANN involves training a feature extractor and domain classifier simultaneously, while DAL involves adding a domain adversarial layer to the feature extractor. Exactly how did they use DAL to reduce the gap between source and target domain? 본 논문에서 제안한 방법에서는 화자 독립적 음성 감정 인식(SER)에서 소스 도메인과 타깃 도메인 간의 도메인 간극을 줄이기 위해 도메인 적대적 계층(DAL)을 사용했습니다. 구체적으로 심층 특징 추출기 이후 판별자 블록에 DAL을 추가하여 소스 데이터와 타깃 데이터의 음성 감정 특징을 획득함으로써 도메인 이동을 더욱 줄이고 음성의 높은 수준의 의미론적 감정 특징을 획득했습니다. 학습 과정에서 특징 추출기는 작업별 손실을 최소화하도록 학습하고, 도메인 적대 계층은 도메인 분류 손실을 최소화하여 도메인 불변 표현을 생성하도록 학습했습니다. 도메인 분류 손실은 소스 도메인과 타깃 도메인을 구분하도록 학습된 도메인 분류기의 출력을 기반으로 계산되었습니다. 도메인 분류 손실을 최소화함으로써 도메인 적대적 계층은 도메인 분류기로 구분할 수 없는 도메인 불변형 표현을 생성하는 방법을 학습했습니다. 이를 통해 DAL은 소스 도메인과 타깃 도메인 간의 도메인 갭을 효과적으로 줄이고 SER 모델의 성능을 개선했습니다. 제안된 방법은 두 개의 벤치마크 데이터 세트에서 평가되었으며, 실험 결과 화자 독립적 SER에서 여러 최신 방법보다 성능이 뛰어난 것으로 나타났습니다. Explain how does DAL work in this paper with generator and discriminator 본 논문에서 제안한 방법에서는 화자 독립적 음성 감정 인식(SER)에서 소스 도메인과 타깃 도메인 간의 도메인 간극을 줄이기 위해 도메인 적대적 계층(DAL)을 사용했습니다. DAL은 생성기-판별기 프레임워크로 구현되었으며, 생성기는 특징 추출기, 판별기는 도메인 적대적 계층 3으로 구성되었습니다. 훈련 과정에서 제너레이터(특징 추출기)는 예측된 감정 레이블과 지상 진실 레이블 간의 교차 엔트로피 손실인 작업별 손실을 최소화하도록 훈련되었습니다. 동시에 판별자(도메인 적대 레이어)는 예측된 도메인 레이블과 기준값 도메인 레이블 간의 이진 교차 엔트로피 손실인 도메인 분류 손실을 최소화하여 도메인 불변 표현을 생성하도록 훈련되었습니다. 도메인 적대적 계층은 라벨 판별자, 소스-대상 도메인 판별자, 화자 판별자라는 세 가지 하위 판별자로 구성되었습니다. 레이블 판별기는 입력 음성 샘플의 감정 레이블을 예측하도록 훈련되었고, 소스-타겟 도메인 판별기는 소스 도메인과 타겟 도메인을 구분하도록 훈련되었습니다. 화자 판별기는 소스 도메인에서 서로 다른 화자를 구별하도록 훈련되었습니다. 생성기와 판별기가 적대적인 방식으로 훈련되도록 하기 위해 생성기와 판별기 사이에 그라디언트 반전 레이어(GRL)를 추가했습니다. GRL은 역전파 중에 기울기를 반전시켜 제너레이터가 판별기가 구별할 수 없는 도메인 불변 표현을 생성할 수 있도록 했습니다. 제너레이터와 판별기를 적대적 방식으로 훈련함으로써 DAL은 소스 도메인과 타깃 도메인 간의 도메인 갭을 효과적으로 줄이고 SER 모델의 성능을 개선했습니다. 제안된 방법은 두 개의 벤치마크 데이터 세트에서 평가되었으며, 실험 결과 화자 독립적 SER에서 여러 최신 방법보다 성능이 뛰어난 것으로 나타났습니다. D. DIFL for Speaker Independent SERThe DIFL framework effectively combines the hierarchical representation, hierarchical alignment, and domain adversarial approach into the deep network, which can be extended to any DCNNs. 이 논문에서 제안한 DIFL(도메인 불변 특징 학습) 방법은 계층적 표현, 계층적 정렬 및 도메인 적대적 접근 방식을 심층 네트워크에 결합하여 다중 소스 UDA의 관점에서 화자 독립적 SER을 처리합니다. DIFL 방식은 계층적 정렬 계층이 있는 특징 추출기 블록과 감정, 화자, 도메인의 여러 판별자가 있는 판별자 블록의 두 가지 주요 블록으로 구성됩니다. 특징 추출기 블록은 입력 음성 샘플에서 높은 수준의 의미적 특징을 추출하는 역할을 담당합니다. 여러 계층으로 구성된 심층 신경망으로 구성되며, 각 계층은 입력 특징의 계층적 표현을 학습합니다. 계층적 정렬 레이어가 특징 추출기 블록에 추가되어 소스 도메인과 대상 도메인을 서로 다른 추상화 수준에서 정렬합니다. 계층적 정렬 레이어는 여러 하위 레이어로 구성되며, 각 하위 레이어는 소스 도메인과 대상 도메인을 특정 추상화 수준에서 정렬합니다. 판별자 블록은 도메인 불변 특징 학습을 촉진하는 역할을 담당합니다. 감정 판별자, 화자 판별자, 도메인 판별자 등 여러 판별자로 구성됩니다. 감정 판별자는 입력 음성 샘플의 감정 레이블을 예측하도록 훈련되며, 화자 판별자는 소스 도메인에서 서로 다른 화자를 구별하도록 훈련됩니다. 도메인 판별기는 소스 도메인과 대상 도메인을 구분하도록 훈련됩니다. 도메인 불변 특징 학습을 촉진하기 위해 DIFL 방법은 최대 평균 불일치(MMD) 메트릭을 사용하여 소스 도메인과 대상 도메인 간의 거리를 측정합니다. MMD 메트릭은 소스 도메인과 대상 도메인 간의 도메인 이동을 최소화하고 적대적 도메인 적응을 촉진하는 데 사용됩니다. 도메인 및 화자 판별자는 점진적으로 도메인 독립적인 특징을 얻기 위해 활용됩니다. 또한 DIFL의 레이블 판별자는 특징의 판별 가능성도 보장할 수 있습니다. 계층적 표현, 계층적 정렬 및 도메인 적대적 접근 방식을 결합함으로써 DIFL 방법은 소스 도메인과 대상 도메인 간의 도메인 격차를 효과적으로 줄이고 SER 모델의 성능을 향상시킬 수 있습니다. 제안된 방법은 두 개의 벤치마크 데이터 세트에서 평가되었으며, 실험 결과 화자 독립적 SER에서 여러 최신 방법보다 성능이 우수한 것으로 나타났습니다. MK-MMD MK-MMD는 다중 커널 최대 평균 불일치의 약자입니다. 두 확률 분포 사이의 거리를 측정하는 데 사용되는 메트릭입니다. 이 논문에서는 화자 독립적 음성 감정 인식(SER) 5에서 소스 도메인과 대상 도메인의 특징 분포 간의 불일치를 측정하는 데 MK-MMD를 사용합니다. MMD(최대 평균 불일치)는 커널 기반 메트릭으로, 고차원 커널 힐버트 공간(RKHS)에서 두 확률 분포 사이의 거리를 측정합니다. MK-MMD는 도메인 간의 불일치를 측정하기 위해 양의 반정확(PSD) 커널의 일련의 선형 조합을 사용하여 MMD를 확장합니다. 이 논문에서는 가우시안 커널(즉, RBF 커널)을 MK-MMD의 커널 함수로 사용합니다. MK-MMD는 도메인 이동을 줄이고 적대적 도메인 적응을 촉진하기 위해 제안된 DIFL(도메인 불변 특징 학습) 방법의 특징 학습 단계에 사용됩니다. 특히 MK-MMD는 다양한 화자에 의해 발생하는 특징 분포의 거리를 측정하는 데 사용되며, 이를 통해 소스-타겟 도메인 간의 불일치를 줄이고 특징 정렬 중 감정 판별 성능 저하를 줄일 수 있습니다. 이를 통해 MK-MMD는 소스 도메인과 타겟 도메인 간의 도메인 갭을 효과적으로 줄이고 SER 모델의 성능을 향상시킬 수 있습니다. GRL, UDA, DIFL의 관계 논문에서 제안한 DIFL(도메인 불변 특징 학습) 방법에서는 심층 신경망에서 도메인 적대적 접근 방식을 구현하기 위해 그라디언트 반전 레이어(GRL)를 사용합니다. GRL은 화자 독립적 음성 감정 인식(SER) 에서 소스 도메인과 타겟 도메인 간의 도메인 격차를 줄이는 데 사용되는 비지도 도메인 적응(UDA) 프레임워크의 핵심 구성 요소입니다. GRL은 DIFL 방식에서 특징 추출기 블록과 판별기 블록 사이에 추가됩니다. 순방향 전파 중에 GRL은 입력을 변경하지 않고 유지하며, 역전파 중에는 기울기를 반전시켜 도메인 불변 특징 학습을 촉진합니다. GRL을 사용하면 특징 추출기 블록이 판별자 블록으로 구별할 수 없는 도메인 불변형 표현을 생성할 수 있습니다. DIFL 방법의 UDA 프레임워크는 최대 평균 불일치(MMD) 메트릭을 사용하여 구현됩니다. MMD 메트릭은 소스 도메인과 대상 도메인의 특징 분포 사이의 거리를 측정하는 데 사용됩니다. MMD 메트릭을 최소화함으로써 DIFL 방법은 소스 도메인과 타깃 도메인 간의 도메인 갭을 줄이고 적대적 도메인 적응을 촉진할 수 있습니다. 딥 뉴럴 네트워크는 DIFL 방법에서 GRL과 UDA를 결합하여 소스 도메인과 타깃 도메인 간의 도메인 이동에 강한 도메인 불변 특징을 학습할 수 있습니다. 이를 통해 DIFL 방법은 소스 도메인과 타겟 도메인 간의 도메인 갭을 효과적으로 줄이고 SER 모델의 성능을 향상시킬 수 있습니다. 제안된 방법은 두 개의 벤치마크 데이터 세트에서 평가되었으며, 실험 결과 화자 독립적 SER에서 여러 최신 방법보다 성능이 뛰어난 것으로 나타났습니다. IV. EXPERIMENTSA. Speech Emotion Databases Emo-DB: The Berlin Database of Emotional Speech (Emo-DB) is a German-language database containing speech samples from 10 actors (5 male, 5 female) expressing 7 different emotions (anger, boredom, disgust, fear, happiness, sadness, and neutral). The database consists of 535 utterances with a total duration of approximately 1 hour. eNTERFACE: The eNTERFACE database is an English-language audio-visual emotion database containing speech samples from 44 subjects from different nationalities expressing 6 basic emotions (anger, disgust, fear, happiness&#x2F;joy, sadness, and surprise). The database consists of 1,290 English sentences extracted from videos with a sampling rate of 48 kHz. CASIA: The Chinese Academy of Sciences Institute of Automation (CASIA) emotional speech database is an acted Chinese-language database containing speech samples from four actors (2 male and 2 female) expressing six different emotional states (anger, fear, happiness, neutral, sadness and surprise). The database consists of 1200 public sentences with a sampling rate of 16 kHz. B. Experimental Setting1) 데이터 전처리실험에 사용된 멜-스펙트로그램은 음성 신호에 대해 512개의 샘플 포인트와 절반이 겹치는 창 길이로 STFT를 수행한 후 사람의 청각 지각에 더 근접한 멜-필터 뱅크를 통과하여 얻은 것으로, 멜-필터 번호는 80입니다. 입력 스펙트로그램의 일관성을 보장하기 위해 사용된 데이터베이스의 샘플링 속도를 16kHz로 정규화하고 단일 채널 데이터를 가져옵니다. 또한 각 데이터베이스의 음성 길이도 정규화하여 전체 훈련 세트 샘플의 평균값에 표준 편차를 더한 값을 균일한 길이로 선택하고, 길이가 부족한 각 문장은 0으로 채우고 긴 문장은 세그먼트 단위로 잘라냅니다. 2) 실험 프로토콜제안한 방법의 성능을 평가하기 위해 [9], [11], [35]에서 제안한 바와 같이 화자 독립적 SER 실험에서 LOSO(Leave-One-Speaker-Out) 교차 검증 전략을 채택합니다. 특히, eNTERFACE 데이터베이스에는 44명의 화자가 포함되어 있지만, 6번째 샘플은 컷이 없는 동영상입니다. 따라서 본 실험에서는 6번째 샘플을 제외한 43개의 피사체와 1,287개의 비디오 클립을 사용합니다. 또한 실험과 평가를 보다 효율적으로 수행하기 위해 화자 독립적 SER [11], [56]의 두 벤치마크 연구에 따라 LOSGO(Leave-One-Speaker-Group-Out) 전략을 채택했습니다. 따라서 eNTERFACE의 화자는 5개의 화자 그룹으로 나뉘고, Emo-DB와 CASIA는 각각 10개의 화자와 4개의 화자 그룹으로 나뉘며, 또한 널리 사용되는 두 가지 평가 기준[11], 즉 가중 평균 회상(WAR)과 가중 평균 회상(UAR)을 채택하여 SER의 인식 성능을 평가하는데, WAR은 ‘정상’ 인식 정확도로 사용되는 반면 UAR은 클래스별 정확도(즉, 클래스당 회상)를 클래스 수로 나눈 값을 반영합니다. 화자와 독립적인 SER에서는 데이터베이스의 감정 카테고리가 불균형하기 때문에 UAR은 모델의 성능을 보다 가시적으로 측정합니다. LOSO &amp; LOSGO LOSO는 데이터 집합을 여러 개의 하위 집합으로 나누고 교차 검증을 반복할 때마다 하나의 하위 집합을 테스트 집합으로 사용하고 나머지 하위 집합을 훈련 집합으로 사용하는 Leave-One-Speaker-Out의 약자입니다. 이 전략은 보이지 않는 스피커에서 모델의 성능을 평가하는 데 사용됩니다. LOSGO는 Leave-One-Speaker-Group-Out의 약자로, 데이터 집합을 여러 화자 그룹으로 나누고 교차 검증의 각 반복에서 하나의 화자 그룹을 테스트 집합으로 사용하고 나머지 화자 그룹을 훈련 집합으로 사용합니다. 이 전략은 보이지 않는 화자 그룹에 대한 모델의 성능을 평가하고 화자 가변성으로 인한 편향을 줄이는 데 사용됩니다. 본 논문에서는 제안된 DIFL(도메인 불변 특징 학습) 방법의 성능을 평가하기 위해 LOSO와 LOSGO 교차 검증 전략을 모두 사용하여 eNTERFACE, Emo-DB 및 CASIA 데이터베이스에 대한 성능을 평가합니다. 그 결과 DIFL 방법이 화자 독립적 SER에서 여러 최신 방법보다 성능이 우수하다는 것을 보여줍니다. WAR, UAR Page 7에 설명된 대로 WAR과 UAR은 음성 감정 인식(SER) 실험에 널리 사용되는 두 가지 평가 기준입니다. WAR은 가중 평균 회상률을 의미하며, 다중 클래스 분류 문제에 일반적으로 사용되는 지표입니다. SER에서 WAR은 ‘정상’ 인식 정확도로 사용되며, 이는 각 클래스의 샘플 수에 따라 가중치를 부여한 모든 감정 클래스의 평균 회상률입니다. WAR은 데이터 세트의 모든 감정을 인식하는 모델의 전반적인 성능을 측정합니다. UAR은 가중치 없는 평균 회상률을 의미하며, SER 실험에 사용되는 또 다른 지표입니다. UAR은 추가된 클래스별 정확도(즉, 클래스당 회상률)를 클래스 수로 나눈 값을 반영합니다. 화자에 독립적인 SER에서는 데이터베이스의 감정 카테고리가 불균형하기 때문에 UAR은 모델의 성능을 보다 통찰력 있게 측정합니다. UAR은 각 클래스의 샘플 수를 고려하지 않고 모든 감정 클래스의 평균 리콜을 측정합니다. UAR은 과소 대표되는 감정 클래스에 대한 모델의 성능을 평가하는 데 유용한 지표입니다. 본 논문에서는 제안된 DIFL(도메인 불변 특징 학습) 방법의 인식 성능을 평가하기 위해 WAR과 UAR을 모두 사용하여 eNTERFACE, Emo-DB, CASIA 데이터베이스에 대한 인식 성능을 평가합니다. 그 결과, DIFL 방식이 여러 최신 방식과 비교하여 WAR과 UAR 모두에서 상당한 개선을 달성하는 것으로 나타났습니다. 일반적으로 WAR과 UAR이 높을수록 데이터 세트에서 감정을 인식하는 모델의 성능이 더 우수하다는 것을 나타냅니다. 그러나 WAR 및 UAR의 해석은 특정 데이터 세트와 당면한 작업에 따라 달라진다는 점에 유의해야 합니다. 예를 들어, 일부 감정 클래스가 다른 클래스보다 샘플 수가 현저히 적은 불균형 데이터 세트의 경우, 모델이 다수 클래스에 편향된 경우 높은 WAR이 반드시 좋은 성능을 나타내는 것은 아닐 수 있습니다. 이러한 경우 UAR은 각 클래스의 샘플 수를 고려하지 않고 모든 감정 클래스의 평균 리콜을 측정하여 모델의 성능을 보다 통찰력 있게 평가할 수 있습니다. 또한 평가 메트릭의 선택은 특정 애플리케이션과 작업의 요구 사항에 따라 달라집니다. 예를 들어, 일부 애플리케이션에서는 전체 WAR 또는 UAR을 최적화하는 것보다 특정 감정 클래스에 대해 높은 정밀도 또는 높은 리콜을 달성하는 것이 더 중요할 수 있습니다. 따라서 WAR과 UAR은 SER 실험에서 모델의 성능을 평가하는 데 유용한 지표이지만 데이터 세트 특성, 작업 요구 사항 및 모델의 한계와 같은 다른 요소도 고려하는 것이 중요합니다. 3) Network Parameters이 논문에서는 음성 감정 인식(SER) 실험을 위해 제안된 DIFL(도메인 불변 특징 학습) 프레임워크의 네트워크 파라미터에 대해 설명합니다. DIFL 프레임워크는 계층적 표현을 위한 baseline 네트워크로 가벼운 VGG 네트워크를 사용하며, 논문 7에서 DIFL_VGG6이라고 합니다. DIFL_VGG6 네트워크는 4개의 컨볼루션 블록과 2개의 완전 연결(fc) 레이어로 구성됩니다. 각 컨볼루션 블록에는 컨볼루션(컨볼루션 커널은 3×3), 배치 정규화, ReLU 및 최대 풀링 연산이 포함되어 있습니다. 4개 레이어의 컨볼루션 커널 수는 각각 64, 128, 256, 512이며, fc의 차원은 4,096입니다. 이 논문에서는 DIFL_VGG6 외에도 DIFL 프레임워크를 다른 두 개의 baseline 네트워크, 즉 VGG19와 VGGish로 확장하여 각각 DIFL_VGG19와 DIFL_VGGish라고 부릅니다. C. Experimental Results and Analysis이 논문에서는 제안된 음성 감정 인식(SER)을 위한 DIFL 프레임워크의 실험 결과를 보고합니다. 이 논문에서는 공개적으로 사용 가능한 세 가지 데이터베이스, 즉 eNTERFACE, Emo-DB, CASIA에서 제안한 방법의 성능을 평가하고 여러 최신 방법과 비교합니다. 이 논문에서는 제안된 방법의 인식 성능을 가중 정확도(WAR)와 무가중 정확도(UAR)라는 두 가지 평가 지표를 통해 보고합니다. WAR은 모든 감정 클래스에서 모델의 평균 정확도를 측정하며, 각 클래스의 샘플 수에 따라 가중치를 부여합니다. UAR은 각 클래스의 샘플 수를 고려하지 않고 모든 감정 클래스에서 모델의 평균 정확도를 측정합니다. 그 결과, 제안된 DIFL 방식이 기준 방식에 비해 WAR과 UAR 모두에서 상당한 개선을 달성한 것으로 나타났습니다. 특히, eNTERFACE 데이터베이스에서 제안된 방법은 80.5%의 WAR과 80.3%의 UAR을 달성하여 최신 방법보다 큰 폭으로 성능이 향상되었습니다. Emo-DB 데이터베이스에서 제안된 방법은 70.6%의 WAR과 70.4%의 UAR을 달성하여 역시 최신 방법보다 우수한 성능을 보였습니다. CASIA 데이터베이스에서 제안된 방법은 70.2%의 WAR과 70.0%의 UAR을 달성하여 최신 방법과 비슷한 수준입니다. 이 논문에서는 실험 결과에 대한 자세한 분석도 제공합니다. 분석 결과, 제안된 DIFL 방법은 도메인 이동에 강하고 보이지 않는 도메인에도 잘 일반화할 수 있는 도메인 불변 특징을 학습하는 데 효과적임을 보여줍니다. 또한 분석 결과 제안된 방법은 SER 작업에서 흔히 발생하는 화자 가변성으로 인한 도메인 이동을 처리하는 데 특히 효과적이라는 것을 보여줍니다. 또한, 분석 결과 제안한 방법이 다양한 감정 클래스의 판별 정보를 포착하고 높은 인식 정확도를 달성할 수 있음을 보여줍니다. 전반적으로 실험 결과와 분석은 제안한 DIFL 프레임워크가 SER 작업에 효과적이며 최신 방법보다 우수하다는 것을 보여줍니다. V. Conclusion이 논문에서는 다중 소스 비지도 도메인 적응(UDA)의 관점에서 음성 감정 인식(SER)을 위한 도메인 불변 특징 학습(DIFL) 프레임워크를 제안합니다. 제안된 DIFL 프레임워크는 도메인 이동에 강하고 보이지 않는 도메인에도 잘 일반화할 수 있는 도메인 불변 특징을 학습하는 것을 목표로 합니다. 이 논문에서는 공개적으로 사용 가능한 세 가지 데이터베이스, 즉 eNTERFACE, Emo-DB, CASIA에서 제안된 방법의 성능을 평가하고 여러 최신 방법과 비교합니다. 실험 결과, 제안한 DIFL 방식이 기준 방식에 비해 가중 정확도(WAR)와 무가중 정확도(UAR) 모두에서 유의미한 개선을 달성한 것으로 나타났습니다. 실험 결과의 분석은 제안된 DIFL 방법이 도메인 이동에 강하고 보이지 않는 도메인에도 잘 일반화할 수 있는 도메인 불변 특징을 학습하는 데 효과적이라는 것을 보여줍니다. 또한 제안된 방법은 SER 작업에서 흔히 발생하는 화자 가변성으로 인한 도메인 이동을 처리하는 데 특히 효과적이라는 것을 분석 결과를 통해 알 수 있습니다. 이 논문은 실험 결과와 분석을 바탕으로 제안된 DIFL 프레임워크가 SER 작업의 도메인 이동 문제를 해결하는 데 효과적이며 공개적으로 사용 가능한 데이터베이스에서 최첨단 성능을 달성할 수 있다고 결론지었습니다. 또한 이 논문은 SER 작업에서 도메인 불변 특징 학습의 중요성을 강조하고 제안된 DIFL 프레임워크가 음성 인식, 화자 확인, 언어 식별과 같은 다른 관련 작업으로 확장될 수 있음을 제안합니다.","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speech Emotion Recognition","slug":"Paper/Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/categories/Paper/Speech-Emotion-Recognition/"}],"tags":[{"name":"Speech Emotion Recognition","slug":"Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/tags/Speech-Emotion-Recognition/"},{"name":"Domain Invariant Feature Learning","slug":"Domain-Invariant-Feature-Learning","permalink":"https://jmj3047.github.io/tags/Domain-Invariant-Feature-Learning/"},{"name":"Domain Adversarial Layer","slug":"Domain-Adversarial-Layer","permalink":"https://jmj3047.github.io/tags/Domain-Adversarial-Layer/"},{"name":"Speaker Independent","slug":"Speaker-Independent","permalink":"https://jmj3047.github.io/tags/Speaker-Independent/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speech Emotion Recognition","slug":"Paper/Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/categories/Paper/Speech-Emotion-Recognition/"}]},{"title":"Introduction to Machine Learning in Production_Quiz","slug":"MLOps1_Quiz","date":"2023-06-28T15:00:00.000Z","updated":"2023-07-13T07:20:20.456Z","comments":true,"path":"2023/06/29/MLOps1_Quiz/","link":"","permalink":"https://jmj3047.github.io/2023/06/29/MLOps1_Quiz/","excerpt":"","text":"개요Coursera ML Ops Course 1 Quiz 1. The Machine Learning Project Lifecycle Link: https://www.coursera.org/learn/introduction-to-machine-learning-in-production/home/week/1 2. Deployment 3. Selecting and Training a Model Link: https://www.coursera.org/learn/introduction-to-machine-learning-in-production/home/week/2 4. Modeling Challenges 5. Data Stage of the ML Production Lifecycle Link: https://www.coursera.org/learn/introduction-to-machine-learning-in-production/home/week/3 6. Scoping(optional)","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"Coursera","slug":"Coursera","permalink":"https://jmj3047.github.io/tags/Coursera/"},{"name":"Quiz","slug":"Quiz","permalink":"https://jmj3047.github.io/tags/Quiz/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}]},{"title":"Introduction to Machine Learning in Production","slug":"MLOps_Part1","date":"2023-06-25T15:00:00.000Z","updated":"2023-07-13T07:16:08.871Z","comments":true,"path":"2023/06/26/MLOps_Part1/","link":"","permalink":"https://jmj3047.github.io/2023/06/26/MLOps_Part1/","excerpt":"","text":"Course Link Lecture 1 in MLOps Overview the key steps involved in a typical machine learning project. It starts with scoping, where the project goals and variables (X and Y) are defined. Data collection follows, including establishing a baseline, labeling, and organizing the data. The next phase is model training, which involves selecting and training the model, as well as conducting error analysis. Iteration is emphasized, with the possibility of updating the model or collecting more data based on error analysis. Before deployment, a final check or audit is recommended to ensure system performance and reliability. Deployment involves writing the necessary software, monitoring the system, and maintaining it. Maintenance may involve further error analysis, model retraining, and incorporating live data feedback to improve the system. The script emphasizes that deployment is not the end but rather the start of ongoing learning and improvement for the system. Example: Speech recognition The challenges and considerations involved in deploying a machine learning model It highlights two major categories of challenges: machine learning&#x2F;statistical issues and software engineering issues. It addresses the concept of concept drift and data drift, which refer to changes in the data distribution and the desired mapping between inputs and outputs. The script also touches upon various software engineering decisions, such as real-time vs batch predictions, cloud vs edge deployment, resource allocation, latency and throughput requirements, logging, and security&#x2F;privacy considerations. It emphasizes the importance of monitoring system performance and adapting to changes in data. Finally, it mentions that the deployment process is ongoing and requires continuous maintenance and updates. Deploy patterns One of the most useful frameworks I have found for thinking about how to deploy a system is to think about deployment not as a 0, 1 is either deploy or not deploy, but instead to design a system thinking about what is the appropriate degree of automation. For example, in visual inspection of smartphones, one extreme would be if there’s no automation, so the human only system. Slightly mode automated would be if your system is running a shadow mode. So your learning algorithms are putting predictions, but it’s not actually used in the factory. So that would be shadow mode. A slightly greater degree of automation would be AI assistance in which given a picture like this of a smartphone, you may have a human inspector make the decision. But maybe an AI system can affect the user interface to highlight the regions where there’s a scratch to help draw the person’s attention to where it may be most useful for them to look. The user interface or UI design is critical for human assistance. But this could be a way to get a slightly greater degree of automation while still keeping the human in the loop. And even greater degree of automation maybe partial automation, where given a smartphone, if the learning algorithm is sure it’s fine, then that’s its decision. It is sure it’s defective, then we just go to algorithm’s decision. But if the learning algorithm is not sure, in other words, if the learning algorithm prediction is not too confident, 0 or 1, maybe only then do we send this to a human. So this would be partial automation. Where if the learning algorithm is confident of its prediction, we go the learning algorithm. But for the hopefully small subset of images where the algorithm is not sure we send that to a human to get their judgment. And the human judgment can also be very valuable data to feedback to further train and improve the algorithm. I find that this partial automation is sometimes a very good design point for applications where the learning algorithms performance isn’t good enough for full automation. And then of course beyond partial automation, there is full automation where we might have the learning algorithm make every single decision. So there is a spectrum of using only human decisions on the left, all the way to using only the AI system’s decisions on the right. And many deployment applications will start from the left and gradually move to the right. And you do not have to get all the way to full automation. You could choose to stop using AI assistance or partial automation or you could choose to go to full automation depending on the performance of your system and the needs of the application. Train focused on modeling Establish a baseline unstructured data and structured data: unstructured data has good Human Level Performance(HLP), while structured data doesn’t and relies on Dictionary data set Prioritizing what to work on To summarize, when prioritizing what to work on, you might decide on the most important categories to work on based on, how much room for improvement there is, such as, compared to human-level performance or according to some baseline comparison. How frequently does that category appear? You can also take into account how easy it is to improve accuracy in that category. For example, if you have some ideas for how to improve the accuracy of speech with car noise, maybe your data augmentation, that might cause you to prioritize that category more highly than some other category where you just don’t have as many ideas for how to improve the system. Then finally, how important it is to improve performance on that category. For example, you may decide that improving performance with car noise is especially important because when you’re driving, you have a stronger desire to do search, especially search on maps and find addresses without needing to use your hands if your hands are supposed to be holding the steering wheel. skewed dataset The learning algorithm with some precision, even the high value of precision is not that useful usually if this recall is so low. here’s a common way of combining precision and recall using this formula, which is called the F_1 score. One intuition behind the F_1 score is that you want an algorithm to do well on both precision and recall, and if it does worse on either precision or recall, that’s pretty bad. F_1 is a way of combining precision and recall that emphasizes whichever of P or R precision or recall is worse. In mathematics, this is technically called a harmonic mean between precision and recall, which is like taking the average but placing more emphasis on whichever is the lower number. If you compute the F_1 score of these two models, it turns out to be 83.4 percent using the formula below here. Model 2 has a very bad recall, so its F_1 score is actually quite low as well and this lets us tell, maybe more clearly that Model 1 appears to be a superior model than Model 2. Performance auditing What is good data?","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}],"tags":[{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Coursera","slug":"Coursera","permalink":"https://jmj3047.github.io/tags/Coursera/"},{"name":"AI Studies","slug":"AI-Studies","permalink":"https://jmj3047.github.io/tags/AI-Studies/"},{"name":"ML Operations","slug":"ML-Operations","permalink":"https://jmj3047.github.io/tags/ML-Operations/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}]},{"title":"Sequence Model_Quiz","slug":"DL5_Quiz","date":"2023-06-18T15:00:00.000Z","updated":"2023-06-19T12:43:37.995Z","comments":true,"path":"2023/06/19/DL5_Quiz/","link":"","permalink":"https://jmj3047.github.io/2023/06/19/DL5_Quiz/","excerpt":"","text":"개요Coursera Deep Learning Course 5 Quiz 1. Recurrent Neural Networks Link: https://www.coursera.org/learn/nlp-sequence-models/home/module/1 2. Natural Language Processing &amp; Word Embeddings Link: https://www.coursera.org/learn/nlp-sequence-models/home/module/2 3.Sequence Models &amp; Attention Mechanism Link: https://www.coursera.org/learn/nlp-sequence-models/home/module/3 4. Transformers Link: https://www.coursera.org/learn/nlp-sequence-models/home/module/4","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"Coursera","slug":"Coursera","permalink":"https://jmj3047.github.io/tags/Coursera/"},{"name":"Quiz","slug":"Quiz","permalink":"https://jmj3047.github.io/tags/Quiz/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}]},{"title":"Domain Invariant Feature Learning for Cross Corpus Speech Emotion Recognition","slug":"DIFL_Cross_Corpus_SER","date":"2023-06-12T15:00:00.000Z","updated":"2023-06-14T05:51:27.335Z","comments":true,"path":"2023/06/13/DIFL_Cross_Corpus_SER/","link":"","permalink":"https://jmj3047.github.io/2023/06/13/DIFL_Cross_Corpus_SER/","excerpt":"","text":"Journal&#x2F;Conference : ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)Year(published year): 2022Author: Yuan Gao, Shogo Okada, Longbiao Wang, Jiaxing Liu, Jianwu DangSubject: Speech Emotion Recognition, Domain Adaptation, Center Loss Domain Invariant Feature Learning for Cross Corpus Speech Emotion Recognition Summary I. Introduction연구의 필요성기존 SER의 접근 방식은 동일한 데이터 셋에서 훈련 및 테스트 됨. 자연환경에서 대규모 주석이 달린 감정 발화를 수집하는 것은 시간이 오래 걸리기 때문에 기존 데이터 세트에는 적은 수의 음성 샘플이 포함되어 있어 강력한 딥러닝 모델을 훈련하기에는 충분하지 않음. 또한 실제 환경에서 음성의 감정 정보는 도메인 정보의 변화로 인해 학습하기 어려움. 따라서 SER 시스템을 미지의 데이터 세트에 적용할 경우에는 인식 성능이 크게 저하되는 경우가 많음. 실제 어플리케이션을 다루기 위해 다양한 데이터 세트를 사용하여 모델을 평가해야 함. 최근 연구에서는 연구자들이 cross corpus SER에서 CNN, RNN, 및 attention의 성능을 평가하기도 함[11,12] contributionSER의 일반화 능력을 더욱 향상 시키기 위해 adversarial domain adaptation 방법을 사용하여 학습 데이터와 테스트 데이터 간의 도메인 차이를 줄였음. 구체적으로 adversarial training을 통해 latent representation의 화자, 코퍼스 및 기타 도메인 정보를 제거함. domain adaptation은 feature extractor와 domain classifier 사이의 gradient를 역전시킴으로써 이루어지며 이를 통해 모델은 비감정적 정보의 학습 손실을 최대화 할 수 있음. 또한 기존 연구에서 일반적으로 사용되는 감정 분류기는 softmax 손실함수를 사용하여 decision boundary를 찾고 감정을 구분함. feature representation에 차별성을 두기 위해 center loss를 통합함, 그리고 그것은 feature extractor를 위한joint supervision처럼 feature representation과 해당 클래스 center의 거리를 최소화 하기 위해 훈련되었음. II. Adversarial Domain Adaptation For Feature ExtractionFig 1에서 볼 수 있듯이, 우리는 특징 추출을 위해 deep CNN과 BLSTM layer을 사용하며, 이 layer의 parameter는 [13]에서 쓰인것과 유사함. 우리는 Domain Adversarial Neural Network(DANN)으로 feature extractor를 조정하였음. 또한 feature representation의 intra-class 변화를 줄이기 위해 center loss를 사용했음. DANN과 center loss 둘다 domain divergence를 해결할수 있음. Domain Adversarial Training이 연구에서는 비감정적인 정보를 제거하기 위해 특징 추출기에 DANN을 통합했음. DANN은 multi-task learning 모델 DANN의 recognition target은 emotion classifier(LE), domain classifier(LD)이다. 본 연구에서 LD의 domain recognition target은 코퍼스, 언어, 성별이다. 하나의 training과정 안에서 domain adaptation과 feature representation 학습을 달성하기 위해 [14]는 domain classifier과 feature extractor 사이에 gradient reversal layer(GRL)을 두었음. GRL은 역전파 과정에서 특정 음의 상수를 도메인 분류 작업의 기울기에 곱할수 있음. 소스 도메인과 타겟 도메인에서 학습한 feature distribution이 우리 모델과 구별되지 않도록 DANN 학습을 시킴. 이렇게 GRL을 통해 domain invariant representation을 추출하여 코퍼스 간 감정인식을 위한 일반화 능력을 향상할수 있음. 제안된 feature extraction 모델의 objective function은 다음과 같음. LE: center loss와 softmax loss를 결합한 emotion classifier의 loss function(자세한 내용은 Center Loss) 이 특정 작업에서는 앞서 언급한 비감정적인 정보를 feature extractor G(x, $\\theta$)가 학습하지 않도록 $\\gamma$를 0.3으로 설정 했음. DANN 학습을 통해 모델은 feature distribution의 domain shift를 제거할 수 있음. domain classifier의 손실함수는 다음과 같이 표현됨 Lg, Ll, Lc는 성별, 언어, 말뭉치의 loss function LE를 최소화 하고 LD를 최대화 하는 안정점을 찾아냄으로써 우리가 제안한 feature extractor는 emotion classifier의 input에서 domain divergence를 크게 줄일 수 있음. Center Loss제안된 feature extractor외에도 softmax loss와 center loss[15]를 joint supervision으로 emotion classifier LE에추가적으로 적용함. softmax loss function은 SER에서 다양한 감정의 decision boundary를 찾기 위해 일반적으로 사용됨. M: 미니 배치의 크기, N: emotion class의 개수. 본 연구에서는 training sample과 test sample에 대해 동일한 emotion annotation을 정의했지만, 서로 다른 데이터 세트의 feature distribution이 분리 가능한 cluster로 나타나지 않았음. 그리고 그것은 cross corpus SER이 일반적인 close-set identification 작업보다 더 어렵게 만듦. 이 문제를 해결하기 위해서 center loss를 도입하여 각 감정 카테고리에 대한 class center c를 학습함으로써 feature distribution의 클래스 내(intra-class) 거리를 줄였음. 이 loss function은 input feature와 해당 class center 사이의 유클리드 거리로 계산됨. Class center c를 보다 효율적으로 업데이트 하기 위해 loss function은 각 미니배치에 대해서 학습되었음. 5번 식에서 m은 새 미니배치에 있는 클래스 i의 샘플 개수. 감정 분류기 전체의 objective function은 다음식으로 정의됨 각 loss term의 가중치를 control하기 위해 $\\lambda$를 0.5로 설정. center loss와 softmax loss를 결합하여 모델을 동통으로 최적화 함으로써 cross corpus SER 작업을 위한 robust한 feature representation을 추출할 수 있음. III. Experimental SetupEmotional Speech Dataset네가지 감정 코퍼스를 사용해 모델을 평가합니다: IEMOCAP, MSP-IMPROV, SAVEE, Emo-DB IEMOCAP: 오디오, 비디오, 얼굴 모션 정보를 포함한 12시간 분량의 시청각 데이터와 10명의 화자와 텍스트 필사본이 포함되어 있음. 실험에는 스크립트 데이터와 즉흥 데이터 모두 5531 발화를 사용. 행복, 슬픔, 분노, 중립의 감정이 기록돼 있음. MSP-IMPROV: 다이나믹 세션에서 상호작용하는 배우로부터 기록된 다중모드 감정 데이터 베이스. 12명의 배우러부터 녹음된 8438개의 감정 문장 발화로 구성되어 있음. 행복, 슬픔, 분노, 중립의 감정 카테고리 SAVEE: 남성 피험자 4명의 audio-visual 녹음을 포함하고 있음. 480개의 원어민 영어 발화로 구성되어 있고 행복, 슬픔, 혐오, 분노, 지루함, 두려움인 6가지 감정에 대해서 60개, 중립에 대해 120개의 발화가 포함됨 Emo-DB: 10명의 전문 배우가 녹음환경에서 연기. 배우들은 각 문장을 7가지 감정 상태(중립, 지루함, 혐오, 슬픔, 분노, 행복, 두려움)로 표현함. 총 535개 발화 Experimental Settings두가지 검증 체계를 사용하여 모델을 평가 cross-corpus evaluation: 모델은 IEMOCAP에 대해서만 훈련하고 나머지는 세개의 말뭉치에서 테스트 Multi-corpus evaluation: 네개의 데이터 세트를 모두 train set(80%)와 test set(20%)으로 나누고 각 코퍼스의 테스트 데이터를 사용하여 모델을 평가. train set과 test set은 화자가 겹치지 않음. optimizer로는 adadelta를 사용했으며 미니배치사이즈는 128. 전처리 과정에서 모든 데이터는 16kHz로 다운샘플링 하였음. input feature로는 spectrogram을 사용하였고 input 발화는 265ms로 분할되며 각 세그먼트에 대해 25ms의 프레임 size로 input spectrogram이 계산됨. input spectrogram의 time X frequency는 32 X 129. IV. Results and AnalysisUnweighted Accuracy(UA)를 평가기준으로 선택 baseline: CNN + BLSTM의 조합. 제안된 두개의 DANN-based approaches 들을 비교 DANN_1: domain classifier의 인식 대상이 화자와 corpus DANN_2: speaker classification이 언어와 성별 인식으로 대체 C: center loss, S: softmax loss multi corpus 실험 결과, domain recognition 대상은 DANN 1의 경우 화자와 코퍼스이고 DANN 2의 경우 성별, 언어, 말뭉치. Multi- corpus Evaluation표2에서 다중 코퍼스 평가 결과를 제시. arousal 인식의 경우 DANN 2는 비교 실험보다 작지만 꾸준한 개선으로 네가지 데이터 세트 모두에서 가장 우수한 성능을 달성. valence의 경우 대부분의 비교 실험은 Emo DB에서 낮은 성능을 보였음. EmoDB 훈련세트는 주로 negative input으로 이루어져 있음. 또한 EmoDB와 다른 데이터 세트의 언어 불일치로 인해 이 데이터세트의 인식 성능은 상대적으로 낮음. 이러한 상황임에도 불구하고 모델은 positive, negative에 대해 비교적 동등한 인식 정확도를 보였고 UA를 10.45%나 향상 시켰음. 또한 제안된 center loss는 모델이 더 많은 discriminative feature representation을 추출하고 평균 정확도를 3.28%나 향상 시킴. 결과는 제안된 모델이 데이터 세트 전반에서 감정 정보를 일반화 할 수 있음을 보여줌 Cross-corpus Evaluation교차 코퍼스 평가결과는 제안된 모델의 효과를 입증함. 표3과 4에서 볼수 있듯이 DANN 기반 모델의 평균 성능은 arousal에서 baseline에 비해 크게 향상된 것으로 나타났음. 또한 네가지 데이터 세트에는 화자 수가 많기 때문에 화자 인식은 이 작업에서 높은 정확도를 달성하기 어려움. 따라서 DANN_2가 DANN_1보다 더 나은 평균 성능을 생성함. 그러나 valence 인식 같은 경우 DANN과 baseline모두 상대적으로 낮은 성능(60%미만)을 보였음. Emo DB의 valence인식의 경우 언어 불일치로 인해 네가지 비교 실험 인식 성능이 확률 수준(chance level) 이하임. 이러한 결과는 DANN 학습이 Valence 인식의 경우에 더 달성하기 어렵다는 것을 나타내며 이는 [20]에도 자세하게 나와있음. V. Conclusion이 논문에서 cross corpus SER 시스템의 일반화 능력을 높이기 위한 adversarial domain adaptation 과 center loss에 대해 조사했음. SER의 domain invariant feature learning의 단계로 특징 추출을 DANN으로 수정하고 다른 데이터 세트에서 도메인 차이를 줄였음. 또한 감정인식을 위한 discriminative feature representation을 학습하기 위해 center loss와 softmax loss function을 통합함. 실험 결과에 따르면 arousal에 비해 deep learning 모델은 valence information을 일반화 시키는게 더 어려움. 제안된 모델은 기존의 딥러닝 기반 모델보다 더 유명한 평균 결과를 달성하여 효과를 입증함.","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speech Emotion Recognition","slug":"Paper/Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/categories/Paper/Speech-Emotion-Recognition/"}],"tags":[{"name":"Speech Emotion Recognition","slug":"Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/tags/Speech-Emotion-Recognition/"},{"name":"Adversarial Domain Adaptation","slug":"Adversarial-Domain-Adaptation","permalink":"https://jmj3047.github.io/tags/Adversarial-Domain-Adaptation/"},{"name":"Center Loss","slug":"Center-Loss","permalink":"https://jmj3047.github.io/tags/Center-Loss/"},{"name":"Domain Invariant Feature Learning","slug":"Domain-Invariant-Feature-Learning","permalink":"https://jmj3047.github.io/tags/Domain-Invariant-Feature-Learning/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speech Emotion Recognition","slug":"Paper/Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/categories/Paper/Speech-Emotion-Recognition/"}]},{"title":"Sequence Model","slug":"DL_Part5","date":"2023-06-02T15:00:00.000Z","updated":"2023-06-19T12:48:18.552Z","comments":true,"path":"2023/06/03/DL_Part5/","link":"","permalink":"https://jmj3047.github.io/2023/06/03/DL_Part5/","excerpt":"","text":"Course Link Lecture 5 in Deep Learning RNN there is one-to-many. So, this was a music generation or sequenced generation as example. And then, there’s many-to-one, that would be an example of sentiment classification. Where you might want to read as input all the text with a movie review. And then, try to figure out that they liked the movie or not. There is many-to-many, so the name entity recognition, the example we’ve been using, was this where Tx is equal to Ty. And then, finally, there’s this other version of many-to-many, where for applications like machine translation, Tx and Ty no longer have to be the same. What is language modeling? given any sentence, its job is to tell you what is the probability of that particular sentence, and by probability of sentence, I mean, if you were to pick up a random newspaper, open a random email, or pick a random webpage, what a language model does is it estimates the probability of that particular sequence of words. Gated Recurrent Unit very effective solution for addressing the vanishing gradient problem and will allow your neural network to capture much longer range dependencies This is another gate Gamma r. You can think of r as standing for relevance. This gate Gamma r tells you how relevant is C^t minus 1 to computing the next candidate for C^t. This gate Gamma r is computed pretty much as you expect with a new parameter matrix w _r, and then the same things as input x_t plus b_r. over many years, researchers have experimented with many different possible versions of how to design these units to try to have longer range connections. To try to have model long-range effects and also address vanishing gradient problems. The GRU is one of the most commonly used versions that researchers have converged to and then found as robust and useful for many different problems. LSTM one element of this is interesting is if you hook up a bunch of these in parallel so that’s one of them and you connect them, connect these temporarily. So there’s the input x 1, then x 2, x 3. So you can take these units and just hook them up as follows where the output a for a period of time, 70 input at the next time set. And similarly for C and I’ve simplified the diagrams a little bit at the bottom. And one cool thing about this, you notice is that this is a line at the top that shows how so long as you said the forget and the update gates, appropriately, it is relatively easy for the LSTM to have some value C0 and have that be passed all the way to the right to have, maybe C3 equals C0. And this is why the LSTM as well as the GRU is very good at memorizing certain values. Even for a long time for certain real values stored in the memory cells even for many, many times steps. peephole connection: one common variation you see of LSTMs So that’s it for the LSTM, as you can imagine, there are also a few variations on this that people use. Perhaps the most common one, is that instead of just having the gate values be dependent only on a t-1, xt. Sometimes people also sneak in there the value c t -1 as well. This is called a peephole connection. if you see peephole connection, what that means is that the gate values may depend not just on a t-1 but and on x t but also on the previous memory cell value. And the peephole connection can go into all three of these gates computations. GRU vs LSTM the advantage of the GRU is that it’s a simpler model. And so it’s actually easier to build a much bigger network only has two gates, so computation runs a bit faster so it scales the building, somewhat bigger models. But the LSTM is more powerful and more flexible since there’s three gates instead of two. If you want to pick one to use, I think LSTM has been the historically more proven choice. So if you had to pick one, I think most people today will still use the LSTM as the default first thing to try. Bidirectional RNN you can have a model that uses RNN, or GRU, LSTM, and is able to make predictions anywhere even in the middle of the sequence, but take into account information potentially from the entire sequence. The disadvantage of the bidirectional RNN is that, you do need the entire sequence of data before you can make predictions anywhere. So, for example, if you’re building a speech recognition system then BRNN will let you take into account the entire speech other friends. But if you use this straightforward implementation, you need to wait for the person to stop talking to get the entire utterance before you can actually process it, and make a speech recognition prediction. So the real time speech recognition applications, there is somewhat more complex models as well rather than just using the standard by the rational RNN as you’re seeing here. But for a lot of natural language processing applications where you can get the entire sentence all at the same time, the standard BRNN and algorithm is actually very effective. NLP word embeddings this is how you can carry out transfer learning using word embeddings. Step 1 is to learn word embeddings from a large text corpus, a very large text corpus or you can also download pre-trained word embeddings online. There are several word embeddings that you can find online under very permissive licenses. And you can then take these word embeddings and transfer the embedding to new task, where you have a much smaller labeled training sets. And use this, let’s say, 300 dimensional embedding, to represent your words. One nice thing also about this is you can now use relatively lower dimensional feature vectors. So rather than using a 10,000 dimensional one-hot vector, you can now instead use maybe a 300 dimensional dense vector. Although the one-hot vector is fast and the 300 dimensional vector that you might learn for your embedding will be a dense vector. And then, finally, as you train your model on your new task, on your named entity recognition task with a smaller label data set, one thing you can optionally do is to continue to fine tune, continue to adjust the word embeddings with the new data. In practice, you would do this only if this task 2 has a pretty big data set. If your label data set for step 2 is quite small, then usually, I would not bother to continue to fine tune the word embeddings. Word2Vec you saw how the Skip-Gram model allows you to construct a supervised learning task. So we map from context to target and how that allows you to learn a useful word embedding. But the downside of that was the Softmax objective was slow to compute. Negative Sampling technique is called negative sampling because what you’re doing is, you have a positive example, the orange and then juice. And then you will go and deliberately generate a bunch of negative examples, negative samplings, hence, the name negative sampling, with which to train four more of these binary classifiers. And on every iteration, you choose four different random negative words with which to train your algorithm on. To summarize, you’ve seen how you can learn word vectors in a Softmax classier, but it’s very computationally expensive. And in this video, you saw how by changing that to a bunch of binary classification problems, you can very efficiently learn words vectors. And if you run this algorithm, you will be able to learn pretty good word vectors. Now of course, as is the case in other areas of deep learning as well, there are open source implementations. And there are also pre-trained word vectors that others have trained and released online under permissive licenses. And so if you want to get going quickly on a NLP problem, it’d be reasonable to download someone else’s word vectors and use that as a starting point. Attention greedy search greedy search is an algorithm from computer science which says to generate the first word just pick whatever is the most likely first word according to your conditional language model. Going to your machine translation model and then after having picked the first word, you then pick whatever is the second word that seems most likely, then pick the third word that seems most likely. it turns out that the greedy approach, where you just pick the best first word, and then, after having picked the best first word, try to pick the best second word, and then, after that, try to pick the best third word, that approach doesn’t really work. Because ‘going’ is much more common word than ‘visiting’ so if you use greedy search to translate, ‘going’ has higher possibility to be chosen. However the best choice of translation is the first sentence. one major difference between this and the earlier language modeling problems is rather than wanting to generate a sentence at random, you may want to try to find the most likely English sentence, most likely English translation. But the set of all English sentences of a certain length is too large to exhaustively enumerate. So, we have to resort to a search algorithm. Beam Search with a beam of three being searched considers three possibilities at a time. Notice that if the beam width was said to be equal to one, say cause there’s only one, then this essentially becomes the greedy search algorithm which we had discussed in the last video but by considering multiple possibilities say three or ten or some other number at the same time beam search will usually find a much better output sentence than greedy search. how do you choose the beam width? Share the pros and cons of setting beam to be very large versus very small. If the beam width is very large, then you consider a lot of possibilities and so you tend to get a better result because you’re consuming a lot of different options, but it will be slower. The memory requirements will also grow and also be computationally slower. Whereas if you use a very small beam width, then you get a worse result because you are just keeping less possibilities in mind as the algorithm is running, but you get a result faster and the memory requirements will also be lower. I would say try out a variety of values of beam as see what works for your application, but when beam is very large, there is often diminishing returns. For many applications, I would expect to see a huge gain as you go from beam of one, which is basically research to three to maybe 10, but the gains as you go from the thousands of thousand beam width might not be as big. even though y* is a better translation, the RNN ascribed y* in lower probability than the inferior translation. So in this case, I will say the RNN model is at fault. So the error analysis process looks as follows. You go through the development set and find the mistakes that the algorithm made in the development set. through this process, you can then carry out error analysis to figure out what fraction of errors are due to beam search versus the RNN model. if you find that beam search is responsible for a lot of errors, then maybe is we’re working hard to increase the beam width. Whereas in contrast, if you find that the RNN model is at fault, then you could do a deeper layer of analysis to try to figure out if you want to add regularization, or get more training data, or try a different network architecture, or something else. Bleu score One of the challenges of machine translation is that, given a French sentence, there could be multiple English translations that are equally good translations of that French sentence. So how do you evaluate a machine translation system if there are multiple equally good answers, unlike, say, image recognition where there’s one right answer? You just measure accuracy. If there are multiple great answers, how do you measure accuracy? The way this is done conventionally is through something called the BLEU score. Attention Model it’s for long sentece translation because encoder-decoder algorithm is hard to remember whole sentence if it is too long. Speech Recognition how do you build a speech recognition system? One method that seems to work well is to use the CTC cost for speech recognition. CTC stands for Connection is Temporal Classification the basic rule for the CTC cost function is to collapse repeated characters not separated by “blank”. So, to be clear, I’m using this underscore to denote a special blank character and that’s different than the space character Trigger word system Transformer The major innovation of the transformer architecture is combining the use of attention based representations and a CNN convolutional neural network style of processing. Self Attention the main difference is that for every word, say for l’Afrique, you have three values called the query, key, and value. These vectors are the key inputs to computing the attention value for each words. what are these query key and value vectors supposed to do? They were indeed using a loose analogy to a concert and databases where you can have queries and also key-value pairs. To recap, associated with each of the five words you end up with a query, a key, and a value. The query lets you ask a question about that word, such as what’s happening in Africa. The key looks at all of the other words, and by the similarity to the query, helps you figure out which words gives the most relevant answer to that question. In this case, visite is what’s happening in Africa, someone’s visiting Africa. Then finally, the value allows the representation to plug in how visite should be represented within A^3, within the representation of Africa. This allows you to come up with a representation for the word Africa that says this is Africa and someone is visiting Africa. multi-head attention Transformer","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Coursera","slug":"Coursera","permalink":"https://jmj3047.github.io/tags/Coursera/"},{"name":"AI Studies","slug":"AI-Studies","permalink":"https://jmj3047.github.io/tags/AI-Studies/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}]},{"title":"Convolutional Neural Networks_Quiz","slug":"DL4_Quiz","date":"2023-05-20T15:00:00.000Z","updated":"2023-06-22T15:28:27.227Z","comments":true,"path":"2023/05/21/DL4_Quiz/","link":"","permalink":"https://jmj3047.github.io/2023/05/21/DL4_Quiz/","excerpt":"","text":"개요 Coursera Deep Learning Course 4 Quiz 1. The Basics of ConvNets Link: https://www.coursera.org/learn/neural-networks-deep-learning/home/module/1 2.Deep Convolutional Models Link: https://www.coursera.org/learn/convolutional-neural-networks/home/module/2 3. Detection Algorithms Link: https://www.coursera.org/learn/convolutional-neural-networks/home/module/3 4. Special Applications: Face Recognition &amp; Neural Style Transfer Link: https://www.coursera.org/learn/convolutional-neural-networks/home/module/4","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"Coursera","slug":"Coursera","permalink":"https://jmj3047.github.io/tags/Coursera/"},{"name":"Quiz","slug":"Quiz","permalink":"https://jmj3047.github.io/tags/Quiz/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}]},{"title":"ARIMA model","slug":"ARIMA","date":"2023-05-15T15:00:00.000Z","updated":"2023-05-17T11:51:58.842Z","comments":true,"path":"2023/05/16/ARIMA/","link":"","permalink":"https://jmj3047.github.io/2023/05/16/ARIMA/","excerpt":"","text":"정상성(stationarity) 시계열은 시계열의 특징이 해당 시계열이 관측된 시간에 무관 추세나 계절성이 있는 시계열은 정상성을 나타내는 시계열이 아님 → 추세와 계절성은 서로 다른 시간에 시계열의 값에 영향을 줄 것이기 때문. 백색 잡음(white noise) 시계열: 정상성을 나타내는 시계열 → 언제 관찰하는지에 상관이 없고 시간에 따라 어떤 시점에서 보더라도 똑같이 보일것이기 때문. (a) 200 거래일 동안의 구글 주식 가격; (b) 200 거래일 동안의 구글 주식 가격의 일일 변동; (c) 미국의 연간 파업 수; (d) 미국에서 판매되는 새로운 단독 주택의 월별 판매액; (e) 미국에서 계란 12개의 연간 가격 (고정 달러); (f) 호주 빅토리아 주에서 매월 도살한 돼지의 전체 수; (g) 캐나다 북서부의 맥킨지 강 지역에서 연간 포획된 스라소니의 전체 수; (h) 호주 월별 맥주 생산량; (i) 호주 월별 전기 생산량. 분명하게 계절성이 보이는 (d), (h), (i)는 후보가 되지 못합니다. 추세가 있고 수준이 변하는 (a), (c), (e), (f), (i)도 후보가 되지 못합니다. 분산이 증가하는 (i)도 후보가 되지 못합니다. 그러면 (b)와 (g)만 정상성을 나타내는 시계열 후보로 남았습니다. 언뜻 보면 시계열 (g)에서 나타나는 뚜렷한 주기(cycle) 때문에 정상성을 나타내는 시계열이 아닌 것처럼 보일 수 있습니다. 하지만 이러한 주기는 불규칙적(aperiodic)입니다 — 먹이를 구하기 힘들만큼 살쾡이 개체수가 너무 많이 늘어나 번식을 멈춰서, 개체수가 작은 숫자로 줄어들고, 그 다음 먹이를 구할 수 있게 되어 개체수가 다시 늘어나는 식이기 때문입니다. 장기적으로 볼 때, 이러한 주기의 시작이나 끝은 예측할 수 없습니다. 따라서 이 시계열은 정상성을 나타내는 시계열입니다. ACF (Autocorrelation Function) 정상성 확인하는 방법 중 하나 x축은 lag, y축은 ACF lag&#x3D;1: 한 시점 미룬 데이터와의 차이를 의미 자기 자신과 자기 자신 이전 데이터와의 correlation &#x3D; Autocorrelation lag 1이다. lag&#x3D;2: 자신 자신 데이터와 두 시점 미룬 데이터와의 correlation &#x3D; Autocorrelation lag 2 lag&#x3D;20: 현재 데이터와 20 시점을 shift한 데이터와의 correlation &#x3D; Autocorrelation lag 20 이 그래프에서는 5시점 shift한 것과 autocorrelation이 꽤 있는 것으로 보임 ACF를 통해서 정상성을 알아보는 방법 정리 일정한 패턴이 없거나 갑자기 떨어지는 패턴 &#x3D;&gt; stationary 일정하게 떨어지거나 올라갔다 내려갔다하면서 굉장히 천천히 떨어지는 패턴 &#x3D;&gt; nonstationary Autoregressive (AR) Models dependent variable인 Y의 lag를 independent variables인 X로 사용하는 모델 첫번째 X가 바로 yt−1, 즉 y의 한 시점전 데이터, yt−2는 두 시점 전 데이터,…, yt−p는 p 시점 전 데이터를 의미한다. 자기자신을 X로 삼기때문에 X1 &#x3D; yt−1, X2 &#x3D;yt−2,..,Xp &#x3D; yt−p로 생각하면 된다. ϕ0는 인털셉 yt를 모델링할 때 yt의 lag된 변수들(자신의 과거 데이터)을 X 삼아서 회귀모델을 만듦 .(Auto &#x3D; self라고 생각하면 됨) multiple regression model과 다른 점 자기자신을 갖고 모델링을 하기 때문에 독립성이 없다. ϕ0 (계수)를 추정할 때 일반적으로 사용했던 최소제곱법은 사용할 수 없다. Moving Average (MA) Models yt를 ε(엡실론&#x3D;error)으로 모델링 t시점의 데이터(yt)를 t 시점의 에러(εt)와 과거의 에러들로 표현 연속적인 에러 term으로 y와d의 관계를 모델링하는 방법 Autoregressive and Moving Average (ARMA) AR과 MA을 합친 모델 t시점의 데이터(yt)를 자기자신의 lag된 값들, t시점의 error와 전 시점의 error들로 표현함 Autoregressive Integrated Moving Average (ARIMA) p: order of the AR part of the model d: order of differencing q: order of the MA part of the model differencing을 했다는 것을 “integrated”로 표현함 I 라는 것은 differencing을 몇번 했는지를 의미 p,d,q AR → p I → d MA → q AR모델에서 p &#x3D; independent variable의 개수를 나타냄 MR모델에서 q, θ의 개수, 즉 파라미터의 개수를 나타냄 d &#x3D; 몇번 differencing을 했냐 시계열 데이터 모델을 구현할 때 주의해야 할 상황 AR, MA, ARMA 이 모델들을 구현하기 위해서는 분석해야되는 데이터가 stationary해야된다. nonstationary인 경우, 이 모델들을 적용할 수 없다. 일상생활엔 nonstationary한 데이터들이 훨씬 더 많다. 따라서 stationary한 데이터로 바꾼 뒤에 이 모델링을 할 수 있다. 어떻게 nonstationary를 stationary로 바꾸는 가장 간단한 방법이 바로 differencing(차분) 차분(differencing) 현 시점 데이터에서 d 시점 이전 데이터를 뺀 것 연이은 관측값들의 차이를 계산하는 것 원래 데이터와 원래 데이터를 한번 shift한 것을 빼주면 결과가 나오는데 이것이 바로 첫번째 differencing한 결과이다. 1차 차분이란 t시점의 데이터와 t-1시점의 데이터의 차이 2차 차분이란 t시점의 데이터와 t-2시점의 데이터의 차이 d차 차분이란 t시점의 데이터와 t-d시점의 데이터의 차이 X(원래 데이터)는 nonstationary여도 differencing을 한 결과(Y)는 stationary로 바뀔 확률이 매우 크다 (a)의 구글(Google) 주식 가격이 정상성을 나타내는 시계열이 아니었지만 패널 (b)의 일별 변화는 정상성을 나타냈다는 것에 주목합시다. 이 그림은 정상성을 나타내지 않는 시계열을 정상성을 나타내도록 만드는 한 가지 방법을 나타냅니다. 로그 같은 변환은 시계열의 분산 변화를 일정하게 만드는데 도움이 될 수 있습니다. 차분(differencing)은 시계열의 수준에서 나타나는 변화를 제거하여 시계열의 평균 변화를 일정하게 만드는데 도움이 될 수 있습니다. 결과적으로 추세나 계절성이 제거(또는 감소)됩니다. 정상성을 나타내지 않는 시계열을 찾아낼 때 데이터의 시간 그래프를 살펴보는 것만큼, ACF 그래프도 유용합니다. 정상성을 나타내지 않는 데이터에서는 ACF가 느리게 감소하지만, 정상성을 나타내는 시계열에서는, ACF가 비교적 빠르게 0으로 떨어질 것입니다. 그리고 정상성을 나타내지 않는 데이터에서 r1 은 종종 큰 양수 값을 갖습니다. ARIMA - Order of Differencing 만약 original 데이터가 stationary이면 differencing은 필요없다. 만약 original 데이터가 constant average trend(일정하게 증가하거나 감소하는 패턴)이면 1차 차분이면 충분하다. 오른쪽의 그래프와 같이 더 복잡한 패턴을 가지고 있다면 2차 차분까지 가야된다. 대부분의 데이터가 2차 차분으로 충분하다. 3차 차분까지 했을 때 stationary가 되는 데이터는 AR,MA,ARMA 모델로는 적합하지 않은 데이터라고 생각하면 됨 1st Differencing (1차 차분) 2nd Differencing (2차 차분) 1차와 2차의 차이가 없으므로 2차 차분까지 할 필요가 없어보임. nonstationary가 stationary로 변했는지 그냥 봤을 때는 잘 모르므로 ACF를 확인하자. 원래 데이터는 ACF에서 일정하게 감소하는 패턴 1차 차분한 것은 일정한 패턴이 없다. Reference https://otexts.com/fppkr/stationarity.html https:&#x2F;&#x2F;velog.io&#x2F;@sjina0722&#x2F;시계열분석-ARIMA-모델","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"ARIMA","slug":"ARIMA","permalink":"https://jmj3047.github.io/tags/ARIMA/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}]},{"title":"Transfer Learning for Speech Emotion Recognition","slug":"Transfer_Learning_SER","date":"2023-05-10T15:00:00.000Z","updated":"2023-06-14T05:46:15.955Z","comments":true,"path":"2023/05/11/Transfer_Learning_SER/","link":"","permalink":"https://jmj3047.github.io/2023/05/11/Transfer_Learning_SER/","excerpt":"","text":"Journal&#x2F;Conference : IEEE 5th Intl Conference on Big Data Security on Cloud (BigDataSecurity), IEEE Intl Conference on High Performance and Smart Computing,(HPSC) and IEEE Intl Conference on Intelligent Data and Security (IDS)Year(published year): 2019Author: Han, Zhijie, Huijuan Zhao, and Ruchuan WangSubject: transfer learning, speech emotion recognition, domain adaption, cross-domain Transfer Learning for Speech Emotion Recognition Summary 이 논문은 감정이 인간의 커뮤니케이션에서 중요한 역할을 하는 것을 고려하여, transfer learning의 이론과 주요 범주를 조사하고 speech emotion recognition에서의 적용을 연구하며, 이를 통해 감정 인식 모델의 성능을 개선하는 방법을 제안합니다. 감정 인식을 위한 레이블 데이터를 수집하는 것은 어려울 수 있지만, 전이 학습은 다른 분야에서 학습된 지식을 활용하여 감정 인식 모델의 성능을 향상시키는 데 사용할 수 있습니다.이렇게 하면 레이블 데이터에 대한 의존도를 낮추고 감정 인식 모델의 성능을 향상시킬 수 있습니다. I. Introduction 음성 감정 인식의 복잡성으로 인해, 고성능의 강력한 인식은 여전히 매우 어려운 과제입니다.Due to the complexity of the speech emotion recognition, high performance and robust recognition is still very challenging. 주된 이유는 다음과 같습니다: 인간의 오디오 생성은 말하는 장면, 말하는 방식, 화자의 나이, 성별, 말하기 습관 등 말하기의 맥락과 직접적으로 관련이 있습니다[11]. human audio generation is directly related to the context of speech, such as the speaking scene, the way of speaking, and the age, gender, and speaking habits of the speaker [11]. 음성 데이터 수집은 복잡하며 백그라운드 노이즈를 처리해야 합니다. The collection of speech data is complex, and need deal with the back ground noise. 감정은 주관적이며 공식적인 감정 정의가 없습니다. Emotion is subjective, and there is no formal emotion definition. 데이터에 레이블을 지정하는 사람의 감정 인식 능력은 감정 데이터의 주석에 영향을 미칩니다[12]. annotations은 완전한 음성 정보 표현에 의존하므로 주석 작업에 시간이 많이 걸리므로 주석이 달린 공공 음성 감정 코퍼스의 수가 제한되어 있습니다. The emotional perception ability of the person who labels the data affects the annotation of the emotion data[12]. Annotation relies on the complete speech information presentation, so the annotation work is time consuming.Therefore, the number of the annotated public speech emotion corpora is limited. 현재 머신러닝 연구는 훈련 세트와 테스트 세트가 동일한 특징공간에 속하고 동일한 분포를 갖는다는 전제하에 이루어지고 있습니다. 그러나 실제 어플리케이션에서 훈련데이터와 테스트 데이터는 일반적으로 이 조건을 충족하지 않습니다. 감정인식은 일반적으로 도메인 간 또는 심지어 언어 간에 이루어 집니다.At present, the research of machine learning is on the premise that train sets and test sets belong to the same feature space and have the same distribution. However, train and test data generally do not meet this condition in practical applications. Emotion recognition is usually cross-domain, or even cross-language. 전이 학습은 한 도메인에서 학습한 지식을 다른 유사한 분야에 적용하여 학습 작업을 줄일 수 있습니다. 한편, annotation에 대한 의존도를 줄여 음성 감정 인식의 성능을 더욱 향상시킬 수 있습니다.Transfer learning uses the knowledge learned in one domain and apply to another similar field, which can reduce the training work. Meanwhile, reduce the dependence on the annotation data, which can better promote the performance of speech emotion recognition. annotation data: Annotation data는 감정 인식을 위해 레이블링된 데이터를 의미합니다. 즉, 사람들이 어떤 감정을 표현하고 있는지를 미리 알려준 데이터를 말합니다. 이러한 데이터는 대규모로 수집하기 어렵기 때문에, transfer learning은 이러한 annotation data에 대한 의존도를 낮추면서 성능을 향상시키는 방법으로 제안되었습니다. 감정 인식을 위해 레이블링 된 데이터를 수집하는 것은 매우 어렵고 시간이 많이 소요되는 작업입니다. 이러한 데이터를 수집하고 레이블링하는 것은 감정 인식 모델의 성능을 향상시키는 데 중요하지만, 이러한 데이터가 부족하거나 잘못된 레이블링으로 인해 모델의 성능에 부정적인 영향을 미칠 수도 있습니다. 따라서 transfer learning은 다른 유사한 분야에서 학습된 지식을 활용하여 감정 인식 모델의 성능을 향상시키는 방법으로 제안되었습니다. 이를 통해 annotation data에 대한 의존도를 낮추면서도 감정 인식 모델의 성능을 개선할 수 있습니다. II. Related WorkTransfer Learning 전이 학습은 데이터, 과제, 모델에서 두 과제의 유사성을 활용하여 특정 분야에서 학습한 모델을 새로운 분야의 학습 과정에 적용하는 것을 말합니다[14]. 일반적으로 도메인과 태스크는 과제를 설명하는 두 가지 개념입니다. 도메인은 데이터와 데이터 distribution라는 두 가지 구성 요소로 구성됩니다. 또한 태스크에는 레이블 공간과 예측 함수의 두 가지 구성 요소가 있습니다. 함수는 그림 1과 같이 머신러닝을 통해 얻어지며, 지식은 소스 데이터를 사용하여 모델을 학습함으로써 학습됩니다.Tansfer learning refers to apply a model learned in a certain field to a learning process in a new field utilizing the similarities of two task in data, tasks and models [14]. Usually, Domain and task are two concepts to describe the tasks. A domain consists of two components: data and data distribution. The task also has two components: label space and prediction function. The function is got through machine learning, as shown in Fig.1, the knowledge is learned through training the model using the source data. 여기서 말하는 데이터 distribution이란, 데이터가 어떻게 분포되어있는지를 의미합니다. 예를들어 어떤 회사에서 고객들의 구매 기록을 수집하고 있다고 가정해 봅시다. 이 회사가 보유한 데이터는 고객의 구매 내역, 지불 방법, 배송지 등이 있을 것 입니다. 고객의 구매 내역은 상품의 가격, 수량, 카테고리 등으로 이루어져 있으며 이러한 데이터를 일반적으로 연속형 변수로 표현됩니다. 반면에 지불 방법은 신용카드, 현금, 모바일 결제 등으로 이루어져 있으며 범주형 변수로 표현됩니다. 또한 배송지는 지역별로 다른 분포를 가질 수 있으며 이러한 데이터는 지리적인 정보를 포함하므로 공간 데이터로 분류가 될 수 있습니다. 이러한 데이터들은 각각 다른 특성과 분포를 가지고 있으며 이러한 특성과 분포를 파악하여 적절한 모델만들어야 합니다. Domain은 데이터가 수집된 환경이나 상황을 의미하며, task는 데이터를 이용하여 수행하고자 하는 작업을 의미합니다. 예를 들어, 자율주행 자동차의 경우 도로 주행(domain)과 장애물 회피(task)와 같은 다양한 작업이 있습니다. 목표 과제에 레이블이 지정된 데이터인지 여부에 따라 전이 학습은 귀납적 전이 학습, 전이적 전이 학습 및 비지도 전이 학습의 세 가지 범주로 나눌 수 있습니다.According to whether the data labeled in the target tasks, transfer learning can divided into the following three categories: inductive transfer learning, transductive transfer learning and unsupervised transfer learning. Source data: 전이 학습에서 모델을 학습하는 데 사용되는 초기 데이터 세트입니다. 이 데이터 세트는 일반적으로 대규모 데이터 세트에서 가져옵니다. Source domain: 소스 데이터가 가져온 도메인입니다. 예를 들어, 소스 데이터가 자연어 처리(NLP) 분야의 텍스트 데이터라면, 소스 도메인은 NLP 분야입니다. Source task: 소스 데이터로 수행하는 작업입니다. 예를 들어, 소스 데이터가 감정 분석 작업에 사용된다면, 소스 작업은 감정 분석입니다. Target data: 전이 학습에서 새로운 작업을 수행하기 위해 사용되는 추가적인 데이터 세트입니다. Target domain: 타겟 데이터가 가져온 도메인입니다. 예를 들어, 타겟 데이터가 의료 분야의 이미지라면, 타겟 도메인은 의료 이미지 분야입니다. Target task: 전이 학습에서 수행하려는 새로운 작업입니다. 예를 들어, 타겟 작업이 의료 이미지에서 질병을 감지하는 것이라면, 타겟 작업은 질병 감지입니다. target data, target domain, target task는 각각 다른 개념입니다. 예를 들어, 소스 도메인은 음악 데이터이고 소스 작업은 음악 장르 분류입니다. 이 경우, 타겟 도메인은 영화 데이터이고 타겟 작업은 영화 장르 분류일 수 있습니다. 여기서 타겟 데이터는 영화 데이터 세트 자체를 의미합니다. 따라서 전이 학습에서는 소스 도메인과 작업에서 학습한 지식을 사용하여 타겟 도메인과 작업에 대한 모델을 구축하고 조정합니다. 즉, 전이 학습에서 “target data”는 새로운 데이터 세트를 의미하며 “target domain”은 해당 데이터가 속한 분야 또는 도메인을 나타내며 “target task”는 새로운 작업 또는 문제를 나타냅니다. 따라서, 전이 학습에서는 소스 데이터와 소스 도메인에서 모델을 학습한 후, 타겟 데이터와 타겟 도메인에서 모델을 조정하여 새로운 작업을 수행합니다. 이를 위해 finetune 방법과 같은 기술이 사용됩니다. 세 전이 학습 모두 source domain에서 학습한 모델을 target domain으로 전이시켜 새로운 task를 수행하는 방법입니다. Label space는 모델이 예측하려는 결과값의 집합입니다. 예를 들어, 이메일 분류 모델에서 label space는 “스팸”과 “스팸이 아님”과 같은 두 가지 값으로 구성됩니다. Inductive transfer learning은 source domain과 target domain이 서로 다른 label space를 가지고 있을 때 사용됩니다. Inductive transfer learning은 target domain에서 labeled data가 적거나 없는 경우에 유용합니다. Transductive transfer learning은 source domain과 target domain이 동일한 label space를 가지고 있을 때 사용됩니다. Transductive transfer learning은 target domain에서 labeled data가 있는 경우에 유용합니다. Unsupervised transfer learning은 source domain과 target domain이 동일한 label space를 가지고 있지 않을 때 사용됩니다. Unsupervised transfer learning은 target domain에서 labeled data가 없는 경우에 유용합니다. Transfer approach에 따라 네가지 유형의 방법이 있습니다. instance-transfer learning: source domain과 target domain이 동일한 feature space를 가지고 있지만, 데이터 분포가 다른 경우에 사용됩니다. 이 방법은 source domain에서 학습한 모델을 target domain으로 전이시켜 새로운 task를 수행하는 방법입니다. 데이터 분포가 다른 경우에도 잘 작동할 수 있습니다. The data in the train set will have different weight to show the similarity between source task and target task. train set의 데이터는 source task와 target task 간의 유사성을 나타내기 위해 서로 다른 가중치를 가집니다. 이는 source domain과 target domain이 동일한 feature space를 가지고 있지만, 데이터 분포가 다른 경우에 사용되며, 이러한 가중치는 데이터 분포의 차이를 보상하기 위해 사용됩니다. feature-representaion transfer learning: source domain과 target domain이 다른 feature space를 가지고 있을 때 사용됩니다. 이 방법은 두 도메인 간의 공통된 feature를 찾아내고, 이를 이용하여 데이터 분포의 차이를 줄이는 방법입니다. 두 도메인 간의 유사성을 찾아내는 것이 중요합니다. The feature space of source task and target task are different, both the feature space of the source and the target task will be transformed to a shared subspace. 만약 source task와 target task의 feature space가 서로 다르다면, 두 feature space를 공유하는 subspace로 변환시켜야 합니다. 이를 위해 feature-representation transfer learning을 사용합니다. parameter-transfer learning: 모델 기반의 transfer learning 방법 중 하나입니다. 이 방법은 먼저 source task에서 모델을 학습시키고, 그 다음 target task에서 모델을 fine-tuning하여 target task에 적용합니다. 하지만 source task와 target task는 서로 다른 데이터 분포를 가지고 있기 때문에, fine-tuning이 필요합니다. 따라서 parameter-transfer learning은 source task와 target task 간의 유사성을 찾아내는 것이 중요합니다. Firstly, the model is trained in the source task, and then it can be used in the target task, because of the diffference of the two task, the target task will need finetune. relational-knowledge transfer learning: source task와 target task 간의 관계가 유사한 경우, source task에서 학습한 관계를 target task로 전달할 수 있는 transfer learning 방법입니다. If there is a relation in the source task is similar to the relation in the target task, then we can transfer the relation from source to target. 모든 transfer learning 방법은 source task와 target task 간의 유사성을 기반으로 하며, 이 유사성은 feature, parameter, relationship 등 여러 가지 요소들에서 나타날 수 있습니다. 따라서 transfer learning의 핵심은 두 task 간의 유사성을 찾아내는 것입니다.In summary, each kind of transfer method is on the basic condition, which is the similarity between source and destination task, whether they are features, parameters, or relationships. Therefore, the key to transfer learning is to find the similarities between tasks. 유사성이 없는 경우에도 transfer learning을 할 수 있지만, 이 경우에는 transfer learning의 성능이 저하될 가능성이 높습니다. 따라서 transfer learning을 적용하기 전에 두 task 간의 유사성을 분석하고, 가능한한 유사한 task를 선택하는 것이 좋습니다. transfer learning과 관련하여 언급해야 할 두가지 핵심 이슈는 domain adaption과 task relateness 입니다. There are two key issues to mention about transfer learning: domain adaption and task relateness. Domain adaptation transfer learning의 한 분야로, source domain과 target domain 간의 차이를 극복하기 위한 방법입니다. 이 방법은 source domain에서 학습한 모델을 target domain에서 적용할 때 발생하는 문제를 해결하기 위해 사용됩니다. 주로 feature space와 class space 간의 차이를 줄이는 것이 목적입니다. transfer learning에서 domain adaptation이 필요한 이유: source domain에서는 이미 label이 있는 데이터가 존재하지만, target domain에서는 label이 없는 데이터만 존재하는 경우가 많습니다. 이 때, transfer learning을 사용하여 source domain의 데이터를 활용하여 모델을 학습시키고, 이 모델을 이용하여 target domain의 입력 데이터의 감정 카테고리를 예측하는 것이 목적입니다. In the source domain the data have label, but the data in the target domain do not have label. Here the objective of transfer learning is to use the data from the source domain to train the model and then use the model to predict the emotion category of the input target data. Task relateness transfer learning에서 고려해야 할 또 다른 중요한 요소입니다. 이는 source task와 target task 간의 유사성을 의미합니다. 두 task가 유사할수록 transfer learning의 성능이 향상됩니다. 이러한 유사성은 feature, parameter, relationship 등 여러 가지 측면에서 나타날 수 있습니다. 따라서 transfer learning을 적용하기 전에 두 task 간의 유사성을 분석하고, 가능한한 유사한 task를 선택하는 것이 좋습니다. 전이 학습에서 작업 간의 유사성은 성공적인 전이를 위한 기초이며, 이 유사성을 찾아 올바르게 사용하는 방법은 핵심적인 문제입니다. In transfer learning, the similarity between tasks is the basis for successful transfer and how to find and correctly use this similarity is a key issue. 음성 감정 인식 연구에는 주로 언어 간, 데이터베이스 간, 모달 간, 애플리케이션 간 전이(예: 음성 인식에서 음성 감정 인식으로의 전이)가 있습니다. 표 II는 전송 범위와 다양한 전송에 대한 간략한 설명을 제공합니다.In the research of speech emotion recognition, there are mainly cross-language, cross-database, cross-modal, cross-application transfer (for example, transfer from speech recognition to speech emotion recognition). Table II give the transfer scope and the brief description about the different transfer. III. Transfer Learning Methods For Speech Emotion Recognition 음성 감정인식의 특성에 따라 이 분야에서 전이학습의 적용은 두가지 범주로 나뉩니다.Based on the characteristics of speech emotion recognition, the application of transfer learning in this field mainly has the following two categories A. Feature based Transfer Learning Feature based transfer learning은 source domain과 target domain이 동일한 feature를 공유한다는 가정에 기반하여, 두 domain 간의 차이를 줄이기 위해 feature transformation을 통해 서로 전달하는 방법입니다.Based on the hypothesis that the source and target domain share the same features, we can use feature based transfer learning. Feature based transfer learning refers to that transfer to each other through feature transformation, to reduce the gap between the source and target domains. 또 다른 방법으로는 source domain과 target domain의 데이터를 통합된 feature space로 변환한 후 전통적인 기계 학습 방법을 사용하여 분류하는 것입니다.Other method is to transform the data both in source and target domains into a unified feature space and then use traditional machine learning methods for classification [25]. Siddique는 연구에서 화자의 얼굴과 음성 간의 관계를 활용하여, 시각적인 영역(faces)에서 표현의 주석(annotation)을 음성 영역으로 전달하여, label이 없는 오디오 데이터 문제를 해결하였습니다. 이러한 방식으로 transfer learning을 적용함으로써, label이 없는 데이터에서도 유용한 정보를 추출할 수 있게 되었습니다. Deng는 autoencoder를 사용하여 source domain과 target domain 간에 feature를 전달하는 방법을 제안하였습니다. Autoencoder는 입력 데이터를 압축하고 재생성하는 인공 신경망 구조로, 이를 통해 source domain의 feature들을 압축하고 target domain으로 전달하여, 두 도메인 간의 차이를 줄일 수 있게 됩니다. Autoencoder는 입력 데이터를 압축하고 재생성하는 인공 신경망 구조입니다. 이를 통해 source domain의 feature들을 압축하고 target domain으로 전달할 수 있습니다. Autoencoder는 두 개의 주요 구성 요소로 이루어져 있습니다. 하나는 인코더(encoder)이고, 다른 하나는 디코더(decoder)입니다. 인코더는 입력 데이터를 저차원의 latent space로 압축하고, 디코더는 latent space에서 원래의 입력 데이터를 재생성합니다. 따라서, transfer learning에서 autoencoder를 사용하여 feature transfer를 수행할 때, 먼저 source domain의 데이터를 인코더에 입력하여 latent space로 압축합니다. 그리고 이렇게 압축된 feature들을 디코더를 통해 target domain으로 전달합니다. 이렇게 전달된 feature들은 target domain에서 새로운 모델을 학습하는 데 사용될 수 있습니다. 인코더에서 압축되기 전의 feature와 디코더에서 압축되어 나온 feature는 일반적으로 다릅니다. 인코더는 입력 데이터를 저차원의 latent space로 압축하고, 디코더는 이 latent space에서 원래의 입력 데이터를 재구성합니다. 이 과정에서, 인코더는 입력 데이터의 중요한 특징을 추출하고, 디코더는 이러한 특징을 사용하여 입력 데이터를 재구성합니다. 따라서, 인코더에서 추출된 feature와 디코더에서 재구성된 feature는 서로 다른 형태를 가지며, 일반적으로 차원이 다릅니다. B. Model&#x2F;Parameter based Transfer Learning Model based transfer learning은 source domain과 target domain이 모델 파라미터를 공유할 수 있다는 아이디어에 기반합니다. 이 방법은 주로 많은 hidden layer를 가진 딥러닝 모델에서 사용됩니다. 이 방법을 사용하면, source domain에서 학습된 모델 파라미터를 target domain에서 새로운 모델을 학습하는 데 사용할 수 있습니다.The main idea of model based transfer learning is that the source and target domain can share the model parameters. This method is mainly used in deep learning model, which has many hidden layers [28]. Zhao는 연령 및 성별 분류 모델을 감정 인식 작업에 전이하는 방법을 제안했습니다. 이 방법은 model based transfer learning의 한 예입니다.Zhao proposed transfer age and gender classification model to the emotion recogntion task [11]. Zhao는 연령 및 성별 분류 모델을 감정 인식 작업에 전이하는 방법을 제안했습니다. 이 방법은 transfer learning의 한 예입니다. 이 논문에서는 hierarchical deep learning을 사용하여 대규모 음성 데이터에서 연령, 성별 및 감정 카테고리를 예측하는 모델을 학습합니다. 이 모델은 먼저 연령과 성별 속성을 추출하고, 그 다음에 감정 카테고리를 예측합니다. 연령과 성별은 감정 카테고리를 예측하는 데에 중요한 특징입니다. 예를 들어, 연령이 어린 사람일수록 행복한 감정을 더 많이 나타내는 경향이 있고, 여성일수록 남성보다 슬픈 감정을 더 많이 나타내는 경향이 있습니다. 하지만 연령과 성별만으로는 감정 카테고리를 완벽하게 예측할 수 없습니다. 따라서 이 논문에서는 hierarchical deep learning 모델을 사용하여 입력 데이터에서 다양한 특징을 추출하고, 이를 활용하여 보다 정확한 감정 카테고리를 예측하였습니다. 이 논문에서는 transfer learning의 개념을 활용하여, 이미 연령 및 성별 분류 작업에서 학습된 모델 파라미터를 활용하여 감정 인식 작업에 적용하는 방법을 제안합니다. 이를 통해, 적은 양의 labeled data로도 효과적인 감정 인식 모델을 학습할 수 있습니다. IV. Deep Transfer Learning For Speech Emotion Recognition 딥러닝과 전이학습을 결합하면 더 나은 결과를 얻을 수 있습니다. 일반적으로 딥러닝에는 두 가지 방법이 사용됩니다. 하나는 Multi-task learning이고, 다른 하나는 finetune 입니다.The combination of deep learning and transfer learning can achieve better results. Usually there are two methods used in deep learning. One is multi-task learning and the other is finetune. A. Multi-Task Learning Multi-task learning과 Transfer learning의 차이점과 공통점 Multi-task learning은 여러 개의 관련된 작업을 함께 학습하는 것을 의미하며, Transfer learning은 knowledge를 source domain에서 target domain으로 전달하는 과정에 중점을 둡니다. Multi-task learning은 Transfer learning의 한 유형으로 볼 수 있습니다. Transfer learning의 핵심 문제는 두 작업 간의 유사성을 찾는 것입니다. 이를 찾지 못하면 학습 과정에서 부정적인 영향을 미칠 수 있습니다. 반면, Multi-task learning의 핵심 문제는 related tasks를 찾는 것입니다. 따라서, 두 가지 방법 모두 작업 간의 유사성을 찾는 것이 중요합니다. B. Finetuning 먼저 소스 데이터에 대해 모델을 학습시킨 다음, 타겟 데이터를 사용하여 모델을 조정하여 타겟 작업에 적응시키는 것입니다. 소스와 타겟 작업은 상호 연관성이 있지만, 일반적으로 작업의 데이터 분포가 동일하지 않습니다. 따라서 모델은 학습된 소스 모델에 따라 조정되어야 합니다. finetune 방법과 전이 학습의 장점 Finetune 방법은 소스 데이터로부터 모델을 학습한 후, 타겟 데이터를 사용하여 모델을 조정하는 방법입니다. 이 방법은 소스 작업과 타겟 작업 간의 차이를 극복할 수 있으며, 딥 뉴럴 네트워크는 무작위 초기화 가중치보다 더 나은 성능을 보입니다. 이러한 방법은 훈련 시간을 절약하면서도 모델의 일반성과 견고성을 향상시킬 수 있습니다. 또한, 사전 훈련(pre-train)이 일반적으로 대규모 데이터 세트에서 수행됩니다. 이렇게 함으로써 훈련 데이터를 확장하여 모델의 일반성과 견고성을 높일 수 있습니다. V. Conclusions인공 지능, 사물 인터넷 및 Fog 컴퓨팅의 급속한 발전으로 인해 연구자들의 관심이 높아지고 있다는 것을 언급하며, 전이 학습(transfer learning)이 기계 학습(machine learning)의 중요한 연구 방향 중 하나임을 강조합니다. 이에 따라, 이 논문에서는 전이 학습의 기본 지식, 범주 및 기본적인 방법을 조사하고, 음성 감정 인식에 대한 전이 학습 응용을 연구합니다. 이 응용에서는 실제 작업과 데이터를 분석하여 부정적인 전이를 피하기 위해 주의해야 하며, 전이 학습을 기반으로 하는 다중 모델 감정 인식 및 도메인 적응(domain adaption)은 중요한 주제입니다.","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speech Emotion Recognition","slug":"Paper/Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/categories/Paper/Speech-Emotion-Recognition/"}],"tags":[{"name":"Speech Emotion Recognition","slug":"Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/tags/Speech-Emotion-Recognition/"},{"name":"Transfer Learning","slug":"Transfer-Learning","permalink":"https://jmj3047.github.io/tags/Transfer-Learning/"},{"name":"Domain Adaptation","slug":"Domain-Adaptation","permalink":"https://jmj3047.github.io/tags/Domain-Adaptation/"},{"name":"Cross-domain","slug":"Cross-domain","permalink":"https://jmj3047.github.io/tags/Cross-domain/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speech Emotion Recognition","slug":"Paper/Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/categories/Paper/Speech-Emotion-Recognition/"}]},{"title":"Convolutional Neural Networks","slug":"DL_Part4","date":"2023-05-07T15:00:00.000Z","updated":"2023-06-22T15:34:36.923Z","comments":true,"path":"2023/05/08/DL_Part4/","link":"","permalink":"https://jmj3047.github.io/2023/05/08/DL_Part4/","excerpt":"","text":"Course Link Lecture 4 in Deep Learning CNN By convention, in machine learning, we usually do not bother with this flipping operation. Technically this operation is maybe better called cross-correlation. But most of the deep learning literature just causes the convolution operator. CNN Case LeNet-5 AlexNet this neural network actually had a lot of similarities to LeNet, but it was much bigger. So whereas the LeNet-5 from previous slide had about 60,000 parameters, this AlexNet that had about 60 million parameters. And the fact that they could take pretty similar basic building blocks that have a lot more hidden units and training on a lot more data, they trained on the image that dataset that allowed it to have a just remarkable performance. Another aspect of this architecture that made it much better than LeNet was using the relu activation function. VGG-16 the 16 in the VGG-16 refers to the fact that this has 16 layers that have weights. And this is a pretty large network, this network has a total of about 138 million parameters. And that’s pretty large even by modern standards. ResNet Residual block: using residual blocks allows you to train much deeper neural networks. And the way you build a ResNet is by taking many of these residual blocks, blocks like these, and stacking them together to form a deep network. the theory, in theory, having a deeper network should only help. But in practice or in reality, having a plain network, so no ResNet, having a plain network that is very deep means that all your optimization algorithm just has a much harder time training. And so, in reality, your training error gets worse if you pick a network that’s too deep. But what happens with ResNet is that even as the number of layers gets deeper, you can have the performance of the training error kind of keep on going down. Even if we train a network with over a hundred layers. Network in network(1 X 1 Convolutions) one way to think about the 32 numbers you have in this one, a one by 32 filter is as if you have one neuron that is taking us input 32 numbers. Multiplying each of these 32 numbers in one slice in the same position, height, and width, but these 32 different channels, multiplying them by 32 weights. One way to think about a one-by-one convolution is that it is basically having a fully connected neural network that applies to each of the 62 different positions. shrinking channels Inception Network what the inception network does, is, more or less, put a lot of these modules together. It turns out that there’s one last detail to the inception network if we read the optional research paper. Which is that there are these additional side-branches that I just added. So what do they do? Well, the last few layers of the network is a fully connected layer followed by a softmax layer to try to make a prediction. What these side branches do is it takes some hidden layer and it tries to use that to make a prediction. So this is actually a softmax output and so is that. And this other side branch, again it is a hidden layer passes through a few layers like a few connected layers. And then has the softmax try to predict what’s the output label. And you should think of this as maybe just another detail of the inception that’s worked. But what is does is it helps to ensure that the features computed. Even in the heading units, even at intermediate layers. That they’re not too bad for protecting the output cause of a image. And this appears to have a regularizing effect on the inception network and helps prevent this network from overfitting. Depth-wise seperable convolution you’ll learn about MobileNets, which is another foundational convolutional neural network architecture used for computer vision. Using MobileNets will allow you to build and deploy new networks that work even in low compute environment, such as a mobile phone. MobileNet In MobileNet v2, there are two main changes. One is the addition of a residual connection. This is just a residual connections that you learned about in the ResNet videos. This residual connection or skip connection, takes the input from the previous layer and sums it or passes it directly to the next layer, does allow ingredients to propagate backward more efficiently. The second change is that it also as an expansion layer, which you learn more about on the next slide, before the depthwise convolution, followed by the pointwise convolution, which we’re going to call projection in a point-wise convolution. The block with red line is called bottleneck block why do we meet these bottleneck blocks? It turns out that the bottleneck block accomplishes two things, One, by using the expansion operation, it increases the size of the representation within the bottleneck block. This allows the neural network to learn a richer function. There’s just more computation over here. But when deploying on a mobile device, on edge device, you will often be heavy memory constraints. The bottleneck block uses the pointwise convolution or the projection operation in order to project it back down to a smaller set of values, so that when you pass this the next block, the amount of memory needed to store these values is reduced back down. EfficientNet With MobileNet, you’ve learned how to build more computationally efficient layers, and with EfficientNet, you can also find a way to scale up or down these neural networks based on the resources of a device you may be working on. Object Detection in this example the ideal bx might be about 0.5 because this is about halfway to the right to the image. by might be about 0.7 since it’s about maybe 70% to the way down to the image. bh might be about 0.3 because the height of this red square is about 30% of the overall height of the image. And bw might be about 0.4 let’s say because the width of the red box is about 0.4 of the overall width of the entire image. to implement sliding windows, previously, what you do is you crop out a region. Let’s say this is 14 by 14 and run that through your convnet and do that for the next region over, then do that for the next 14 by 14 region, then the next one, then the next one, then the next one, then the next one and so on, until hopefully that one recognizes the car. But now, instead of doing it sequentially, with this convolutional implementation that you saw in the previous slide, you can implement the entire image, all maybe 28 by 28 and convolutionally make all the predictions at the same time by one forward pass through this big convnet and hopefully have it recognize the position of the car. So that’s how you implement sliding windows convolutionally and it makes the whole thing much more efficient. Application Face recognition One of the challenges of face recognition is that you need to solve the one-shot learning problem. What that means is that for most face recognition applications you need to be able to recognize a person given just one single image, or given just one example of that person’s face. And, historically, deep learning algorithms don’t work well if you have only one training example. to input two faces and tell you how similar or how different they are. A good way to do this is to use a Siamese network. this idea of running two identical, convolutional neural networks on two different inputs and then comparing them, sometimes that’s called a Siamese neural network architecture. Neural Style Transfer just to wrap this up, you can now define the overall cost function as alpha times the content cost between c and G plus beta times the style cost between s and G and then just create in the sense or a more sophisticated optimization algorithm if you want in order to try to find an image G that normalize, that tries to minimize this cost function j of G. And if you do that, you can generate pretty good looking neural artistic and if you do that you’ll be able to generate some pretty nice novel artwork.","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Coursera","slug":"Coursera","permalink":"https://jmj3047.github.io/tags/Coursera/"},{"name":"AI Studies","slug":"AI-Studies","permalink":"https://jmj3047.github.io/tags/AI-Studies/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}]},{"title":"(Error solved) Accessing non-existent property 'lineno' of module exports inside circular dependency","slug":"Error_node_14126","date":"2023-05-06T15:00:00.000Z","updated":"2023-05-07T09:49:23.606Z","comments":true,"path":"2023/05/07/Error_node_14126/","link":"","permalink":"https://jmj3047.github.io/2023/05/07/Error_node_14126/","excerpt":"","text":"(node:14126) Warning, Accessing non-existent property ‘lineno’ of module exports inside circular dependency어느날 부터인가 블로그 업로드 전에 hexo sever 를 입력하면 밑에 node 관련된 warning이 뜨기 시작했다. For some reason, when I type hexo sever before uploading a blog, a warning about nodes appears below. 1234567891011MacBookPro myblog % hexo serverINFO Validating configINFO Start processingINFO Hexo is running at http://localhost:4000/ . Press Ctrl+C to stop.(node:14126) Warning: Accessing non-existent property &#x27;lineno&#x27; of module exports inside circular dependency(Use `node --trace-warnings ...` to show where the warning was created)(node:14126) Warning: Accessing non-existent property &#x27;column&#x27; of module exports inside circular dependency(node:14126) Warning: Accessing non-existent property &#x27;filename&#x27; of module exports inside circular dependency(node:14126) Warning: Accessing non-existent property &#x27;lineno&#x27; of module exports inside circular dependency(node:14126) Warning: Accessing non-existent property &#x27;column&#x27; of module exports inside circular dependency(node:14126) Warning: Accessing non-existent property &#x27;filename&#x27; of module exports inside circular dependency 게시물이 업로드 되는데에는 문제가 없었지만, 찝찝해서 해결하려고 찾아본 결과: I didn’t have any issues with the post uploading, but when I went to troubleshoot, I found that this code: 1rm -rf node_modules package-lock.json &amp;&amp; npm install &amp;&amp; npm run 위 명령어를 입력했더니 해결 되었다. 대충 찾아봤더니 npm과 node의 버전 문제 아니면 package-lock.json의 문제 둘중에 하나 인 거 같았다. I entered the above command and it worked. After a quick search, I realized it was either a version issue with npm and node or a problem with package-lock.json. Reference: https://github.com/nodejs/help/issues/2347","categories":[{"name":"Setting","slug":"Setting","permalink":"https://jmj3047.github.io/categories/Setting/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://jmj3047.github.io/tags/Hexo/"},{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"}],"keywords":[{"name":"Setting","slug":"Setting","permalink":"https://jmj3047.github.io/categories/Setting/"}]},{"title":"Structuring Machine Learning Projects_Quiz","slug":"DL3_Quiz","date":"2023-05-06T15:00:00.000Z","updated":"2023-06-03T13:54:32.965Z","comments":true,"path":"2023/05/07/DL3_Quiz/","link":"","permalink":"https://jmj3047.github.io/2023/05/07/DL3_Quiz/","excerpt":"","text":"개요Coursera Deep Learning Course 3 Quiz 1. Bird Recognition in the City of Peacetopia (Case Study) Link: https://www.coursera.org/learn/machine-learning-projects/home/module/1 2. Autonomous Driving (Case Study) Link: https://www.coursera.org/learn/machine-learning-projects/home/module/2","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"Coursera","slug":"Coursera","permalink":"https://jmj3047.github.io/tags/Coursera/"},{"name":"Quiz","slug":"Quiz","permalink":"https://jmj3047.github.io/tags/Quiz/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}]},{"title":"Structuring Machine Learning Projects","slug":"DL_Part3","date":"2023-05-04T15:00:00.000Z","updated":"2023-06-19T12:49:03.236Z","comments":true,"path":"2023/05/05/DL_Part3/","link":"","permalink":"https://jmj3047.github.io/2023/05/05/DL_Part3/","excerpt":"","text":"Course Lecture 3 in Deep Learning Why ML Strategy F1 score Classifier A is 90% recall: That of all of the images in, say, your dev sets that really are cats, classifier A accurately pulled out 90% of them. It turns out that there’s often a tradeoff between precision and recall, and you care about both. → When the classifier says something is a cat, there’s a high chance it really is a cat. But of all the images that are cats, you also want it to pull a large fraction of them as cats. (분류기가 고양이라고 말하면 그게 실제로 고양이일 확률이 높기를 원합니다. 그러나 고양이 이미지 중에서도 많은 이미지를 고양이로 분류할 수 있기를 원합니다.) So it might be reasonable to try to evaluate the classifiers in terms of its precision and its recall. The problem with using precision recall as your evaluation metric is that if classifier A does better on recall, which it does here, the classifier B does better on precision, then you’re not sure which classifier is better. F1 score: In the machine learning literature, the standard way to combine precision and recall. The details of F1 score aren’t too important, but informally, you can think of this as the average of precision, P, and recall, R. Formally, the F1 score is defined by this formula, it’s 2&#x2F; 1&#x2F;P + 1&#x2F;R. In mathematics, this function is called the harmonic mean of precision P and recall R. But less formally, you can think of this as some way that averages precision and recall. To summarize, if there are multiple things you care about by say there’s one as the optimizing metric that you want to do as well as possible on and one or more as satisficing metrics were you’ll be satisfice. So long as it does better than some threshold you can now have an almost automatic way of quickly looking at multiple core size and picking the, quote, best one. Avoidale bias In machine learning, avoidable bias is the difference between the training error and the Bayes error. The Bayes error is the theoretical minimum error rate that is possible for a given machine learning model. Avoidable bias is the error that is due to the machine learning model, rather than the data that it is trained on. There are a number of factors that can contribute to avoidable bias, including: The choice of machine learning algorithm The size and quality of the training data The way that the model is trained There are a number of things that can be done to reduce avoidable bias, including: Choosing a machine learning algorithm that is appropriate for the task at hand Using a large and high-quality training dataset Using a variety of techniques to train the model, such as cross-validation and regularization It is important to note that it is not always possible to eliminate avoidable bias entirely. However, by taking steps to reduce avoidable bias, it is possible to improve the accuracy and fairness of machine learning models. Here are some examples of avoidable bias in machine learning: A model that is trained on a dataset that is biased towards men may be more likely to predict that a job applicant is a man, even if the applicant’s qualifications are equal to those of a woman. A model that is trained on a dataset that is biased towards white people may be more likely to predict that a criminal is white, even if the crime rate is equal for all races. A model that is trained on a dataset that is biased towards wealthy people may be more likely to predict that a person is wealthy, even if the person’s income is equal to that of a person from a lower socioeconomic status. Avoidable bias can have a significant impact on the fairness and accuracy of machine learning models. It is important to be aware of the potential for avoidable bias and to take steps to reduce it. what you want is to maybe keep improving your training performance until you get down to Bayes error but you don’t actually want to do better than Bayes error. You can’t actually do better than Bayes error unless you’re overfitting. And this, the difference between your training area and the dev error, there’s a measure still of the variance problem of your algorithm. The term avoidable bias acknowledges that there’s some bias or some minimum level of error that you just cannot get below which is that if Bayes error is 7.5%, you don’t actually want to get below that level of error. So rather than saying that if you’re training error is 8%, then the 8% is a measure of bias in this example, you’re saying that the avoidable bias is maybe 0.5% or 0.5% is a measure of the avoidable bias whereas 2% is a measure of the variance and so there’s much more room in reducing this 2% than in reducing this 0.5%. Relationship between human level performance, bayes error and variance If you’re trying to understand bias and variance where you have an estimate of human-level error for a task that humans can do quite well, you can use human-level error as a proxy or as a approximation for Bayes error. so the difference between your estimate of Bayes error tells you how much avoidable bias is a problem, how much avoidable bias there is. And the difference between training error and dev error, that tells you how much variance is a problem, whether your algorithm’s able to generalize from the training set to the dev set. And the big difference between our discussion here and what we saw in an earlier course was that instead of comparing training error to 0%. And just calling that the estimate of the bias. In contrast, in this video we have a more nuanced analysis in which there is no particular expectation that you should get 0% error. Because sometimes Bayes error is non zero and sometimes it’s just not possible for anything to do better than a certain threshold of error. Human-level performance is a measure of how well humans can perform a task. Bayes error is the lowest possible error that a machine learning model can achieve on a task, given the amount of data that is available. Variance is a measure of how much the model’s performance varies on different datasets The relationship between human-level performance, Bayes error, and variance can be understood in the following way: Human-level performance is a lower bound on Bayes error. This is because humans have access to all of the information that is available to the machine learning model, and they are able to use their knowledge and experience to make better decisions. Variance is a measure of how much the model’s performance is affected by noise in the data. The more noise there is in the data, the higher the variance will be. As the model’s variance increases, the gap between its performance and human-level performance will also increase. This is because the model will be more likely to make mistakes on data that is not representative of the training data. In general, the goal of machine learning is to develop models that have low bias and low variance. Low bias means that the model is not making systematic errors, and low variance means that the model’s performance is not affected by noise in the data. There are a number of techniques that can be used to reduce bias and variance in machine learning models. These techniques include: Data augmentation: This involves creating new data points by modifying existing data points. This can help to reduce the amount of noise in the data. Regularization: This involves adding a penalty to the loss function that is being minimized. This can help to reduce the model’s complexity and make it more robust to noise in the data. Ensembling: This involves combining the predictions of multiple models. This can help to reduce the variance of the model’s predictions. Structured data in ML All four of these examples are actually learning from structured data, where you might have a database of what ads users have clicked on, database of products you’ve bought before, databases of how long it takes to get from A to B, database of previous loan applications and their outcomes. These are not natural perception problems, so these are not computer vision, or speech recognition, or natural language processing tasks. Humans tend to be very good in natural perception task. So it is possible, but it’s just a bit harder for computers to surpass human-level performance on natural perception tasks. Finally, all of these are problems where there are teams that have access to huge amounts of data. So for example, the best systems for all four of these applications have probably looked at far more data of that application than any human could possibly look at. And so, that’s also made it relatively easy for a computer to surpass human-level performance. Reducing bias and variance The process we’ve seen in the last several videos, if you want to improve the performance of your machine learning system, I would recommend looking at the difference between your training error and your proxy for Bayes error and just gives you a sense of the avoidable bias. In other words, just how much better do you think you should be trying to do on your training set. Then look at the difference between your dev error and your training error as an estimate of how much of a variance problem you have. In other words, how much harder you should be working to make your performance generalized from the training set to the dev set that it wasn’t trained on explicitly. How to build ML Strategy Carrying Out Error Analysis: you should find a set of mislabeled examples, either in your dev set, or in your development set. And look at the mislabeled examples for false positives and false negatives. And just count up the number of errors that fall into various different categories. what I would recommend you do, if you’re starting on building a brand new machine learning application, is to build your first system quickly and then iterate. What I mean by that is I recommend that you first quickly set up a dev&#x2F;test set and metric. So this is really deciding where to place your target. And if you get it wrong, you can always move it later, but just set up a target somewhere. Then I recommend you build an initial machine learning system quickly. Find the training set, train it and see. Start to see and understand how well you’re doing against your dev&#x2F;test set and your valuation metric. When you build your initial system, you will then be able to use bias&#x2F;variance analysis which we talked about earlier as well as error analysis which we talked about just in the last several videos, to prioritize the next steps. Transfer Learning If you retrain all the parameters in the neural network, then this initial phase of training on image recognition is sometimes called pre-training, because you’re using image recognitions data to pre-initialize or really pre-train the weights of the neural network. And then if you are updating all the weights afterwards, then training on the radiology data sometimes that’s called fine tuning. speech recognition model to wake word&#x2F;trigger word detection model you’ve trained a speech recognition system to output your transcripts. And let’s say that you now want to build a “wake words” or a “trigger words” detection system. So, recall that a wake word or the trigger word are the words we say in order to wake up speech control devices in our houses such as saying “Alexa” to wake up an Amazon Echo or “OK Google” to wake up a Google device or “hey Siri” to wake up an Apple device or saying “Ni hao baidu” to wake up a baidu device. In order to do this, you might take out the last layer of the neural network again and create a new output node. But sometimes another thing you could do is actually create not just a single new output, but actually create several new layers to your neural network to try to predict the labels Y for your wake word detection problem. Then again, depending on how much data you have, you might just retrain the new layers of the network or maybe you could retrain even more layers of this neural network. When does transfer learning make sense? Transfer learning makes sense when you have a lot of data for the problem you’re transferring from and usually relatively less data for the problem you’re transferring to. For speech recognition, maybe you’ve trained the speech recognition system on 10000 hours of data. So, you’ve learned a lot about what human voices sounds like from that 10000 hours of data, which really is a lot. But for your trigger word detection, maybe you have only one hour of data. So, that’s not a lot of data to fit a lot of parameters. So in this case, a lot of what you learn about what human voices sound like, what are components of human speech and so on, that can be really helpful for building a good wake word detector, even though you have a relatively small dataset or at least a much smaller dataset for the wake word detection task. So in both of these cases, you’re transferring from a problem with a lot of data to a problem with relatively little data. Transfer learning has been most useful if you’re trying to do well on some Task B, usually a problem where you have relatively little data. So for example, in radiology, you know it’s difficult to get that many x-ray scans to build a good radiology diagnosis system. In that case, you might find a related but different task, such as image recognition, where you can get maybe a million images and learn a lot of load-over features from that, so that you can then try to do well on Task B on your radiology task despite not having that much data for it. Multi task Learning In multi-task learning, you start off simultaneously, trying to have one neural network do several things at the same time. And then each of these task helps hopefully all of the other task. when does multi-task learning make sense? One is if your training on a set of tasks that could benefit from having shared low-level features. So for the autonomous driving example, it makes sense that recognizing traffic lights and cars and pedestrians, those should have similar features that could also help you recognize stop signs, because these are all features of roads. Second, this is less of a hard and fast rule, so this isn’t always true. But what I see from a lot of successful multi-task learning settings is that the amount of data you have for each task is quite similar. When you recall from transfer learning, you learn from some task A and transfer it to some task B. So if you have a million examples for task A then and 1,000 examples for task B, then all the knowledge you learned from that million examples could really help augment the much smaller data set you have for task B. In multi-task learning you usually have a lot more tasks than just two. So maybe you have, previously we had 4 tasks but let’s say you have 100 tasks. And you’re going to do multi-task learning to try to recognize 100 different types of objects at the same time. So what you may find is that you may have 1,000 examples per task and so if you focus on the performance of just one task, let’s focus on the performance on the 100th task, you can call A100. If you are trying to do this final task in isolation, you would have had just a thousand examples to train this one task, this one of the 100 tasks that by training on these 99 other tasks. These in aggregate have 99,000 training examples which could be a big boost, could give a lot of knowledge to argument this otherwise, relatively small 1,000 example training set that you have for task A100. And symmetrically every one of the other 99 tasks can provide some data or provide some knowledge that help every one of the other tasks in this list of 100 tasks. So the second bullet isn’t a hard and fast rule but what I tend to look at is if you focus on any one task, for that to get a big boost for multi-task learning, the other tasks in aggregate need to have quite a lot more data than for that one task. And so one way to satisfy that is if a lot of tasks like we have in this example on the right, and if the amount of data you have in each task is quite similar. But the key really is that if you already have 1,000 examples for 1 task, then for all of the other tasks you better have a lot more than 1,000 examples if those other other task are meant to help you do better on this final task. And finally multi-task learning tends to make more sense when you can train a big enough neural network to do well on all the tasks. The alternative to multi-task learning would be to train a separate neural network for each task. Rather than training one neural network for pedestrian, car, stop sign, and traffic light detection, you could have trained one neural network for pedestrian detection, one neural network for car detection, one neural network for stop sign detection, and one neural network for traffic light detection. So what a researcher, Rich Carona, found many years ago was that the only times multi-task learning hurts performance compared to training separate neural networks is if your neural network isn’t big enough. But if you can train a big enough neural network, then multi-task learning certainly should not or should very rarely hurt performance. And hopefully it will actually help performance compared to if you were training neural networks to do these different tasks in isolation. End to end deep learning Briefly, there have been some data processing systems, or learning systems that require multiple stages of processing. And what end-to-end deep learning does, is it can take all those multiple stages, and replace it usually with just a single neural network. One of the challenges of end-to-end deep learning is that you might need a lot of data before it works well. For example, if you’re training on 3,000 hours of data to build a speech recognition system, then the traditional pipeline, the full traditional pipeline works really well. It’s only when you have a very large data set, you know one could say 10,000 hours of data, anything going up to maybe 100,000 hours of data that the end-to end-approach then suddenly starts to work really well. When you have a smaller data set, the more traditional pipeline approach actually works just as well. Often works even better. And you need a large data set before the end-to-end approach really shines. And if you have a medium amount of data, then there are also intermediate approaches where maybe you input audio and bypass the features and just learn to output the phonemes of the neural network, and then at some other stages as well. Reference avoidable bias, variance, human level performance","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Coursera","slug":"Coursera","permalink":"https://jmj3047.github.io/tags/Coursera/"},{"name":"AI Studies","slug":"AI-Studies","permalink":"https://jmj3047.github.io/tags/AI-Studies/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}]},{"title":"An ongoing review of speech emotion recognition","slug":"SER_Review","date":"2023-04-25T15:00:00.000Z","updated":"2023-04-27T14:20:29.636Z","comments":true,"path":"2023/04/26/SER_Review/","link":"","permalink":"https://jmj3047.github.io/2023/04/26/SER_Review/","excerpt":"","text":"Journal&#x2F;Conference : Science DirectYear(published year): 2023Author: Javier de Lope, Manuel GrañaSubject: Speech Emtion Recognition, Speech feature extraction Summary This paper provides an overview of recent and classical approaches to speech emotion recognition (SER) using machine learning and deep learning techniques. SER is an active area of research that involves recognizing emotional states from speech signals, which can have applications in fields such as human-computer interaction, psychology, and healthcare. Classical machine learning approaches for SER include support vector machines (SVMs), k-nearest neighbors (kNN), decision trees, Gaussian mixture models (GMMs), among others. These approaches typically involve extracting features from speech signals using techniques such as Mel Frequency Cepstral Coefficients (MFCCs) or prosodic features like pitch or energy. Recent deep learning approaches for SER include convolutional neural networks (CNNs), recurrent neural networks (RNNs), long short-term memory (LSTM) networks, among others. These approaches usually encompass both the feature extraction and classification phases and have shown promising results in some datasets. Transfer learning is a technique used in DL that involves reusing a pre-trained neural network model on a new task or dataset. In SER, transfer learning has been applied to CNNs and RNNs to leverage pre-trained models on related tasks and reduce the amount of training data required. Dataset 12개 데이터 셋, 2004년~2022년도에 발행된 93개의 논문 Table1에는 VAM이 빠져잇어 11개의 데이터 셋만 비교함 DES 덴마크어 emotion Database 4명의 전문 배우(여성 2명, 남성 2명)의 녹음 30분 분량의 연설로 구성 2개의 독립된 단어 (예, 아니오), 9개의 짧은 구문(4개 질문, 5개 평서문), 두개의 구절로 구성 중립, 놀람, 행복, 슬픔, 분노 총 5개의 감정 EMODB 독일어 emotion Database 10명의 배우(여성 5명, 남성 5명) 1487초(평균 2.77초) 짧은 문장 5개, 긴 문장 5개 일부 감정 표현에는 동일한 발화자가 기록한 두가지 버전이 있음 약 800개의 문장 제공(중복 포함), 535개 발화(중복제거) 분노, 중립, 화남, 지루함, 행복, 슬픔, 혐오 총 7가지 감정 eNTERFACE 89 영어 audio-visual emotion Database 14개의 국적의 42명(여성 19%, 남성 81%) 6개의 단편 스토리에 대한 반응을 표현 행복, 슬펌, 놀라움, 분노, 혐오감, 두려움 총 6가지 감정 IEMOCAP Interactive Emotional Dyadic Motion Capture Database collected by University of South California 10명의 배우가 대본 시나리오와 즉흥 시나리오에서 얼굴, 머리, 손 등에 마커를 부착한채 녹음이 진행됨 12시간분량의 데이터 오디오는 3-15초 사이로 나눠지며 3-4명의 사람이 라벨링을 함 분노, 슬픔, 행복, 혐오감, 두려움, 좌절, 흥분, 중립 등 10가지 감정으로 확장됨(원래 설계는 분노, 슬픔, 행복, 좌절, 중립 5개 였음) SAVEE 영어 Audio-Visual Expressed Emotion Database 4명의 영국 남성 배우의 오디오 및 비디오 녹음 120개의 발화를 연기, 총 480개의 문장 감정별로 15개의 문장(공통 문장 3개, 감정별 문장 2개, 일반 문장 10개) 중립, 분노, 혐오, 공포, 행복, 슬픔, 놀람 총 7가지 감정 Thai DB Audiovisual Thai Emotion Database 6명의 드라마 전공생들 1~7음절로 구성된 일반적인 태국어 단어 1000개 → 최종으로는 972개 단어 행복, 슬픔, 놀라움, 분노, 두려움, 혐오감 총 6가지 감정 INTER1SP 스페인어 emotion database 남성 1명과 여성 1명의 Spanish 전문 배우의 녹음 단어, 숫자, 문장을 포함하는 184개 발화를 포함. 각 화자 마다 4시간 분량의 데이터, 6040개의 샘플이 포함 분노, 혐오, 공포, 행복, 슬픔, 놀라움, 중립등의 감정이 고려됨 TESS Toronto emotion database 2명의 전문 배우 200개의 단어, 2800개의 샘플 분노, 혐오, 두려움, 행복, 유쾌함, 슬픔, 중립 총 7가지 감정 RAVDESS 영어 Audio-Visual Database of Emotional Speech and Song 24명의 전문배우(여성 12명, 남성 12명) 2개의 문구, 1440개 음성 오디오 샘플(오디오 하나당 약 3초) 7356개의 오디오 및 비디오 클립으로 구성(25GB) 중립, 평온, 행복, 슬픔, 분노, 두려움, 혐오, 놀라움 총 8가지 감정 JL-Corpus 뉴질랜드 emotion database 5개의 기본감정과 5개의 보조 감정 [58]논문 외에는 다른곳에서 사용한 언급을 찾을수 없음 MSP-PODCAST 영어 emotion database 오픈 오디오 데이터 소스로 자연스러운 (일반인의) 녹음 27시간(2.75~11초), 18000개의 자연스러운 감정 문장 300명의 평가자가 라벨링 8개의 감정(기본 arousal, valence, dominance + extended list of emotions) Machine Learning Conventional machine learning approaches found in the literature. VK SVM &#x3D; various kernels for the SVM RSVM &#x3D; radial basis function kernel SVM TSVM &#x3D; twins SVM, LR &#x3D; logistic regression MLP &#x3D; multilayer perceptron HuM &#x3D; Humoments ELMDT &#x3D; extreme learning machine decision tree BN &#x3D; Bayes networks GMM &#x3D; Gaussian mixture model EDT &#x3D; Ensembles of decision trees kNN &#x3D; k nearest neighbors NNMF &#x3D; non negative matrix factorization. Deep Learning 그림 1에는 CNN과 LSTM을 결합한 음성 감정 인식을 위한 일반적인 딥러닝 구조에 대해 설명합니다. CNN을 사용해 감정 특징을 학습하거나 기존의 수작업 특징 없이 CNN과 RNN을 연결해 기존 방식보다 더 높은 정확도를 달성한 초기 접근법의 예를 소개합니다. 이 문장들은 CNN 및 LSTM과 같은 딥러닝 모델을 사용하여 음성에서 감정을 인식하는 다양한 연구들을 설명합니다. 연구자들은 이러한 모델을 다양한 방식으로 결합하여 사용하였으며, CNN을 사용하여 affective feature를 학습하거나, CNN과 RNN을 결합하여 새로운 아키텍처를 만들어 감정을 인식하는 등 다양한 방법을 사용하였습니다. 많은 연구에서는 기존의 방법과 비교하여 높은 정확도를 보고하였습니다. 연구들은 MFCC, LFE(Log Mel-Filterbank Energies*)*, spectrograms 및 Mel spectrograms 등 다양한 유형의 데이터를 결합하고, 병렬화된 convolutional recurrent neural networks, multi-CNN, 1D convolutional layers 등 다양한 아키텍처를 사용합니다. 이러한 실험들은 다양한 데이터베이스에서 수행되었습니다. CNN, LSTM, transformers 등 딥러닝 모델을 사용하여 음성에서 감정을 인식하는 다양한 연구들을 설명합니다. 연구자들은 훈련 및 테스트를 위해 다양한 데이터셋을 사용하며, 데이터 증강 및 깊은 메트릭 러닝과 같은 다양한 기술을 적용하여 감정 인식 결과를 개선하려고 노력합니다. DL network, recurrent neural networks(RNN) 및 gated recurrent units (GRU) 등의 딥러닝 모델을 사용하여 음성에서 감정을 분류하기 위한 다양한 연구들을 설명합니다. 연구들은 spectrograms 및 MFCC와 같은 다양한 기술을 사용하며, 기존의 최신 방법보다 더 나은 정확도를 보고하고 있습니다. 하나의 연구에서는 전통적인 데이터 증강 기술을 사용하지 않는 DL network를 제안하며, 더 큰 이미지 차원이 더 높은 정확도를 보이지만 계산 복잡성이 증가한다는 것을 보고하고 있습니다. 이미지 스펙트럼에 적용되는 잘 알려진 DL 아키텍처를 transfer learning하는 것이 많은 DL 접근 방법에 사용됩니다. 많은 논문들이 CNN을 이용하여 스펙트럼 이미지를 처리하며, 그 중 Stolar et al. [130], Badshah et al. [12]은 AlexNet 모델을 transfer learning 하는 방법을 사용합니다. 또한, Huang and Bao [52], Zhang et al. [163], Gerczuk et al. [42], Popova et al. [106], Wang et al.[143] 등은 스펙트럼 이미지나 MFCC를 특징으로 하는 DL transfer learning 방법을 사용합니다. Tripathi et al.은 ResNet 기반의 신경망을 제안하여 어려운 샘플에 더 많은 중요도를 부여하는 focal loss로 감정 인식을 훈련시켰습니다. 이는 다양한 클래스 간에 중요한 클래스 불균형이 존재할 때 정확도를 향상시키려는 것입니다. Park et al.은 특성 입력에 직접 적용되는 음성 인식용 데이터 증강 방법을 제안하여, 주파수 채널 블록 및 시간 단계 블록을 마스킹하는 방법 등으로 특성을 왜곡합니다. 이 방법을 사용하여 훈련 세트를 보강하여 언어 모델의 도움 없이도 상태가 좋은 결과를 얻을 수 있었다고 보고하고 있습니다. Yi et al.는 생성 적대 신경망(GAN)을 사용하여 데이터 증강을 수행합니다. Shilandari et al. 및 Latif et al.도 데이터 증강을 위해 GAN을 제안합니다. Bakhshi et al.은 CNN에서 사용하기 위해 오디오 녹음에서 질감 있는 이미지를 생성합니다. Zeng et al. [156]은 오디오 파일에서 생성 된 스펙트로그램을 기능으로 사용하며 LSTM에서 사용하는 게이트 메커니즘과 유사한 ResNet 아키텍처를 기반으로 한 DL 접근 방식을 제안합니다. Jannat et al. [58]은 오디오 기능만 사용하여 Inception-v3 딥러닝 아키텍처를 사용하여 행복과 슬픔에 대해 66.41 %의 정확도 (교차 검증)를 달성하는 다중 모달 접근 방식을 사용합니다. Sanchez-Gutierrez 및 Gonzalez-Perez [113]는 딥러닝 네트워크에서 유용한 뉴런 노드를 식별하고 제거하여 오류율을 감소시키기 위해 여러 판별적 측정 방법을 적용하며, Manohar 및 Logashanmugam [84]은 감정 분류에 대한 딥러닝 네트워크의 성능을 높이기 위한 기능 선택 방법을 제안합니다. Wang et al. [145]는 이미지와 오디오 녹음으로부터 감정을 인식하는 멀티모달 시스템을 제안한다. 특히, 오디오 서브시스템은 CNN과 LSTM 네트워크를 사용하며, 녹음에서 생성된 스펙트로그램 이미지를 입력으로 사용한다. Heredia et al. [50]는 소셜 로봇에서 감정을 감지하기 위한 멀티모달 (비디오, 오디오 및 텍스트) DL 아키텍처를 제안하고 있다. Conclusion 이제는 예측 모델의 비교 및 새로운 발전을 지탱할 데이터의 가용성이 과학의 핵심 중 하나이다. SER 분야에서는 지역적인 작은 규모의 데이터셋이 많다. 최근 데이터셋 중 일부는 아직 활용되지 않았으며, 오래된 데이터셋 중 일부만 활용되어 새로운 데이터셋이 제안될 때마다 오래되어 새로운 결론이 되지 않을 수 있다. SER 분야에서 중요한 신호 특성과 DL 아키텍처의 보급이 이미 시작되었지만, 이러한 접근 방식은 데이터 샘플링의 민감성 및 유효성 검사에 대한 분석과 검증이 필요하다. 또한, 이 분야에서는 새로운 아키텍처 및 기능이 빠르게 나타나고 있으므로, 고객과의 감성적 상호작용의 높은 가치 때문에 성능의 큰 향상이 곧 나타날 것으로 기대된다","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speech Emotion Recognition","slug":"Paper/Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/categories/Paper/Speech-Emotion-Recognition/"}],"tags":[{"name":"Speech Emotion Recognition","slug":"Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/tags/Speech-Emotion-Recognition/"},{"name":"Speech Feature Extraction","slug":"Speech-Feature-Extraction","permalink":"https://jmj3047.github.io/tags/Speech-Feature-Extraction/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speech Emotion Recognition","slug":"Paper/Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/categories/Paper/Speech-Emotion-Recognition/"}]},{"title":"Imporoving Deep Neural Networks, Hyper parameter tuning, Regularization and Optimization_Quiz","slug":"DL2_Quiz","date":"2023-04-22T15:00:00.000Z","updated":"2023-06-03T13:33:05.137Z","comments":true,"path":"2023/04/23/DL2_Quiz/","link":"","permalink":"https://jmj3047.github.io/2023/04/23/DL2_Quiz/","excerpt":"","text":"개요Coursera Deep Learning Course 2 Quiz 1.Practical aspects of Deep Learning Link: https://www.coursera.org/learn/deep-neural-network/home/module/1 2.Optimization Algorithms Link: https://www.coursera.org/learn/deep-neural-network/home/module/2 3.Hyperparameter tuning, Batch Normalization, Programming Frameworks Link: https://www.coursera.org/learn/deep-neural-network/home/module/3","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"Coursera","slug":"Coursera","permalink":"https://jmj3047.github.io/tags/Coursera/"},{"name":"Quiz","slug":"Quiz","permalink":"https://jmj3047.github.io/tags/Quiz/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}]},{"title":"wav2vec 2.0, A Framework for Self-Supervised Learning of Speech Representations","slug":"Wav2Vec","date":"2023-04-20T15:00:00.000Z","updated":"2023-04-24T00:29:44.647Z","comments":true,"path":"2023/04/21/Wav2Vec/","link":"","permalink":"https://jmj3047.github.io/2023/04/21/Wav2Vec/","excerpt":"","text":"Journal&#x2F;Conference : IEEEYear(published year): 2020Author: Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael AuliSubject: self-supervised learning, speech representation Self-supervised learning은 레이블이 없는 데이터에서 일반적인 데이터 표현을 학습하고, 이를 레이블이 있는 데이터에서 세부 조정하는 패러다임입니다. 이 방법은 자연어 처리 분야에서 특히 성공적으로 적용되어 왔으며, 컴퓨터 비전 분야에서도 활발한 연구 주제 중 하나입니다. Self-supervised learning은 지도 학습과 달리 레이블링 작업에 대한 인력과 시간을 절약할 수 있으며, 대규모 데이터셋에서 효과적으로 작동할 수 있습니다. wav2vec 2.0은 음성 처리 분야에서 self-supervised learning을 사용하여 강력한 음성 표현을 학습하는 방법 중 하나입니다. WER는 ‘단어 오류 비율’을 의미하는 용어로, 음성 인식 시스템의 성능을 평가하는 데 자주 사용됩니다. 첫 페이지의 초록에서는 wav2vec 2.0이 모든 레이블 데이터를 사용할 때 Librispeech의 깨끗한&#x2F;다른 테스트 세트에서 각각 1.8&#x2F;3.3 WER을 달성한다고 언급되었습니다. 이것은 시스템이 깨끗한 테스트 세트에서 100 단어당 평균 1.8개의 오류와 다른 테스트 세트에서 100 단어당 평균 3.3개의 오류를 만든다는 것을 의미합니다. Latent representations은 입력 데이터의 특징을 나타내는 벡터입니다. 이러한 벡터는 일반적으로 인공 신경망의 중간 계층에서 추출됩니다. wav2vec 2.0에서는 음성 입력을 잠재 공간에서 마스킹하고, 이를 함께 학습하는 양자화된 잠재 표현을 사용하여 대조적인 작업을 해결합니다. 이를 통해 wav2vec 2.0은 레이블이 없는 음성 데이터로부터 강력한 음성 표현을 학습할 수 있습니다. 잠재 공간은 인공 신경망의 hidden layer와 유사한 개념입니다. 인공 신경망에서 입력 데이터는 여러 개의 hidden layer를 거쳐 출력층으로 전달됩니다. 이때 hidden layer에서 추출된 벡터를 잠재 공간이라고 부릅니다. wav2vec 2.0에서도 음성 입력은 여러 개의 hidden layer를 거쳐 잠재 공간에서 마스킹되고, 이를 함께 학습하는 양자화된 잠재 표현을 사용하여 대조적인 작업을 해결합니다. wav2vec 2.0에서는 음성 입력을 잠재 공간에서 마스킹하는 방법을 사용합니다. 이 방법은 레이블이 없는 데이터로부터 강력한 음성 표현을 학습하기 위해 사용됩니다. 이를 위해 wav2vec 2.0은 입력 음성 신호를 잠재 공간으로 변환하고, 일부 잠재 벡터를 무작위로 마스킹합니다. 그런 다음, 모델은 마스킹된 벡터를 예측하도록 훈련됩니다. 이러한 방식으로 wav2vec 2.0은 레이블이 없는 데이터에서도 유효한 정보를 추출할 수 있습니다. wav2vec 2.0 모델은 다음과 같은 단계로 데이터가 흘러갑니다: 입력 데이터인 raw audio X는 multi-layer convolutional feature encoder f를 통해 잠재 공간의 벡터 z1, …, zT로 변환됩니다. 잠재 벡터들 z1, …, zT는 Transformer g를 통해 c1, …, cT로 변환됩니다. 이때, c1, …, cT는 전체 시퀀스 [9, 5, 4]에서 정보를 캡처하는 벡터입니다. feature encoder의 출력은 양자화 모듈 Z를 통해 qt로 이산화됩니다. 이렇게 얻어진 qt는 self-supervised objective (§ 3.2)에서 target을 나타내기 위해 사용됩니다. 이러한 방식으로 wav2vec 2.0은 레이블이 없는 데이터에서도 유효한 정보를 추출할 수 있습니다. 또한, wav2vec 2.0은 vq-wav2vec [5]와 비교하여 연속적인 음성 표현에 대한 context representations을 구축하고 self-attention을 사용하여 전체 시퀀스의 종속성을 캡처합니다. 전체 시퀀스에서 정보를 캡처한다는 것은, 입력 데이터의 전체 시퀀스에 대한 정보를 모델이 학습하고 이를 잘 반영하여 출력을 생성한다는 것을 의미합니다. wav2vec 2.0 모델에서는 입력 데이터인 raw audio X가 feature encoder를 통해 잠재 공간의 벡터 z1, …, zT로 변환되고, 이 벡터들이 Transformer를 통해 c1, …, cT로 변환됩니다. 이때, c1, …, cT는 전체 시퀀스 [9, 5, 4]에서 정보를 캡처하는 벡터입니다. 따라서 wav2vec 2.0 모델은 입력 데이터의 전체 시퀀스에 대한 정보를 잘 반영하여 출력을 생성할 수 있습니다. 양자화 모듈(Quantization module)은 wav2vec 2.0 모델에서 self-supervised training을 위해 사용되는데, 이 모듈은 feature encoder의 출력인 z 벡터를 유한한 speech representations 집합으로 이산화(discretize)합니다. 이러한 이산화된 벡터들은 self-supervised objective (§ 3.2)에서 타겟을 나타내는 데 사용됩니다. wav2vec 2.0 모델에서는 product quantization [25]이라는 방법을 사용하여 z 벡터를 이산화합니다. 이 방법은 벡터 공간을 여러 개의 서브 공간으로 분할하고, 각 서브 공간에 대해 centroid를 계산하여 해당 서브 공간 내의 점들을 가장 가까운 centroid와 매칭시키는 방식으로 동작합니다. 이렇게 생성된 매칭된 centroid들의 인덱스를 사용하여 z 벡터를 이산화합니다. z 벡터를 유한한 speech representations 집합으로 이산화한다는 것은, 연속적인 값을 일정한 간격으로 나누어서 유한한 speech representations 집합으로 변환하는 것을 의미합니다. 이러한 변환은 self-supervised objective (§ 3.2)에서 타겟을 나타내는 데 사용됩니다. Feature encoder는 입력 데이터인 raw audio X를 잠재 공간의 벡터 z1, …, zT로 변환하는데 사용됩니다. 이 encoder는 여러 블록으로 구성되어 있으며, 각 블록은 시간적 컨볼루션(temporal convolution)을 포함하고 있습니다. 이후 layer normalization [1]과 GELU activation function [21]이 적용됩니다. 또한, Feature encoder에 입력되는 raw waveform은 zero mean과 unit variance로 정규화됩니다. 이렇게 정규화된 입력 데이터는 Encoder의 총 stride에 따라 T개의 time-steps로 Transformer (§ 4.2)에 입력됩니다. 따라서 Feature encoder는 입력 데이터를 잘 처리하여 Transformer에 전달하는 역할을 합니다. Contextualized representations with Transformers는 wav2vec 2.0 모델에서 사용되는 기술 중 하나입니다. 이 기술은 feature encoder의 출력인 z 벡터를 context network로 전달하여, 입력 데이터의 전체 시퀀스에 대한 정보를 잘 반영하는 벡터 c1, …, cT를 생성합니다. 이때, context network는 Transformer architecture [55, 9, 33]을 따릅니다. Transformer는 입력 데이터의 전체 시퀀스에 대한 정보를 잘 반영할 수 있는 구조로, 이전의 RNN과 같은 모델보다 더욱 효과적으로 입력 데이터를 처리할 수 있습니다. Transformer architecture는 딥러닝 모델 중 하나로, 입력 데이터의 전체 시퀀스에 대한 정보를 잘 반영할 수 있는 구조를 가지고 있습니다. 이 구조는 RNN과 같은 모델보다 더욱 효과적으로 입력 데이터를 처리할 수 있습니다. Transformer architecture는 self-attention mechanism을 사용하여 입력 데이터의 전체 시퀀스에 대한 정보를 잘 반영합니다. Self-attention mechanism은 입력 데이터 내에서 서로 다른 위치의 정보를 상호작용시켜서, 해당 위치의 정보가 다른 위치에서 어떻게 사용되는지 학습합니다. Transformer architecture는 크게 두 가지 부분으로 나뉩니다. 첫 번째 부분은 encoder이며, 입력 데이터를 임베딩하고 여러 층의 self-attention과 feed-forward network layer로 구성됩니다. 두 번째 부분은 decoder이며, encoder에서 생성된 벡터들을 기반으로 출력 시퀀스를 생성하는데 사용됩니다. Transformer architecture는 자연어 처리 분야에서 많이 사용되며, 특히 기계 번역 분야에서 좋은 성능을 보입니다. wav2vec 2.0 모델에서도 Transformer architecture가 feature encoder와 context network에 적용되어, 입력 데이터의 전체 시퀀스에 대한 정보를 잘 반영하는 벡터들을 생성합니다. 또한, wav2vec 2.0 모델에서는 absolute positional information을 인코딩하는 고정된 positional embeddings 대신 relative positional embedding을 사용합니다. 이러한 relative positional embedding은 convolutional layer와 유사하게 동작하며 [37, 4, 57], 입력 데이터의 상대적인 위치 정보를 인코딩합니다. 마지막으로, convolutional layer의 출력값에 GELU activation function을 적용하고 입력값에 더해준 후 layer normalization을 적용합니다. 이러한 과정을 거치면서 생성된 벡터 c1, …, cT는 입력 데이터의 전체 시퀀스에 대한 정보를 잘 반영하고 있습니다. Quantization module은 wav2vec 2.0 모델에서 사용되는 기술 중 하나입니다. 이 모듈은 self-supervised training을 위해 feature encoder의 출력인 z 벡터를 이산화(discretize)하여 사용합니다 Quantization module은 product quantization [25]이라는 방법을 사용하여 z 벡터를 이산화합니다. 이 방법은 G개의 codebook(또는 group)을 사용하며, 각 codebook은 V개의 entry로 구성됩니다. 따라서, 각 codebook에서 하나의 entry를 선택하고, 선택된 entry들을 concatenate하여 하나의 벡터 q를 생성합니다. 이러한 과정으로 생성된 q 벡터는 입력 데이터 X에 대한 잠재적인 표현(representation)으로 사용됩니다. 이때, q 벡터는 연속적인 값을 일정한 간격으로 나누어서 유한한 speech representations 집합으로 변환하는 것을 의미합니다. Quantization module은 wav2vec 2.0 모델에서 self-supervised objective (§ 3.2)에서 타겟을 나타내는 데 사용됩니다. 즉, 입력 데이터 X와 q 벡터 사이의 관계를 학습하여, 입력 데이터 X에 대한 잠재적인 표현(representation)을 생성하는 데 활용됩니다. wav2vec 2.0 모델의 training은 크게 pre-training과 fine-tuning 두 단계로 나뉩니다. 이 두 단계에서 사용되는 데이터는 모두 self-supervised learning을 기반으로 합니다. Pre-training 단계에서는, 입력 데이터 X를 이용하여 feature encoder와 context network를 학습합니다. 이때, 입력 데이터 X는 일정한 비율로 time steps가 mask되어 있습니다. Masking된 time steps에 대해서, 모델은 해당 time step의 quantized latent audio representation을 식별해야 합니다. Fine-tuning 단계에서는, labeled data를 이용하여 pre-trained model을 fine-tuning합니다. Fine-tuning에 사용되는 labeled data는 speech recognition task나 speaker identification task 등과 같은 downstream task에서 수집된 데이터입니다. 따라서, wav2vec 2.0 모델의 training 과정은 다음과 같습니다. Pre-training: 입력 데이터 X를 이용하여 feature encoder와 context network를 학습합니다. Masked language modeling 방식을 사용하여, 일부 time steps가 mask됩니다. Masking된 time steps에 대해서, 모델은 해당 time step의 quantized latent audio representation을 식별해야 합니다. Fine-tuning: labeled data를 이용하여 pre-trained model을 fine-tuning합니다. Downstream task에서 수집된 labeled data를 사용합니다. Fine-tuned model은 downstream task에서 좋은 성능을 보입니다. quantized latent audio representation을 식별해야 한다는 것은, masked time step에서 입력 데이터 X에 대한 feature encoder의 출력인 z 벡터를 이산화하여 생성된 q 벡터 중에서, 정확히 어떤 q 벡터가 해당 masked time step에 대한 올바른 표현(representation)인지를 식별해야 한다는 것을 의미합니다. 즉, wav2vec 2.0 모델은 pre-training 단계에서 입력 데이터 X의 일부 time steps를 mask하고, 이러한 masked time steps에 대해서 올바른 quantized latent audio representation을 식별하는 데 초점을 둡니다. 이러한 방식으로 모델은 입력 데이터 X에 대한 잠재적인 표현(representation)을 학습하게 됩니다. 이러한 pre-training 단계에서 학습된 모델은 fine-tuning 단계에서 downstream task에 적용됩니다. Fine-tuning 단계에서는 labeled data를 이용하여 pre-trained model을 fine-tuning하며, 이때 모델은 downstream task에서 좋은 성능을 보이도록 학습됩니다 입력 데이터가 context network를 학습한다는 것은, wav2vec 2.0 모델에서 입력 데이터 X의 잠재적인 표현(representation)을 생성하기 위해 context network가 사용된다는 것을 의미합니다. Context network는 feature encoder의 출력인 z 벡터를 입력으로 받아, 전체 시퀀스에 대한 정보를 잘 반영하는 c 벡터들을 생성합니다. 이때, c 벡터들은 Transformer architecture와 함께 사용되어 입력 데이터 X의 전체 시퀀스에 대한 정보를 잘 반영하는 벡터들을 생성합니다. 따라서, wav2vec 2.0 모델에서 입력 데이터 X는 feature encoder와 context network를 통해 잠재적인 표현(representation)으로 변환됩니다. 이러한 방식으로 모델은 self-supervised learning을 기반으로 하여 입력 데이터 X에 대한 좋은 표현(representation)을 학습하게 됩니다. link : https://arxiv.org/abs/2006.11477","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speech Representations","slug":"Paper/Speech-Representations","permalink":"https://jmj3047.github.io/categories/Paper/Speech-Representations/"}],"tags":[{"name":"Speech Representations","slug":"Speech-Representations","permalink":"https://jmj3047.github.io/tags/Speech-Representations/"},{"name":"Self-Supervised Learning","slug":"Self-Supervised-Learning","permalink":"https://jmj3047.github.io/tags/Self-Supervised-Learning/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speech Representations","slug":"Paper/Speech-Representations","permalink":"https://jmj3047.github.io/categories/Paper/Speech-Representations/"}]},{"title":"The Geneva Minimalistic Acoustic Parameter Set (GeMAPS) for Voice Research andAffective Computing","slug":"eGeMAPS","date":"2023-04-20T15:00:00.000Z","updated":"2023-04-20T21:06:58.521Z","comments":true,"path":"2023/04/21/eGeMAPS/","link":"","permalink":"https://jmj3047.github.io/2023/04/21/eGeMAPS/","excerpt":"","text":"Journal&#x2F;Conference : IEEEYear(published year): 2016Author: Florian Eyben, Klaus R. Scherer, Bjorn W. Schuller, Johan Sundberg, Elisabeth Andre, Carlos Busso, Laurence Y. Devillers, Julien Epps, Petri Laukka, Shrikanth S. Narayanan, and Khiet P. TruongSubject: Acoustic Parameter Set, eGeMAPS Summary 이 논문은 음성 특징 추출을 위한 최소한의 매개 변수 세트를 제안합니다. 제안된 매개 변수 세트는 다양한 감정 분류 작업에서 높은 성능을 보입니다. binary arousal 및 binary valence 분류에 대한 요약 결과가 Table 2에 나와 있습니다. FAU AIBO를 제외한 모든 데이터베이스와 9개의 최고 SVM 복잡도(C&#x3D;0.0025)에서 UAR(Unweighted Average Recall)을 평균화합니다. 각 화자에 대한 표준화와 균형 잡힌 훈련 세트를 위해 인스턴스 업샘플링이 수행됩니다. 88개 파라미터 정리 기본 GeMAPS, 62개 파라미터 Pitch Jitter Formant 1 frequency Formant 2 frequency Formant 3 frequency Formant 1 Shimmer Loudness Harmonics-to-noise ratio (HNR) Alpha Ratio Hammarberg Index Spectral Slope 0-500 Hz Spectral Slope 500-1500 Hz Formant 1 relative energy Formant 2 relative energy Formant 3 relative energy Harmonic difference H1-H2 Harmonic difference H1-A3 위 18개의 산술평균과 변동계수를 적용 -&gt; 36개 + loudness 8가지 함수 추가 적용 파라미터+ pitch 8가지 함수 추가 적용 파라미터 -&gt; 52개 Alpha Ratio의 산술평균 Hammarberg Index의 산술평균 0-500 Hz 스펙트럼 기울기의 산술평균 (무성음 구간 전체) 500-1500 Hz 스펙트럼 기울기의 산술평균 (무성음 구간 전체) the rate of loudness peaks continuously voiced regions의 평균 길이(F0 &gt; 0) continuously voiced regions의 표준편차(F0 &gt; 0) unvoiced regions (approximating pauses)의 평균 길이(F0 &#x3D; 0) unvoiced regions (approximating pauses)의 표준편차(F0 &#x3D; 0) continuous voiced regions의 초당 개수 (pseudo syllable rate) extended GeMAPS(eGeMAPS), 추가 26개 파라미터 7개의 LLD에 대해 산술평균과 변동계수 적용 -&gt; 14개의 discriptor 추가 MFCC 1 (MFCC의 첫번째 계수) MFCC 2 MFCC 3 MFCC 4 Spectral flux Formant 2 bandwidth Formant 2 bandwidth 11개의 discriptor가 추가 무성영역에서만 spectral flux의 산술 평균 유성영역에서만 spectral flux의 산술 평균 유성영역에서만 spectral flux의 변동 계수 유성 영역에서만 MFCC 1의 변동 계수 유성 영역에서만 MFCC 2의 변동 계수 유성 영역에서만 MFCC 3의 변동 계수 유성 영역에서만 MFCC 4의 변동 계수 유성 영역에서만 MFCC 1의 산술 평균 유성 영역에서만 MFCC 2의 산술 평균 유성 영역에서만 MFCC 3의 산술 평균 유성 영역에서만 MFCC 4의 산술 평균 equivalent soud level 이렇게 하면 확장된 파라미터에는 14+11+1 26개의 파라미터가 추가 됨 0.abstract GeMAPS는 음성과 감정 컴퓨팅 분야에서 사용되는 기본 표준 음향 파라미터 세트입니다. 이 세트는 다양한 자동 음성 분석 영역에 적용될 수 있습니다. GeMAPS는 감정적 생리적 변화를 색인화하는 데 잠재력이 있습니다. 이전 연구에서 입증된 가치와 이론적 중요성을 가지며, 자동 추출 가능합니다. GeMAPS는 미래 연구 평가의 공통 기준을 제공하고, 다른 파라미터 세트 또는 동일한 파라미터의 다른 구현으로 인한 차이를 제거하기 위해 만들어졌습니다. 1. Introduction 이 단락에서는 다양한 감정 상태의 음성 표현에 대한 관심이 오랫동안 지속되어 왔으며, 다양한 분야의 연구자들이 이 주제에 대해 연구해 왔다고 설명합니다. 정신과 의사는 감정 상태를 진단하기 위해 노력해 왔으며, 심리학자와 커뮤니케이션 연구자들은 감정의 신호를 전달하는 목소리의 능력을 탐구해 왔습니다. 언어학자와 음성학자들은 언어 생산과 지각에서 정서적 실용적 정보의 역할을 발견해 왔습니다. 최근에는 컴퓨터 과학자와 엔지니어들이 화자의 태도와 감정을 자동으로 인식하고 조작하여 인간 사용자가 정보 기술에 더 쉽게 접근하고 신뢰할 수 있도록 하려는 시도를 하고 있습니다. 이러한 연구의 대부분은 음성 신호에서 음향 매개변수를 추출하여 다양한 감정과 기타 정서적 성향이 발성 패턴을 통해 어떻게 표현되는지 이해하는 방법을 사용합니다. 기본 이론적 가정은 정서적 과정으로 인한 자율신경 흥분과 근육 긴장의 변화를 음향 파형의 다양한 파라미터로 추정할 수 있다는 것입니다. GeMAPS는 음성과 감정 컴퓨팅 분야에서 사용되는 기본 표준 음향 파라미터 세트로, 이전 연구에서는 다양한 파라미터들이 사용되었지만 일관된 방식으로 추출되지 않았습니다. 따라서 GeMAPS는 다양한 자동 음성 분석 영역에 적용될 수 있는 기본 표준 세트로서, 감정적 생리적 변화를 색인화하는 데 잠재력이 있으며, 이전 연구에서 입증된 가치와 이론적 중요성을 가지고 있습니다. 2.Related Work CEICES는 더 엔지니어링 중심적인 “수집기(collector)” 접근 방식으로, 분류 실험에서 성공적인 파라미터들을 모두 포함합니다. 반면, GeMAPS는 다양한 출처의 교차학제적 증거와 이론적 중요성 또는 몇 가지 파라미터를 기반으로 하는 최소한의 파라미터 세트를 합의하는 보다 교차학제적인 시도입니다. 초기 조사[15]와 최근 개요[17]는 정서적 음성 연구에 관한 수십 년간의 심리학 문헌을 잘 요약하고 있으며, 제시된 경험적 데이터를 통해 강도(음량) F0(기본 주파수) 평균 변동성 및 범위 음성 신호의 고주파 콘텐츠&#x2F;에너지가 스트레스(강도, F0 평균) 분노 및 슬픔(모든 매개변수) 지루함(F0 변동성 및 범위)과 같은 전형적인 음성 정서 표현과 상관관계를 보인다는 결론을 내렸습니다. speech와 articulation rate은 감정표현에 영향을 미치지 않는다는 것으로 나타났습니다. [16]은 F0 및 스펙트럼 분포와 관련된 매개변수가 감정적 발화 내용에 대한 중요한 단서임을 확인함 [17]과 같이 대부분의 연구는 청각적 감정분석을 다루며 accoustic arousal에 대한 단서인 상당히 일관된 매개변수를 보고함 [24]에 보면, 그냥 음량을 측정하는거보다 다양한 주파수 대역의 신호에너지에 human-hearing’s frequency sensitivity에 따라 가중치를 부과하면 vocal affect dimension과 더 많은 상관관계가 있는 것으로 나타났음. 또한 spectral flux가 단일 피처에 대해 전반적으로 가장 높은 상관관계를 보였음. [17],[25] : particular valence(감정의 긍정성 또는 부정성을 나타내는 지표) [25]에서는 LOI(Level of Interest, 대화 참여자의 관심 수준을 나타내는 지표입니다. LOI는 boredom(지루함), overneutral(중립), joyful interaction(즐거운 상호작용)으로 구분) 같은 경우 MFCC의 중요성이 크다는 것을 보여줌, lower order MFCC는 스펙트럼 전체범위(first coefficient, 첫번째 계수) 또는 다양한 작은 sub-bands(second and higher coefficient, 두번째 및 상위계수)에서 어느정도 spectral tilt(slope)를 측정합니다. spectral slope(Spectral slope는 주파수 대역에서 파워 스펙트럼의 기울기를 나타내는 지표입니다. 이 논문에서는 0-500 Hz 및 500-1500 Hz 대역에서의 스펙트럼 기울기를 계산하는 방법에 대해 설명하고 있습니다. 이러한 스펙트럼 기울기는 음성 특징 추출 및 음성 감정 인식과 같은 다양한 연구 분야에서 사용됩니다.) 이 연구의 목표는 이전의 관련 연구 결과에 따라 관련 파라미터를 선택하는 것 많은 automatically extracted brute-force 파라미터 세트는 formant 파라미터를 안정적으로 추출하기 어렵기 때문에 이를 무시하지만, voice research and automatic classification를 위해서는 formant 파라미터가 매우 중요. formant는 다양한 형태의 감정과 정신상태에 민감한 것으로 나타났으며 거의 sota 수준의 cognitive load classification result[27]와 우울증 인식 및 평가 결과[31],[38]를 제공하며 다른 시스템의 feature dimension에 비해 훨씬 적은수의 formant로도 경쟁력 있는 감정인식 성능을 제공할수 있기 때문 cognitive load: 인간의 인지 능력이 처리해야 하는 정보의 양과 복잡성에 따라 발생하는 정신적인 부담을 의미 그래서 이 논문에 제안된 파라미터세트에는 formant도 포함되어 있음. fundamental frequency(기본 주파수[6])와 진폭&#x2F;강도의 중요성이 입증되었기 때문에, 강력한 fundamental frequency 측정과 pseudo-auditory loudness(유사 청각적 음량) 측정이 제안된 세트에 포함되어 있습니다. 분포 변화를 포착하기 위해 시간에 따른 다양한 통계가 두 매개변수에 적용됩니다. 고주파 콘텐츠와 스펙트럼 밸런스를 강력하게 표현하기 위해 alpha 비율, Hammerberg 지수, 스펙트럼 기울기를 설명자로 고려합니다. Alpha Ratio는 음성 신호의 저주파 대역과 고주파 대역 간의 에너지 비율을 나타내는 지표입니다. 이 논문에서는 Alpha Ratio가 50-1000 Hz와 1-5 kHz 대역에서의 에너지 합의 비율로 정의되며, Hammarberg index와 유사한 방식으로 계산됩니다. Hammarberg index는 음성 신호의 저주파 대역과 고주파 대역 간의 에너지 비율을 나타내는 지표입니다. 이 논문에서는 Hammarberg index가 0-2 kHz와 2-5 kHz 대역에서 가장 큰 에너지 피크 간의 비율로 정의되며, 고정된 pivot point인 2 kHz를 기준으로 계산 vocal timbre(보컬 음색)은 MFCC로 인코딩되고, F0의 period-to-period jitter와 shimmer를 사용하여 음성 발화 신호의 질을 평가. Vocal timbre는 음성의 톤 색깔이나 음색을 나타내는 용어 F0의 period-to-period jitter와 shimmer는 음성 발화 신호의 안정성과 규칙성을 나타내는 지표입니다. Jitter는 연속된 F0 주기 간 최고점의 차이를 나타내며, shimmer는 연속된 F0 주기 간 최고점의 크기 차이를 나타냅니다. 이러한 지표들은 음성 발화 신호의 질을 평가하는 데 사용 모음 기반 음성 연구를 허용하고 특정 작업에 대한 관련성이 입증된 formant 파라미터도 세트에 포함되어 있습니다. 3.Acoustic Parameter Recommendation 여기에 제시된 권장 사항은 제네바에서 열린 음성 및 언어 과학자들의 학제 간 회의에서 고안되었으며 뮌헨공과대학교(TUM)에서 더욱 발전시켰습니다. 세가지 기준에 따라 매개변수의 선택이 이루어졌음 정서적 과정 중 음성 생성의 생리적 변화를 지표화 할수 있는 음향 매개변수의 잠재력 voice production 과정에서 감정적 변화가 생기면서 생리학적 변화를 나타내는 음향 파라미터의 potential. 과거 문헌에서 해당 매개변수가 사용된 빈도 및 성공 여부(related works 참조) 이론적 중요성([1],[2]) 두가지 버전의 acoustic parameter set가 권장됨 저자의 이전 연구에서 가장 중요한 것으로 밝혀진 prosodic, excitation, vocal tract, and spectral descriptors를 구현하는 최소한의 파라미터 세트 문학([40])에서 pure prosodic 및 스펙트럼 마라미터 세트보다 automatic affect recognition의 정확도를 높이는 것으로 일관되게 알려진 소량의 cepstral descriptors를 포함하는 최소한의 세트에 대한 확장 제안하는 음향 파라미터 세트: 여기서는 두 가지 버전의 음향 파라미터 세트를 제안하고 있습니다. 첫 번째는 저자들의 이전 연구에서 가장 중요하다고 판단된 prosodic, excitation, vocal tract 및 spectral descriptors를 구현한 최소한의 파라미터 세트입니다. 두 번째는 최소한의 파라미터 세트에 cepstral descriptors를 추가한 확장된 버전입니다. 이러한 cepstral descriptors는 [40]과 같은 literature에서 자주 언급되며, 순수한 prosodic 및 spectral parameter set보다 자동 감정 인식의 정확도를 높이는 것으로 알려져 있습니다. 자동 파라미터 선택에 대한 연구 결과와 MFCCs의 기본 함수에 대한 설명, [23], [24]와 같은 자동 파라미터 선택에 관한 연구들은, 감정 및 언어적 음성 분석 작업에서 낮은 차수의 MFCCs가 더 중요하다는 것을 시사합니다. MFCCs를 계산할 때 사용되는 이산 코사인 변환(DCT-II) 기본 함수를 살펴보면, 낮은 차수의 MFCCs가 스펙트럼 기울기와 스펙트럼 에너지의 전체 분포와 관련이 있다는 것이 분명합니다. 높은 차수의 MFCCs는 보다 정교한 에너지 분포를 반영하며, 음성 속성(non-verbal voice attribute)보다는 음운 내용(phonetic content)을 식별하는 데 더 중요할 것으로 추정됩니다. 3.1 Minimalistic Parameter Set 18개의 Low-Level descriptors Frequency related parameters(1~6): Pitch, logarithmic F0 on a semitone frequency scale, starting at 27.5 Hz (semitone 0). 피치, 27.5Hz(반음 0)에서 시작하는 반음 주파수 스케일에서 로그 F0입니다. 이 문장은 F0 값에 대한 설명입니다. F0 값은 음성 신호에서 기본 주파수를 나타내며, 이 값은 세미톤 주파수 스케일에서 로그함수로 변환됩니다. 이 스케일은 27.5 Hz (세미톤 0)에서 시작하며, 각 세미톤 간의 주파수 차이는 2^(1&#x2F;12)로 계산됩니다. 따라서, 예를 들어, 세미톤 1은 29.136 Hz이고, 세미톤 2는 30.868 Hz입니다. 이러한 변환을 통해 F0 값을 보다 직관적으로 이해할 수 있습니다. Jitter, deviations in individual consecutive F0 period lengths. 지터는 개별 연속 F0 주기 길이의 편차입니다. 이 문장은 음성 파라미터 중 하나인 Jitter에 대한 설명입니다. Jitter는 연속적인 F0 주기의 길이에서 발생하는 변동을 나타내는 지표입니다. 즉, F0 주기의 길이가 일정하지 않고 변동이 크면 Jitter 값이 높아집니다. 이러한 변동은 음성 신호에서 발생하는 국소적인 성대 운동 불규칙성으로 인해 발생할 수 있습니다. Jitter 값은 음성 장애 진단 및 감정 분석 등에 사용됩니다. Formant 1 frequency, Formant 2 frequency, Formant 3 frequency, centre frequency of first, second, and third formant 1,2,3 번째 formant의 중심 주파수 Formant 1, bandwidth of first formant. 첫번째 formant의 중심 파라미터 Energy&#x2F;Amplitude related parameters(7~9): Shimmer, difference of the peak amplitudes of consecutive F0 periods. 쉬머는 연속된 F0 기간의 피크 진폭의 차이입니다. 이 문장은 음성 파라미터 중 하나인 Shimmer에 대한 설명입니다. Shimmer는 연속적인 F0 주기에서 peak amplitude의 차이를 나타내는 지표입니다. 즉, F0 주기에서 peak amplitude의 차이가 크면 Shimmer 값이 높아집니다. 이러한 변동은 음성 신호에서 발생하는 국소적인 성대 운동 불규칙성으로 인해 발생할 수 있습니다. Shimmer 값은 음성 장애 진단 및 감정 분석 등에 사용됩니다. Loudness, estimate of perceived signal intensity from an auditory spectrum. 청각 스펙트럼에서 감지된 신호강도의 estimate Harmonics-to-noise ratio (HNR), relation of energy in harmonic components to energy in noise-like components. Harmonics 대 잡음비(HNR)는 Harmonics 성분의 에너지와 잡음 성분의 에너지를 고조파 성분의 에너지와 잡음 성분의 에너지의 관계입니다. 이 문장은 음성 파라미터 중 하나인 Harmonics-to-noise ratio (HNR)에 대한 설명입니다. HNR은 음성 신호에서 harmonic components와 noise-like components 간의 에너지 비율을 나타내는 지표입니다. 즉, HNR 값이 높을수록 음성 신호에서 harmonic components의 비중이 높아지고, HNR 값이 낮을수록 noise-like components의 비중이 높아집니다. 이러한 지표는 음성 신호에서 발생하는 잡음과 왜곡 등을 분석하고, 음성 장애 진단 및 감정 분석 등에 사용됩니다. Spectral (balance) parameters(10~18): Alpha Ratio, ratio of the summed energy from 50-1000 Hz and 1-5 kHz 50-1000Hz, 1-5kHz에서 합산된 에너지의 비율 음성 신호의 저주파 대역과 고주파 대역 간의 에너지 비율을 나타내는 지표 Alpha Ratio는 저주파 영역과 고주파 영역의 에너지 비율을 나타내는 파라미터입니다. 구체적으로, 50-1000 Hz와 1-5 kHz에서의 총 에너지 비율을 나타냅니다. Alpha Ratio는 음성 신호의 성질을 분석하는 데 사용됩니다. 예를 들어, Alpha Ratio가 높은 경우 저주파 성분이 많은 음성 신호일 가능성이 높습니다. 이러한 정보는 음성 질환 진단 및 치료에 유용하게 사용될 수 있습니다. Hammarberg Index, ratio of the strongest energy peak in the 0-2 kHz region to the strongest peak in the 2–5kHz region. 0-2kHz 영역에서 가장 강한 에너지 피크와 2-5kHz 영역에서 가장 강한 피크의 비율 이 문장은 음성 파라미터 중 하나인 Hammarberg Index에 대한 설명입니다. Hammarberg Index는 음성 신호에서 0-2 kHz 영역과 2-5 kHz 영역에서 가장 강한 에너지 peak 간의 비율을 나타내는 지표입니다. 즉, Hammarberg Index 값이 높을수록 0-2 kHz 영역에서의 에너지 peak가 높아지고, Hammarberg Index 값이 낮을수록 2-5 kHz 영역에서의 에너지 peak가 높아집니다. 이러한 지표는 음성 신호에서 발생하는 고음과 저음 등을 분석하고, 음성 장애 진단 및 감정 분석 등에 사용됩니다. Spectral Slope 0-500 Hz Spectral Slope 500-1500 Hz, linear regression slope of the logarithmic power spectrum within the two given bands. 0-500 Hz 과 500-1500 Hz, 주어진 두 대역 내에서 로그 파워 스펙트럼의 선형 회귀 기울기 이 문장은 음성 파라미터 중 하나인 Spectral Slope 0-500 Hz and 500-1500 Hz에 대한 설명입니다. Spectral Slope는 주어진 두 개의 주파수 범위 (0-500 Hz 및 500-1500 Hz) 내에서 로그 스케일의 파워 스펙트럼의 선형 회귀 기울기를 나타내는 지표입니다. 즉, Spectral Slope 값이 높을수록 파워 스펙트럼이 빠르게 감소하고, Spectral Slope 값이 낮을수록 파워 스펙트럼이 천천히 감소합니다. 이러한 지표는 음성 신호에서 발생하는 저역대와 고역대의 에너지 분포를 분석하고, 음성 장애 진단 및 감정 분석 등에 사용됩니다. 파워 스펙트럼은 시간 도메인의 신호를 주파수 도메인으로 변환한 것입니다. 즉, 파워 스펙트럼은 주파수별로 신호의 에너지를 나타내는 그래프입니다. 파워 스펙트럼을 계산하면 주파수 영역에서 신호의 성분을 분석할 수 있습니다. 이러한 분석은 음성 인식, 음성 합성, 음성 변조 및 음성 감정 분석 등에 사용됩니다. Formant 1 relative energy Formant 2 relative energy Formant 3 relative energy, as well as the ratio of the energy of the spectral harmonic peak at the first, second, third formant’s centre frequency to the energy of the spectral peak at F0. Formant 1, 2, 3 상대 에너지는 첫 번째, 두 번째, 세 번째 포먼트의 중심 주파수에서 spectral harmonic peak의 에너지와 F0에서 spectral peak의 에너지의 비율입니다. Harmonic difference H1-H2, ratio of energy of the first F0 harmonic (H1) to the energy of the second F0 harmonic (H2). 첫 번째 F0 Harmonic(H1)의 에너지와 두 번째 F0 Harmonic(H2)의 에너지 둘 사이의 비율입니다. Harmonic difference H1-A3, ratio of energy of the first F0 harmonic (H1) to the energy of the highest harmonic in the third formant range (A3). Harmonic 차이 H1-A3은 첫 번째 F0 harmonic(H1)의 에너지와 세 번째 formant 범위(A3)에서 가장 높은 harmonic의 에너지의 비율입니다. 첫 번째 F0 harmonic(H1)와 세 번째 formant 에너지(A3) 비율을 계산 LLD(Low-Level Descriptors)는 음성 신호에서 추출된 저수준 특징을 나타내는 지표입니다. 이러한 지표들은 시간에 따라 변화하므로, 3 프레임 길이의 대칭 이동 평균 필터(symmetric moving average filter*)*를 사용하여 시간적으로 smoothing(평활화)됩니다. smoothing은 발성 영역 내에서만 수행되기 때문에 pitch, jitter 및 shimmer와 같은 경우, 0(무성음)에서 0이 아님으로의 전환은 smoothing하지 않습니다. Smoothing은 시계열 데이터에서 잡음이나 불규칙성을 제거하고, 데이터의 전반적인 추세를 부드럽게 만드는 기술입니다. 이를 위해 대표적으로 이동 평균 필터와 같은 필터링 기법이 사용됩니다. 이동 평균 필터는 일정한 길이의 윈도우를 설정하고, 해당 윈도우 내의 데이터들의 평균값을 구하여 각각의 데이터에 대해 적용하는 방식으로 smoothing을 수행합니다. 이러한 LLD에 대해 산술평균(arithmetic mean)과 변동 계수 (coefficient of variation, 표준 편차를 산술 평균으로 정규화 한 것)가 모든 18개의 LLD에 대해 적용되어 36개의 파라미터가 생성됩니다. 음성 파라미터 추출 과정에서 LLD에 대한 통계적인 지표 계산 방법 산술평균은 각 LLD의 값들을 모두 더한 후, LLD의 개수로 나누어 평균값을 구하는 것입니다. 변동 계수는 표준 편차를 산술 평균으로 정규화하여, 데이터의 변동성을 상대적으로 비교할 수 있는 지표입니다. 이러한 산술평균과 변동 계수를 각각 18개의 LLD에 대해 적용하여, 36개의 파라미터가 생성됩니다. 음량과 피치에는 20번째, 50번째, 80번째 백분위수, 20~80번째 백분위수 범위, 상승&#x2F;하강 신호 부분의 기울기 평균과 표준편차 등 8가지 함수가 추가로 적용됩니다. 이 문장은 음성 파라미터 추출 과정에서 음량과 피치에 대해 적용되는 추가적인 함수들에 대한 설명입니다. 이러한 함수들은 20번째, 50번째, 80번째 백분위수, 20~80번째 백분위수 범위, 상승&#x2F;하강 신호 부분의 기울기 평균과 표준편차 등 총 8가지 함수로 구성됩니다. 이러한 함수들은 음성 신호에서 추출된 저수준 특징을 더욱 상세하게 분석하기 위해 사용됩니다. 예를 들어, 백분위수는 데이터의 분포를 파악하는 데 사용되며, 상승&#x2F;하강 신호 부분의 기울기 평균과 표준편차는 음성의 강도나 높낮이 변화를 분석하는 데 사용됩니다. 이러한 파라미터들은 음성 인식 및 감정 분석 등 다양한 분야에서 활용됩니다. 20,50,80 번째 백분위수: 각각 전체 데이터의 20%, 50%, 80%에 해당하는 값을 의미합니다. 예를 들어, 만약 어떤 데이터 집합에서 50번째 백분위수가 10이라면, 이는 해당 데이터 집합에서 중간값(median)이 10이라는 것을 의미합니다. 이러한 백분위수들은 데이터의 분포를 파악하고, 대표값을 추정하는 데 사용됩니다. 따라서 음성 신호에서 추출된 파라미터들에 대해 적용되는 백분위수들은 해당 파라미터들의 분포를 파악하고, 대표값을 추정하는 데 사용 상승&#x2F;하강 신호 부분의 기울기 평균과 표준편차: 음성 신호에서 추출된 파라미터들 중 하나로, 음성의 강도나 높낮이 변화를 분석하는 데 사용됩니다. 이러한 파라미터는 상승&#x2F;하강 신호 부분에서의 기울기를 계산하여, 해당 구간에서의 음성 변화 정도를 나타내는 지표입니다. 기울기 평균은 해당 구간에서의 기울기 값들을 모두 더한 후, 구간 길이로 나누어 평균값을 구하는 것이며, 표준편차는 해당 구간에서의 기울기 값들의 흩어진 정도를 나타내는 지표입니다. 이러한 파라미터들은 음성 인식 및 감정 분석 등 다양한 분야에서 활용 상승&#x2F;하강 신호 부분의 기울기 구하는 방법: 상승&#x2F;하강 신호 부분의 기울기는 일반적으로 미분을 통해 구합니다. 음성 신호에서는 일반적으로 Short-Time Energy(STE) 또는 Zero Crossing Rate(ZCR)와 같은 저수준 특징을 사용하여, 상승&#x2F;하강 신호 부분을 검출한 후, 해당 구간에서의 기울기를 계산합니다. 예를 들어, STE를 사용하여 음성 신호에서 상승&#x2F;하강 신호 부분을 검출한 후, 해당 구간에서의 기울기를 계산하기 위해서는 해당 구간 내 STE 값들의 차이를 계산하여, 이를 구간 길이로 나누어 기울기 값을 얻을 수 있습니다. loudness에 적용되는 함수들을 제외한 모든 함수는 음성영역(F0가 0이 아닌것)에서만 적용됨. 이렇게 36개 파라미터 + loudness 8가지 함수 추가 적용 파라미터 + pitch 8가지 함수 추가 적용 파라미터 = 52개의 파라미터가 최종 또한 모든 무성음 세그먼트에 대한 알파 비율의 산술 평균, 하마버그 지수, 0-500Hz 및 500-1500Hz의 스펙트럼 슬로프가 포함되어 총 56개의 파라미터가 있습니다. 또한 6개의 시간적 특징이 포함되어 있습니다. Alpha Ratio의 산술평균 Hammarberg Index의 산술평균 0-500 Hz 스펙트럼 기울기의 산술평균 (무성음 구간 전체) 500-1500 Hz 스펙트럼 기울기의 산술평균 (무성음 구간 전체) 음성 신호에서 voiced 또는 unvoiced 구간의 최소 길이가 정해져 있지 않으므로 이러한 구간이 한 프레임만큼 짧을 수도 있다. 그러나 F0 contour의 Viterbi-based smoothing은 에러로 인해 단일 voiced frame이 누락되는 것을 효과적으로 방지합니다. the rate of loudness peaks: 음성 신호에서 초당 loudness peak의 수를 나타내는 파라미터 continuously voiced regions의 평균 길이(F0 &gt; 0), continuously voiced regions의 표준편차(F0 &gt; 0), unvoiced regions (approximating pauses)의 평균 길이(F0 &#x3D; 0), unvoiced regions (approximating pauses)의 표준편차(F0 &#x3D; 0), continuous voiced regions의 초당 개수 (pseudo syllable rate) 음성 신호에서 연속적으로 발생하는 voiced 구간의 개수를 초당 단위로 나타내는 파라미터입니다. 이러한 파라미터는 음성 신호에서 발생하는 언어적인 정보를 추출하는 데 사용 파라미터들은 음성 신호에서 발생하는 pause나 강세 등과 같은 다양한 정보를 추출하는 데 사용 3.2 Extended Parameter Set 3.1의 파라미터에는 cepstral parameter나 dynamic parameter가 거의 들어가지 않음 Cepstral parameter는 음성 신호의 주파수 특성을 분석하는 데 사용되는 파라미터입니다. 이러한 파라미터는 Mel-Frequency Cepstral Coefficients (MFCCs)와 같은 변환을 통해 추출됩니다. MFCCs는 음성 신호를 일련의 주파수 대역으로 분할하고, 각 대역에서 적절한 계수를 추출하여 구성됩니다. 이러한 계수들은 음성 신호의 주파수 특성을 잘 나타내며, 음성 인식 및 감정 인식과 같은 다양한 어플리케이션에서 유용하게 사용됩니다. Dynamic parameter는 시간적인 변화를 나타내는 파라미터입니다. 이러한 파라미터는 음성 신호의 동적인 특징을 분석하는 데 사용됩니다. 예를 들어, Delta 및 Delta-Delta 계수와 같은 동적 파라미터는 MFCCs와 같은 정적 파라미터에 추가하여 사용됩니다. 이러한 동적 파라미터는 음성 신호의 빠른 변화를 잘 나타내며, 음성 인식 및 감정 인식과 같은 다양한 어플리케이션에서 유용하게 사용됩니다. 즉, 이 minimal Parameter Set은 Delta 회귀 계수 및 차이 특징과 같은 동적 파라미터를 포함하지 않습니다. 대신, 상승 및 하강하는 F0와 음량 세그먼트의 기울기만이 일부 동적 정보를 포함합니다. 즉, 이 Parameter Set에서는 두 개의 연속된 프레임 간 차이를 나타내는 파라미터가 포함되어 있지 않으며, 상승 및 하강하는 F0와 음량 세그먼트의 기울기만이 일부 동적 정보를 추출할 수 있는 파라미터로 사용됩니다. [23],[40],[41]에서는 affective state를 모델링하는데 cepstral 파라미터가 매우 성공적인 것으로 입증되었다. 따라서 확장된 set에서는 minimalistic set에 추가로 7개의 LLD를 포함하는 것으로 제안함 Spectral (balance&#x2F;shape&#x2F;dynamics) parameters: MFCC 1 (MFCC의 첫번째 계수) MFCC 2 MFCC 3 MFCC 4, Mel-Frequency Cepstral Coefficients 1-4. MFCC는 여러 개의 계수로 구성됩니다. 일반적으로 MFCC는 12개에서 40개까지의 계수를 사용합니다. 이러한 계수는 음성 신호에서 추출된 특징을 나타내며, 일반적으로 음성 인식, 감정 인식 및 화자 인식과 같은 작업에 사용됩니다. 더 낮은 순서의 MFCC는 주파수 스펙트럼의 기울기와 전체 스펙트럼 에너지 분포와 관련이 있으며, 더 높은 순서의 MFCC는 보다 세부적인 에너지 분포를 나타냅니다. 이러한 다양한 계수를 조합하여 음성 처리 작업에서 보다 정확한 결과를 얻을 수 있습니다. Spectral flux, difference of the spectra of two consecutive frames. Spectral flux는 음성 신호의 스펙트럼 변화를 측정하는 파라미터 중 하나입니다. 이 파라미터는 연속된 프레임 간 스펙트럼 차이의 제곱합을 계산하여 구합니다. 이러한 계산은 음성 신호의 주파수 성분이 얼마나 빠르게 변화하는지를 나타내며, 음성 신호의 에너지 분포가 어떻게 변화하는지를 추적할 수 있습니다. Spectral flux는 음성 인식 및 감정 인식과 같은 다양한 어플리케이션에서 유용하게 사용 Frequency related parameters: Formant 2 bandwidth Formant 3 bandwidth, added for completeness of Formant 1-3 parameters. Formant 1-3 파라미터의 완성도를 위해 Formant2,3 대역폭이 추가 함수로써 산술평균과 변동계수는 이 7가지 추가 LLD에 모두 적용됨. 세그먼트(유,무성 모두)를 포함하여 유성영역에만 함수가 적용되는 대역폭은 제외됨. → 최종적으로 14개의 discriptor가 추가가 됨 또한 무성 영역에서만 spectral flux의 산술 평균, 유성영역에서만 spectral flux의 산술 평균과 변동 계수 및 MFCC 1-4가 포함됨. → 이 결과 11개의 discriptor가 추가됨. 추가적으로 equivalent soud level이 포함됨. Equivalent sound level (LEq)은 프레임당 RMS 에너지의 평균값을 로그 스케일로 변환하여 계산됩니다. 이를 통해 음성 신호의 평균적인 에너지 레벨을 측정할 수 있습니다. LEq는 주로 소음 및 음향 관련 어플리케이션에서 사용되며, 음성 감정 인식과 같은 분야에서도 유용하게 사용됩니다. RMS (Root Mean Square) 에너지 평균값은 음성 신호의 에너지를 측정하는 방법 중 하나입니다. 이 값은 각 프레임에서 음성 신호의 진폭 값을 제곱한 후, 평균을 구한 값입니다. RMS 에너지는 음성 신호의 전체적인 에너지 레벨 이렇게 하면 총 26개의 추가 파라미터가 생성 결과적으로 extended Geneva Minimalistic Acoustic Parameter Set(eGeMAPS)는 88개의 파라미터가 포함되어 있음. 4.Baseline Evaluation 위에서 제안된 두가지 파라미터세트는 각각 binary arousal와 binary valence dimensions에서 자동 인식 작업을 수행하기 위해 평가되었습니다. 이를 위해, 여러 가지 감정적인 음성 데이터베이스에서 제공된 원래 라벨을 binary dimensional labels (Arousal&#x2F;Valence)로 매핑하여 사용하였습니다. Binary arousal와 binary valence dimensions는 감정 인식 분야에서 사용되는 두 가지 이진 차원입니다. Arousal 차원은 감정의 활성화 수준을 나타내며, 낮은 수준의 활성화는 침착하고 집중력이 높은 상태를 나타내고, 높은 수준의 활성화는 흥분하거나 공포와 같은 강한 감정을 나타냅니다. Valence 차원은 감정의 긍정성&#x2F;부정성 정도를 나타내며, 긍정적인 감정은 높은 valence 값을 가지고 부정적인 감정은 낮은 valence 값을 가집니다. 이러한 이진 차원들을 사용하여 음성 신호에서 특정한 감정을 자동으로 인식하는 작업이 가능합니다. 원래 데이터 세트에 라벨링 된 감정 TUM AVIC 데이터베이스의 Levels of Interest Geneva Multimodal Emotion Portrayals (GEMEP) corpus 및 German Berlin Emotional Speech database (EMO-DB) professional opera singers의 노래에서 나타난 감정 (GeSiE) FAU AIBO corpus의 어린이들의 발화에서 나타난 valence German talk-show recordings (Vera-am-Mittag corpus)에서 나타난 실제 감정 제안된 minimal set은 INTERSPEECH 2009 Emotion Challenge, INTERSPEECH 2010 Paralinguistic Challenge, INTERSPEECH 2011 Speaker State Challenge, INTERSPEECH 2012 Speaker Trait Challenge 및 INTERSPEECH 2013 Computational Paralingusitics ChallengE (ComParE)와 같은 대규모 brute-forced baseline acoustic feature set과 비교되었습니다. 4.1 Database 4.1.1 FAU AIBO: FAU AIBO는 세계 최초의 국제 감정 챌린지의 공식 말뭉치로 사용되었습니다. 이 데이터베이스는 Sony 애완동물 로봇 Aibo와 상호작용하는 어린이들의 발화 녹음을 포함하고 있습니다. 따라서 spontaneous 하며, 감정적으로 색칠된 독일어 발화를 포함합니다. 어린이들은 Aibo 로봇이 방향에 대한 목소리 명령에 반응한다고 알려졌지만, 실제로 로봇은 때로는 불복종적으로 행동하여 어린이들로부터 강한 감정 반응을 유도하기 위해 인간 조작자에 의해 제어되었습니다. 이 녹음은 MONT와 OHM이라는 두 개의 학교에서 총 51명의 어린이 (10-13세, 21명 남성, 30명 여성)으로부터 수집되었으며, 쉬는 시간을 제외하고 약 9.2시간의 발화가 포함되어 있습니다. 4.1.2 TUM Audiovisual Interest Corpus (TUM-AVIC): TUM Audiovisual Interest Corpus (TUM-AVIC)는 spontaneous한 affective interactions을 포함하는 audiovisual 녹음을 담고 있는 데이터베이스입니다. 이 데이터베이스는 INTERSPEECH 2010 Paralinguistics Challenge를 위한 데이터셋으로 사용되었습니다. 이 데이터셋은 제품 프레젠터가 대상자를 상품 프레젠테이션을 통해 안내하는 과정에서 수집되었습니다. 사용된 언어는 영어이지만, 대부분의 제품 프레젠터는 독일어 원어민입니다. 대상자들은 주로 유럽 및 아시아 국적입니다. 이 데이터베이스에는 21명의 대상자 (여성 10명)의 녹음이 포함되어 있습니다. LOI는 각 sub-turn마다 (화자 turn의 수동적인 pause 기반 sub-division을 통해 찾아낸) 세 가지 라벨로 표시됩니다. 이 세 가지 라벨은 boredom (대상자가 대화나 주제에 지루함을 느끼며 매우 수동적이며 대화를 따르지 않음; loi1로도 알려짐), over neutral (대상자가 대화를 따르고 참여하지만 주제에 관심이 있는지 아니면 무관심한지 판단할 수 없음; loi2로도 알려짐), joyful interaction (대상자가 대화하고 주제에 대해 더 배우고 싶어하는 강한 욕구를 보여주며, 즉, 그/그녀는 토론에 큰 관심을 가짐; loi3으로도 알려짐)입니다. 이 평가에서는 [47]의 3,002개의 구문(sub-turns)이 사용되었습니다. 이는 예를 들어 [46]에서 사용된 high interlabeller agreement를 가진 996개의 구문보다 더 많습니다. high interlabeller agreement는 서로 다른 라벨러들이 동일한 라벨을 부여하는 정도를 나타내는 지표입니다. 즉, 서로 다른 라벨러들이 동일한 구문에 대해 동일한 라벨을 부여하는 경우, 그 구문은 high interlabeller agreement를 가진다고 할 수 있습니다. 이것은 데이터셋의 신뢰성과 일관성을 보장하기 위해 중요한 지표 중 하나입니다. 4.1.3 Berlin Emotional Speech Database: Berlin Emotional Speech Database 또는 EMO-DB는 자동 감정 분류의 효과를 테스트하기 위해 매우 잘 알려져 있고 널리 사용되는 데이터베이스입니다. 이 데이터베이스는 Levels of Emotion Annotation Space (LEAS)와 함께 개발되었습니다. EMO-DB에는 10명의 배우가 7가지 감정(분노, 경멸, 두려움, 기쁨, 슬픔, 수치심, 중립)을 나타내는 독일어 단어 및 문장을 발화한 녹음이 포함되어 있습니다. 이 데이터베이스에는 대략 5시간의 오디오 녹음이 포함되어 있습니다. 4.1.4 The Geneva Multimodal Emotion Portrayals: The Geneva Multimodal Emotion Portrayals(GEMEP)은 10명의 프랑스어 배우가 연기한 1,260개의 멀티모달 감정 표현을 수집한 데이터베이스입니다. 이 데이터베이스는 얼굴 표정, 음성, 제스처 등 다양한 모달리티를 사용하여 감정을 나타냅니다. GEMEP 데이터베이스는 많은 연구에서 사용되며, 이 연구에서도 사용되었습니다. 구체적으로는 12가지 감정이 있으며 총 24개러 분류 된다. 4.1.5 Geneva Singing Voice Emotion Database(GeSiE): 3명의 가수가 녹음한 감정적인 노래를 수집한 데이터베이스입니다. 이 데이터베이스는 5명의 전문 오페라 가수가 추가로 녹음한 것으로 확장되었습니다. 총 8명의 가수들이 10가지 감정 범주에서 세 가지 다른 구절과 음계를 부르며 녹음되었습니다. 이 데이터베이스는 많은 연구에서 사용되고 있습니다. Vera-Am-Mittag: 독일 TV 쇼 “Vera am Mittag”에서 추출한 947개의 감정적인 발화로 구성되어 있습니다. 이 쇼에서 주인공인 Vera는 게스트들 간의 토론을 주관합니다. 이 데이터베이스는 매우 자발적이고 감정적으로 매우 다양한 상태를 포함하고 있습니다. 이 데이터베이스의 감정은 활성화, 가치 및 지배력&#x2F;권력 세 가지 차원으로 설명됩니다.비디오로 구성된 데이터베이스입니다. 이 쇼에서 주인공인 Vera는 게스트들 간의 토론을 주관합니다. 이 데이터베이스는 많은 연구에서 사용되고 있습니다. 이 부분에서는 EmoReact 데이터베이스의 주석 방법에 대한 정보가 제공됩니다. 주석자들은 각각의 감정 차원에 대해 다섯 개의 이미지 중 하나를 선택할 수 있는 아이콘 기반 방법을 사용했습니다. 주석자들은 먼저 수동으로 분할된 발화를 듣고, 그 발화에서 가장 잘 설명하는 감정 차원에 대한 아이콘을 선택해야 했습니다. 이 아이콘의 선택은 후에 각 차원마다 [-1;1] 범위 내에서 균등하게 분포된 다섯 가지 범주로 매핑되었으며, 주석자의 확신도를 고려하는 가중치 함수를 적용하여 평균값을 계산했습니다. 비교적 평가를 가능하게 하기 위해 연속적인 가치와 활성화 라벨은 네 개의 클래스로 이산화되었으며, 이는 activation-valence 공간의 네 사분면(q1:positive-active, q2:positive-passive, q3:negative-passive, and q4:negative-active)을 나타냅니다. 4.2 Common Mapping of Emotions 이 부분에서는 모든 데이터셋에서 결과와 특징 집합의 성능을 비교할 수 있도록 하기 위해, 각각의 데이터셋에 대한 특정한 감정 라벨을 공통적 binary arousal and valence representation으로 매핑했다는 것을 설명합니다. 이 매핑은 [43], [47], [49] (GEMEP의 경우)에서 제안된 대로 [53]를 참고하여 수행되었습니다. GeSiE의 경우 GEMEP에서 사용된 절차와 유사하게 매핑이 수행되었습니다. Table 1은 감정 범주를 바이너리 활성화 및 가치 라벨로 매핑한 결과를 보여줍니다. Note, that for FAU AIBO: 원래 5개의 클래스 레이블 특성상 binary valence에 대한 매핑만 가능 4.3 Experimental Protocol AIBO에 대한 실험을 제외한 모든 실험은 LOSO(Leave-One-Speaker(Group)-Out) 교차 검증을 사용하여 수행됩니다. LOSO는 “Leave-One-Speaker-Out”의 약어로, 각 실험에서 하나의 화자를 제외한 모든 화자 데이터를 사용하여 모델을 학습하고, 나머지 화자의 데이터를 사용하여 모델을 평가하는 교차 검증 방법입니다. 이 방법은 모델이 다른 화자들에 대해서도 일반화될 수 있는지 확인하기 위해 사용됩니다. 즉, 각각의 테스트 세트는 한 명의 화자에 대한 데이터만 포함하며, 이러한 방식으로 모든 화자에 대한 평가가 수행됩니다. (교차검증의 일종) GeSiE 데이터셋처럼 화자 수가 8명 이하인 경우, 각 화자의 데이터를 하나의 교차 검증 폴드로 처리합니다. 그러나 8명 이상인 경우, 화자 ID가 무작위로 8개의 그룹으로 나누어지고, 이에 따라 데이터가 8개의 폴드로 분할됩니다. 그런 다음 교차 검증은 각각 7개 폴드에서 데이터를 사용하여 8개의 다른 모델을 훈련하고, 첫 번째 모델에 대한 테스트를 위해 첫 번째 폴드를 제외하고 두 번째 모델에 대한 테스트를 위해 두 번째 폴드를 제외하는 식으로 수행됩니다. 이렇게 하면 전체 데이터셋에 대한 예측이 훈련 및 테스트 데이터 중복 없이 생성됩니다. FAU AIBO의 경우, OHM에서 학습하고 MONT에서 평가하는 방식과 MONT에서 학습하고 OHM에서 평가하는 방식으로 두 개의 교차 검증 폴드를 사용합니다. 교차 검증 폴드는 교차 검증에서 사용되는 데이터 세트의 하위 집합입니다. 전체 데이터 세트를 여러 개의 폴드로 나누어 각각을 테스트 세트와 학습 세트로 사용하여 모델을 반복적으로 학습하고 평가하는 방법에서, 각각의 폴드는 서로 다른 데이터를 포함하며, 전체 데이터 세트를 대표할 수 있는 크기와 분포를 가지도록 구성됩니다. 예를 들어, 10개의 폴드로 나누어 교차 검증을 수행하는 경우, 10개의 서로 다른 하위 집합으로 데이터가 분할되며, 각각의 폴드는 10%씩의 데이터를 포함합니다. 이러한 방식으로 모든 데이터가 테스트 및 학습에 사용되며, 모델이 일반화될 수 있는지 확인할 수 있습니다. paralinguistics 분야에서 가장 널리 사용되는 정적 분류기로서, support-vector machines(SVMs)가 선택되었습니다. SVMs는 WEKA [54]에서 구현된 순차 최소 최적화 알고리즘으로 학습됩니다. 모델 복잡도 C의 값 범위가 평가되며, 결과는 매개 변수 세트의 성능과 관련하여 더 안정적인 결과를 얻기 위해 전체 범위에서 평균화됩니다. c값의 범위는 C1&#x3D;0.000025, C2&#x3D;0.00005, C3&#x3D;0.000075, C4&#x3D;0.0001, …, C15&#x3D;0.075, C16&#x3D;0.1, C17&#x3D;0.25으로 총 17개의 값이 평가 대상입니다. 이 범위에서 모델 복잡도를 평가하고 결과를 평균화하여 매개 변수 세트의 성능을 더 안정적으로 평가합니다. SVMs를 구현하는 데 필요한 데이터 균형 문제에 대해 설명하고 있습니다. SVMs를 사용할 때, 다수 클래스에 대한 사전 편향을 피하기 위해 각 클래스에 대해 동일한 수의 인스턴스가 필요합니다. 이를 위해 각각의 훈련 파티션은 균형을 맞추어야 합니다. Up-sampling은 소수 클래스의 데이터를 복제하여 다수 클래스와 동일한 수의 데이터를 생성하는 방식으로 수행됩니다. 이렇게 함으로써, 모델이 다수 클래스에 대해 사전 편향을 학습하지 않도록 하고, 소수 클래스의 정보를 더 잘 반영할 수 있도록 합니다. Up-sampling은 일반적으로 무작위로 선택된 소수 클래스 샘플을 복제하여 데이터 세트의 크기를 증가시키는 방식으로 수행됩니다. VMs를 수치적으로 효율적으로 만들기 위해 모든 음향 매개 변수가 공통 값 범위로 정규화되어야 함을 설명하고 있습니다. 이를 위해 z-정규화, 즉 평균이 0이고 분산이 1인 정규화가 수행됩니다. 이 논문에서는 정규화 매개 변수를 계산하고 적용하는 세 가지 다른 방법을 조사합니다. 첫째, 전체 훈련 파티션에서 평균과 분산을 계산하는 방법(std)입니다. 둘째, 각 화자에 대해 개별적으로 평균과 분산을 계산하는 방법(spkstd)입니다. 이 방법은 [55]와 유사합니다. 셋째, 훈련 및 테스트 파티션 각각에 대해 개별적으로 평균과 분산을 계산하는 방법(stdI)입니다. 4.4 Result 제안된 최소한의 매개 변수 세트와 Interspeech Challenges 시리즈에서 사용된 대규모 brute-forced 매개 변수 세트 간의 결과를 비교합니다. brute-forced 매개 변수 세트는 Interspeech Challenges는 2009년 [43] (InterSp09)의 감정, 2010년 [36] (InterSp10)의 연령 및 성별, 관심도 수준, 2011년 [44] (InterSp11)의 화자 상태, 2012년 [45] (InterSp12)의 화자 특성 및 2013 및 2014년 [12], [37] (ComParE)의 전산언어학에 대한 시리즈로 구성이 되어있습니다. binary arousal and binary valence classification의 결과 매개 변수 세트를 제외한 모든 변수를 제거하기 위해 결과는 다섯 개의 데이터베이스(all, FAU AIBO 제외)와 C&#x3D;0.0025부터 시작하는 가장 높은 아홉 개의 SVM 복잡도 설정에서 평균화됩니다. 더 높은 복잡도 설정에서만 평균을 계산하는 결정은 작은 특징 집합의 경우 이 임계값보다 낮은 복잡도에서 성능이 크게 저하되기 때문입니다. 이러한 저하가 평균화를 편향시키기 때문입니다. UAR은 Unweighted Average Recall의 약자로, 각 클래스에 대한 재현율을 평균화한 값입니다. 이 부분에서는 FAU AIBO를 제외한 모든 데이터베이스와 9개의 최고 SVM 복잡도(C&gt;&#x3D;0.0025)에서 UAR을 평균화합니다. 이는 가중치가 적용되지 않은 평균입니다. 또한, 각 화자에 대한 표준화와 균형 잡힌 훈련 세트를 위해 인스턴스 업샘플링이 수행됩니다. 각 클래스에 대한 재현율은 분류기가 실제 양성인 샘플을 얼마나 잘 찾아냈는지를 나타내는 지표입니다. 즉, 분류기가 참 양성(True Positive)으로 분류한 샘플 수를 실제 양성(True Positive + False Negative)인 전체 샘플 수로 나눈 값입니다. 이 값은 해당 클래스의 분류 성능을 평가하는 데 사용됩니다. 평균 결과에서 GeMAPS 세트의 높은 효율성을 확인할수 있습니다. eGeMAPS 세트는 arousal에서 거의 80%의 UAR에 도달하여 가장 우수한 성능을 보였습니다. eGeMAPS 세트는 GEMEP 데이터베이스의 이진 각성 분류와 GeSiE 데이터베이스의 이진 가치 분류에서 최상의 결과를 보입니다. eGeMAPS 세트는 항상 GeMAPS 세트보다 우수하거나 동일하며, 이는 추가 파라미터(MFCC 및 스펙트럼 플럭스 특히)의 중요성을 시사합니다. 이는 특히 가치에서 GeMAPS와 eGeMAPS 간의 평균 차이가 더 큰 경우에 그러하며, 이로써 음향 가치에 대한 그러한 파라미터의 중요성을 제안합니다. 큰 규모의 파라미터 세트들에 비해 전반적으로 GeMAPS 세트는 최소한의 크기임에도 불구하고 놀랍게도 비슷한 성능을 보입니다. 향후 연구에서는 제안된 최소한의 세트가 교차 데이터베이스 분류 실험에서 더 나은 일반화를 얻을 수 있는지 조사해야 합니다. 5.Discussion and Conclusion GeMAPS는 수동 상호작용이나 보정 없이 오디오 파형에서 음향 파라미터 세트를 추출하는 자동 추출 시스템을 기반으로 합니다. 하지만 특정 현상과 관련성이 있거나 상관관계가 있는 것으로 밝혀진 모든 파라미터를 자동으로 안정적으로 추출할 수 있는 것은 아닙니다. 예를 들어 모음기반 포먼트 분석에는 신뢰할수 있는 자동모음감지및 분류 시스템이 필요합니다. 따라서 GeMAPS에서는 깨끗한 음향 조건에서 감독없이 안정적으로 추출할 수 있는 파라미터만 포함했습니다. 검증 실험은 데이터 베이스간 최상의 비교 가능성을 위해 binary classification 실험으로 제한되었습니다. 자동 추출을 통한 표준 파라미터 세트의 잠재적 위험 중 하나는 발성 현상과의 연결이 무시될 수 있다는 것입니다. 파라미터 세트를 선택할 때, 이러한 연결을 강조하고 수집 기준 중 하나로 기본 발성 메커니즘을 사용했습니다. 향후 연구를 통해 이러한 기초를 강화하고 새로운 통찰력을 제공할 것으로 기대됩니다. 예를 들어, 각성과 빠른 발성 및&#x2F;또는 조음 제스처와 관련이 있다고 예상할 수 있으며, 평화로운 성격은 느린 제스처로부터 비롯된다고 합니다. 따라서 앞으로는 발성의 음향 출력을 소리 크기, 피치 등 기본 파라미터를 넘어 생리학적으로 관련된 파라미터로 이해를 확장하는 것이 가치가 있습니다. 이러한 맥락에서 성대 접착은 특히 관련성이 높은 파라미터입니다. 접착 증가는 닫힌 단계를 길게 하고 횡성대 공기 흐름 펄스의 진폭을 줄입니다. 이로 인해 음성 원천 기본값의 감쇄 또는 더 구체적으로는 음성 원천 부분 음 두 개 사이의 레벨 차이를 줄일 것으로 예상됩니다. 이러한 방사음에서 이 레벨 차이는 주로 첫 번째 포먼트의 주파수에 영향을 받으며, 이는 발성의 감정 색채에 이차적인 중요성을 가집니다. GeMAPS의 향후 발전은 음성 원천 파라미터를 직접 측정하기 위해 음향 출력 신호를 역 필터링하는 기술의 추가를 포함할 수 있습니다. 이러한 분석은 감정 표현의 다양한 음향 출력 특성의 생리적 상관 관계를 확인할 수 있게 하여 감정적 각성이 음성 생산에 미치는 메커니즘에 대한 지식을 강화합니다.","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speech Emotion Recognition","slug":"Paper/Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/categories/Paper/Speech-Emotion-Recognition/"}],"tags":[{"name":"Speech Emotion Recognition","slug":"Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/tags/Speech-Emotion-Recognition/"},{"name":"Acoustic Parameter Set","slug":"Acoustic-Parameter-Set","permalink":"https://jmj3047.github.io/tags/Acoustic-Parameter-Set/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speech Emotion Recognition","slug":"Paper/Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/categories/Paper/Speech-Emotion-Recognition/"}]},{"title":"Neural Networks and Deep Learning_Quiz","slug":"DL1_Quiz","date":"2023-04-20T15:00:00.000Z","updated":"2023-06-03T13:33:11.015Z","comments":true,"path":"2023/04/21/DL1_Quiz/","link":"","permalink":"https://jmj3047.github.io/2023/04/21/DL1_Quiz/","excerpt":"","text":"개요Coursera Deep Learning Course 1 Quiz 1.Introduction to Deep Learning Link: https://www.coursera.org/learn/neural-networks-deep-learning/home/module/1 2.Neural Network Basics Link: https://www.coursera.org/learn/neural-networks-deep-learning/home/module/2 3.Shallow Neural Networks Link: https://www.coursera.org/learn/neural-networks-deep-learning/home/module/3 4.Key Concepts on Deep Neural Networks Link: https://www.coursera.org/learn/neural-networks-deep-learning/home/module/4","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"Coursera","slug":"Coursera","permalink":"https://jmj3047.github.io/tags/Coursera/"},{"name":"Quiz","slug":"Quiz","permalink":"https://jmj3047.github.io/tags/Quiz/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}]},{"title":"Imporoving Deep Neural Networks, Hyper parameter tuning, Regularization and Optimization","slug":"DL_Part2","date":"2023-04-20T15:00:00.000Z","updated":"2023-06-03T13:34:01.660Z","comments":true,"path":"2023/04/21/DL_Part2/","link":"","permalink":"https://jmj3047.github.io/2023/04/21/DL_Part2/","excerpt":"","text":"Course Lecture 2 in Deep Learning Regularization useful technique for reducing variance. There is a little bit of a bias variance tradeoff when you use regularization. It might increase the bias a little bit, although often not too much if you have a huge enough network. If you suspect your neural network is over fitting your data, that is, you have a high variance problem, one of the first things you should try is probably regularization. The other way to address high variance is to get more training data that’s also quite reliable. But you can’t always get more training data, or it could be expensive to get more data. But adding regularization will often help to prevent overfitting, or to reduce variance in your network. Why does regularization help with overfitting? Why does it help with reducing variance problems? Simplicity and Generalization: Regularization encourages the model to be simpler by penalizing large weights. Simpler models tend to generalize better to new data because they are less likely to fit to the noise in the training data. This is achieved by adding a penalty term to the loss function that is proportional to the magnitude of the weights (parameters). This penalty term could be the L1 norm (sum of absolute values of weights) or L2 norm (sum of squares of weights) of the model parameters. The L1 norm leads to Lasso regularization and the L2 norm leads to Ridge regularization. Reducing Model Complexity: By penalizing large weights, regularization effectively reduces the model’s complexity by discouraging learning a highly flexible model, which would lead to overfitting. In this way, it helps in reducing the variance of the model. High variance is a sign of overfitting where the model is too sensitive to the small fluctuations in the training set and hence may not perform well on the unseen data. Feature Selection (for L1 regularization): In the case of L1 regularization, it can shrink some of the weights to zero, effectively performing feature selection. This means the model becomes simpler and more interpretable, and can improve generalization by reducing overfitting to irrelevant features. Dropout Regularization the thing to remember is that drop out is a regularization technique, it helps prevent overfitting. And so unless my avram is overfitting, I wouldn’t actually bother to use drop out. So as you somewhat less often in other application areas, there’s just a computer vision, you usually just don’t have enough data so you almost always overfitting, which is why they tend to be some computer vision researchers swear by drop out by the intuition. One big downside of drop out is that the cost function J is no longer well defined on every iteration. You’re randomly, calling off a bunch of notes. Other Regularization: Data Augmentation, Early Stopping Normalizing Inputs: If your input features came from very different scales, maybe some features are from 0-1, sum from 1-1000, then it’s important to normalize your features. If your features came in on similar scales, then this step is less important although performing this type of normalization pretty much never does any harm. Gradient Checking Implementation Notes Optimization exponentially weighted averages bias correction Adam Optimization Algorithm Adam: Adaptive moment estimation Beta_1 is computing the mean of the derivatives. This is called the first moment Beta_2 is used to compute exponentially weighted average of the squares, and that’s called the second moment. Local Optima first, you’re actually pretty unlikely to get stuck in bad local optima so long as you’re training a reasonably large neural network, save a lot of parameters, and the cost function J is defined over a relatively high dimensional space. But second, that plateaus are a problem and you can actually make learning pretty slow. And this is where algorithms like momentum or RmsProp or Adam can really help your learning algorithm as well. And these are scenarios where more sophisticated observation algorithms, such as Adam, can actually speed up the rate at which you could move down the plateau and then get off the plateau. Hyperparameter Batch norm means that, especially from the perspective of one of the later layers of the neural network, the earlier layers don’t get to shift around as much, because they’re constrained to have the same mean and variance. And so this makes the job of learning on the later layers easier. It turns out batch norm has a second effect, it has a slight regularization effect. So one non-intuitive thing of a batch norm is that each mini-batch, I will say mini-batch X_t, has the values Z_t, has the values Z_l, scaled by the mean and variance computed on just that one mini-batch.","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Coursera","slug":"Coursera","permalink":"https://jmj3047.github.io/tags/Coursera/"},{"name":"AI Studies","slug":"AI-Studies","permalink":"https://jmj3047.github.io/tags/AI-Studies/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}]},{"title":"Neural Networks and Deep Learning","slug":"DL_Part1","date":"2023-04-18T15:00:00.000Z","updated":"2023-05-31T13:16:29.416Z","comments":true,"path":"2023/04/19/DL_Part1/","link":"","permalink":"https://jmj3047.github.io/2023/04/19/DL_Part1/","excerpt":"","text":"Course Lecture 1 of Deep Learning Course Derivatives https://community.deeplearning.ai/t/derivation-of-dl-dz/165 vectorizing logistic regression Why we like to use that cost function for logistic regression? maximum likelihood estimation by minimizing this cost function J(w,b) we’re really carrying out maximum likelihood estimation with the logistic regression model. Under the assumption that our training examples were IID, or identically independently distributed. Getting your Matrix dimensions right Lecture note: https://community.deeplearning.ai/t/dls-course-1-lecture-notes/11862","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Coursera","slug":"Coursera","permalink":"https://jmj3047.github.io/tags/Coursera/"},{"name":"AI Studies","slug":"AI-Studies","permalink":"https://jmj3047.github.io/tags/AI-Studies/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}]},{"title":"Gen App Builder","slug":"Gen_App_Builder","date":"2023-04-17T15:00:00.000Z","updated":"2023-04-18T09:07:08.073Z","comments":true,"path":"2023/04/18/Gen_App_Builder/","link":"","permalink":"https://jmj3047.github.io/2023/04/18/Gen_App_Builder/","excerpt":"","text":"개요 구글에서 새롭게 발표한 gen app builder에 대해서 알아보자 어떤 기능이 있고 official doc이 발표된 게 많이 없지만 있는걸 최대한 활용해서 설명한다(유투브에 나온 데모 설명 포함) 웹사이트 설명 오피셜 문서는 아니지만 사이트에 간단하게 설명이 잘 되어 있어서 첨부. Gen App Builder의 도움으로 기계 학습 경험이 없는 개발자도 엔터프라이즈급 생성 AI 앱을 만들 수 있습니다. 이 강력한 도구는 코드 없는 접근 방식을 제공하여 개발자가 몇 분 또는 몇 시간 내에 고품질 환경을 구축할 수 있도록 합니다. 주요 기능은 다음과 같습니다. 코딩 필요 없음: Gen App Builder를 사용하면 사용자는 코드를 작성하지 않고도 맞춤형 비즈니스 애플리케이션을 만들 수 있습니다. 이 플랫폼은 시각적 인터페이스와 드래그 앤 드롭 기능을 제공하여 사용자가 애플리케이션을 쉽게 설계하고 구축 할 수 있도록 합니다 . 다른 Google 클라우드 서비스와의 통합: Gen App Builder는 ****Cloud SQL 및 Cloud Storage 와 같은 다른 Google Cloud 서비스와 통합되어 데이터를 쉽게 저장하고 관리할 수 있습니다. 사용자 지정 가능한 템플릿: 이 플랫폼은 사용자가 애플리케이션을 빠르고 쉽게 만들 수 있도록 사용자 지정 가능한 템플릿을 제공합니다. 템플릿은 고객 관계 관리, 인사 및 프로젝트 관리를 비롯한 다양한 사용 사례에 사용할 수 있습니다. 실시간 협업: 이 App Builder를 사용하면 사용자가 애플리케이션에서 실시간으로 협업할 수 있습니다. 이를 통해 팀이 쉽게 협력하고 피드백을 공유할 수 있습니다. 확장성: Gen App Builder로 구축된 애플리케이션은 확장성이 뛰어나고 많은 양의 데이터와 사용자를 처리할 수 있습니다. 보안: Google Cloud의 보안 기능은 App Builder에 내장되어 있어 ****보안 위협으로부터 애플리케이션과 데이터를 보호합니다 . 구글 오피셜 문서 구글 오피셜 문서에 있는 설명을 간략하게 추린 것이다. 생성형 앱 빌더를 사용한 새로운 생성형 AI 기반 검색 및 대화형 환경 빌드 생성형 앱 빌더를 사용하여 머신러닝 기술을 활용한 엔터프라이즈급 생성형 AI 애플리케이션 개발 빠르게 고품질 환경을 구축하고 애플리케이션 및 웹사이트에 통합 기반 모델과 정보 검색을 결합해 관련성 높은 맞춤 정보 제공 텍스트, 이미지, 기타 미디어로 응답할 수 있는 멀티모달 앱 구축 서드 파티 앱 및 서비스와 연결하고 트랜잭션 지원 기능 제공 차세대 대화형 AI 경험 및 어시스턴트 생성형 앱 빌더를 사용해 원활한 대화형 방식으로 기업 데이터 세트를 활용하고 사용자 경험 개선 AI 기반 앱으로 모든 소스의 정보 종합 및 구체적인 실행 가능한 답변 제공 고객 서비스 분야에서 수익, 고객 만족도, 충성도 향상 도모 멀티모달 기능과 대화형 UI 트랜잭션을 사용하여 제품 구매 과정 돕기 다양한 산업 및 사용 사례에 적용 가능, 예: 소비재, 공공 서비스, 금융, 인트라넷 등 Google 품질 수준의 검색과 기반 모델의 결합 조직 전체 데이터에서 올바른 정보 찾기 중요, 기존 도구로는 어려움 생성형 앱 빌더로 Google 품질 검색 기능과 생성형 AI 결합하여 관련성 높은 맞춤 정보 찾기 지원 코딩 경험 없이 몇 분~시간 만에 대화형 검색 환경 구축 가능 멀티모달 검색 환경을 통해 텍스트, 이미지, 동영상 검색 지원 자연스럽게 인용 제공 및 맞춤 결과 지원으로 사용자 만족도 향상 데이터 사용률 증가, 프로세스 효율성 개선, 직원과 고객 만족도 높이기 다양한 소스의 복잡한 데이터와 상호작용 능력으로 고객 서비스 개선 가능 대화형 처리 기능과 결합하여 고객 참여도, 직원 생산성 개선 가능 개발자와 기업이 새로운 경험과 수익 기회를 실현할 것으로 기대 유투브 데모 설명 상황: 투자 전략을 수립하기 위해 반도체 시장을 평가해야 하는 애널리스트, 엔지니어들이 회사 홈페이지에 gen app builder를 사용하여서 검색 엔진을 만들어줌 세계적으로 반도체 부족 현상이 일어남 → 어떤 산업에서 가장 영향을 많이 미쳤는지를 검색 엔진으로 물어봄 결과: internal, external이 표시되어서 회사가 제공한 데이터냐 아니냐에 따라서 결과가 나옴, 각각 결과마다 ai 엔진이 자동적으로 요약해서 보여줌 셋중에 하나를 더 알고 싶다면 클릭한다. 그럼 글들을 자동적으로 정리해서 목차를 만들어줌 기업에서 구독하고 있는 금융 저널들에 중요한 정보가 담겨 있음. 그래서 앱 설정해서 이 정보들을 포함해 달라고 설정 가능, 그리고 찾은 정보들을 전부 합치고 정리해서 보여달라고도 설정 가능 이렇게 정보를 추가 한 후 interest rate에 대해서 follow-up 질문을 함 검색한 정보와 함께 검색 overall summary를 같이 보여줌 두 질문으로 산업군의 이해가 높아짐 이 모든것을 회사 url, 데이터를 cloudstorage에 올려서 (구글은 이 정보를 회사 허가 하에만 사용) search로 할건지 chat으로 할건지를 선택한 다음 만들면 끝이남 → 서치 엔진을을 쉽게 만들어줌 만들고 나서 커스터마이징도 가능 → 하단 네비게이션 바에 있는 것들을 우리가 조정해서 사용할 수 있음 deploy 코드를 따로 배포 → 회사 html에 삽입하면 서치 엔진이 됨, api로도 제공 Reference https://www.analyticsvidhya.com/blog/2023/04/gen-app-builder-google-clouds-latest-generative-ai-tools/ https://cloud.google.com/blog/ko/products/ai-machine-learning/create-generative-apps-in-minutes-with-gen-app-builder?_ga&#x3D;2.206347863.-517135162.1681093488&amp;hl&#x3D;ko https://www.youtube.com/watch?v=kOmG83wGfTs","categories":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}],"tags":[{"name":"Gen App Builder","slug":"Gen-App-Builder","permalink":"https://jmj3047.github.io/tags/Gen-App-Builder/"},{"name":"AI tool","slug":"AI-tool","permalink":"https://jmj3047.github.io/tags/AI-tool/"}],"keywords":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}]},{"title":"Hi,KIA A Speech Emotion Recognition Dataset for Wake-Up Words","slug":"SER_Hi_KIA","date":"2023-04-16T15:00:00.000Z","updated":"2023-05-11T14:36:21.812Z","comments":true,"path":"2023/04/17/SER_Hi_KIA/","link":"","permalink":"https://jmj3047.github.io/2023/04/17/SER_Hi_KIA/","excerpt":"","text":"Journal&#x2F;Conference : IEEEYear(published year): 2022 SeptemberAuthor: Taesu Kim, SeungHeon Doh, Gyunpyo Lee, Hyungseok Jeon, Juhan Nam, Hyeon-Jeong SukSubject: Emotion Recognition, Wake-Up Words Hi,KIA: A Speech Emotion Recognition Dataset for Wake-Up Words Summary 새로운 공개 데이터셋인 Hi, KIA, 감정 레이블이 지정된 WUW 데이터셋을 제안함 Hi, KIA라는 새로운 공개 데이터셋을 제안하고 이를 이용한 감정 인식 모델을 개발함 제안된 데이터셋은 488개의 한국어 발화 샘플로 구성되어 있으며, 분노, 기쁨, 슬픔, 중립 등 4가지 감정 상태로 레이블링되어 있음 개발된 두가지 분류 모델은 전통적인 hand-craft feature와 pre-trained neural network를 이용한 transfer-learning 접근 방식을 사용하여 구현되었으며 이 데이터셋에서 짧은 발화 수준의 감정 인식에 대한 기준 결과를 제시함 이러한 결과들은 앞으로 VUI 기반 어플리케이션에 활용될 것으로 기대됨 I. Introduction 음성 인식 기술은 음성 사용자 인터페이스(VUIs)를 통해 다양한 응용 프로그램에서 사용되고 있다. VUIs는 사용자에게 보이지 않는 인터페이스로서, 기존의 인터페이스보다 감정적인 커뮤니케이션을 더 잘 전달할 수 있다. 특히, 차량 내 VUIs는 운전 중 안전을 보장하고 운전자의 감정적인 경험을 향상시키는 데 큰 관심을 받고 있다. WUW(Wake-up words)를 발화하여 VUIs를 활성화하는 것이 가장 일반적인 사용 방법 중 하나이다. 이 연구에서는 Hi,KIA 데이터셋을 사용하여 WUW 발화 시 사용자의 감정 상태를 분류하는 모델을 개발하였다. Hi,KIA 데이터셋은 488개의 한국어 발화 샘플로 구성되어 있으며, 4가지 감정 상태(분노, 기쁨, 슬픔, 중립)로 레이블링되어 있다. 이 모델은 차량 내 VUIs에서 WUW를 발화한 사용자의 감정 상태를 인식하는 데 활용될 수 있다. Wake up word emotion recognition 화자의 음성을 기반으로 화자의 감정 상태를 감지하는 프로세스. 여기에는 감정 상태와 관련이 있는 것으로 알려진 피치, 강도 및 스펙트럼 특성과 같은 다양한 음향적 특징에 대해 화자의 음성을 분석하는 작업이 포함. 감정 데이터를 사용한 이유 사용자와 자연스러운 대화를 할 수 있는 VUI(Voice User Interface) 기반 어플리케이션의 개발이 필요 이러한 어플리케이션에서는 사용자의 감정 상태를 파악하여 적절한 대응을 제공하는 것이 중요 예를 들어, 운전 중인 차량에서 음성 인식 기반의 VUI 어플리케이션을 사용할 때, 운전자가 분노 상태일 때는 안전 운전에 방해가 되는 메시지를 보여주지 않도록 하거나, 기분 좋은 상태일 때는 더욱 친근한 메시지를 보여줄 수 있음 SER 데이터셋들과 비교 SER는 감정 발화와 라벨이 포함된 데이터셋을 필요로 함 IEMOCAP, EmoDB, RAVDESS, TESS 등의 SER 데이터셋이 있음 이 데이터 셋들은 text-independent emotion recognition을 기반으로 만들어짐 이 말을 다른말로 하면 lexical information과는 상관 없이 사용자의 감정을 인식하는 시스템이라는 것 WUW SER 데이터셋은 짧은 발화와 키워드로 제한됨 최근에 Release된 Ok Aura의 경우 데이터 셋도 많고 스피커도 많지만 labeled된 데이터가 218개 밖에 없음. II. DATASETA. Scenario Selection 시나리오는 “Hi, KIA (WUW)”로 시작하는 텍스트 스크립트로 제공되며, 감정 상태를 유발하는 문맥 문장으로 끝남 affective computing 분야에서 경험이 있는 8명의 대학원생들이 VUI가 특정 감정에서 사용되는 시나리오를 제안하도록 요청 이전 연구를 기반으로 운전자의 다섯 가지 감정(화남, 스트레스, 기쁨, 두려움, 슬픔)이 제시 학생들은 각각의 감정에 대해 최소한 두 개의 시나리오를 제안하였으며, 중복된 것들을 병합하여 총 53개의 시나리오가 수집 시나리오는 그림 1(a)에 표시된 valence-arousal 좌표계로 매핑되었습니다. 4사분면에 해당하는 감정 범주는 시나리오가 거의 없기 때문에 제외되었습니다. arousal: 감정의 흥분 정도를 이야기하는 척도 이 값이 작을수록 차분한 감정인데, 지루함이나 편안함, 졸림 등이 해당 arousal이 큰 감정에는 흥분감, 분노, 공포 등 valence: 감정의 긍정 혹은 부정적인 정도 공포는 아주 부정적인 valence를 가지고 지루함이나 흥분감은 중간 정도의 valence를, 행복이나 편안함은 긍정적인 valence를 가짐 시나리오는 valence-arousal 좌표계를 기반으로 화남, 기쁨, 슬픔 및 중립 카테고리로 클러스터링되었습니다. 각 카테고리에서 대표적인 3개의 시나리오가 선택되어 “Hi, KIA” 데이터셋 구축에 사용되었습니다. 선택된 시나리오에 대한 녹음 가이드로 사용될 문장과 함께 삽화가 있는 카드를 준비 각 카드에는 차 내부 위에 녹음 가이드로 사용될 문장이 제시 해당 문장의 상황을 설명하는 참조 이미지를 배경으로 제공 B. Recording and Post-processing 4명의 목소리 배우들을 온라인으로 모집. 평균 연령은 31.38세이며 표준 편차는 3.90세 마우스 위치를 마이크로부터 30cm 이상 떨어뜨리도록 12개 시나리오에 대해 총 576개의 오디오 파일을 수집 C. Human Validation 8명의 대학원생들이 576개의 녹음 파일을 무작위로 선정하여, 각 녹음 파일이 ‘화남’, ‘기쁨’, ‘슬픔’, ‘중립’ 중 어떤 감정을 나타내는지 분류 만약 녹음 파일이 인식하기 어려웠다면 ‘unknown’으로 분류 이 결과를 바탕으로, 모든 인간 평가자가 다른 예측을 한 88개의 녹음 파일은 제거 이를 통해 최종적으로 488개의 녹음 파일이 “Hi, KIA” dataset에 포함됨. 인간 평가자들의 응답을 결합하고 Figure 2에 나와 있는 혼동 행렬을 계산 우리는 평가자들이 ‘슬픈’ 감정을 분류하는 데 비교적 능숙하다는 것을 발견 반면에, ‘화난’과 ‘중립’ 목소리를 식별하는 데 어려움이 있었으며, 그들은 ‘화난’을 ‘중립’으로 평가하고 ‘중립’을 ‘슬픈’으로 평가 high-arousals 목소리를 감정적인 기저 상태로 인식: 참가자들은 ‘angry’와 ‘happy’를 ‘neutral’ 감정으로 관찰 이는 사람들이 WUW(wake up words)에서 high arousals 인상을 인식하는 데 어려움이 있다는 것을 나타냅니다. III. WAKE-UP WORD EMOTION RECOGNITION 데이터셋의 크기가 작기 때문에 두 가지 학습 전략을 탐구 하나는 도메인 지식에 기반한 수작업 오디오 특징을 사용하는 것이고, 다른 하나는 대규모 데이터셋으로 학습된 모델의 일반화 능력을 활용하여 작은 데이터셋으로 사전 학습된 신경망 모델을 세밀하게 조정하는 것 One is using hand-craft audio features based on domain knowledge. The other is fine-tuning a pretrainedneural network model with the small dataset by leveraging the generalization capability of the model trained with a largescale dataset. A. Hand-craft Features 주파수, 에너지 및 스펙트럼 도메인 기능을 포함하는 확장된 Geneva Minimalistic Acoustic Parameter Set(eGeMAPS) [22]를 사용 88차원의 eGeMAPS 특징은 고정된 평균과 표준 편차 값을 사용하여 z-점수로 표준화 Figure 3은 전체 데이터셋에서 에너지와 음높이 분포의 두 바이올린 그림을 보여줌 일반적인 경향은 고기분 상태 그룹(‘화난’, ‘기쁜’)이 저기분 상태 그룹(‘슬픈’, ‘보통’)과 잘 구별됨 딥 뉴럴 네트워크 기반의 기능과 비교하기 위해, 문장 수준의 eGeMAPS 기능을 로지스틱 회귀 분류기의 input으로 사용 B. Fine-Tuning with Pretrained Wav2vec 2.0 최근 연구에서는 음성 감정 인식을 위해 딥 러닝을 사용하는 것이 일반적입니다. 그러나 annotation이 달린 데이터의 부족으로 인해 이러한 방법들은 제한되어 있습니다. 이 문제를 해결하기 위해, Wav2vec와 같은 대규모 사전 학습된 신경망을 사용한 전이 학습이 감정 인식 정확도를 향상시켰습니다. Hi, KIA와 함께, 우리는 Wav2vec2.0을 사용하여 유사한 전이 학습을 수행하였습니다. Pretrained Wav2vec2.0 Wav2vec2.0 [29]은 원시 오디오 신호에서 의미 있는 표현을 추출하기 위해 훈련된 transformer 기반 모델입니다. Wav2vec2.0은 CNN을 기반으로 한 로컬 인코더, transformer를 기반으로 한 컨텍스트 네트워크 및 양자화 모듈로 구성되어 있습니다. 로컬 인코더는 원시 파형에서 직접 low-level representation을 추출합니다. 이러한 표현을 기반으로 컨텍스트 네트워크는 대조 손실(contrastive loss)을 사용하여 과거 표현에서 미래 표현을 예측하도록 학습됩니다. 컨텍스트 네트워크의 출력은 학습된 high-level representations입니다. Fine-tuning methods 우리는 Wav2vec2.0을 사용하여 특징을 추출하고 average pooling을 통해 문장 수준의 기능을 얻었습니다. 이전 연구를 따라[28], 우리는 Wav2vec2.0의 모듈에 대한 다양한 세부 조정 전략을 탐구하였습니다. 첫째, 감정 지도 없이 긴급한 libri-speech corpus로 학습된 vanila Wav2vec2.0에서 감정 인식 성능을 측정하는 것입니다(세부 조정 없음). 둘째, 로컬 인코더 또는 컨텍스트 네트워크(각각 저수준 또는 고수준 표현에 대한 책임)를 세밀하게 조정하는 것입니다. 마지막으로 전체 네트워크를 세밀하게 조정합니다 C. Experiment SetupData split and Metrics speaker independence를 위해 8-fold cross-validation을 실행: 7명을 train, validation에 사용하고 1명을 test로 사용 → 그 결과가 WA(Weighted Accuracy)와 UA(Unweighted Accuracy)로 보고 됨 WA: 모든 클래스를 대상으로 한 전체 정확도 UA: 각 클래스의 평균 정확도 Hyper-parameters 사전 훈련된 모델인 wav2vec2.0-base를 사용하여 실험을 수행 2개의 transformer 블록과 7개의 컨볼루션 블록으로 구성 각각 512 채널 Huggingface transformers repository [30]를 기반 AdamW [31]를 사용하여 모델을 최적화하였으며, 학습률은 5e^5, epoch는 200 full audio data: 16,000 Hz sampling rate와 1 batch size IV. RESULTS Table III는 분류 모델과 인간 검증의 분류 성능을 보여줍니다. Human Validation: 8명의 평가자의 평균 성능으로 계산 Fine-tuning이 없는 Wav2vec2.0 특징은 hand-craft 특징보다 성능이 좋지 않음. 이는 자기 지도 학습만으로 high-level emotion feature를 추출하는 것이 어렵다는 것을 나타냄 Wav2vec2.0을 fine-tuning하면 분류 정확도가 크게 향상 세 가지 설정 중에서 Context Net을 fine-tuning하는 것이 가장 잘 작동하며, WA(%)와 UA(%)에서 각각 68.64%와 68.51%를 달성합니다. 이는 작은 데이터셋의 경우 high-level representation과 관련된 매개변수를 업데이트하는 것이 모든 매개변수를 업데이트하는 것보다 더 효율적임을 나타냄 Context Net를 fine-tuning하면 인간 검증보다 우수한 결과를 얻을 수 있음 → 이것은 감정 인식이 주관적이기 때문. 4명의 남성과 4명의 여성이 포함된 8개의 폴드에서 WA(가중치 정확도)를 보여줌 여기서 주목할 점: wav2vec2.0 feature가 대부분의 여성 폴드에서 인간 검증 성능을 능가한다는 것 Human Validation Performance는 남성과 여성 폴드 모두에서 비교적 안정적 Hand-craft 특징과 ‘Wav2vec2.0 FT’는 남성과 여성 폴드 사이에 성능 차이를 보이며, 특히 M1, M2, M4 및 F5 폴드에서 인간 검증보다 성능이 낮음 hand-craft 특징과 Wav2vec2.0 Context Net fine-tuning에 의한 혼동 행렬 두 모델 모두 ‘happy’, ‘sad’와 같은 감정의 활성화 및 가치 차이를 구별하는 데 좋음 그러나 hand-craft 특징은 high-arousals(‘angry’, ‘happy’) 및 sad-neutral pair 내에서 가치 차이를 이해하는 데 약함 → Wav2vec2.0 컨텍스트 네트워크 fine-tuning에서 완화 Figure 2와 Figure 5를 비교하면, Wav2vec2.0 컨텍스트 네트워크 fine-tuning은 high arousals 감정과 neutral 감정을 구별하는 데 인간 검증보다 우수한 성능을 보임. V. CONCLUSIONS 이 논문은 감정 라벨이 지정된 WUW 데이터 세트인 Hi, KIA를 제안 감정적인 짧은 말을 수집하기 위해 신중하게 설계된 절차를 설명 Human Validation을 수행한 후 → 488개의 녹음으로 구성된 데이터 세트를 완성 이는 한국어 악센트와 발화 수준의 4가지 감정 클래스 주석이 포함된 샷 길이 음성 데이터 세트 이 데이터 세트에서 hand-crafted 특징과 transfer learning을 사용하여 짧은 발화 수준 음성 감정 인식에 대한 기준 결과를 제시 결과는 4가지 감정 인식에서 높은 정확도를 달성할 수 있다는 것을 보여줍니다.","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speech Emotion Recognition","slug":"Paper/Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/categories/Paper/Speech-Emotion-Recognition/"}],"tags":[{"name":"Speech Emotion Recognition","slug":"Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/tags/Speech-Emotion-Recognition/"},{"name":"Wake-Up Words","slug":"Wake-Up-Words","permalink":"https://jmj3047.github.io/tags/Wake-Up-Words/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speech Emotion Recognition","slug":"Paper/Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/categories/Paper/Speech-Emotion-Recognition/"}]},{"title":"Ecommerce Reorder Prediction","slug":"Ecommerce_Reorder_Pred","date":"2023-04-05T15:00:00.000Z","updated":"2023-04-06T09:04:17.102Z","comments":true,"path":"2023/04/06/Ecommerce_Reorder_Pred/","link":"","permalink":"https://jmj3047.github.io/2023/04/06/Ecommerce_Reorder_Pred/","excerpt":"","text":"개요 유저들의 재주문 여부 예측하기 instacart kaggle: https://www.kaggle.com/competitions/instacart-market-basket-analysis/leaderboard 2위 한 모델 github: https://github.com/KazukiOnodera/Instacart 필요 메모리 약 300GB RAM이 필요. 개인용 LB에서는 약 60GB RAM만으로도 0.4073을 얻을 수 있음을 확인. 또한 메모리가 충분하지 않고 높은 점수를 얻고 싶다면 xgb.train의 xgb_model을 사용하여 연속 훈련을 시도. 데이터 구조 주요 approach 두 가지 모델: 재주문 예측 및 재주문 없음 예측 재주문 모델의 키는 user_id와 product_id 없음 모델의 키는 user_id 더 나은 예측을 위해 더 많은 훈련 데이터를 사용해야 한다고 생각했습니다. prior 데이터를 훈련 데이터로로 사용하기로 결정 튜닝 결과, 최적의 윈도우 수는 3개 재주문 예측 재주문 없음 예측 모델 구조 Feature Engineering User: what this user like Item: what this item like User x Item: How do user feel about the item Datetime: What this day and hour like *For None model, I can’t user above features except user and datetime, so I convert those to stats(min, mean, max, sum, std…) Feature Importance for reorder Feature Importance for None Importance Findings for reorder 상식적으로 과거에 여러 번 구매한 품목은 재주문될 확률이 높다는 것을 알 수 있습니다. 하지만 재주문되지 않는 품목에 대한 패턴이 있을 수 있습니다 → 이 패턴을 파악하여 사용자가 언제 상품을 재주문하지 않는지 파악할 수 있습니다. user_id : 54035 이 유저는 콜라를 항상 재주문하였다. 그러나 8번 주문을 보면 유저는 재주문 하지 않았다 그 이유는 fridge-pack-cola를 대신 샀기 때문. 이러한 행동 유형을 잡기 위해서 피처를 만들었다. days_last_order_max는 days_since_last_order_this_item과 useritem_order_days_max를 뺀 것. days_since_last_order_this_item는 유저와 아이템 사이의 피처이다. → 이는 마지막 오더에서 얼마나 많은 일수가 지나갔는지에 대한 것 useritem_order_days_max 는 유저와 아이템 사이의 피처이다 → 이는 주문의 최대 주문 기간(일)을 의미한다. index 0을 보면, 이는 유저가 주문을 14일 전에 샀고 주문 최대 기간이 30일인걸 알수 있다. 이 피처는 유저가 그 아이템을 사는것에 싫증을 느끼는지 아닌지에 대한 것 이미 우리는 야채보다 과일의 재주문율이 더 많은걸 알고 있다. 얼마나 더 자주인지 알아보자 item_10to1_ratio 피처를 만듦: 주문된 경우와 주문되지 않은 경우의 재주문 비율로 정의 유저 A가 item A를 order_number 1과 4를 샀다. 유저B는 item_A를 order_number 1과 3을 샀다. item_10to1_ratio는 0.5 : 유저 a가 먼저 1,4를 사고 유저 b가 1,3을 샀는데 산 1,3 중에 재주문 된 order_number가 1이어서 2개 중에 재주문율 1개 그래서 ratio는 0.5 Importance Findings for reorder useritem_sum_pos_cart(사용자 A, 아이템 B)는 사용자 A의 카트에서 아이템 B가 속하는 평균 위치입니다. useritem_sum_pos_cart-mean(사용자 A)은 모든 항목에 대한 위 기능의 평균입니다. 따라서 이 feature는 기본적으로 사용자의 카트에서 아이템의 평균 위치를 캡처하며, 한 번에 많은 아이템을 구매하지 않는 사용자는 ‘재주문이 아님’일 가능성이 더 높다는 것을 알 수 있습니다. total_buy는 total order의 숫자 만약 유저a가 item A를 과거에 3번 샀다면, total_buy는 3이 됨 그래서 total_buy_max는 유저당 주문의 최대값 우리는 이를 통해 유저가 재주문할지 안할지를 예측 t-1_is_None(User A)는 binary feature: 유저의 전 주문이 재주문인지 아닌지를 알려줌 만약 이 전주문이 ‘재주문하지 않음’이라면, 다음 주문은 재주문하지 않을 가능성이 30%가 있는것. &#x2F; F1 maximization F1 최대화에 관해서는 Faron이 커널을 발표하기 전까지는 그 논문을 읽지 않았습니다. 하지만 저는 F1 최대화 덕분에 높은 점수를 받았습니다. 설명해드리겠습니다. F1을 최대화하기 위해 저는 예측된 확률에 따라 y_true를 생성합니다. 그리고 더 높은 확률에서 F1을 확인합니다. 예를 들어 {A: 0.3, B: 0.5, C: 0.4}와 같이 항목과 확률이 정렬되어 있다고 가정해 보겠습니다. 그런 다음 y_true를 여러 번 생성합니다. 제 경우에는 9999번 생성했습니다. 이제 [[A,B],[B],[B,C],[C],[B],[None]…..]와 같이 많은 y_true가 생겼습니다. 위에서 언급했듯이 다음으로 할 일은 [B], [B,C], [B,C,A]에서 F1을 확인하는 것입니다. 그런 다음 F1 피크 아웃을 추정하고 계산을 중지하고 다음 순서로 넘어갈 수 있습니다. 이 방법에서는 [A],[A,B],[A,B,C],[B]…와 같은 모든 패턴을 확인할 필요가 없다는 것을 알 수 있습니다. “더 멀리 가기 위한 팁”이라는 제 코멘트에서 이 방법을 알아낸 분도 있을 것 같습니다. 그러나 이 방법은 시간이 많이 걸리고 시드에 따라 달라집니다. 그래서 저는 결국 Faron의 커널을 사용했습니다. 다행히도 Faron의 커널을 사용해서 거의 동일한 결과를 얻었습니다. py_model&#x2F;pyx_get_best_items.pyx를 참고하세요. 모델 평가 지표인 F1 score: 단일 메트릭에서 정확도와 리콜을 모두 캡처하는 방법 그래서 우리는 재주문 확률을 이진 형태로 바꿔야 함. 하지만 이 변환을 수행하려면 임계값을 알아야 함. 처음에는 그리드 검색을 사용하여 0.2라는 보편적인 임계값을 찾았지만 Kaggle 토론 게시판에서 주문에 따라 임계값이 달라야 한다는 의견을 봄 왜그래야 하는지에 대해서 밑의 예시를 살펴 보자 첫 번째 예에서 임계값은 0.9에서 0.3 사이입니다. 두 번째 예에서는 임계값이 0.2보다 낮습니다. 앞서 설명했듯이 각 주문에는 각각의 임계값이 있어야 합니다. 하지만 위의 계산을 사용하면 모든 확률 패턴을 먼저 준비해야 합니다. 따라서 다른 계산을 생각해 내야했습니다. 모델에서 품목 A가 0.9의 확률로 재주문되고 품목 B가 0.3의 확률로 재주문될 것으로 예측한다고 가정해 보겠습니다. 그런 다음 이 확률을 사용하여 9,999개의 대상 레이블(A와 B가 재주문될지 여부)을 시뮬레이션합니다. 예를 들어 시뮬레이션된 레이블은 다음과 같이 보일 수 있습니다. 그런 다음 확률이 가장 높은 항목부터 시작하여 F1 점수가 정점에 달했다가 감소할 때까지 항목(예: [A], [A, B], [A, B, C] 등)을 추가하여 각 레이블 세트에 대한 예상 F1 점수를 계산합니다. A, B, AB…와 같은 모든 패턴을 계산할 필요는 없습니다. 항목B를 선택해야 한다면 항목A도 선택해야 하기 때문입니다. ‘재주문 없음’(None)에 대해 생각하는 한 가지 방법은 (1 - 항목 A) * (1 - 항목 B) * …의 확률로 생각하는 것입니다. 하지만 또 다른 방법은 None을 특수한 경우로 예측하는 것입니다. None 모델을 사용하고 None을 다른 항목으로 취급하면 F1 점수를 0.400에서 0.407로 높일 수 있습니다.","categories":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"Kaggle","slug":"Kaggle","permalink":"https://jmj3047.github.io/tags/Kaggle/"}],"keywords":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}]},{"title":"Unsupervised Learning, Recommenders, Reinforcement Learning","slug":"ML_Part3","date":"2023-04-03T15:00:00.000Z","updated":"2023-04-13T10:57:37.183Z","comments":true,"path":"2023/04/04/ML_Part3/","link":"","permalink":"https://jmj3047.github.io/2023/04/04/ML_Part3/","excerpt":"","text":"Course Lecture 3 in Machine Learning course Unsupervised Learning1. clustering Usage: Grouping similar news, DNA analysis, Astronomical data analysis k-means algorithm local minimum Occurs when optimization algorithm converges to a suboptimal solution K-means is sensitive to initial centroid placement Different initializations can lead to different local minima Multiple runs with random initializations can mitigate this issue elbow method Technique to determine optimal number of clusters (k) Plot within-cluster sum of squares (WCSS) against k Look for “elbow” point in the plot Elbow indicates diminishing returns in WCSS reduction with increasing k Choose k at elbow point as optimal number of clusters Evaluate K-means based on how well it performs on that later purpose → not depending on mathematics but business. 2. anomaly detection gaussian(normal) distribution anomaly detection algorithm Anomlay detection VS Supervised Learning Just train the system to decide if a new smartphone that you just manufactured has any scratches in it. If you just see scratch smartphones over and over and you want to check if your phones are scratched, then supervised learning works well. Whereas if you suspect that they are going to be brand new ways for something go wrong in the future, then anomaly detection will work well. Anomaly detection tries to find brand new positive examples that may be unlike anything you’ve seen before. Where supervised learning looks at your positive examples and tires to decide if a future example is similar to the positive examples that you’ve already seen. Recommender Systems With this framework for recommended systems one possible way to approach the problem is to look at the movies that users have not rated. And to try to predict how users would rate those movies because then we can try to recommend to users things that they are more likely to rate as five stars. 1. collaborative filtering Using per-item features Collaborative filtering In which of the following situations will a collaborative filtering system be the most appropriate learning algorithm (compared to linear or logistic regression)? You run an online bookstore and collect the ratings of many users. You want to use this to identify what books are “similar” to each other (i.e., if a user likes a certain book, what are other books that they might also like?) → You can find “similar” books by learning feature values using collaborative filtering. Binary lables: favs, likes and clicks 2. Recommender systems implementation detail mean normalization It seems more reasonable to think Eve is likely to rate this movie 2.5 rather than think Eve will rate all movie zero stars just because she hasn’t rated any movies yet. In fact the effect of this algorithm is it will cause the initial guesses for the new user Eve to be just equal to the mean of whatever other users have rated these five movies. It turns out that by normalizing the mean of the different movies ratings to be zero, the optimization algorithm for the recommended system will also run just a little bit faster. But it does make algorithm behave much better for users who have rated no movies or very small numbers of movies. And the predictions will become more reasonable. There’s one other alternative that you could use which is to instead normalize the columns of this matrix to have zero mean. And that would be a reasonable thing to do too. But I think in this application, normalizing the rows so that you can give reasonable ratings for a new user seems more important than normalizing the columns. Limitations of Collaborative Filtering 3. Content-based filtering Collaborative filtering: Recommend items based on similar user ratings Algorithm uses existing ratings to make recommendations Content-based filtering: Recommend items based on user and item features Requires features of users and items Finds good matches based on these features Can potentially find better matches than collaborative filtering alone 4. Recommending from a large catalogue During the retrieval step, retrieving more items will tend to result in better performance. But the algorithm will end up being slower to analyze or to optimize the trade off between how many items to retrieve to retrieve 100 or 500 or 1000 items. 5. Principle Componant Analysis(PCA) the idea of PCA is to find one or more new axes, such as z so that when you measure your datas coordinates on the new axis, you end up still with very useful information about the car. PCA is a powerful algorithm for taking data with a lot of features, with a lot of dimensions or high-dimensional data, and reducing it to two or three features to two or three dimensional data so you can plot it and visualize it and better understand what’s in your data. Linear regression: One target variable Y Measures distance between fitted line and Y Distances measured in the direction of the y-axis PCA (Principal Component Analysis): Can have many features (e.g., X1, X2, … X50) All features treated equally Finds axis Z to retain maximum variance Projects data onto Z while preserving variance But when you have more than two features, which is most of the case, the difference between linear regression and PCA and what the algorithms do is very large. These algorithms are used for totally different purposes and give you very different answers. you should use linear regression if you’re trying to predict the value of y, and you should use PCA if you’re trying to reduce the number of features in your data set, say to visualize it. PCA (Principal Component Analysis) other applications: Data compression (less popular now): Reduce features for smaller storage or transmission costs Example: 50 features per car reduced to 10 principal components Speed up training of supervised learning models (less popular now): Reduce high-dimensional features to smaller set Used to make a difference for older learning algorithms (e.g., Support Vector Machines) Modern developments: Improved storage and networking capabilities Modern machine learning algorithms like deep learning handle high-dimensional data more effectively Less need for PCA in these applicationsReinforcement 1. Reinforcement learning introduction the key idea is rather than you needing to tell the algorithm what is the right output y for every single input, all you have to do instead is specify a reward function that tells it when it’s doing well and when it’s doing poorly. And it’s the job of the algorithm to automatically figure out how to choose good actions. the goal is given a board position to pick a good action using a policy Pi. This formalism of a reinforcement learning application actually has a name. It’s called a Markov decision process in a Markov decision process, the future depends only on where you are now, not on how you got here. 2. State-action value function In reinforcement learning, there’s a key equation called the Bellman equation that will help us to compute the state action value function.","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Coursera","slug":"Coursera","permalink":"https://jmj3047.github.io/tags/Coursera/"},{"name":"AI Studies","slug":"AI-Studies","permalink":"https://jmj3047.github.io/tags/AI-Studies/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}]},{"title":"Ecommerce Recommendation System","slug":"Ecommerce_Rec","date":"2023-04-02T15:00:00.000Z","updated":"2023-04-06T09:01:21.752Z","comments":true,"path":"2023/04/03/Ecommerce_Rec/","link":"","permalink":"https://jmj3047.github.io/2023/04/03/Ecommerce_Rec/","excerpt":"","text":"개요 kaggle instacart 데이터로 추천 모델링 시스템을 만들기 빅쿼리, Vertex AI를 사용하여 모델을 만들고 예측하기 참고한 유투브의 추천시스템은 평점 feature가 있지만 이 데이터에는 존재 하지 않음. 따라서 재주문 여부와 주문회차를 평점으로 가정하고 모델에 도입함 200만 유저 중에 10000명으로 제한하였고, 모델 평가에 대한 지표는 nDCG를 사용함 모델에 대한 간략한 설명 사용한 데이터셋 및 cpu&#x2F;gpu 성능 Instacart 데이터셋 세부 특성 확인 : Instacart data dictionary version 1: vCPU 4개, 15GB RAM, GPU 없음 → 텐서플로우가 돌아가지 않음 비용: 월 202 달러, 시간당 0.27 달러 version2: vCPU 8개, 52GB RAM, GPU NVIDIA Tesla P4 1개 비용: 월 940 달러, 시간당 1.27 달러 Data import (BQ → Jupyter) 이미 빅쿼리에 csv 파일을 적재함 주피터 커널 안에 ‘%%’ 를 사용하면 빅쿼리 문법을 사용할 수 있음 123%%bigquery aislesSELECT * FROM `your-project-id.your-view.aisles`ORDER BY 1 12print(aisles.isna().sum())aisles 123%%bigquery departmentsSELECT * FROM `your-project-id.your-view.departments`ORDER BY 1 12print(departments.isna().sum())departments 이런 형식으로 order_product_prior, order_product_train, orders, products 데이터 프레임을 생성 데이터 전처리: 결측치 처리 및 샘플링123# 결측치 처리(&#x27;days_since_prior_order&#x27;:첫 구매인경우 nan값이 입력되어 있음 =&gt; 0으로 대체)#orders[&#x27;days_since_prior_order&#x27;] = orders[&#x27;days_since_prior_order&#x27;].fillna(0)orders = orders.dropna() 12print(orders.shape)orders[&#x27;eval_set&#x27;].value_counts() 1orders[(orders[&#x27;user_id&#x27;]==10)] # =&gt; 주문데이터 분석: 유저당 prior주문기록, train주문기록이 있는 유저들이 있음. 1orders[orders[&#x27;user_id&#x27;].isin(orders[&#x27;user_id&#x27;][orders[&#x27;eval_set&#x27;]==&#x27;test&#x27;])] # test data -&gt; order_product 정보가 없음 1orders[&#x27;user_id&#x27;].value_counts() 1234orders_user_id_eval_set = []for i in orders[&#x27;user_id&#x27;][orders[&#x27;eval_set&#x27;]==&#x27;train&#x27;].unique(): i = int(i) orders_user_id_eval_set.append(i) 12345678import randomrandom.seed(2021)# 1만명의 유저만 샘플링 (첫시도)user = random.sample(orders_user_id_eval_set, 10000)#user = orders[&#x27;user_id&#x27;][orders[&#x27;eval_set&#x27;]==&#x27;train&#x27;].unique().tolist() #샘플링 안하고 시도 (실패)# orders에서 test dataset 관련 기록(prior포함) 제외orders_2 = orders[orders[&#x27;user_id&#x27;].isin(user)] 1print(&quot;유저 인원:&quot;,len(user), &quot;// 주문건수 합:&quot;,len(orders_2)) 12# order_product_prior에서 test dataset 관련 기록 제외order_product_prior = order_product_prior[order_product_prior[&#x27;order_id&#x27;].isin(orders_2[&#x27;order_id&#x27;])] 데이터 조합123# 제품 정보 : product + aisles + department dataproduct_m = products.merge(aisles).merge(departments)product_m 1234import pandas as pd# product one-hot encoding data product_enc = pd.get_dummies(product_m, columns=[&#x27;aisle&#x27;], prefix=[None])product_encㄹ 12order_product = pd.concat([order_product_prior,order_product_train]) order_product 12# 주문 항목 정보 : product_m + order_productorder_detail = order_product.merge(product_m,how=&#x27;left&#x27;) #.sample(n=100000, random_state=2021) 1order_detail#.sort_values(by=order_detail[&#x27;order_id&#x27;]) 1data = orders_2.merge(order_detail)#, on =&#x27;order_id&#x27;) 1data 1data.isna().sum() 데이터 탐색적 분석(EDA)주문 행태 분석12345# 요일별 주문 현황data[&#x27;order_dow&#x27;].value_counts()data.hist(&#x27;order_dow&#x27;,grid=False, bins=7) 12345# 시간대별 주문 현황data[&#x27;order_hour_of_day&#x27;].value_counts()data.hist(&#x27;order_hour_of_day&#x27;,grid=False, bins=24) 1234# 주문 횟수 현황 data[&#x27;order_number&#x27;].value_counts()data.hist(&#x27;order_number&#x27;,grid=False, bins=24) 1234# 재주문 여부 현황 data[&#x27;reordered&#x27;].value_counts()data.hist(&#x27;reordered&#x27;,grid=False) 유투브 추천 알고리즘 적용데이터 전처리데이터 Encoding1234567data[&#x27;user_id&#x27;] = data[&#x27;user_id&#x27;].astype(int)data[&#x27;product_id&#x27;] = data[&#x27;product_id&#x27;].astype(int)data[&#x27;order_id&#x27;] = data[&#x27;order_id&#x27;].astype(int)data[&#x27;days_since_prior_order&#x27;] = data[&#x27;days_since_prior_order&#x27;].astype(int)data = data.set_index([&#x27;user_id&#x27;]).sort_index()data = data.reset_index() 12345678910111213141516171819# 유저 인덱스 인코딩user_ids = data[&quot;user_id&quot;].unique().tolist()user2user_encoded = &#123;x: i for i, x in enumerate(user_ids)&#125;#userencoded2user = &#123;i: x for i, x in enumerate(user_ids)&#125;# 주문 인덱스 인코딩order_ids = data[&quot;order_id&quot;].unique().tolist()order2order_encoded = &#123;x: i for i, x in enumerate(order_ids)&#125;#order_encoded2order = &#123;i: x for i, x in enumerate(order_ids)&#125;# 상품 인덱스 인코딩product_ids = data[&quot;product_id&quot;].unique().tolist()product2product_encoded = &#123;x: i for i, x in enumerate(product_ids)&#125;#product_encoded2product = &#123;i: x for i, x in enumerate(product_ids)&#125;# 상품 이름 인코딩pd_name_ids = data[&quot;product_name&quot;].unique().tolist()pd_name2pd_name_encoded = &#123;x: i for i, x in enumerate(pd_name_ids)&#125;#pd_name_encoded2pd_name = &#123;i: x for i, x in enumerate(pd_name_ids)&#125; 12345678910111213141516171819202122232425# 상품 대분류 인덱스 인코딩department_ids = []for i in data[&quot;department_id&quot;].unique(): i = int(i) department_ids.append(i)department2department_encoded = &#123;x: i for i, x in enumerate(department_ids)&#125;#department_encoded2department = &#123;i: x for i, x in enumerate(department_ids)&#125;# 상품 소분류 인덱스 인코딩aisle_ids = []for i in data[&quot;aisle_id&quot;].unique(): i = int(i) aisle_ids.append(i)aisle2aisle_encoded = &#123;x: i for i, x in enumerate(aisle_ids)&#125;#aisle_encoded2aisle = &#123;i: x for i, x in enumerate(aisle_ids)&#125;# 상품 대분류명 인덱스 인코딩dept_name_ids = data[&quot;department&quot;].unique().tolist()dept_name2dept_name_encoded = &#123;x: i for i, x in enumerate(dept_name_ids)&#125;#dept_name_encoded2dept_name = &#123;i: x for i, x in enumerate(dept_name_ids)&#125;# 상품 소분류명 인덱스 인코딩aisle_name_ids = data[&quot;aisle&quot;].unique().tolist()aisle_name2aisle_name_encoded = &#123;x: i for i, x in enumerate(aisle_name_ids)&#125;#aisle_name_encoded2aisle_name = &#123;i: x for i, x in enumerate(aisle_name_ids)&#125; 12345678910# 인코딩으로 바꾸기data[&quot;user&quot;] = data[&quot;user_id&quot;].map(user2user_encoded)data[&quot;product&quot;] = data[&quot;product_id&quot;].map(product2product_encoded)data[&quot;order&quot;] = data[&quot;order_id&quot;].map(order2order_encoded)data[&quot;pd_name&quot;] = data[&quot;product_name&quot;].map(pd_name2pd_name_encoded)# data[&quot;department&quot;] = data[&quot;department_id&quot;].map(department2department_encoded)# data[&quot;aisle&quot;] = data[&quot;aisle&quot;].map(aisle2aisle_encoded)# data[&quot;dept_name&quot;] = data[&quot;department&quot;].map(dept_name2dept_name_encoded)# data[&quot;aisle_name&quot;] = data[&quot;aisle&quot;].map(aisle_name2aisle_name_encoded) User 기준으로 데이터 조정(feature engineering) 구매자 기준으로 데이터 프레임 재생성 feature engineering 추가 기능 12345order_hist = data.groupby([&#x27;user&#x27;])[&#x27;order_id&#x27;].unique().apply(list).reset_index()product_hist = data.groupby([&#x27;user&#x27;])[&#x27;product_id&#x27;].apply(list).reset_index()order_dow_hist = data.groupby([&#x27;user&#x27;])[&#x27;order_dow&#x27;].apply(list).reset_index() # unique().적용해보기order_hour_of_day_hist = data.groupby([&#x27;user&#x27;])[&#x27;order_hour_of_day&#x27;].apply(list).reset_index()days_since_prior_order_hist = data.groupby([&#x27;user&#x27;])[&#x27;days_since_prior_order&#x27;].apply(list).reset_index() 1data.groupby([&#x27;user&#x27;])[&#x27;order_dow&#x27;].unique().apply(list 1order_product_hist = data.groupby([&#x27;order&#x27;])[&#x27;product_id&#x27;].apply(list).reset_index() 1order_hist # 사용자의 주문목록 1product_hist # 사용자가 구매한 상품 1order_product_hist 1order_dow_hist 1order_hour_of_day_hist 1days_since_prior_order_hist 123# User dataset 생성 (학습에 사용할 데이터, prior order:[data[&#x27;eval_set&#x27;]==&#x27;prior&#x27;])user_data = data[[&#x27;user&#x27;,&#x27;user_id&#x27;]].merge(order_hist, how=&#x27;left&#x27;).merge(product_hist, how=&#x27;left&#x27;).merge(order_dow_hist, how=&#x27;left&#x27;).merge(order_hour_of_day_hist, how = &#x27;left&#x27;).merge(days_since_prior_order_hist,how=&#x27;left&#x27;) #eval_setuser_data 12user_data = user_data.drop_duplicates(&#x27;user&#x27;) # 중복데이터 삭제user_data.shape (10000, 7) 1data_product_prior=data[&#x27;product&#x27;][data[&#x27;eval_set&#x27;]==&#x27;prior&#x27;] predict_label 생성 및 데이터 분할123user_data[&#x27;predict_labels&#x27;] = user_data[&#x27;product_id&#x27;].apply(lambda x: int(random.uniform(0,data[&#x27;product_id&#x27;].max())))#user_data[&#x27;predict_labels&#x27;] = user_data[&#x27;product_id&#x27;].apply(lambda x: int(random.uniform(0,data[&#x27;product&#x27;].max())))# (random.uniform(0,data[&quot;product&quot;][data[&#x27;eval_set&#x27;]==&#x27;prior&#x27;].max())) train 데이터의 product중 하나 (=&gt; 알맞은 데이터가 들어가는지 코드 검증 필요) 1user_data 1234train_data = user_data[(user_data.user&gt;=30) &amp; (user_data.user&lt;=39)]test_data = user_data[(user_data.user&gt;=40) &amp; (user_data.user&lt;=59)] 후보모델(candidate generator model)1data[&quot;product_id&quot;].max() 49688 1data[&quot;product&quot;].max() 35406 12345678# 하이퍼파라미터 정의EMBEDDING_DIMS = 16DENSE_UNITS = 64DROPOUT_PCT = 0.1ALPHA = 0.1NUM_CLASSES = data[&quot;product_id&quot;].max() + 2 LEARNING_RATE = 0.1 12import osos.environ[&#x27;TF_CPP_MIN_LOG_LEVEL&#x27;] = &#x27;2&#x27; 123456789101112131415161718192021222324252627282930313233343536# custom layersimport tensorflow as tfclass MaskedEmbeddingsAggregatorLayer(tf.keras.layers.Layer): def __init__(self, agg_mode=&#x27;sum&#x27;, **kwargs): super(MaskedEmbeddingsAggregatorLayer, self).__init__(**kwargs) if agg_mode not in [&#x27;sum&#x27;, &#x27;mean&#x27;]: raise NotImplementedError(&#x27;mode &#123;&#125; not implemented!&#x27;.format(agg_mode)) self.agg_mode = agg_mode @tf.function def call(self, inputs, mask=None): masked_embeddings = tf.ragged.boolean_mask(inputs, mask) if self.agg_mode == &#x27;sum&#x27;: aggregated = tf.reduce_sum(masked_embeddings, axis=1) elif self.agg_mode == &#x27;mean&#x27;: aggregated = tf.reduce_mean(masked_embeddings, axis=1) return aggregated def get_config(self): # this is used when loading a saved model that uses a custom layer return &#123;&#x27;agg_mode&#x27;: self.agg_mode&#125; class L2NormLayer(tf.keras.layers.Layer): def __init__(self, **kwargs): super(L2NormLayer, self).__init__(**kwargs) @tf.function def call(self, inputs, mask=None): if mask is not None: inputs = tf.ragged.boolean_mask(inputs, mask).to_tensor() return tf.math.l2_normalize(inputs, axis=-1) def compute_mask(self, inputs, mask): return mask 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889# modelingimport tensorflow as tfimport datetimeimport osinput_user = tf.keras.Input(shape=(None, ), name=&#x27;user&#x27;) input_product_hist = tf.keras.layers.Input(shape=(None,), name=&#x27;product_hist&#x27;)input_order_dow_hist = tf.keras.layers.Input(shape=(None,), name=&#x27;order_dow_hist&#x27;)input_order_hour_of_day_hist = tf.keras.Input(shape=(None, ), name=&#x27;order_hour_of_day_hist&#x27;)input_days_since_prior_order_hist = tf.keras.Input(shape=(None, ), name=&#x27;days_since_prior_order_hist&#x27;)# layer 구성features_embedding_layer = tf.keras.layers.Embedding(input_dim=NUM_CLASSES, output_dim=EMBEDDING_DIMS, mask_zero=True, trainable=True, name=&#x27;features_embeddings&#x27;)labels_embedding_layer = tf.keras.layers.Embedding(input_dim=NUM_CLASSES, output_dim=EMBEDDING_DIMS, mask_zero=True, trainable=True, name=&#x27;labels_embeddings&#x27;)avg_embeddings = MaskedEmbeddingsAggregatorLayer(agg_mode=&#x27;mean&#x27;, name=&#x27;aggregate_embeddings&#x27;)dense_1 = tf.keras.layers.Dense(units=DENSE_UNITS, name=&#x27;dense_1&#x27;)dense_2 = tf.keras.layers.Dense(units=DENSE_UNITS, name=&#x27;dense_2&#x27;)dense_3 = tf.keras.layers.Dense(units=DENSE_UNITS, name=&#x27;dense_3&#x27;)l2_norm_1 = L2NormLayer(name=&#x27;l2_norm_1&#x27;)dense_output = tf.keras.layers.Dense(NUM_CLASSES, activation=tf.nn.softmax, name=&#x27;dense_output&#x27;)# feature 투입features_embeddings = features_embedding_layer(input_user)l2_norm_features = l2_norm_1(features_embeddings)avg_features = avg_embeddings(l2_norm_features)labels_product_embeddings = labels_embedding_layer(input_product_hist)l2_norm_product = l2_norm_1(labels_product_embeddings)avg_product = avg_embeddings(l2_norm_product)labels_order_dow_embeddings = labels_embedding_layer(input_order_dow_hist)l2_norm_order_dow = l2_norm_1(labels_order_dow_embeddings)avg_order_dow = avg_embeddings(l2_norm_order_dow)labels_order_hour_embeddings = labels_embedding_layer(input_order_hour_of_day_hist)l2_norm_order_hour = l2_norm_1(labels_order_hour_embeddings)avg_order_hour = avg_embeddings(l2_norm_order_hour)labels_since_prior_embeddings = labels_embedding_layer(input_days_since_prior_order_hist)l2_norm_since_prior = l2_norm_1(labels_since_prior_embeddings)avg_since_prior = avg_embeddings(l2_norm_since_prior)print(avg_features)print(avg_order_dow)print(avg_order_hour)print(avg_since_prior)# 임베딩 벡터들 연결concat_inputs = tf.keras.layers.Concatenate(axis=1)([avg_product, avg_order_dow, avg_order_hour, avg_since_prior ])# Dense Layersdense_1_features = dense_1(concat_inputs)dense_1_relu = tf.keras.layers.ReLU(name=&#x27;dense_1_relu&#x27;)(dense_1_features)dense_1_batch_norm = tf.keras.layers.BatchNormalization(name=&#x27;dense_1_batch_norm&#x27;)(dense_1_relu)dense_2_features = dense_2(dense_1_relu)dense_2_relu = tf.keras.layers.ReLU(name=&#x27;dense_2_relu&#x27;)(dense_2_features)dense_2_batch_norm = tf.keras.layers.BatchNormalization(name=&#x27;dense_2_batch_norm&#x27;)(dense_2_relu)dense_3_features = dense_3(dense_2_relu)dense_3_relu = tf.keras.layers.ReLU(name=&#x27;dense_3_relu&#x27;)(dense_3_features)dense_3_batch_norm = tf.keras.layers.BatchNormalization(name=&#x27;dense_3_batch_norm&#x27;)(dense_3_relu)outputs = dense_output(dense_3_batch_norm)#Optimizeroptimiser = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)#--- prep modelmodel = tf.keras.models.Model( inputs=[input_product_hist, input_order_dow_hist, input_order_hour_of_day_hist, input_days_since_prior_order_hist ], outputs=[outputs])logdir = os.path.join(&quot;logs&quot;, datetime.datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;))tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)model.compile(optimizer=optimiser, loss=&#x27;sparse_categorical_crossentropy&#x27;, metrics=[&#x27;acc&#x27;]) model.summary() 1train_data 12345678# 학습(training)history = model.fit([tf.keras.preprocessing.sequence.pad_sequences(train_data[&#x27;product_id&#x27;]), tf.keras.preprocessing.sequence.pad_sequences(train_data[&#x27;order_dow&#x27;]), tf.keras.preprocessing.sequence.pad_sequences(train_data[&#x27;order_hour_of_day&#x27;]), #+ 1e-10, dtype=float tf.keras.preprocessing.sequence.pad_sequences(train_data[&#x27;days_since_prior_order&#x27;]) ],train_data[&#x27;predict_labels&#x27;].values, #batch_size=16, steps_per_epoch=1, epochs=300) 12# 모델 저장model.save(&quot;candidate_generation_v2.h5&quot;) 12345678# 모델 예측결과 추출pred = model.predict([tf.keras.preprocessing.sequence.pad_sequences(test_data[&#x27;product_id&#x27;]), tf.keras.preprocessing.sequence.pad_sequences(test_data[&#x27;order_dow&#x27;]), tf.keras.preprocessing.sequence.pad_sequences(test_data[&#x27;order_hour_of_day&#x27;]), #+ 1e-10, dtype=float tf.keras.preprocessing.sequence.pad_sequences(test_data[&#x27;days_since_prior_order&#x27;]) ])pred 123456789# candidate generation: ###### 각 user당 top-7개의 추천 후보 데이터(predict_label)를 뽑아낸다.import numpy as npN = 7k = np.sort((-pred).argsort()[:,:N])print(k)k = k.flatten()k[k&gt;data[&quot;product&quot;].max()]=0k = np.unique(k) 1k 순위 모델 1,2안(ranking model 1,2)1안)재주문여부(reordered), 1:like, 0:dislike12345678# load candidate_generation model = tf.keras.models.load_model( &#x27;candidate_generation_v2.h5&#x27;, custom_objects=&#123; &#x27;L2NormLayer&#x27;:L2NormLayer, &#x27;MaskedEmbeddingsAggregatorLayer&#x27;:MaskedEmbeddingsAggregatorLayer &#125;) 123456789101112131415161718192021222324# 아이템의 속성(ex.aisle)을 불러오는 함수 aisle_cols = aisles[&#x27;aisle&#x27;].values.tolist()# type(aisle_cols)type(aisle_cols)aisle_colsaisles_encoded = &#123;x: i for i, x in enumerate(aisle_cols)&#125;# movies-&gt; product_enc# genres-&gt; aisles# aisles -&gt; [&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, ...]def get_aisles(products, aisles): def get_all_aisles(ai): active = [str(aisles_encoded[aisle]) for aisle, a in zip(aisles, ai) if a==1] if len(active) == 0: return &#x27;0&#x27; return &#x27;,&#x27;.join((active)) products[&#x27;all_aisles&#x27;] = [ get_all_aisles(ai) for ai in zip(*[products[aisle] for aisle in aisles]) # 문제없음. ]get_aisles(product_enc, aisle_cols) 1product_enc.head(1) 1data[&#x27;reordered&#x27;].value_counts() 12ratings = data[[&#x27;product_id&#x27;, &#x27;reordered&#x27;]]ratings 1234# normalizedef normalize_col(df,col_name): df[col_name] = (df[col_name] - df[col_name].min()) / (df[col_name].max() - df[col_name].min()) return df 12345678910111213141516171819202122232425262728293031#-- rating 데이터 형태로 정리하기 #-- 레퍼런스 코드와 대조하여 정리. # 레퍼런스: https://github.com/HYEZ/Deep-Youtube-Recommendations/blob/master/neural_net.ipynb# dataframe: movies-&gt;product_enc# movie_id -&gt; product_id# title -&gt; product_name -&gt; pd_name# title2title_encoded -&gt;pd_name2pd_name_encoded# genre -&gt; aisleproduct_data = product_enc.set_index([&#x27;product_id&#x27;]).sort_index()# using candidate result &#x27;k&#x27;product_data = product_data.loc[k+1]product_data[&quot;pd_name_d&quot;] = product_data[&quot;product_name&quot;].map(pd_name2pd_name_encoded)print(product_data.head())# 레퍼런스와 대조하여 feature 정리: # rating -&gt; reordered (1: like / 0: dislike) # unix_timestamp-&gt; order_hour_of_day / 굳이 필요없으면 사용하지 않기. ratings_cols = [&#x27;user_id&#x27;, &#x27;product_id&#x27;, &#x27;reordered&#x27;, &#x27;order_hour_of_day&#x27;]ratings = data[[&#x27;product_id&#x27;, &#x27;reordered&#x27;, &#x27;user_id&#x27;, &#x27;order_hour_of_day&#x27; ]]# 모델에 사용할 new_datanew_data = product_data.merge(ratings, on=&#x27;product_id&#x27;) # rating 추가print(new_data.columns)aisle_occurences = new_data[aisle_cols].sum().to_dict()aisles_encoded = &#123;x: i for i, x in enumerate(aisle_cols)&#125;get_aisles(product_data, aisle_cols) 1new_data.columns 123456789101112131415161718192021222324252627282930313233343536373839new_data = new_data[[&#x27;product_id&#x27;, &#x27;user_id&#x27;, &#x27;reordered&#x27;, &#x27;order_hour_of_day&#x27;, &#x27;all_aisles&#x27;, &#x27;pd_name_d&#x27;]]new_data[&#x27;product_type&#x27;] = np.where(new_data[&#x27;reordered&#x27;] ==1, &#x27;like&#x27;, &#x27;dislike&#x27;) # 1이면 likeproduct_list = new_data.groupby([&#x27;user_id&#x27;,&#x27;product_type&#x27;])[&#x27;product_id&#x27;].apply(list).reset_index()aisle_list = new_data.groupby([&#x27;user_id&#x27;])[&#x27;all_aisles&#x27;].unique().apply(list).reset_index()aisle_list[&#x27;all_aisles&#x27;]=aisle_list[&#x27;all_aisles&#x27;].apply(lambda x: list(set(&#x27;,&#x27;.join(x))) ) # 중복제거aisle_list[&#x27;all_aisles&#x27;]=aisle_list[&#x27;all_aisles&#x27;].apply(lambda x:[ x for x in x if x.isdigit() ]) # normalizenew_data = normalize_col(new_data, &#x27;order_hour_of_day&#x27;)order_hour_of_day_list = new_data.groupby([&#x27;user_id&#x27;])[&#x27;order_hour_of_day&#x27;].unique().apply(list).reset_index()pd_name_list = new_data.groupby([&#x27;user_id&#x27;])[&#x27;pd_name_d&#x27;].apply(list).reset_index()print(product_list)dataset = product_list.pivot(index=&#x27;user_id&#x27;, columns=&#x27;product_type&#x27;, values=&#x27;product_id&#x27;).reset_index()dataset.fillna(new_data[&quot;product_id&quot;].max()+1, inplace=True)dataset[&#x27;like&#x27;] =dataset[&#x27;like&#x27;].apply(lambda x: x if type(x) is list else [])dataset[&#x27;dislike&#x27;] =dataset[&#x27;dislike&#x27;].apply(lambda x: x if type(x) is list else [])dataset = pd.merge(dataset, pd_name_list, how=&#x27;left&#x27;)dataset = pd.merge(dataset, aisle_list, how=&#x27;left&#x27;)dataset = pd.merge(dataset, order_hour_of_day_list, how=&#x27;left&#x27;)dataset[&#x27;predict_labels&#x27;] = dataset[&#x27;like&#x27;].apply(lambda x: int(random.uniform(1,new_data[&quot;product_id&quot;].max()))) dataset[&#x27;like&#x27;]=dataset[&#x27;like&#x27;].apply(lambda x: [new_data[&quot;product_id&quot;].max()+1] if x == [] else x)dataset[&#x27;dislike&#x27;]=dataset[&#x27;dislike&#x27;].apply(lambda x: [new_data[&quot;product_id&quot;].max()+1] if x == [] else x)# train_data=dataset[(dataset.user_id &gt;= 1)&amp;# (dataset.user_id &lt;= 5)]# test_data=dataset[(dataset.user_id &gt;= 6)&amp;# (dataset.user_id &lt;= 9)]train_data_r=dataset[(dataset.index &gt;= 0)&amp; (dataset.index &lt;= 9)]test_data_r=dataset[(dataset.index &gt;= 20)&amp; (dataset.index &lt;= 24)]print(dataset.index) 1train_data_r 1test_data_r 1234567# 하이퍼파라미터 정의EMBEDDING_DIMS = 16DENSE_UNITS = 64DROPOUT_PCT = 0.1ALPHA = 0.0NUM_CLASSES=new_data[&quot;product_id&quot;].max() + 3LEARNING_RATE = 0.003 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586# 모델#---inputsimport tensorflow as tfimport datetimeimport osinput_name = tf.keras.Input(shape=(None, ), name=&#x27;product_name&#x27;)inp_item_liked = tf.keras.layers.Input(shape=(None,), name=&#x27;like&#x27;)inp_item_disliked = tf.keras.layers.Input(shape=(None,), name=&#x27;dislike&#x27;)input_aisle = tf.keras.Input(shape=(None, ), name=&#x27;aisle&#x27;)input_order_hour = tf.keras.Input(shape=(None, ), name=&#x27;order_hour&#x27;)#--- layersfeatures_embedding_layer = tf.keras.layers.Embedding(input_dim=NUM_CLASSES, output_dim=EMBEDDING_DIMS, mask_zero=True, trainable=True, name=&#x27;features_embeddings&#x27;)labels_embedding_layer = tf.keras.layers.Embedding(input_dim=NUM_CLASSES, output_dim=EMBEDDING_DIMS, mask_zero=True, trainable=True, name=&#x27;labels_embeddings&#x27;)avg_embeddings = MaskedEmbeddingsAggregatorLayer(agg_mode=&#x27;mean&#x27;, name=&#x27;aggregate_embeddings&#x27;)dense_1 = tf.keras.layers.Dense(units=DENSE_UNITS, name=&#x27;dense_1&#x27;)dense_2 = tf.keras.layers.Dense(units=DENSE_UNITS, name=&#x27;dense_2&#x27;)dense_3 = tf.keras.layers.Dense(units=DENSE_UNITS, name=&#x27;dense_3&#x27;)l2_norm_1 = L2NormLayer(name=&#x27;l2_norm_1&#x27;)dense_output = tf.keras.layers.Dense(NUM_CLASSES, activation=tf.nn.softmax, name=&#x27;dense_output&#x27;)#--- featuresfeatures_embeddings = features_embedding_layer(input_name)l2_norm_features = l2_norm_1(features_embeddings)avg_features = avg_embeddings(l2_norm_features)labels_liked_embeddings = labels_embedding_layer(inp_item_liked)l2_norm_liked = l2_norm_1(labels_liked_embeddings)avg_liked = avg_embeddings(l2_norm_liked)labels_disliked_embeddings = labels_embedding_layer(inp_item_disliked)l2_norm_disliked = l2_norm_1(labels_disliked_embeddings)avg_disliked = avg_embeddings(l2_norm_disliked)labels_aisle_embeddings = labels_embedding_layer(input_aisle)l2_norm_aisle = l2_norm_1(labels_aisle_embeddings)avg_aisle = avg_embeddings(l2_norm_aisle)labels_order_hour_embeddings = labels_embedding_layer(input_order_hour)l2_norm_order_hour = l2_norm_1(labels_order_hour_embeddings)avg_order_hour = avg_embeddings(l2_norm_order_hour)# 임베딩 벡터들 연결concat_inputs = tf.keras.layers.Concatenate(axis=1)([avg_features, avg_liked, avg_disliked, avg_aisle, avg_order_hour ])# Dense Layersdense_1_features = dense_1(concat_inputs)dense_1_relu = tf.keras.layers.ReLU(name=&#x27;dense_1_relu&#x27;)(dense_1_features)dense_1_batch_norm = tf.keras.layers.BatchNormalization(name=&#x27;dense_1_batch_norm&#x27;)(dense_1_relu)dense_2_features = dense_2(dense_1_relu)dense_2_relu = tf.keras.layers.ReLU(name=&#x27;dense_2_relu&#x27;)(dense_2_features)# dense_2_batch_norm = tf.keras.layers.BatchNormalization(name=&#x27;dense_2_batch_norm&#x27;)(dense_2_relu)dense_3_features = dense_3(dense_2_relu)dense_3_relu = tf.keras.layers.ReLU(name=&#x27;dense_3_relu&#x27;)(dense_3_features)dense_3_batch_norm = tf.keras.layers.BatchNormalization(name=&#x27;dense_3_batch_norm&#x27;)(dense_3_relu)outputs = dense_output(dense_3_batch_norm)#Optimizeroptimiser = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)#--- prep modelmodel = tf.keras.models.Model( inputs=[input_name, inp_item_liked, inp_item_disliked, input_aisle, input_order_hour, ], outputs=[outputs])logdir = os.path.join(&quot;logs&quot;, datetime.datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;))tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)model.compile(optimizer=optimiser, loss=&#x27;sparse_categorical_crossentropy&#x27;, metrics=[&#x27;acc&#x27;])model.summary() Model: &quot;model_3&quot; 12#학습train_data_r.columns 1234567ratings1 = model.fit([tf.keras.preprocessing.sequence.pad_sequences(train_data_r[&#x27;pd_name_d&#x27;])+ 1e-10, tf.keras.preprocessing.sequence.pad_sequences(train_data_r[&#x27;like&#x27;])+ 1e-10, tf.keras.preprocessing.sequence.pad_sequences(train_data_r[&#x27;dislike&#x27;])+ 1e-10, tf.keras.preprocessing.sequence.pad_sequences(train_data_r[&#x27;all_aisles&#x27;])+ 1e-10, tf.keras.preprocessing.sequence.pad_sequences(train_data_r[&#x27;order_hour_of_day&#x27;], dtype=float) + 1e-10, ],train_data_r[&#x27;predict_labels&#x27;].values, steps_per_epoch=1, epochs=300) 123456789# prediction pred_1 = model.predict([tf.keras.preprocessing.sequence.pad_sequences(test_data_r[&#x27;pd_name_d&#x27;])+ 1e-10, tf.keras.preprocessing.sequence.pad_sequences(test_data_r[&#x27;like&#x27;])+ 1e-10, tf.keras.preprocessing.sequence.pad_sequences(test_data_r[&#x27;dislike&#x27;])+ 1e-10, tf.keras.preprocessing.sequence.pad_sequences(test_data_r[&#x27;all_aisles&#x27;])+ 1e-10, tf.keras.preprocessing.sequence.pad_sequences(test_data_r[&#x27;order_hour_of_day&#x27;], dtype=float) + 1e-10 ])pred_1 ranking 결과 1234567891011121314# ranking###### 각 user당 top-7개의 추천 데이터를 뽑아낸다.N = 7# k = np.sort((-pred_1).argsort()[:,:N]) # np.argsort(): probability의 마이너스값을 작은 값부터 순서대로 데이터의 인덱스를 반환. ==&gt; 즉, 양의값으로는 큰 값부터 반환한 셈.# np.sort(): 인덱스 순으로 다시 정렬. 확률이 더 높은 것부터 보고싶으므로 레퍼런스에 있는 코드이지만 사용하지 않음. ranking_1 = (-pred_1).argsort()[:, :N]ranking_1[ranking_1&gt;new_data[&#x27;product_id&#x27;].max()]=0 print(ranking_1)ranking_1_probability = np.sort(pred_1[:, :N])print(ranking_1_probability) 2안) order_number: 18번째 이상 like, 18번째 미만 dislike12345678# load candidate_generation model = tf.keras.models.load_model( &#x27;candidate_generation_v2.h5&#x27;, custom_objects=&#123; &#x27;L2NormLayer&#x27;:L2NormLayer, &#x27;MaskedEmbeddingsAggregatorLayer&#x27;:MaskedEmbeddingsAggregatorLayer &#125;) 123456789101112131415161718192021222324# 아이템의 다른 속성(ex. aisle)을 불러오는 기능aisle_cols = aisles[&#x27;aisle&#x27;].values.tolist()# type(aisle_cols)type(aisle_cols)aisle_colsaisles_encoded = &#123;x: i for i, x in enumerate(aisle_cols)&#125;# movies-&gt; product_enc# genres-&gt; aisles# aisles -&gt; [&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, ...]def get_aisles(products, aisles): def get_all_aisles(ai): active = [str(aisles_encoded[aisle]) for aisle, a in zip(aisles, ai) if a==1] if len(active) == 0: return &#x27;0&#x27; return &#x27;,&#x27;.join((active)) products[&#x27;all_aisles&#x27;] = [ get_all_aisles(ai) for ai in zip(*[products[aisle] for aisle in aisles]) # 문제없음. ]get_aisles(product_enc, aisle_cols) 1product_enc.head(1) 1data.head() 1data[[&#x27;order_number&#x27;]].value_counts() 12# 주문데이터의 평균 주문회차 data[&#x27;order_number&#x27;].mean() 18.063830654172047 1data.shape (1570703, 19) 1234567# order_number분포 확인 import matplotlib.pyplot as pltdata[&#x27;order_number&#x27;].value_counts()data.hist(&#x27;order_number&#x27;,grid=False, bins = 30)plt.minorticks_on()plt.tick_params(axis=&#x27;both&#x27;, which =&#x27;both&#x27;, direction=&#x27;in&#x27;, pad=8, top=True, right=True) 1data[&#x27;order_number&#x27;].min(), data[&#x27;order_number&#x27;].max(), data[&#x27;order_number&#x27;].mean() (2, 100, 18.063830654172047) 1234567891011121314151617# ratings_2 (order_number: 18이상 like / 18미만 dislike)# 레퍼런스: https://www.delftstack.com/ko/howto/python-pandas/how-to-create-dataframe-column-based-on-given-condition-in-pandas/# data[&#x27;ratings_2&#x27;] = data[&#x27;order_number&#x27;][(data[&#x27;order_number&#x27;]&gt;=18)]import numpy as np conditionlist = [ data[&#x27;order_number&#x27;].to_numpy() &gt;= 18, data[&#x27;order_number&#x27;].to_numpy()&lt;18]choicelist = [1, 0]data[&#x27;ratings_2&#x27;] = np.select(conditionlist, choicelist)print(data[&#x27;ratings_2&#x27;].value_counts())print(data.columns)# 데이터 data[&#x27;ratings_2&#x27;] = 1,0 값 확인# data[[&#x27;order_number&#x27;, &#x27;ratings_2&#x27;]][(data[&#x27;order_number&#x27;]&lt;18)].sort_values(by=&#x27;order_number&#x27;, ascending=False)data[[&#x27;order_number&#x27;, &#x27;ratings_2&#x27;]][(data[&#x27;order_number&#x27;]&gt;=18)].sort_values(by=&#x27;order_number&#x27;, ascending=False) 1234# normalizedef normalize_col(df,col_name): df[col_name] = (df[col_name] - df[col_name].min()) / (df[col_name].max() - df[col_name].min()) return df 12345678910111213141516171819202122232425262728293031#-- rating 데이터 형태로 정리하기 #-- 레퍼런스 코드와 대조하여 정리. # 레퍼런스: https://github.com/HYEZ/Deep-Youtube-Recommendations/blob/master/neural_net.ipynb# dataframe: movies-&gt;product_enc# movie_id -&gt; product_id# title -&gt; product_name -&gt; pd_name# title2title_encoded -&gt;pd_name2pd_name_encoded# genre -&gt; aisle# 1안에서 사용한 product_data 그대로 사용. 아래는 에러 발생시 대안 코드. # product_data_2 = product_enc.set_index([&#x27;product_id&#x27;]).sort_index()# using candidate result &#x27;k&#x27;# product_data_2 = product_data_2.loc[k+1]# product_data_2[&quot;pd_name_d&quot;] = product_data_2[&quot;product_name&quot;].map(pd_name2pd_name_encoded)# print(product_data.head())# 레퍼런스와 대조하여 feature 정리: # rating -&gt; reordered (1: like / 0: dislike) # unix_timestamp-&gt; order_hour_of_day / 굳이 필요없으면 사용하지 않기. ratings_2_cols = [&#x27;user_id&#x27;, &#x27;product_id&#x27;, &#x27;ratings_2&#x27;, &#x27;order_hour_of_day&#x27;]ratings_2 = data[[&#x27;product_id&#x27;, &#x27;ratings_2&#x27;, &#x27;user_id&#x27;, &#x27;order_hour_of_day&#x27; ]]# 모델에 사용할 new_datanew_data_2 = product_data.merge(ratings_2, on=&#x27;product_id&#x27;) # rating 추가print(new_data.columns)aisle_occurences = new_data_2[aisle_cols].sum().to_dict()aisles_encoded = &#123;x: i for i, x in enumerate(aisle_cols)&#125;get_aisles(product_data, aisle_cols) 12print(product_data.columns)print(new_data_2.columns) 123456789101112131415161718192021222324252627282930313233343536373839new_data_2 = new_data_2[[&#x27;product_id&#x27;, &#x27;user_id&#x27;, &#x27;ratings_2&#x27;, &#x27;order_hour_of_day&#x27;, &#x27;all_aisles&#x27;, &#x27;pd_name_d&#x27;]]new_data_2[&#x27;product_type&#x27;] = np.where(new_data_2[&#x27;ratings_2&#x27;] ==1, &#x27;like&#x27;, &#x27;dislike&#x27;) # 1이면 likeproduct_list_2 = new_data_2.groupby([&#x27;user_id&#x27;,&#x27;product_type&#x27;])[&#x27;product_id&#x27;].apply(list).reset_index()aisle_list_2 = new_data_2.groupby([&#x27;user_id&#x27;])[&#x27;all_aisles&#x27;].unique().apply(list).reset_index()aisle_list_2[&#x27;all_aisles&#x27;]=aisle_list_2[&#x27;all_aisles&#x27;].apply(lambda x: list(set(&#x27;,&#x27;.join(x))) ) # 중복제거aisle_list_2[&#x27;all_aisles&#x27;]=aisle_list_2[&#x27;all_aisles&#x27;].apply(lambda x:[ x for x in x if x.isdigit() ]) # normalizenew_data_2 = normalize_col(new_data_2, &#x27;order_hour_of_day&#x27;)order_hour_of_day_list_2 = new_data_2.groupby([&#x27;user_id&#x27;])[&#x27;order_hour_of_day&#x27;].unique().apply(list).reset_index()pd_name_list_2 = new_data_2.groupby([&#x27;user_id&#x27;])[&#x27;pd_name_d&#x27;].apply(list).reset_index()print(product_list_2)dataset_2 = product_list_2.pivot(index=&#x27;user_id&#x27;, columns=&#x27;product_type&#x27;, values=&#x27;product_id&#x27;).reset_index()dataset_2.fillna(new_data_2[&quot;product_id&quot;].max()+1, inplace=True)dataset_2[&#x27;like&#x27;] =dataset_2[&#x27;like&#x27;].apply(lambda x: x if type(x) is list else [])dataset_2[&#x27;dislike&#x27;] =dataset_2[&#x27;dislike&#x27;].apply(lambda x: x if type(x) is list else [])dataset_2 = pd.merge(dataset_2, pd_name_list_2, how=&#x27;left&#x27;)dataset_2 = pd.merge(dataset_2, aisle_list_2, how=&#x27;left&#x27;)dataset_2 = pd.merge(dataset_2, order_hour_of_day_list_2, how=&#x27;left&#x27;)dataset_2[&#x27;predict_labels&#x27;] = dataset_2[&#x27;like&#x27;].apply(lambda x: int(random.uniform(1,new_data_2[&quot;product_id&quot;].max()))) dataset_2[&#x27;like&#x27;]=dataset_2[&#x27;like&#x27;].apply(lambda x: [new_data_2[&quot;product_id&quot;].max()+1] if x == [] else x)dataset_2[&#x27;dislike&#x27;]=dataset_2[&#x27;dislike&#x27;].apply(lambda x: [new_data_2[&quot;product_id&quot;].max()+1] if x == [] else x)# train_data=dataset[(dataset.user_id &gt;= 1)&amp;# (dataset.user_id &lt;= 5)]# test_data=dataset[(dataset.user_id &gt;= 6)&amp;# (dataset.user_id &lt;= 9)]train_data_r_2=dataset_2[(dataset_2.index &gt;= 0)&amp; (dataset_2.index &lt;= 9)]test_data_r_2=dataset_2[(dataset_2.index &gt;= 20)&amp; (dataset_2.index &lt;= 24)]print(dataset_2.index) 1train_data_r_2 1test_data_r_2 1234567# 하이퍼 파라미터 정의EMBEDDING_DIMS = 16DENSE_UNITS = 64DROPOUT_PCT = 0.1ALPHA = 0.0NUM_CLASSES=new_data_2[&quot;product_id&quot;].max() + 3LEARNING_RATE = 0.003 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586# 모델#---inputsimport tensorflow as tfimport datetimeimport osinput_name = tf.keras.Input(shape=(None, ), name=&#x27;product_name&#x27;)inp_item_liked = tf.keras.layers.Input(shape=(None,), name=&#x27;like&#x27;)inp_item_disliked = tf.keras.layers.Input(shape=(None,), name=&#x27;dislike&#x27;)input_aisle = tf.keras.Input(shape=(None, ), name=&#x27;aisle&#x27;)input_order_hour = tf.keras.Input(shape=(None, ), name=&#x27;order_hour&#x27;)#--- layersfeatures_embedding_layer = tf.keras.layers.Embedding(input_dim=NUM_CLASSES, output_dim=EMBEDDING_DIMS, mask_zero=True, trainable=True, name=&#x27;features_embeddings&#x27;)labels_embedding_layer = tf.keras.layers.Embedding(input_dim=NUM_CLASSES, output_dim=EMBEDDING_DIMS, mask_zero=True, trainable=True, name=&#x27;labels_embeddings&#x27;)avg_embeddings = MaskedEmbeddingsAggregatorLayer(agg_mode=&#x27;mean&#x27;, name=&#x27;aggregate_embeddings&#x27;)dense_1 = tf.keras.layers.Dense(units=DENSE_UNITS, name=&#x27;dense_1&#x27;)dense_2 = tf.keras.layers.Dense(units=DENSE_UNITS, name=&#x27;dense_2&#x27;)dense_3 = tf.keras.layers.Dense(units=DENSE_UNITS, name=&#x27;dense_3&#x27;)l2_norm_1 = L2NormLayer(name=&#x27;l2_norm_1&#x27;)dense_output = tf.keras.layers.Dense(NUM_CLASSES, activation=tf.nn.softmax, name=&#x27;dense_output&#x27;)#--- featuresfeatures_embeddings = features_embedding_layer(input_name)l2_norm_features = l2_norm_1(features_embeddings)avg_features = avg_embeddings(l2_norm_features)labels_liked_embeddings = labels_embedding_layer(inp_item_liked)l2_norm_liked = l2_norm_1(labels_liked_embeddings)avg_liked = avg_embeddings(l2_norm_liked)labels_disliked_embeddings = labels_embedding_layer(inp_item_disliked)l2_norm_disliked = l2_norm_1(labels_disliked_embeddings)avg_disliked = avg_embeddings(l2_norm_disliked)labels_aisle_embeddings = labels_embedding_layer(input_aisle)l2_norm_aisle = l2_norm_1(labels_aisle_embeddings)avg_aisle = avg_embeddings(l2_norm_aisle)labels_order_hour_embeddings = labels_embedding_layer(input_order_hour)l2_norm_order_hour = l2_norm_1(labels_order_hour_embeddings)avg_order_hour = avg_embeddings(l2_norm_order_hour)# 임베딩 벡터들 연결concat_inputs = tf.keras.layers.Concatenate(axis=1)([avg_features, avg_liked, avg_disliked, avg_aisle, avg_order_hour ])# Dense Layersdense_1_features = dense_1(concat_inputs)dense_1_relu = tf.keras.layers.ReLU(name=&#x27;dense_1_relu&#x27;)(dense_1_features)dense_1_batch_norm = tf.keras.layers.BatchNormalization(name=&#x27;dense_1_batch_norm&#x27;)(dense_1_relu)dense_2_features = dense_2(dense_1_relu)dense_2_relu = tf.keras.layers.ReLU(name=&#x27;dense_2_relu&#x27;)(dense_2_features)# dense_2_batch_norm = tf.keras.layers.BatchNormalization(name=&#x27;dense_2_batch_norm&#x27;)(dense_2_relu)dense_3_features = dense_3(dense_2_relu)dense_3_relu = tf.keras.layers.ReLU(name=&#x27;dense_3_relu&#x27;)(dense_3_features)dense_3_batch_norm = tf.keras.layers.BatchNormalization(name=&#x27;dense_3_batch_norm&#x27;)(dense_3_relu)outputs = dense_output(dense_3_batch_norm)#Optimizeroptimiser = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)#--- prep modelmodel = tf.keras.models.Model( inputs=[input_name, inp_item_liked, inp_item_disliked, input_aisle, input_order_hour, ], outputs=[outputs])logdir = os.path.join(&quot;logs&quot;, datetime.datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;))tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)model.compile(optimizer=optimiser, loss=&#x27;sparse_categorical_crossentropy&#x27;, metrics=[&#x27;acc&#x27;])model.summary() Model: &quot;model_4&quot; 12# 학습train_data_r_2.columns 1234567ranking_2 = model.fit([tf.keras.preprocessing.sequence.pad_sequences(train_data_r_2[&#x27;pd_name_d&#x27;])+ 1e-10, tf.keras.preprocessing.sequence.pad_sequences(train_data_r_2[&#x27;like&#x27;])+ 1e-10, tf.keras.preprocessing.sequence.pad_sequences(train_data_r_2[&#x27;dislike&#x27;])+ 1e-10, tf.keras.preprocessing.sequence.pad_sequences(train_data_r_2[&#x27;all_aisles&#x27;])+ 1e-10, tf.keras.preprocessing.sequence.pad_sequences(train_data_r_2[&#x27;order_hour_of_day&#x27;], dtype=float) + 1e-10, ],train_data_r_2[&#x27;predict_labels&#x27;].values, steps_per_epoch=1, epochs=300) 12345678# prediction pred_2 = model.predict([tf.keras.preprocessing.sequence.pad_sequences(test_data_r_2[&#x27;pd_name_d&#x27;])+ 1e-10, tf.keras.preprocessing.sequence.pad_sequences(test_data_r_2[&#x27;like&#x27;])+ 1e-10, tf.keras.preprocessing.sequence.pad_sequences(test_data_r_2[&#x27;dislike&#x27;])+ 1e-10, tf.keras.preprocessing.sequence.pad_sequences(test_data_r_2[&#x27;all_aisles&#x27;])+ 1e-10, tf.keras.preprocessing.sequence.pad_sequences(test_data_r_2[&#x27;order_hour_of_day&#x27;], dtype=float) + 1e-10 ])pred_2 ranking 결과 12345678910111213141516# ranking###### 각 user당 top-7개의 추천 데이터를 뽑아낸다.N = 7# k = np.sort((-pred_1).argsort()[:,:N]) # np.argsort(): probability의 마이너스값을 작은 값부터 순서대로 데이터의 인덱스를 반환. ==&gt; 즉, 양의값으로는 큰 값부터 반환한 셈.# np.sort(): 인덱스 순으로 다시 정렬. 확률이 더 높은 것부터 보고싶으므로 레퍼런스에 있는 코드이지만 사용하지 않음. ranking_2 = (-pred_2).argsort()[:, :N]ranking_2[ranking_2&gt;new_data_2[&#x27;product_id&#x27;].max()]=0 print(ranking_2)ranking_2_probability = np.sort(pred_2[:, :N])print(ranking_2_probability)# 추출된 인덱스 값들은 user별 구매목록라벨(여러개 상품들이 있는). 유사도 높은 다른 유저들의 구매목록이 추천된 셈. 순위모델 1,2안 평과 결과(ranking-eval(nDCG score))12# 1안의 유저별 추천 &#x27;상품묶음&#x27; 라벨들. ranking_1 123456# -- 라벨의 like_id값 비교용 # pd_name_d# train_data_r[[&#x27;predict_labels&#x27;,&#x27;pd_name_d&#x27;]]train_df1 = train_data_r[[&#x27;like&#x27;, &#x27;predict_labels&#x27;]]train_df1 = train_df1.rename(columns = &#123;&#x27;like&#x27;:&#x27;like_id&#x27;, &#x27;predict_labels&#x27;: &#x27;label&#x27;&#125;)train_df1 12345678910# predict_labels: 유저별 라벨 역할. # pd_name_d : user A가 구매한 상품명 인덱스 수치 묶음.# 비슷한 유저의 상품 목록을 추천하는 것으로 생각. true_df1 = test_data_r[[&#x27;user_id&#x27;, &#x27;like&#x27;, &#x27;predict_labels&#x27;]]# pred_data_1true_df1 = true_df1.rename(columns=&#123;&#x27;like&#x27;: &#x27;true_like_id&#x27; ,&#x27;predict_labels&#x27;: &#x27;true_labels&#x27;&#125;)true_df1 = true_df1[[&#x27;user_id&#x27;, &#x27;true_like_id&#x27;]]true_df1 rank_1안 최종 ndcg 값(함수 사용)1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# user 당 추천목록 데이터 추출하는 기능 def user_rec(n , rank, rec_df, train_df): # 전달인자(argument) 설명 : ## n: 몇번째 유저인지(the order of test-user starting from 0) ## rank: 랭킹 결과 (ranking result of ranking model (array type)) ## rec_df: 유저당 추천된 라벨과 라벨에 해당하는 상품 아이디 데이터프레임 (recommended label and product_id for each user) ## train_df: 랭킹 모델학습에 사용된 학습데이터에서 필요한 like, predict_labels만 추출한 데이터프레임(train_data&#x27;s &#x27;like and predict_labels&#x27;features used for ranking_model training) for label in rank[n-1]: # user2번째(array index=1) DF 만들기. rec_id = train_df[&#x27;like_id&#x27;][(train_df[&#x27;label&#x27;]==label)].tolist() rec_df = rec_df.append(pd.DataFrame([[label, rec_id]], columns=[&#x27;label&#x27;, &#x27;rec_id&#x27;]), ignore_index=True) # rec_id = train_data[&#x27;product_id&#x27;][(train_data[&#x27;label&#x27;]==label)].values return rec_df# ---user1# user1_rec_data = pd.DataFrame(columns=[&#x27;label&#x27;, &#x27;rec_id&#x27;, &#x27;T/F&#x27;])user1_rec_data = pd.DataFrame(columns=[&#x27;label&#x27;, &#x27;rec_id&#x27;])rec_df1 = user_rec(1, ranking_1, user1_rec_data, train_df1)print(&#x27;user1_rec_data&#x27;)print(rec_df1)# ---user2# user2_rec_data = pd.DataFrame(columns=[&#x27;label&#x27;, &#x27;rec_id&#x27;, &#x27;T/F&#x27;])user2_rec_data = pd.DataFrame(columns=[&#x27;label&#x27;, &#x27;rec_id&#x27;])rec_df2 = user_rec(2, ranking_1, user2_rec_data, train_df1)print(&#x27;user2_rec_data&#x27;)print(rec_df2)# ---user3user3_rec_data = pd.DataFrame(columns=[&#x27;label&#x27;, &#x27;rec_id&#x27;])rec_df3 = user_rec(3, ranking_1, user3_rec_data, train_df1)print(&#x27;user3_rec_data&#x27;)print(rec_df3)# ---user4user4_rec_data = pd.DataFrame(columns=[&#x27;label&#x27;, &#x27;rec_id&#x27;])rec_df4 = user_rec(4, ranking_1, user4_rec_data, train_df1)print(&#x27;user4_rec_data&#x27;)print(rec_df4)# ---user5user5_rec_data = pd.DataFrame(columns=[&#x27;label&#x27;, &#x27;rec_id&#x27;])rec_df5 = user_rec(5, ranking_1, user5_rec_data, train_df1)print(&#x27;user5_rec_data&#x27;)print(rec_df5) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# 각 유저별 추천목록의 상품들 중 true-like한 것이 있는지 T/F(trud:1/false:0)으로 데이터프레임에 넣어주는 기능.def true_like(n, rec_df, true_df): # 전달인자(argument)설명: ## n: 몇번째 유저인지 (the order of test-user starting from 0) ## rec_df: 유저당 추천된 라벨과 라벨에 해당하는 상품 아이디 데이터프레임 (recommended label and product_id for each user) ## true_df: 랭킹모델의 예측에 사용된 test_data중에서 필요한 &#x27;user_id, true_like_id&#x27;특성만 추출한 데이터프레임(test_data&#x27;s &#x27;user_id and true_like_id&#x27;features used for ranking_model predicting) tf_list = [] rec_id = rec_df[&#x27;rec_id&#x27;].tolist() # rec_id = rec_df[&#x27;rec_id&#x27;].values # true데이터인 test_data의 형태상, row에 있는 유저 n-1번째 row를 고정해야 함. true = true_df[&#x27;true_like_id&#x27;].tolist()[n-1] for rec in rec_id[:][:]: if len(rec) == 0: tf_list.append(0.0) elif len(rec)&gt;=1: tf = true[0] in rec[0] if tf == True: tf_list.append(1) else: tf_list.append(0) rec_df[&#x27;T/F&#x27;] = pd.DataFrame(tf_list) print(&quot;true_like_id:&quot;, true) return rec_df# ---user1의 추천목록에 대한 T/F(true-like or false) 결과 rec_df1 = true_like(1, rec_df1, true_df1)print(rec_df1)# ---user2의 추천목록에 대한 T/F(true-like or false) 결과 rec_df2 = true_like(2, rec_df2, true_df1)print(rec_df2)# ---user3의 추천목록에 대한 T/F(true-like or false) 결과 rec_df3 = true_like(3, rec_df3, true_df1)print(rec_df3)# ---user4의 추천목록에 대한 T/F(true-like or false) 결과 rec_df4 = true_like(4, rec_df4, true_df1)print(rec_df4)# ---user5의 추천목록에 대한 T/F(true-like or false) 결과 rec_df5 = true_like(5, rec_df5, true_df1)print(rec_df5) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# 각 유저별 dcg, idcg, ndcg 계산하는 기능import numpy as npdef get_ndcg(rec_df): # rec_df: 유저당 추천된 &#x27;라벨&#x27;과 &#x27;라벨에 해당하는 상품 아이디&#x27;, &#x27;T/F(true_like여부)&#x27;특성이 포함된 데이터프레임 (dataframe about recommended &#x27;label&#x27; and &#x27;product_id&#x27;, &#x27;T/F(true_like or not)&#x27; for each user) rec = rec_df[&#x27;rec_id&#x27;].tolist() t = rec_df[&#x27;T/F&#x27;].tolist() dcg = 0.0 #dcg 계산 for i, j in enumerate(t): # i, j: T/F안의 인덱스와 값들(1.0, 0.0..)을 돈다. if j == 1.0: dcg += (1.0/np.log2(i+1+1)) else: dcg += 0 #idcg 계산 idcg = sum((1.0/np.log2(i+1+1) for i in range(0, len(t)+1))) #ndcg 계산 ndcg = dcg / idcg # set 으로 결과 산출할 경우, # return dcg, idcg, ndcg #dataframe으로 결과 산출할 경우, ndcg_df = pd.DataFrame(columns=[&#x27;dcg&#x27;, &#x27;idcg&#x27;, &#x27;ndcg&#x27;]) ndcg_df = ndcg_df.append(pd.DataFrame([[dcg, idcg, ndcg]], columns=[&#x27;dcg&#x27;, &#x27;idcg&#x27;, &#x27;ndcg&#x27;]), ignore_index=True) return ndcg_df# --- user1의 dcg, idcg, ndcg ndcg_df1 = get_ndcg(rec_df1)print(ndcg_df1)# --- user2의 dcg, idcg, ndcg ndcg_df2 = get_ndcg(rec_df2)print(ndcg_df2)# --- user3의 dcg, idcg, ndcg ndcg_df3 = get_ndcg(rec_df3)print(ndcg_df3)# --- user4의 dcg, idcg, ndcg ndcg_df4 = get_ndcg(rec_df4)print(ndcg_df4)# --- user5의 dcg, idcg, ndcg ndcg_df5 = get_ndcg(rec_df5)print(ndcg_df5) 123456789101112131415# user별 dcg, idcg, ndcg 결과를 합쳐주는 함수 기능. import pandas as pddef concat_result(df1, df2): df3 = pd.concat([df1, df2]) return df3result = concat_result(ndcg_df1, ndcg_df2)result = concat_result(result, ndcg_df3)result = concat_result(result, ndcg_df4)result = concat_result(result, ndcg_df5)result.index=[&#x27;test-user1&#x27;, &#x27;test-user2&#x27;, &#x27;test-user3&#x27;, &#x27;test-user4&#x27;, &#x27;test-user5&#x27;]print(&#x27;[ ranking_1안의 유저별 dcg, idcg, ndcg ] \\n &#x27;, result)print(&#x27;-------------------\\n&#x27;)print(&#x27;[ ranking_1안의 dcg, idcg, ndcg평균 ] \\n&#x27;, result.mean()) rank_2안(order_number 기준 like&#x2F;dislike 에 따른 순위 목록)의 nDCG12# 1안의 유저별 추천 &#x27;상품묶음&#x27; 라벨들. ranking_2 1234# pd_name_d# train_data_r[[&#x27;predict_labels&#x27;,&#x27;pd_name_d&#x27;]]train_df2 = train_data_r_2[[&#x27;like&#x27;, &#x27;predict_labels&#x27;]].rename(columns=&#123;&#x27;like&#x27;: &#x27;like_id&#x27;, &#x27;predict_labels&#x27;:&#x27;label&#x27;&#125;)train_df2 123456# predict_labels: 유저별 라벨 역할. # pd_name_d : user A가 구매한 상품명 인덱스 수치 묶음.# 비슷한 유저의 상품 목록을 추천하는 것으로 생각. true_df2 = test_data_r_2[[&#x27;user_id&#x27;, &#x27;like&#x27;]].rename(columns=&#123;&#x27;like&#x27;: &#x27;true_like_id&#x27;&#125;)true_df2 rank_2안 최종 ndcg 값1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# user 당 추천목록 데이터 추출하는 기능 def user_rec(n , rank, rec_df, train_df): # 전달인자(argument) 설명 : ## n: 몇번째 유저인지(the order of test-user starting from 0) ## rank: 랭킹 결과 (ranking result of ranking model (array type)) ## rec_df: 유저당 추천된 라벨과 라벨에 해당하는 상품 아이디 데이터프레임 (recommended label and product_id for each user) ## train_df: 랭킹 모델학습에 사용된 학습데이터에서 필요한 like, predict_labels만 추출한 데이터프레임(train_data&#x27;s &#x27;like and predict_labels&#x27;features used for ranking_model training) for label in rank[n-1]: # user2번째(array index=1) DF 만들기. rec_id = train_df[&#x27;like_id&#x27;][(train_df[&#x27;label&#x27;]==label)].tolist() rec_df = rec_df.append(pd.DataFrame([[label, rec_id]], columns=[&#x27;label&#x27;, &#x27;rec_id&#x27;]), ignore_index=True) # rec_id = train_data[&#x27;product_id&#x27;][(train_data[&#x27;label&#x27;]==label)].values return rec_df# ---user1# user1_rec_data = pd.DataFrame(columns=[&#x27;label&#x27;, &#x27;rec_id&#x27;, &#x27;T/F&#x27;])user1_rec_data = pd.DataFrame(columns=[&#x27;label&#x27;, &#x27;rec_id&#x27;])rec_df1 = user_rec(1, ranking_2, user1_rec_data, train_df2)print(&#x27;user1_rec_data&#x27;)print(rec_df1)# ---user2# user2_rec_data = pd.DataFrame(columns=[&#x27;label&#x27;, &#x27;rec_id&#x27;, &#x27;T/F&#x27;])user2_rec_data = pd.DataFrame(columns=[&#x27;label&#x27;, &#x27;rec_id&#x27;])rec_df2 = user_rec(2, ranking_2, user2_rec_data, train_df2)print(&#x27;user2_rec_data&#x27;)print(rec_df2)# ---user3user3_rec_data = pd.DataFrame(columns=[&#x27;label&#x27;, &#x27;rec_id&#x27;])rec_df3 = user_rec(3, ranking_2, user3_rec_data, train_df2)print(&#x27;user3_rec_data&#x27;)print(rec_df3)# ---user4user4_rec_data = pd.DataFrame(columns=[&#x27;label&#x27;, &#x27;rec_id&#x27;])rec_df4 = user_rec(4, ranking_2, user4_rec_data, train_df2)print(&#x27;user4_rec_data&#x27;)print(rec_df4)# ---user5user5_rec_data = pd.DataFrame(columns=[&#x27;label&#x27;, &#x27;rec_id&#x27;])rec_df5 = user_rec(5, ranking_2, user5_rec_data, train_df2)print(&#x27;user5_rec_data&#x27;)print(rec_df5) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# 각 유저별 추천목록의 상품들 중 true-like한 것이 있는지 T/F(trud:1/false:0)으로 데이터프레임에 넣어주는 기능.def true_like(n, rec_df, true_df): # 전달인자(argument)설명: ## n: 몇번째 유저인지 (the order of test-user starting from 0) ## rec_df: 유저당 추천된 라벨과 라벨에 해당하는 상품 아이디 데이터프레임 (recommended label and product_id for each user) ## true_df: 랭킹모델의 예측에 사용된 test_data중에서 필요한 &#x27;user_id, true_like_id&#x27;특성만 추출한 데이터프레임(test_data&#x27;s &#x27;user_id and true_like_id&#x27;features used for ranking_model predicting) tf_list = [] rec_id = rec_df[&#x27;rec_id&#x27;].tolist() # rec_id = rec_df[&#x27;rec_id&#x27;].values # true데이터인 test_data의 형태상, row에 있는 유저 n-1번째 row를 고정해야 함. true = true_df[&#x27;true_like_id&#x27;].tolist()[n-1] for rec in rec_id[:][:]: if len(rec) == 0: tf_list.append(0.0) elif len(rec)&gt;=1: tf = true[0] in rec[0] if tf == True: tf_list.append(1) else: tf_list.append(0) rec_df[&#x27;T/F&#x27;] = pd.DataFrame(tf_list) print(&quot;true_like_id:&quot;, true) return rec_df# ---user1의 추천목록에 대한 T/F(true-like or false) 결과 rec_df1 = true_like(1, rec_df1, true_df2)print(rec_df1)# ---user2의 추천목록에 대한 T/F(true-like or false) 결과 rec_df2 = true_like(2, rec_df2, true_df2)print(rec_df2)# ---user3의 추천목록에 대한 T/F(true-like or false) 결과 rec_df3 = true_like(3, rec_df3, true_df2)print(rec_df3)# ---user4의 추천목록에 대한 T/F(true-like or false) 결과 rec_df4 = true_like(4, rec_df4, true_df2)print(rec_df4)# ---user5의 추천목록에 대한 T/F(true-like or false) 결과 rec_df5 = true_like(5, rec_df5, true_df2)print(rec_df5) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# 각 유저별 dcg, idcg, ndcg 계산하는 기능import numpy as npdef get_ndcg(rec_df): # rec_df: 유저당 추천된 &#x27;라벨&#x27;과 &#x27;라벨에 해당하는 상품 아이디&#x27;, &#x27;T/F(true_like여부)&#x27;특성이 포함된 데이터프레임 (dataframe about recommended &#x27;label&#x27; and &#x27;product_id&#x27;, &#x27;T/F(true_like or not)&#x27; for each user) rec = rec_df[&#x27;rec_id&#x27;].tolist() t = rec_df[&#x27;T/F&#x27;].tolist() dcg = 0.0 #dcg 계산 for i, j in enumerate(t): # i, j: T/F안의 인덱스와 값들(1.0, 0.0..)을 돈다. if j == 1.0: dcg += (1.0/np.log2(i+1+1)) else: dcg += 0 #idcg 계산 idcg = sum((1.0/np.log2(i+1+1) for i in range(0, len(t)+1))) #ndcg 계산 ndcg = dcg / idcg # set 으로 결과 산출할 경우, # return dcg, idcg, ndcg #dataframe으로 결과 산출할 경우, ndcg_df = pd.DataFrame(columns=[&#x27;dcg&#x27;, &#x27;idcg&#x27;, &#x27;ndcg&#x27;]) ndcg_df = ndcg_df.append(pd.DataFrame([[dcg, idcg, ndcg]], columns=[&#x27;dcg&#x27;, &#x27;idcg&#x27;, &#x27;ndcg&#x27;]), ignore_index=True) return ndcg_df# --- user1의 dcg, idcg, ndcg ndcg_df1 = get_ndcg(rec_df1)print(ndcg_df1)# --- user2의 dcg, idcg, ndcg ndcg_df2 = get_ndcg(rec_df2)print(ndcg_df2)# --- user3의 dcg, idcg, ndcg ndcg_df3 = get_ndcg(rec_df3)print(ndcg_df3)# --- user4의 dcg, idcg, ndcg ndcg_df4 = get_ndcg(rec_df4)print(ndcg_df4)# --- user5의 dcg, idcg, ndcg ndcg_df5 = get_ndcg(rec_df5)print(ndcg_df5) 12345678910111213141516# user별 dcg, idcg, ndcg 결과를 합쳐주는 함수 기능. import pandas as pddef concat_result(df1, df2): df3 = pd.concat([df1, df2]) return df3result = concat_result(ndcg_df1, ndcg_df2)result = concat_result(result, ndcg_df3)result = concat_result(result, ndcg_df4)result = concat_result(result, ndcg_df5)result.index=[&#x27;test-user1&#x27;, &#x27;test-user2&#x27;, &#x27;test-user3&#x27;, &#x27;test-user4&#x27;, &#x27;test-user5&#x27;]print(&#x27;[ ranking_2안의 유저별 dcg, idcg, ndcg ] \\n &#x27;, result)print(&#x27;-------------------\\n&#x27;)print(&#x27;[ ranking_2안의 dcg, idcg, ndcg평균 ] \\n&#x27;, result.mean()) 결론 및 향후 보완할 사항(1) ranking_2안의 전체 평균 결과는 nDCG 스코어에서 0.1 높음. ranking_1안 ndcg 평균: 0.28 ranking_2안 ndcg 평균: 0.38 (2) 유저별로 nDCG스코어가 ranking_1안이 높은 경우가 있고, ranking_2안이 높은 경우가 있으므로, 개인별 맞춤 제안을 하면 좋을 것, 특히 0이 나오는 경우도 있어서 이 경우는 신경 써서 제안을 해야 함. user2: 0.37 (ranking1_ndcg) &lt; 0.57 (ranking2_ndcg) user1: 0.52 (ranking1_ndcg) &lt; 0.00 (ranking2_ndcg) 12345678910111213141516171819202122232425262728[ ranking_1안의 유저별 dcg, idcg, ndcg ] dcg idcg ndcgtest-user1 2.061606 3.953465 0.521468test-user2 1.487137 3.953465 0.376160test-user3 1.487137 3.953465 0.376160test-user4 0.500000 3.953465 0.126471test-user5 0.000000 3.953465 0.000000-------------------[ ranking_1안의 dcg, idcg, ndcg평균 ] dcg 1.107176idcg 3.953465ndcg 0.280052[ ranking_2안의 유저별 dcg, idcg, ndcg ] dcg idcg ndcgtest-user1 0.000000 3.953465 0.000000test-user2 2.264010 3.953465 0.572665test-user3 1.500000 3.953465 0.379414test-user4 1.500000 3.953465 0.379414test-user5 2.351116 3.953465 0.594698-------------------[ ranking_2안의 dcg, idcg, ndcg평균 ] dcg 1.523025idcg 3.953465ndcg 0.385238 (3) ranking 1,2 안의 차이점: (ranking_1안) reorder(재주문 여부)로 rating을 매긴 개념으로, 재주문을 한 경우(1)라면 like, 재주문을 안 한 경우(0)라면 dislike로 상품을 구분한 feature를 랭킹모델에 추가하여 추천목록을 생성한 방법. (ranking_2안) order_number(주문회차: 몇번째 구매인지)로 rating을 매긴 개념으로, 평균 주문회차인 18 이상인 경우(1) like, 18 미만(0)이면 dislike로 상품을 구분한 feature를 랭킹모델에 추가하여 추천목록을 생성한 방법. Reference https://jalynne-kim.medium.com/추천모델-이커머스-추천모델링-딥러닝모델-프로젝트-회고-d5017cb1335f","categories":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}],"tags":[{"name":"Recommendation System","slug":"Recommendation-System","permalink":"https://jmj3047.github.io/tags/Recommendation-System/"},{"name":"Deep Nueral Networks","slug":"Deep-Nueral-Networks","permalink":"https://jmj3047.github.io/tags/Deep-Nueral-Networks/"},{"name":"Vertex AI","slug":"Vertex-AI","permalink":"https://jmj3047.github.io/tags/Vertex-AI/"}],"keywords":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}]},{"title":"Deep Nueral Networks for YouTube Recommendations","slug":"DNN_Youtube_Rec","date":"2023-03-30T15:00:00.000Z","updated":"2023-04-05T08:46:31.915Z","comments":true,"path":"2023/03/31/DNN_Youtube_Rec/","link":"","permalink":"https://jmj3047.github.io/2023/03/31/DNN_Youtube_Rec/","excerpt":"","text":"Journal&#x2F;Conference: GoogleYear(published year): 2016Author: Paul Covington, Jay Adams, Emre SarginSubject: Recommendation, Youtube Deep Nueral Networks for YouTube Recommendations GPT 요약 딥러닝이 어떻게 유튜브 추천의 성능을 향상시켰어? 딥러닝은 유튜브 추천 시스템의 성능을 향상시키는 데 큰 역할을 합니다. 이전에는 작은 문제에서 잘 작동하는 많은 추천 알고리즘이 유튜브와 같은 대규모 문제에서 실패했었습니다. 그러나 딥러닝을 사용하면 이러한 문제를 극복할 수 있습니다. 이 논문에서는 딥 후보 생성 모델과 별도의 딥 랭킹 모델을 설명하며, 이들이 어떻게 유튜브 추천 시스템의 성능을 향상시켰는지 자세히 다루고 있습니다. 이전에는 유튜브 추천 시스템에서 사용되는 전통적인 추천 알고리즘은 사용자의 이전 기록과 관련된 비슷한 동영상을 찾아서 추천하는 방식이었습니다. 그러나 이러한 방식은 대규모 데이터에서는 제대로 작동하지 않았습니다. 따라서 딥러닝을 사용하여 유튜브 추천 시스템을 개선하였습니다. 논문에서는 딥 후보 생성 모델과 별도의 딥 랭킹 모델을 소개합니다. 딥 후보 생성 모델은 많은 후보 동영상 중에서 가장 관련성이 높은 동영상을 선택하는 모델입니다. 이 모델은 다양한 신호를 사용하여 학습됩니다. 예를 들어, 동영상 제목, 설명, 태그 및 시청 기록 등이 있습니다. 딥 랭킹 모델은 후보 동영상 중에서 가장 관심 있는 동영상을 선택하는 모델입니다. 이 모델은 예상 시청 시간을 예측하는 데 사용됩니다. 이전에는 클릭 확률을 예측하는 것이 일반적이었지만, 이 논문에서는 예상 시청 시간을 예측하여 보다 정확한 추천을 제공합니다. 딥러닝을 사용하면 이전에는 해결할 수 없었던 대규모 데이터에서의 추천 문제를 해결할 수 있습니다. 따라서 딥러닝은 유튜브 추천 시스템의 성능을 크게 향상시켰습니다. feature engineering을 어떻게 했어? 저희는 보통 랭킹 모델에서 수백 개의 특징을 사용합니다. 이러한 특징은 범주형과 연속형으로 거의 반반씩 구성됩니다. 그러나 딥러닝은 특징 공학을 대체할 수 있는 잠재력이 있지만, 저희가 다루는 데이터의 본질 때문에 바로 딥러닝 모델에 입력하기에는 적합하지 않습니다. 따라서 저희는 여전히 많은 시간을 특징 공학에 할애하고 있습니다. 이를 위해 우리는 각 동영상과 관련된 정보를 수집하고, 이를 기반으로 다양한 특징을 추출합니다. 예를 들어, 동영상 제목, 설명, 태그 및 시청 기록 등이 있습니다. 이러한 특징들은 후보 생성 및 랭킹 모델에서 사용됩니다. 따라서 우리가 선택하는 특징들은 추천 시스템의 성능에 큰 영향을 미칩니다. 딥 랭킹 모델과 딥 후보 모델의 차이점이 뭐야? 딥 후보 생성 모델과 딥 랭킹 모델은 유튜브 추천 시스템에서 서로 다른 역할을 합니다. 딥 후보 생성 모델은 많은 후보 동영상 중에서 가장 관련성이 높은 동영상을 선택하는 모델입니다. 이 모델은 다양한 신호를 사용하여 학습됩니다. 예를 들어, 동영상 제목, 설명, 태그 및 시청 기록 등이 있습니다. 이러한 신호를 사용하여 후보 동영상을 생성하고, 이들 중에서 가장 관련성이 높은 동영상을 선택합니다. 반면에 딥 랭킹 모델은 후보 동영상 중에서 가장 관심 있는 동영상을 선택하는 모델입니다. 이 모델은 예상 시청 시간을 예측하는 데 사용됩니다. 이전에는 클릭 확률을 예측하는 것이 일반적이었지만, 이 논문에서는 예상 시청 시간을 예측하여 보다 정확한 추천을 제공합니다. 따라서 딥 후보 생성 모델과 딥 랭킹 모델은 서로 다른 목적과 기능을 가지고 있습니다. 이를 이용해서 어떻게 사용자에게 추천하는거야? 유튜브 추천 시스템은 크게 두 단계로 나뉩니다. 첫 번째는 딥 후보 생성 모델을 사용하여 수백만 개의 동영상 중에서 수백 개의 후보 동영상을 선택하는 것입니다. 두 번째는 딥 랭킹 모델을 사용하여 이러한 후보 동영상 중에서 가장 관심 있는 동영상을 선택하는 것입니다. 따라서 유저에게 추천하기 위해서는, 먼저 유저의 시청 기록, 검색 기록, 좋아요 및 싫어요 등과 같은 정보를 수집합니다. 그런 다음 이 정보를 사용하여 딥 후보 생성 모델을 실행하여 수백 개의 후보 동영상을 생성합니다. 그 다음, 딥 랭킹 모델을 실행하여 이러한 후보 동영상 중에서 가장 관심 있는 동영상을 선택합니다. 이 모델은 예상 시청 시간을 예측하는 데 사용됩니다. 예측된 시청 시간이 긴 동영상일수록, 유저가 해당 동영상에 대해 관심이 있을 가능성이 높습니다. 따라서 유튜브 추천 시스템은 딥 후보 생성 모델과 딥 랭킹 모델을 결합하여 유저에게 맞춤형 추천을 제공합니다. 전통적인 추천 알고리즘보다 더 성능이 나은지 어떻게 알아? 이 논문에서는 딥 후보 생성 모델과 딥 랭킹 모델을 기존의 전통적인 추천 알고리즘과 비교하여 성능을 평가하였습니다. 기존의 전통적인 추천 알고리즘은 주로 협업 필터링, 콘텐츠 기반 필터링 등이 사용되었습니다. 이러한 알고리즘들은 유저와 아이템 간의 상호작용 정보를 사용하여 추천을 수행합니다. 논문에서는 A&#x2F;B 테스트를 사용하여 딥 후보 생성 모델과 딥 랭킹 모델을 기존의 전통적인 추천 알고리즘과 비교하였습니다. 이를 위해, 일부 유저에게는 기존의 추천 알고리즘을 사용하여 동영상을 추천하고, 다른 일부 유저에게는 딥 후보 생성 모델과 딥 랭킹 모델을 사용하여 동영상을 추천하였습니다. 결과적으로, 딥 후보 생성 모델과 딥 랭킹 모델이 기존의 전통적인 추천 알고리즘보다 더 나은 성능을 보였습니다. 이러한 결과는 딥러닝이 복잡한 문제를 해결하는 데 있어서 매우 유용하다는 것을 보여주며, 유튜브 추천 시스템에서 딥러닝이 어떻게 활용될 수 있는지를 보여줍니다. 추천 알고리즘의 모델 구조에 대해서 input layer, hidden layer, output을 포함해서 자세하게 알려줘 이 논문에서는 딥 후보 생성 모델과 딥 랭킹 모델을 사용하여 유튜브 추천 시스템의 성능을 향상시켰습니다. 각 모델의 구조는 다음과 같습니다. 딥 후보 생성 모델 Input layer: 유저의 시청 기록, 검색 기록, 좋아요 및 싫어요 등과 같은 정보를 입력 받습니다. Hidden layers: 이러한 입력 정보를 처리하는 여러 개의 은닉층이 있습니다. 이 은닉층들은 각각 다양한 신호를 사용하여 후보 동영상을 생성합니다. Output layer: 후보 동영상 중에서 가장 관련성이 높은 동영상을 선택합니다. 딥 랭킹 모델 Input layer: 후보 동영상의 정보를 입력 받습니다. Hidden layers: 이러한 입력 정보를 처리하는 여러 개의 은닉층이 있습니다. 이 은닉층들은 예상 시청 시간을 예측하는 데 사용됩니다. Output layer: 예상 시청 시간이 가장 긴 동영상을 선택합니다. 두 모델 모두 다양한 신호를 사용하여 복잡한 문제를 해결하기 위해 여러 개의 은닉층을 가지고 있으며, 딥러닝의 특징인 역전파 알고리즘을 사용하여 학습됩니다. 이러한 모델 구조는 유튜브 추천 시스템에서 매우 효과적이며, 대규모 데이터에서 복잡한 문제를 해결하는 데 있어서 딥러닝이 매우 유용하다는 것을 보여줍니다. 전체 논문 내용 개요랑 제목 포함해서 요약해줘 이 논문의 제목은 “Deep Neural Networks for YouTube Recommendations”입니다. 이 논문에서는 유튜브 추천 시스템에 대한 딥러닝 기반의 접근 방식을 제안하고, 이를 통해 성능을 크게 향상시켰습니다. 유튜브는 세계에서 가장 큰 비디오 공유 플랫폼 중 하나이며, 추천 시스템은 수억 명의 사용자가 매일 사용하는 핵심 기능 중 하나입니다. 이 논문에서는 유튜브 추천 시스템이 어떻게 구성되어 있는지에 대한 개요를 제공하며, 딥 후보 생성 모델과 딥 랭킹 모델이 어떻게 작동하는지에 대해 자세히 설명합니다. 딥 후보 생성 모델은 유저의 시청 기록, 검색 기록, 좋아요 및 싫어요 등과 같은 정보를 입력으로 받아서 수백만 개의 동영상 중에서 수백 개의 후보 동영상을 생성합니다. 딥 랭킹 모델은 이러한 후보 동영상 중에서 가장 관심 있는 동영상을 선택하는데 사용됩니다. 이 모델은 예상 시청 시간을 예측하는 데 사용되며, 예측된 시청 시간이 긴 동영상일수록 유저가 해당 동영상에 대해 관심이 있을 가능성이 높습니다. 논문에서는 A&#x2F;B 테스트를 사용하여 딥 후보 생성 모델과 딥 랭킹 모델이 기존의 전통적인 추천 알고리즘보다 더 나은 성능을 보인다는 것을 실험적으로 입증하였습니다. 이러한 결과는 딥러닝이 복잡한 문제를 해결하는 데 있어서 매우 유용하다는 것을 보여주며, 유튜브 추천 시스템에서 딥러닝이 어떻게 활용될 수 있는지를 보여줍니다. 4.ranking 부분을 feature과 modeling부분을 포함해서 요약해줘 Ranking is a crucial step in the YouTube recommendation system, which uses impression data to specialize and calibrate candidate predictions for the particular user interface. During ranking, many more features describing the video and the user’s relationship to the video are accessed, because only a few hundred videos are being scored rather than the millions scored in candidate generation. The ranking model is separate from the candidate generation model and uses deep neural networks to predict expected watch time for each candidate video. The model has several hidden layers that process input information and use backpropagation algorithm to learn from large-scale data. The output layer selects the video with the highest predicted watch time as the recommended video. 순위는 노출 데이터를 사용하여 특정 사용자 인터페이스에 대한 후보 예측을 전문화하고 보정하는 YouTube 추천 시스템에서 중요한 단계입니다. 순위를 매기는 동안에는 후보 생성에서 점수를 매기는 수백만 개의 동영상이 아닌 수백 개의 동영상만 점수를 매기기 때문에 동영상과 사용자와 동영상과의 관계를 설명하는 더 많은 기능에 액세스합니다. 순위 모델은 후보 생성 모델과는 별개의 모델이며 심층 신경망을 사용하여 각 후보 동영상의 예상 시청 시간을 예측합니다. 이 모델에는 입력 정보를 처리하고 대규모 데이터에서 학습하기 위해 역전파 알고리즘을 사용하는 여러 개의 숨겨진 레이어가 있습니다. 출력 레이어는 예상 시청 시간이 가장 높은 동영상을 추천 동영상으로 선택합니다. 그래서 결론이 뭐야? The paper proposes a deep neural network architecture for recommending YouTube videos, which is split into two distinct problems: candidate generation and ranking. The deep candidate generation model generates hundreds of relevant videos for the user based on their history and context, while the deep ranking model selects the most relevant video from these candidates using impression data and many more features. The paper shows that the proposed approach outperforms traditional recommendation algorithms in A&#x2F;B testing. 이 논문에서는 후보 생성 및 순위 지정이라는 두 가지 문제로 나뉘어져 있는 YouTube 동영상 추천을 위한 심층 신경망 아키텍처를 제안합니다. 딥 후보 생성 모델은 사용자의 기록과 컨텍스트를 기반으로 수백 개의 관련 동영상을 생성하고, 딥 랭킹 모델은 노출 데이터와 더 많은 기능을 사용하여 이러한 후보 중에서 가장 관련성이 높은 동영상을 선택합니다. 이 논문은 제안된 접근 방식이 A&#x2F;B 테스트에서 기존 추천 알고리즘보다 우수한 성능을 보인다는 것을 보여줍니다. Summary 딥러닝을 사용하여 성능 향상을 한 Youtube의 추천 시스템을 소개한다. deep candidate generation model과 deep ranking model을 설명 YouTube represents one of the largest scale and most sophisticated industrial recommendation systems in existence. In this paper, we describe the system at a high level and focus on the dramatic performance improvements brought by deep learning. The paper is split according to the classic two-stage information retrieval dichotomy: first, we detail a deep candidate generation model and then describe a separate deep ranking model. We also provide practical lessons and insights derived from designing, iterating and maintaining a massive recommendation system with enormous userfacing impact. YouTube는 현존하는 가장 큰 규모와 정교한 산업 추천 시스템 중 하나입니다. 이 paper에서는 이 시스템을 높은 수준에서 설명하고 딥러닝이 가져온 극적인 성능 향상에 초점을 맞춥니다. 이 paper는 기존의 2단계 정보 검색 이분법에 따라 먼저 deep candidate generation model을 자세히 설명한 다음 별도의 deep ranking model을 설명합니다. 또한 사용자에게 막대한 영향을 미치는 대규모 추천 시스템을 설계, 반복 및 유지 관리하면서 얻은 실질적인 교훈과 인사이트를 제공합니다. INTRODUCTIONYouTube is the world’s largest platform for creating, sharing and discovering video content. YouTube recommendations are responsible for helping more than a billion users discover personalized content from an ever-growing corpus of videos. In this paper we will focus on the immense impact deep learning has recently had on the YouTube video recommendations system. Figure 1 illustrates the recommendations on the YouTube mobile app home. Recommending YouTube videos is extremely challenging from three major perspectives: YouTube는 동영상 콘텐츠를 제작, 공유 및 검색할 수 있는 세계 최대의 플랫폼입니다. YouTube 추천은 10억 명 이상의 사용자가 계속 증가하는 동영상 모음에서 개인화된 콘텐츠를 발견할 수 있도록 지원합니다. 이 paper에서는 최근 딥러닝이 YouTube 동영상 추천 시스템에 미친 막대한 영향에 초점을 맞출 것입니다. 그림 1은 YouTube 모바일 앱 홈의 추천 시스템을 보여줍니다. YouTube 동영상 추천은 크게 세 가지 관점에서 매우 까다로운 작업입니다: Scale: Many existing recommendation algorithms proven to work well on small problems fail to operate on our scale. Highly specialized distributed learning algorithms and efficient serving systems are essential for handling YouTube’s massive user base and corpus. 규모: 작은 문제에서는 잘 작동하는 것으로 입증된 기존의 많은 추천 알고리즘은 YouTube의 규모에서는 작동하지 않습니다. YouTube의 방대한 사용자 기반과 말뭉치를 처리하려면 고도로 전문화된 분산 학습 알고리즘과 효율적인 서빙 시스템이 필수적입니다. Freshness: YouTube has a very dynamic corpus with many hours of video are uploaded per second. The recommendation system should be responsive enough to model newly uploaded content as well as the latest actions taken by the user. Balancing new content with well established videos can be understood from an exploration&#x2F;exploitation perspective.(강화학습) 신선도: YouTube는 초당 수 시간 분량의 동영상이 업로드되는 매우 역동적인 코퍼스를 보유하고 있습니다. 추천 시스템은 새로 업로드된 콘텐츠와 사용자가 최근에 수행한 작업을 모델링할 수 있을 만큼 반응성이 뛰어나야 합니다. 새로운 콘텐츠와 잘 알려진 동영상의 균형을 맞추는 것은 탐색&#x2F;활용 관점에서 이해할 수 있습니다. Noise: Historical user behavior on YouTube is inherently difficult to predict due to sparsity and a variety of unobservable external factors. We rarely obtain the ground truth of user satisfaction and instead model noisy implicit feedback signals. Furthermore, metadata associated with content is poorly structured without a well de ned ontology. Our algorithms need to be robust to these particular characteristics of our training data. 노이즈: YouTube의 과거 사용자 행동은 희소성과 관찰할 수 없는 다양한 외부 요인으로 인해 본질적으로 예측하기 어렵습니다. 사용자 만족도에 대한 실측 데이터를 거의 확보하지 못하고 대신 노이즈가 많은 암시적 피드백 신호를 모델링합니다. 또한 콘텐츠와 관련된 메타데이터는 잘 정의된 온톨로지 없이는 제대로 구조화되지 않습니다. 학습 데이터의 이러한 특정 특성에 맞게 알고리즘을 강력하게 설계해야 합니다. In conjugation with other product areas across Google, YouTube has undergone a fundamental paradigm shift towards using deep learning as a general-purpose solution for nearly all learning problems. Our system is built on Google Brain [4] which was recently open sourced as TensorFlow [1]. TensorFlow provides a flexible framework for experimenting with various deep neural network architectures using large scale distributed training. Our models learn approximately one billion parameters and are trained on hundreds of billions of examples. YouTube는 Google의 다른 제품 영역과 함께 거의 모든 학습 문제에 대한 범용 솔루션으로 딥 러닝을 사용하는 방향으로 근본적인 패러다임의 변화를 겪었습니다. YouTube의 시스템은 최근 TensorFlow[1]로 오픈 소스화된 Google Brain[4]을 기반으로 구축되었습니다. 텐서플로는 대규모 분산 학습을 사용하여 다양한 심층 신경망 아키텍처를 실험할 수 있는 유연한 프레임워크를 제공합니다. 유니티의 모델은 약 10억 개의 매개변수를 학습하고 수천억 개의 예제를 통해 훈련됩니다. In contrast to vast amount of research in matrix factorization methods [19], there is relatively little work using deep neural networks for recommendation systems. Neural networks are used for recommending news in [17], citations in [8] and review ratings in [20]. Collaborative filtering is formulated as a deep neural network in [22] and autoencoders in [18]. Elkahky et al. used deep learning for cross domain user modeling [5]. In a content-based setting, Burges et al. used deep neural networks for music recommendation [21]. 행렬 인수분해 방법[19]에 대한 방대한 양의 연구와 달리, 추천 시스템에 심층 신경망을 사용한 연구는 상대적으로 적습니다. 신경망은 [17]에서 뉴스 추천, [8]에서 인용, [20]에서 리뷰 평가에 사용되었습니다. 협업 필터링은 [22]에서 심층 신경망으로, [18]에서 자동 인코더로 공식화되었습니다. Elkahky 등은 크로스 도메인 사용자 모델링에 딥러닝을 사용했습니다[5]. 콘텐츠 기반 환경에서 Burges 등은 음악 추천에 심층 신경망을 사용했습니다[21]. The paper is organized as follows: A brief system overview is presented in Section 2. Section 3 describes the candidate generation model in more detail, including how it is trained and used to serve recommendations. Experimental results will show how the model benefits from deep layers of hidden units and additional heterogeneous signals. Section 4 details the ranking model, including how classic logistic regression is modified to train a model predicting expected watch time (rather than click probability). Experimental results will show that hidden layer depth is helpful as well in this situation. Finally, Section 5 presents our conclusions and lessons learned. paper는 다음과 같이 구성되어 있습니다: 섹션 2에서는 간략한 시스템 개요를 소개합니다. 섹션 3에서는 후보 생성 모델이 어떻게 학습되고 추천을 제공하는 데 사용되는지 등 후보 생성 모델에 대해 자세히 설명합니다. 실험 결과를 통해 모델이 심층 계층의 숨겨진 유닛과 추가적인 이질적인 신호를 통해 어떤 이점을 얻을 수 있는지 보여줍니다. 섹션 4에서는 클릭 확률이 아닌 예상 시청 시간을 예측하는 모델을 훈련하기 위해 고전적인 로지스틱 회귀를 수정하는 방법을 포함하여 순위 모델을 자세히 설명합니다. 실험 결과를 통해 숨겨진 레이어 깊이가 이러한 상황에서도 유용하다는 것을 보여줍니다. 마지막으로 섹션 5에서는 결론과 교훈을 제시합니다. SYSTEM OVERVIEWThe overall structure of our recommendation system is illustrated in Figure 2. The system is comprised of two neural networks: one for candidate generation and one for ranking. The candidate generation network takes events from the user’s YouTube activity history as input and retrieves a small subset (hundreds) of videos from a large corpus. These candidates are intended to be generally relevant to the user with high precision. The candidate generation network only provides broad personalization via collaborative filtering. The similarity between users is expressed in terms of coarse features such as IDs of video watches, search query tokensand demographics. 추천 시스템의 전체 구조는 그림 2에 나와 있습니다. 이 시스템은 후보 생성용 네트워크와 랭킹용 네트워크의 두 가지 신경망으로 구성됩니다. 후보 생성 네트워크는 사용자의 YouTube 활동 기록에서 이벤트를 입력으로 받아 대규모 말뭉치에서 작은 하위 집합(수백 개)의 동영상을 검색합니다. 이러한 후보 동영상은 일반적으로 사용자와 관련성이 높고 정밀도가 높아야 합니다. 후보 생성 네트워크만 협업 필터링을 통해 광범위한 개인화를 제공합니다. 사용자 간의 유사성은 동영상 시청의 ID, 검색 쿼리 토큰 및 인구 통계와 같은 거친 기능으로 표현됩니다. Presenting a few “best” recommendations in a list requires a fine-level representation to distinguish relative importance among candidates with high recall. The ranking network accomplishes this task by assigning a score to each video according to a desired objective function using a rich set of features describing the video and user. The highest scoring videos are presented to the user, ranked by their score. 목록에 몇 가지 ‘최고의’ 추천을 제시하려면 recall률이 높은 후보들 간의 상대적 중요성을 구분할 수 있는 세밀한 수준의 표현이 필요합니다. 랭킹 네트워크는 동영상과 사용자를 설명하는 풍부한 기능 세트를 사용하여 원하는 목적 함수에 따라 각 동영상에 점수를 할당함으로써 이 작업을 수행합니다. 가장 높은 점수를 받은 동영상이 점수에 따라 순위에 따라 사용자에게 표시됩니다. The two-stage approach to recommendation allows us to make recommendations from a very large corpus (millions) of videos while still being certain that the small number of videos appearing on the device are personalized and engaging for the user. Furthermore, this design enables blending candidates generated by other sources, such as those described in an earlier work [3]. 추천에 대한 2단계 접근 방식을 통해 수백만 개에 달하는 대규모 동영상 코퍼스에서 추천을 생성하는 동시에 디바이스에 표시되는 소수의 동영상이 사용자에게 맞춤화되고 흥미를 끌 수 있도록 할 수 있습니다. 또한 이 설계는 이전 연구[3]에서 설명한 것과 같이 다른 소스에서 생성된 후보를 블렌딩할 수 있게 해줍니다. During development, we make extensive use of o ine metrics (precision, recall, ranking loss, etc.) to guide iterative improvements to our system. However for the final determination of the effectiveness of an algorithm or model, we rely on A&#x2F;B testing via live experiments. In a live experiment, we can measure subtle changes in click-through rate, watch time, and many other metrics that measure user engagement. This is important because live A&#x2F;B results arenot always correlated with offline experiments. 개발 과정에서 정확도, 리콜, 순위 손실 등 다양한 지표를 광범위하게 활용하여 시스템의 반복적인 개선을 유도합니다. 그러나 알고리즘이나 모델의 효과를 최종적으로 결정하기 위해서는 라이브 실험을 통한 A&#x2F;B 테스트에 의존합니다. 실시간 실험에서는 클릭률, 시청 시간 및 사용자 참여를 측정하는 기타 여러 지표의 미묘한 변화를 측정할 수 있습니다. 실시간 A&#x2F;B 결과가 오프라인 실험과 항상 상관관계가 있는 것은 아니기 때문에 이 점이 중요합니다. CANDIDATE GENERATIONDuring candidate generation, the enormous YouTube corpus is winnowed down to hundreds of videos that may be relevant to the user. The predecessor to the recommender described here was a matrix factorization approach trained under rank loss [23]. Early iterations of our neural network model mimicked this factorization behavior with shallow networks that only embedded the user’s previous watches. From this perspective, our approach can be viewed as a nonlineargeneralization of factorization techniques. 후보를 생성하는 동안 방대한 YouTube 말뭉치가 사용자와 관련이 있을 수 있는 수백 개의 동영상으로 좁혀집니다. 여기에 설명된 추천의 전신은 순위 손실 하에서 훈련된 행렬 인수분해 접근 방식이었습니다[23]. 신경망 모델의 초기 반복은 사용자의 이전 시청만 포함하는 얕은 네트워크를 사용하여 이러한 인수 분해 동작을 모방했습니다. 이러한 관점에서 볼 때, 우리의 접근 방식은 인수분해 기법을 비선형적으로 일반화라고 볼 수 있습니다. Recommendation as ClassificationWe pose recommendation as extreme multiclass classification where the prediction problem becomes accurately classifying a specific video watch wt at time t among millions of videos i (classes) from a corpus V based on a user U and context C, 추천을 극단적인 다중 클래스 분류로 설정하면, 예측 문제는 사용자 U와 컨텍스트 C를 기반으로 말뭉치 V의 수백만 개의 동영상 i(클래스) 중에서 특정 동영상 시청 횟수 t를 정확하게 분류하는 것입니다, where u 2 Rn represents a high-dimensional \\embedding” of the user, context pair and the vj 2 RN represent embeddings of each candidate video. In this setting, an embedding is simply a mapping of sparse entities (individual videos, users etc.) into a dense vector in RN. The task of the deep neural network is to learn user embeddings u as a function of the user’s history and context that are useful for discriminating among videos with a softmax classifier. 여기서 u 2 Rn은 사용자, 컨텍스트 pair 및 vj 2 RN의 고차원 ‘임베딩’을 나타냅니다. 이 설정에서 임베딩은 단순히 스파스 엔티티(개별 동영상, 사용자 등)를 RN의 밀도가 높은 벡터에 매핑하는 것입니다. 심층 신경망의 임무는 소프트맥스 분류기를 사용하여 동영상을 구분하는 데 유용한 사용자 히스토리 및 컨텍스트의 함수로서 사용자 임베딩을 학습하는 것입니다. Although explicit feedback mechanisms exist on YouTube (thumbs up&#x2F;down, in-product surveys, etc.) we use the implicit feedback [16] of watches to train the model, where a user completing a video is a positive example. This choice is based on the orders of magnitude more implicit user history available, allowing us to produce recommendations deep in the tail where explicit feedback is extremely sparse. YouTube에는 명시적인 피드백 메커니즘(좋아요&#x2F;싫어요, 제품 내 설문조사 등)이 존재하지만, 저희는 사용자가 동영상을 완료하는 것이 긍정적인 예인 시청의 암묵적 피드백[16]을 사용하여 모델을 훈련합니다. 이러한 선택은 사용 가능한 암시적 사용자 기록이 훨씬 더 많기 때문에 명시적 피드백이 극히 드문 꼬리 부분에 대한 추천을 생성할 수 있다는 점을 기반으로 합니다. Efficient Extreme MulticlassTo efficiently train such a model with millions of classes, we rely on a technique to sample negative classes from the background distribution (”candidate sampling”) and then correct for this sampling via importance weighting [10]. For each example the cross-entropy loss is minimized for the true label and the sampled negative classes. In practice several thousand negatives are sampled, corresponding to more than 100 times speedup over traditional softmax. A popular alternative approach is hierarchical softmax [15], but we weren’t able to achieve comparable accuracy. In hierarchical softmax, traversing each node in the tree involves discriminating between sets of classes that are often unrelated, making the classification problem much more difficult and degrading performance. 수백만 개의 클래스로 이러한 모델을 효율적으로 훈련하기 위해 배경 분포에서 음수 클래스를 샘플링하는 기법(‘후보 샘플링’)을 사용하고 이 샘플링에 중요도 가중치를 부여하여 중요도 가중치[10]를 통해 이 샘플링을 보정합니다. 각 예제에서 교차 엔트로피 손실은 실제 레이블과 샘플링된 음수 클래스에 대해 최소화됩니다. 실제로는 수천 개의 개의 네거티브가 샘플링되며, 이는 기존 소프트맥스보다 100배 이상의 속도 향상에 해당합니다. 널리 사용되는 대안적 접근 방식은 계층적 소프트맥스[15]이지만, 비슷한 정확도를 달성할 수 없었습니다. 계층적 소프트맥스에서는 트리의 각 노드를 탐색할 때 종종 서로 관련이 없는 클래스 집합을 구별해야 하므로 분류 문제가 훨씬 더 어려워지고 성능이 저하됩니다. At serving time we need to compute the most likely N classes (videos) in order to choose the top N to present to the user. Scoring millions of items under a strict serving latency of tens of milliseconds requires an approximate scoring scheme sublinear in the number of classes. Previous systems at YouTube relied on hashing [24] and the classifier described here uses a similar approach. Since calibrated likelihoods from the softmax output layer are not needed at serving time, the scoring problem reduces to a nearest neighbor search in the dot product space for which general purpose libraries can be used [12]. We found that A&#x2F;B results were not particularly sensitive to the choice of nearest neighbor search algorithm. 서빙 시간에 사용자에게 표시할 상위 N개를 선택하려면 가장 가능성이 높은 N개의 클래스(동영상)를 계산해야 합니다. 수십 밀리초의 엄격한 서빙 지연 시간 아래에서 수백만 개의 항목에 점수를 매기려면 클래스 수에 따라 대략적인 점수 체계가 필요합니다. YouTube의 이전 시스템은 해싱에 의존했으며[24], 여기에 설명된 분류기는 유사한 접근 방식을 사용합니다. 소프트맥스 출력 레이어의 보정된 가능성은 서빙 시점에 필요하지 않으므로 점수 문제는 범용 라이브러리를 사용할 수 있는 도트 프로덕트 공간에서 가장 가까운 이웃 검색으로 축소됩니다[12]. A&#x2F;B 결과는 최인접 이웃 검색 알고리즘의 선택에 특별히 민감하지 않다는 것을 발견했습니다. Model ArchitectureInspired by continuous bag of words language models [14], we learn high dimensional embeddings for each video in a fixed vocabulary and feed these embeddings into a feedforward neural network. A user’s watch history is represented by a variable-length sequence of sparse video IDs which is mapped to a dense vector representation via the embeddings. The network requires fixed-sized dense inputs and simply averaging the embeddings performed best among several strategies (sum, component-wise max, etc.). Importantly, the embeddings are learned jointly with all other model parameters through normal gradient descent back-propagation updates. Features are concatenated into a wide first layer, followed by several layers of fully connected Rectified Linear Units (ReLU) [6]. Figure 3 shows the general network architecture with additional non-video watch features described below. 연속 단어 가방 언어 모델[14]에서 영감을 받아 각 동영상에 대한 고차원 임베딩을 고정 어휘로 학습하고 이러한 임베딩을 피드포워드 신경망에 공급합니다. 사용자의 시청 기록은 임베딩을 통해 고밀도 벡터 표현에 매핑되는 가변 길이의 스파스 비디오 ID 시퀀스로 표현됩니다. 네트워크에는 고정 크기의 고밀도 입력이 필요하며 여러 전략(합계, 구성 요소별 최대값 등) 중에서 가장 성능이 좋은 임베딩의 평균을 구합니다. 중요한 것은 임베딩이 일반 경사 하강 역전파 업데이트를 통해 다른 모든 모델 파라미터와 함께 학습된다는 점입니다. 특징은 넓은 첫 번째 레이어로 연결되고, 그 다음에는 완전히 연결된 여러 레이어의 정류 선형 단위(ReLU)[6]로 연결됩니다. 그림 3은 아래에 설명된 non-video 감시 기능이 추가된 일반적인 네트워크 아키텍처를 보여줍니다. Heterogeneous SignalsA key advantage of using deep neural networks as a generalization of matrix factorization is that arbitrary continuous and categorical features can be easily added to the model. Search history is treated similarly to watch history - each query is tokenized into unigrams and bigrams and each token is embedded. Once averaged, the user’s tokenized, embedded queries represent a summarized dense search history. Demographic features are important for providing priors so that the recommendations behave reasonably for new users. The user’s geographic region and device are embedded and concatenated. Simple binary and continuous features such as the user’s gender, logged-in state and age are input directly into the network as real values normalized to [0,1]. 행렬 인수분해의 일반화로 심층 신경망을 사용할 때의 주요 장점은 임의의 연속 및 범주형 특징을 모델에 쉽게 추가할 수 있다는 것입니다. 검색 기록은 시청 기록과 유사하게 취급되며, 각 쿼리는 유니그램과 빅그램으로 토큰화되고 각 토큰은 임베드됩니다. 평균을 내면 사용자의 토큰화된 임베디드 쿼리는 조밀하게 요약된 검색 기록을 나타냅니다. 인구통계학적 특징은 추천이 신규 사용자에게 합리적으로 작동하도록 우선순위를 제공하는 데 중요합니다. 사용자의 지리적 지역과 디바이스가 임베드되어 연결됩니다. 사용자의 성별, 로그인 상태, 나이와 같은 간단한 이진 및 연속형 특징은 [0,1]로 정규화된 실제 값으로 네트워크에 직접 입력됩니다. “Example Age” FeatureMany hours worth of videos are uploaded each second to YouTube. Recommending this recently uploaded (”fresh”) content is extremely important for YouTube as a product. We consistently observe that users prefer fresh content, though not at the expense of relevance. In addition to the first-order effect of simply recommending new videos that users want to watch, there is a critical secondary phenomenon of bootstrapping and propagating viral content [11]. YouTube에는 매초 몇 시간 분량의 동영상이 업로드됩니다. 최근에 업로드된(‘새로운’) 콘텐츠를 추천하는 것은 제품으로서 YouTube에 매우 중요합니다. 사용자들은 관련성을 희생하더라도 새로운 콘텐츠를 선호한다는 사실을 지속적으로 관찰하고 있습니다. 단순히 사용자가 보고 싶어하는 새로운 동영상을 추천하는 1차 효과 외에도, 바이럴 콘텐츠를 부트스트랩하고 전파하는 중요한 2차 현상이 있습니다[11]. Machine learning systems often exhibit an implicit bias towards the past because they are trained to predict future behavior from historical examples. The distribution of video popularity is highly non-stationary but the multinomial distribution over the corpus produced by our recommender will reflect the average watch likelihood in the training window of several weeks. To correct for this, we feed the age of the training example as a feature during training. At serving time, this feature is set to zero (or slightly negative) to reflect that the model is making predictions at the very end of the training window. 머신 러닝 시스템은 과거의 사례를 통해 미래의 행동을 예측하도록 학습되기 때문에 과거에 대한 암묵적인 편향성을 보이는 경우가 많습니다. 동영상 인기도 분포는 매우 비고정적이지만, 추천자가 생성한 말뭉치에 대한 다항식 분포는 몇 주 동안의 훈련 기간 동안의 평균 시청 가능성을 반영합니다. 이를 보정하기 위해 훈련 중에 훈련 예제의 연령을 피처로 제공합니다. 서빙 시간에는 이 피처가 0(또는 약간 음수)으로 설정되어 모델이 훈련 기간의 맨 마지막에 예측을 하고 있음을 반영합니다. Figure 4 demonstrates the efficacy of this approach on an arbitrarily chosen video [26]. Label and Context SelectionIt is important to emphasize that recommendation often involves solving a surrogate problem and transferring the result to a particular context. A classic example is the assumption that accurately predicting ratings leads to effective movie recommendations [2]. We have found that the choice of this surrogate learning problem has an outsized importance on performance in A&#x2F;B testing but is very difficult to measure with offline experiments. 추천에는 종종 대체 모델 문제를 해결하고 그 결과를 특정 컨텍스트로 옮기는 작업이 포함된다는 점을 강조하는 것이 중요합니다. 대표적인 예로 평점을 정확하게 예측하면 효과적인 영화 추천이 가능하다는 가정을 들 수 있습니다[2]. 우리는 이 대리 학습 문제의 선택이 A&#x2F;B 테스트의 성능에 매우 중요하지만 오프라인 실험으로 측정하기는 매우 어렵다는 것을 발견했습니다. Training examples are generated from all YouTube watches (even those embedded on other sites) rather than just watches on the recommendations we produce. Otherwise, it would be very difficult for new content to surface and the recommender would be overly biased towards exploitation. If users are discovering videos through means other than our recommendations, we want to be able to quickly propagate this discovery to others via collaborative filtering. Another key insight that improved live metrics was to generate a fixed number of training examples per user, effectively weighting our users equally in the loss function. This prevented a small cohort of highly active users from dominating the loss. Training examples는 추천 동영상뿐만 아니라 모든 YouTube 동영상(다른 사이트에 임베드된 동영상도 포함)에서 생성됩니다. 그렇지 않으면 새로운 콘텐츠가 노출되기가 매우 어렵고 추천이 지나치게 악용에 편향될 수 있습니다. 사용자가 추천 이외의 경로를 통해 동영상을 발견하는 경우, 공동 필터링을 통해 이러한 발견을 다른 사용자에게 신속하게 전파할 수 있기를 바랍니다. 실시간 지표를 개선한 또 다른 주요 인사이트는 사용자당 고정된 사용자당 훈련 예제 수를 고정하여 손실 함수에서 모든 사용자에게 동일한 가중치를 부여하는 것입니다. 이를 통해 활동성이 높은 소수의 사용자 집단이 손실을 지배하는 것을 방지할 수 있었습니다. Somewhat counter-intuitively, great care must be taken to withhold information from the classifier in order to prevent the model from exploiting the structure of the site and overfitting the surrogate problem. Consider as an example a case in which the user has just issued a search query for “taylor swift”. Since our problem is posed as predicting the next watched video, a classifier given this information will predict that the most likely videos to be watched are those which appear on the corresponding search results page for “taylor swift”. Unsurpisingly, reproducing the user’s last search page as homepage recommendations performs very poorly. By discarding sequence information and representing search queries with an unordered bag of tokens, the classifier is no longer directly aware of the origin of the label. 다소 직관적이지 않을 수 있지만, 모델이 사이트의 구조를 악용하여 대리 문제에 과도하게 적합하지 않도록 분류기에서 정보를 보류하는 데 세심한 주의를 기울여야 합니다. 사용자가 방금 ‘테일러 스위프트’에 대한 검색 쿼리를 실행한 경우를 예로 들어 보겠습니다. 우리의 문제는 다음에 시청할 동영상을 예측하는 것이므로, 이 정보가 주어진 분류기는 ‘테일러 스위프트’에 대한 해당 검색 결과 페이지에 표시되는 동영상이 시청될 가능성이 가장 높은 동영상이라고 예측할 것입니다. 당연히 사용자가 마지막으로 검색한 페이지를 홈페이지 추천 페이지로 재생산하는 것은 매우 저조한 성능을 보입니다. 시퀀스 정보를 버리고 정렬되지 않은 토큰 백으로 검색 쿼리를 표현하면 분류기는 더 이상 레이블의 출처를 직접 인식하지 못합니다. Natural consumption patterns of videos typically lead to very asymmetric co-watch probabilities. Episodic series are usually watched sequentially and users often discover artists in a genre beginning with the most broadly popular before focusing on smaller niches. We therefore found much better performance predicting the user’s next watch, rather than predicting a randomly held-out watch (Figure 5). Many collaborative filtering systems implicitly choose the labels and context by holding out a random item and predicting it from other items in the user’s history (5a). This leaks future information and ignores any asymmetric consumption patterns. In contrast, we “rollback” a user’s history by choosing a random watch and only input actions the user took before the held-out label watch (5b). 동영상의 자연스러운 소비 패턴은 일반적으로 매우 비대칭적인 공동 시청 확률로 이어집니다. 에피소드 시리즈는 일반적으로 순차적으로 시청되며, 사용자는 가장 널리 알려진 장르부터 시작하여 작은 틈새 장르에 집중하면서 해당 장르의 아티스트를 발견하는 경우가 많습니다. 따라서 무작위로 보류된 시청을 예측하는 것보다 사용자의 다음 시청을 예측하는 것이 훨씬 더 나은 성능을 보였습니다(그림 5). 많은 협업 필터링 시스템은 암묵적으로 레이블과 컨텍스트를 선택합니다. 레이블과 컨텍스트를 암시적으로 선택합니다(그림 5a). 이는 미래 정보를 유출하고 비대칭적인 소비 패턴을 무시합니다. 이와는 대조적으로, 무작위 시계를 선택하여 사용자의 기록을 ‘롤백’하고 사용자가 보류된 레이블 시계 이전에 수행한 작업만 입력합니다(5b). Experiments with Features and DepthAdding features and depth significantly improves precision on holdout data as shown in Figure 6. In these experiments, a vocabulary of 1M videos and 1M search tokens were embedded with 256 floats each in a maximum bag size of 50 recent watches and 50 recent searches. The softmax layer outputs a multinomial distribution over the same 1M video classes with a dimension of 256 (which can be thought of as a separate output video embedding). These models were trained until convergence over all YouTube users, corresponding to several epochs over the data. Network structure followed a common “tower” pattern in which the bottom of the network is widest and each successive hidden layer halves the number of units (similar to Figure 3). The depth zero network is effectively a linear factorization scheme which performed very similarly to the predecessor system. Width and depth were added until the incremental benefit diminished and convergence became difficult: 그림 6에서 볼 수 있듯이 특징과 깊이를 추가하면 홀드아웃 데이터의 정밀도가 크게 향상됩니다. 이 실험에서는 1백만 개의 동영상과 1백만 개의 검색 토큰으로 구성된 어휘를 최근 시청 50개와 최근 검색 50개의 최대 가방 크기에 각각 256개의 플로트로 임베드했습니다. 소프트맥스 레이어는 동일한 1M 동영상 클래스에 대해 256 차원으로 다항식 분포를 출력합니다(별도의 출력 동영상 임베딩으로 생각할 수 있음). 이러한 모델은 데이터의 여러 시대에 해당하는 모든 YouTube 사용자에 대해 수렴할 때까지 학습되었습니다. 네트워크 구조는 네트워크의 맨 아래가 가장 넓고 연속되는 각 숨겨진 레이어가 유닛 수를 절반으로 줄이는 일반적인 “타워” 패턴을 따랐습니다(그림 3과 유사). 깊이 제로 네트워크는 사실상 선형 인수분해 방식이며, 이전 시스템과 매우 유사하게 작동했습니다. 점진적 이득이 감소하고 수렴이 어려워질 때까지 폭과 깊이가 추가되었습니다: Depth 0: A linear layer simply transforms the concatenation layer to match the softmax dimension of 256 Depth 1: 256 ReLU Depth 2: 512 ReLU → 256 ReLU Depth 3: 1024 ReLU → 512 ReLU → 256 ReLU Depth 4: 2048 ReLU → 1024 ReLU → 512 ReLU → 256 ReLU RankingThe primary role of ranking is to use impression data to specialize and calibrate candidate predictions for the particular user interface. For example, a user may watch a given video with high probability generally but is unlikely to click on the specific homepage impression due to the choice of thumbnail image. During ranking, we have access to many more features describing the video and the user’s relationship to the video because only a few hundred videos are being scored rather than the millions scored in candidate generation. Ranking is also crucial for ensembling different candidate sources whose scores are not directly comparable. 랭킹의 주요 역할은 노출 데이터를 사용하여 특정 사용자 인터페이스에 대한 후보 예측을 전문화하고 보정하는 것입니다. 예를 들어, 사용자가 특정 동영상을 일반적으로 높은 확률로 시청하지만 썸네일 이미지 선택으로 인해 특정 홈페이지 노출을 클릭할 가능성은 낮을 수 있습니다. 순위를 매기는 동안에는 후보 생성에서 수백만 개의 동영상이 점수화되는 대신 수백 개의 동영상만 점수화되기 때문에 동영상과 사용자와의 관계를 설명하는 더 많은 기능에 액세스할 수 있습니다. 순위는 점수가 직접적으로 비교할 수 없는 다양한 후보 소스를 조합하는 데에도 중요합니다. We use a deep neural network with similar architecture as candidate generation to assign an independent score to each video impression using logistic regression (Figure 7). The list of videos is then sorted by this score and returned to the user. Our final ranking objective is constantly being tuned based on live A&#x2F;B testing results but is generally a simple function of expected watch time per impression. Ranking by click-through rate often promotes deceptive videos that the user does not complete (\\clickbait”) whereas watch time better captures engagement [13, 25]. 후보 생성과 유사한 아키텍처를 가진 심층 신경망을 사용하여 로지스틱 회귀를 통해 각 동영상 노출에 독립적인 점수를 부여합니다(그림 7). 그런 다음 동영상 목록은 이 점수에 따라 정렬되어 사용자에게 반환됩니다. 최종 순위 목표는 실시간 A&#x2F;B 테스트 결과를 기반으로 지속적으로 조정되고 있지만, 일반적으로 노출당 예상 시청 시간이라는 간단한 함수를 사용합니다. 클릭률로 순위를 매기면 사용자가 완료하지 않는 기만적인 동영상(”클릭베이트”)을 홍보하는 경우가 많은 반면, 시청 시간은 참여도를 더 잘 파악할 수 있습니다[13, 25]. Feature RepresentationOur features are segregated with the traditional taxonomy of categorical and continuous&#x2F;ordinal features. The categorical features we use vary widely in their cardinality - some are binary (e.g. whether the user is logged-in) while others have millions of possible values (e.g. the user’s last search query). Features are further split according to whether they contribute only a single value (”univalent”) or a set of values (”multivalent”). An example of a univalent categorical feature is the video ID of the impression being scored, while a corresponding multivalent feature might be a bag of the last N video IDs the user has watched. We also classify features according to whether they describe properties of the item (”impression”) or properties of the user&#x2F;context (”query”). Query features are computed once per request while impression features are computed for each item scored. 우리의 feature는 범주형 기능과 연속형&#x2F;서수형 기능이라는 전통적인 분류 체계로 구분됩니다. 우리가 사용하는 범주형 피처는 카디널리티가 매우 다양합니다. 일부 피처는 이진(예: 사용자의 로그인 여부)인 반면, 다른 피처는 수백만 개의 가능한 값(예: 사용자의 마지막 검색 쿼리)을 가집니다. 기능은 단일 값만 제공하는지(‘1항’) 또는 여러 값의 집합을 제공하는지(‘2항’)에 따라 더 세분화됩니다. 단항 범주형 특징의 예로는 점수가 매겨진 노출의 동영상 ID를 들 수 있으며, 해당 다항 특징으로는 사용자가 마지막으로 시청한 N개의 동영상 ID를 묶은 것을 들 수 있습니다. 또한 항목의 속성(‘노출’)을 설명하는지, 아니면 사용자&#x2F;컨텍스트의 속성(‘쿼리’)을 설명하는지에 따라 피처를 분류합니다. 쿼리 피처는 요청당 한 번 계산되며, 노출 피처는 점수가 매겨진 각 항목에 대해 계산됩니다. Feature EngineeringWe typically use hundreds of features in our ranking models, roughly split evenly between categorical and continuous. Despite the promise of deep learning to alleviate the burden of engineering features by hand, the nature of our raw data does not easily lend itself to be input directly into feedforward neural networks. We still expend considerable engineering resources transforming user and video data into useful features. The main challenge is in representing a temporal sequence of user actions and how these actions relate to the video impression being scored. 일반적으로 순위 모델에는 수백 개의 피처가 사용되며, 대략 범주형과 연속형으로 고르게 나뉩니다. 수작업으로 기능을 엔지니어링하는 부담을 덜어주는 딥러닝의 장점에도 불구하고, 원시 데이터의 특성상 피드포워드 신경망에 직접 입력하는 것은 쉽지 않습니다. 여전히 사용자 및 비디오 데이터를 유용한 기능으로 변환하는 데 상당한 엔지니어링 리소스를 투입하고 있습니다. 주요 과제는 사용자 행동의 시간적 순서와 이러한 행동이 점수화되는 비디오 노출과 어떻게 연관되는지를 표현하는 것입니다. We observe that the most important signals are those that describe a user’s previous interaction with the item itself and other similar items, matching others’ experience in ranking ads [7]. As an example, consider the user’s past history with the channel that uploaded the video being scored - how many videos has the user watched from this channel? When was the last time the user watched a video on this topic? These continuous features describing past user actions on relateditems are particularly powerful because they generalize well across disparate items. We have also found it crucial to propagate information from candidate generation into ranking in the form of features, e.g. which sources nominated this video candidate? What scores did they assign? Features describing the frequency of past video impressions are also critical for introducing \\churn” in recommendations (successive requests do not return identical lists). If a user was recently recommended a video but did not watch it then the model will naturally demote this impression on the next page load. Serving up-to-the-second impression and watch history is an engineering feat onto itself outside the scope of this paper, but is vital for producing responsiverecommendations. Embedding Categorical FeaturesSimilar to candidate generation, we use embeddings to map sparse categorical features to dense representations suitable for neural networks. Each unique ID space (\\vocabulary”) has a separate learned embedding with dimension that increases approximately proportional to the logarithm of the number of unique values. These vocabularies are simple look-up tables built by passing over the data once before training. Very large cardinality ID spaces (e.g. video IDs or search query terms) are truncated by including only the top N after sorting based on their frequency in clicked impressions. Out-of-vocabulary values are simply mapped to the zero embedding. As in candidate generation, multivalent categorical feature embeddings are averaged before being fed in to the network. Importantly, categorical features in the same ID space also share underlying emeddings. For example, there exists a single global embedding of video IDs that many distinct featuresuse (video ID of the impression, last video ID watched by the user, video ID that \\seeded” the recommendation, etc.). Despite the shared embedding, each feature is fed separately into the network so that the layers above can learn specialized representations per feature. Sharing embeddings is important for improving generalization, speeding up training and reducing memory requirements. The overwhelming majority of model parameters are in these high-cardinality embedding spaces - for example, one million IDs embedded in a 32 dimensional space have 7 times more parameters than fully connected layers 2048 units wide. TBC.. Link : https://research.google/pubs/pub45530/","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Recommendation System","slug":"Paper/Recommendation-System","permalink":"https://jmj3047.github.io/categories/Paper/Recommendation-System/"}],"tags":[{"name":"Recommendation System","slug":"Recommendation-System","permalink":"https://jmj3047.github.io/tags/Recommendation-System/"},{"name":"Deep Nueral Networks","slug":"Deep-Nueral-Networks","permalink":"https://jmj3047.github.io/tags/Deep-Nueral-Networks/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Recommendation System","slug":"Paper/Recommendation-System","permalink":"https://jmj3047.github.io/categories/Paper/Recommendation-System/"}]},{"title":"Advanced Learning Algorithms","slug":"ML_Part2","date":"2023-03-29T15:00:00.000Z","updated":"2023-04-28T10:44:28.837Z","comments":true,"path":"2023/03/30/ML_Part2/","link":"","permalink":"https://jmj3047.github.io/2023/03/30/ML_Part2/","excerpt":"","text":"Course Lecture 2 in Machine Learning Course Neural Networks matrix multiplication: it is a binary operation that takes a pair of matrices and produces another matrix. It is defined as the product of an m×n matrix and an n×p matrix, resulting in an m×p matrix. The product is calculated by taking the dot product of the rows of the first matrix with the columns of the second matrix. The entry in the i-th row and j-th column of the resulting matrix is obtained by multiplying each element of the i-th row of the first matrix by the corresponding element of the j-th column of the second matrix and then summing the products. Nueral Network training ML advice One nice thing about transfer learning as well is maybe you don’t need to be the one to carry out supervised pre-training. For a lot of neural networks, there will already be researchers they have already trained a neural network on a large image and will have posted a trained neural networks on the Internet, freely licensed for anyone to download and use. What that means is rather than carrying out the first step yourself, you can just download the neural network that someone else may have spent weeks training and then replace the output layer with your own output layer and carry out either Option 1 or Option 2 to fine tune a neural network that someone else has already carried out supervised pre-training on, and just do a little bit of fine tuning to quickly be able to get a neural network that performs well on your task. Downloading a pre-trained model that someone else has trained and provided for free is one of those techniques where by building on each other’s work on machine learning community we can all get much better results. By the generosity of other researchers that have pre-trained and posted their neural networks online. One restriction of pre-training though, is that the image type x has to be the same for the pre-training and fine-tuning steps. If your goal is to build a speech recognition system to process audio, then a neural network pre-trained on images probably won’t do much good on audio. Instead, you want a neural network pre-trained on audio data, there you then fine tune on your own audio dataset and the same for other types of applications. You can pre-train a neural network on text data and If your application has a save feature input x of text data, then you can fine tune that neural network on your own data. To summarize, these are the two steps for transfer learning. Step 1 is download neural network with parameters that have been pre-trained on a large dataset with the same input type as your application. Decision Tree entropy function measure of the impurity of a set of data. starts from zero, goes up to one, and then comes back down to zero as a function of the fraction of positive examples in your sample. similar with Gini crtiteria if there’s a node with a lot of examples in it with high entropy that seems worse than if there was a node with just a few examples in it with high entropy. Because entropy, as a measure of impurity, is worse if you have a very large and impure dataset compared to just a few examples and a branch of the tree that is very impure. information gain it measures the reduction in entropy that you get in your tree resulting from making a split. recursive algorithm the way you build a decision tree at the root is by building other smaller decision trees in the left and right sub-branches recursion refers to writing code that calls itself.","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Coursera","slug":"Coursera","permalink":"https://jmj3047.github.io/tags/Coursera/"},{"name":"AI Studies","slug":"AI-Studies","permalink":"https://jmj3047.github.io/tags/AI-Studies/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}]},{"title":"Supervised Machine Learning, Regression and Classification","slug":"ML_Part1","date":"2023-03-25T15:00:00.000Z","updated":"2023-04-18T03:38:12.136Z","comments":true,"path":"2023/03/26/ML_Part1/","link":"","permalink":"https://jmj3047.github.io/2023/03/26/ML_Part1/","excerpt":"","text":"Course Lecture 1 in Machine Learning Course Regression with multiple input variables1. vectorization Many CPUs have “vector” or “SIMD” instruction sets which apply the same operation simultaneously to two, four, or more pieces of data. Modern x86 chips have the SSE instructions, many PPC chips have the “Altivec” instructions, and even some ARM chips have a vector instruction set, called NEON. “Vectorization” (simplified) is the process of rewriting a loop so that instead of processing a single element of an array N times, it processes (say) 4 elements of the array simultaneously N&#x2F;4 times. I chose 4 because it’s what modern hardware is most likely to directly support for 32-bit floats or ints. The difference between vectorization and loop unrolling: Consider the following very simple loop that adds the elements of two arrays and stores the results to a third array. 12for (int i=0; i&lt;16; ++i) C[i] = A[i] + B[i]; Unrolling this loop would transform it into something like this: 123456for (int i=0; i&lt;16; i+=4) &#123; C[i] = A[i] + B[i]; C[i+1] = A[i+1] + B[i+1]; C[i+2] = A[i+2] + B[i+2]; C[i+3] = A[i+3] + B[i+3];&#125; Vectorizing it, on the other hand, produces something like this: 12for (int i=0; i&lt;16; i+=4) addFourThingsAtOnceAndStoreResult(&amp;C[i], &amp;A[i], &amp;B[i]); Where “addFourThingsAtOnceAndStoreResult” is a placeholder for whatever intrinsic(s) your compiler uses to specify vector instructions. 2. feature scaling Feature scaling is a method used to normalize the range of independent variables or features of data. It is generally performed during the data preprocessing step. It is important because it can help machine learning algorithms work properly and converge faster. There are several methods for feature scaling, including rescaling (min-max normalization), mean normalization, and standardization (Z-score normalization) Rescaling (min-max normalization): This method rescales the range of features to a new range, such as [0, 1] or [-1, 1]. The general formula for a min-max of [0, 1] is given as: x&#39; = (x - min) / (max - min) where x is an original value and x’ is the normalized value. Mean normalization: This method involves subtracting the mean of a feature from each data point and then dividing by the range (max - min) or standard deviation of that feature. Standardization (Z-score normalization): This method involves subtracting the mean of a feature from each data point and then dividing by the standard deviation of that feature. This results in each feature having a mean of 0 and a standard deviation of 1. 3. feature engineering(audio data) Feature engineering in audio data involves using domain knowledge to extract relevant features from raw audio data. This can include techniques such as converting audio files into spectrograms, which produce a high-dimensional space of data that can be further reduced by applying a convolutional neural network model. Spectrogram generation: This involves converting audio files into spectrograms, which produce a high-dimensional space of data that can be further reduced by applying a convolutional neural network model. Time series analysis: This involves analyzing audio data as a set of time series and extracting relevant features from it. Sound engineering: This involves using domain knowledge from the field of sound engineering to extract relevant features from audio data. 4. polynomial regression(다항 회귀) Polynomial regression is a form of regression analysis in which the relationship between the independent variable x and the dependent variable y is modeled as an nth degree polynomial in x. Polynomial regression fits a nonlinear relationship between the value of x and the corresponding conditional mean of y, denoted E(y|x). Although polynomial regression fits a nonlinear model to the data, as a statistical estimation problem it is linear, in the sense that the regression function E(y|x) is linear in the unknown parameters that are estimated from the data. example Suppose we have a dataset with the following x and y values: x = [1, 2, 3, 4, 5] y = [2, 6, 12, 20, 30] If we plot these points on a graph, we can see that the relationship between x and y is not linear. However, we can use polynomial regression to fit a curve to these points. First, we need to decide on the degree of the polynomial. In this case, let’s use a second-degree polynomial (a quadratic equation). This means our model will have the form: y = a * x^2 + b * x + c, where a, b, and c are coefficients that we need to estimate from the data. We can use the polyfit function from the numpy library to estimate these coefficients: 123456import numpy as npx = np.array([1, 2, 3, 4, 5])y = np.array([2, 6, 12, 20, 30])coefficients = np.polyfit(x, y, deg=2) This will return an array of coefficients [a, b, c]. In this case, the coefficients are [1.0, -1.7763568394002505e-15, 1.0]. Now that we have the coefficients of our polynomial model, we can use it to make predictions for new x values. For example: 12x_new = 6y_new = coefficients[0] * x_new**2 + coefficients[1] * x_new + coefficients[2] This will give us a predicted y value of **42**for x=6 Classification Classification and Logistic Regression Cost function Gradient descent Overfitting Reference https://stackoverflow.com/questions/1422149/what-is-vectorization https://en.wikipedia.org/wiki/Feature_scaling https://scikit-learn.org/stable/auto_examples&#x2F;preprocessing&#x2F;plot_scaling_importance.html https://towardsdatascience.com/what-is-feature-scaling-why-is-it-important-in-machine-learning-2854ae877048 https://bkshin.tistory.com/entry/머신러닝-8-Feature-Scaling-Feature-Selection https://link.springer.com/chapter/10.1007/978-1-4842-8925-9_9 https://en.wikipedia.org/wiki/Polynomial_regression","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Coursera","slug":"Coursera","permalink":"https://jmj3047.github.io/tags/Coursera/"},{"name":"AI Studies","slug":"AI-Studies","permalink":"https://jmj3047.github.io/tags/AI-Studies/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}]},{"title":"BQML로 게임사 주가 예측하기","slug":"BQML_Stock_Predict","date":"2023-03-19T15:00:00.000Z","updated":"2023-03-27T10:45:16.078Z","comments":true,"path":"2023/03/20/BQML_Stock_Predict/","link":"","permalink":"https://jmj3047.github.io/2023/03/20/BQML_Stock_Predict/","excerpt":"","text":"개요 데이터로 BQML을 통해서 주가 예측을 해보자 넥슨, 컴투스, 넷마블, nc 소프트의 주가를 예측 5년동안 데이터로 학습하고 넥슨 게임사의 2023-03월달의 주가 예측을 시행함 데이터 흐름: API 크롤링 → 빅쿼리, 빅쿼리ML → Looker Studio 목적 5년치 주가 데이터를 활용힌 넥슨 게임즈의 주가 등락 예측과 경쟁 3사와의 비교 BQML 만으로 괜찮은 결과가 나올수 있는지 확인 모델들 비교를 해보며 주제에 어떤 모델이 적합할지 확인 데이터 데이터 크롤링 → 빅쿼리 적재 하는 방법 대상 회사 정의 2018년 이전에 상장한 게임 회사 매출 1~5위 순위 중 위 조건을 충족하는 게임사는 넥슨, 넷마블, ncsoft 였으며, 그 외 순위의 위 조건을 충족하는 컴투스까지 포함하여 총 4개 게임사 게임사 선택 이유: 경쟁사가 많고, 매출 기준으로 순위권 회사들이 상장을 많이 함. 피처 설명 5년치 데이터를 가져 올수 있는 파이썬 api중 가장 많은 데이터를 가져올수 있는 marcap data를 사용하였음 Adj_Close(수정종가) 피처는 yfinance에서만 있었고, 모든 날짜에 대해서 있는건 아니지만, 이를 통해서 액면분할 된 주가를 모델에 제대로 반영할수 있어서 추가 하게 되었음 marcap데이터 수집 당시 2월 말까지만 수집을 하였고 이로 인해 3월 예측 데이터를 finance data reader로 진행함 ml_label: 훈련을 수월하게 하기 위해 임의로 만듦, volume을 기준으로 training:evaluation:prediction &#x3D; 8:1:1로 나누고 prediction 데이터는 3월 데이터 까지 포함되어 있음. 데이터 개수 marcap + yfinance 의 데이터를 8:1:1로 나누고 예측 데이터에 finance data reader에서 2023-03 기간동안의 데이터를 합쳐서 학습을 진행 전일대비 등락률로 예측을 진행하였고 정확한 수치가 아닌 양수면 오름, 음수면 내림으로 환산하여 예측의 정확도를 판별 데이터 기간 marcap: 2018-01-02 ~ 2023-02-28 yfinance: 2018-01-02 ~ 2023-02-28 finance data reader: 2023-03-02 ~ 2023-03-21 모델 선택 및 비교 Big Query ML에서 추천하는 주식 데이터 예측 모델로는 Linear Regression, Boosted Trees Regressor, Auto ML Table Regressor, DNN Regressor, Wide &amp; Deep Regressor 이 있음 총 데이터가 만개를 넘지 않을정도로 적고(5104행), 시간이 오래 걸리지 않는 Linear Regression, Boosted Trees Regression 모델을 사용하여 예측 모델 예측 결과 Linear Regression이 모델 학습 속도는 적게 걸리나 오류나 과적합 면에서 Boosted Trees Regression의 결과가 훨씬 나음 실제 등락이 크게 오르거나 내려갈때의 값을 Linear Regression 모델 보다 Boosted Trees Regression 모델이 더 잘 예측함 Model Evaluation 값 지표 설명 예측 ml_ label이 prediction으로 책정된 데이터를 예측한 결과와 실제 Chages Ratio 값을 비교 정확한 수치를 비교하는것이 아닌 등락으로 비교 -&gt; 비율이 양수라면 값이 오른것, 음수라면 내린것으로 간주 feature importance 시각화 결론 애초에 데이터부터 잘못 설계가 된 실험 전날 데이터를 갖고 다음날을 예측 해야 하는데 당일 데이터로 당일을 예측해 버림 → 안정확한게 더 이상한것 Reference BigQuery ML을 사용하여 펭귄 체중 예측 Mean_Absolute_Error MAE,MSE,RMSE R2_Explained_Variance_Score","categories":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}],"tags":[{"name":"Big Query","slug":"Big-Query","permalink":"https://jmj3047.github.io/tags/Big-Query/"},{"name":"BigQueryML","slug":"BigQueryML","permalink":"https://jmj3047.github.io/tags/BigQueryML/"},{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"Looker Studio","slug":"Looker-Studio","permalink":"https://jmj3047.github.io/tags/Looker-Studio/"}],"keywords":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}]},{"title":"Datastream, Dataflow, BigQuery ML, Looker Studio를 사용하여 수요 예측 빌드 및 시각화","slug":"Dataflow_BQML_Looker","date":"2023-03-12T15:00:00.000Z","updated":"2023-03-19T07:02:20.896Z","comments":true,"path":"2023/03/13/Dataflow_BQML_Looker/","link":"","permalink":"https://jmj3047.github.io/2023/03/13/Dataflow_BQML_Looker/","excerpt":"","text":"개요 이 노트북의 예제로 실습 진행 데이터 흐름: oracle → pub&#x2F;sub → datastream → dataflow → bigquery → looker 전체적인 process compute engine에서 vm 인스턴스 생성 저장장소 Cloud Storage Bucket 생성 객체 변경사항에 대한 알림을 Pub&#x2F;sub으로 전송하도록 bucket 구성 Data stream을 만들어 google cloud storage에 오라클 데이터 복제 DataFlow로 복제된 데이터를 json파일로 빅쿼리에 적재 빅쿼리로 데이터 분석 및 ML 루커로 시각화 본 포스트는 위 전체 프로세스에서 5~6번에 해당하는 내용(이전 내용 확인) DataFlow로 복제된 데이터를 json파일로 변환하여 Bigquery 적재 하는 방법을 알아보자 BQML의 ARIMA_PLUS 대해서 알아보자 루커를 사용하지 않고 루커 스튜디오로 시각화 사례에서 활용된 DataFlow Dataflow Datastream to BigQuery 스트리밍 템플릿에 배포하여 Datastream에서 캡처한 변경사항을 BigQuery에 복제 UDF를 만들고 사용하여 이 템플릿의 기능을 확장 수신 데이터를 처리할 수 있는 UDF 만들기 UDF를 만들어 backfill된 데이터와 모든 새 수신 데이터에서 다음 작업을 수행 backfill: 데이터 파이프라인을 운용할때 이미 지난 날짜를 기준으로 재처리하는 작업, 메우는 작업, 버그가 있거나 어떤 이유로 로직이 변경됐을때 전체 데이터를 새로 말아주어야 할때 컬럼 등의 메타 데이터가 변경되었을 때 이를 반영하기 위한 append 성의 작업이 필요할때 고객 결제 수단과 같은 민감한 정보를 수정 데이터 계보와 검색을 위해 Oracle 소스 테이블을 BigQuery에 추가 이 로직은 Datastream에서 생성된 JSON 파일을 입력 매개변수로 사용하는 자바스크립트 파일에서 캡처됨 Cloud Shell 세션에서 다음 코드를 복사하여 retail_transform.js 파일에 저장 Oracle에서 추출한 json 파일을 암호화 해서 새로운 json 파일을 생성함. 123456789101112function process(inJson) &#123; var obj = JSON.parse(inJson), includePubsubMessage = obj.data &amp;&amp; obj.attributes, data = includePubsubMessage ? obj.data : obj; data.PAYMENT_METHOD = data.PAYMENT_METHOD.split(&#x27;:&#x27;)[0].concat(&quot;XXX&quot;); data.ORACLE_SOURCE = data._metadata_schema.concat(&#x27;.&#x27;, data._metadata_table); return JSON.stringify(obj);&#125; retail_transform.js file을 저장할 Cloud Storage 버킷을 만든 후 자바스크립트 파일을 새로 만든 버킷에 업로드합니다. 위에서 생성된 json 파일을 새로운 버켓에 저장함 1234gsutil mb gs://js-$&#123;BUCKET_NAME&#125;gsutil cp retail_transform.js \\gs://js-$&#123;BUCKET_NAME&#125;/utils/retail_transform.js Dataflow 작업 만들기 Cloud Shell에서 데드 레터 큐(DLQ) 버킷을 만든다: 이 버킷은 DataFlow에서 사용됨 dead-letter-queue: 하나 이상의 Source Queue가 성공적으로 컨슘되지 못한 메세지들을 재전송하기 위해 사용하는 별도의 큐. DLQ에 쌓인 메세지들을 보면 왜 이 메세지들이 컨슈머에 의해 처리되지 못했는지를 알 수 있다. 1gsutil mb gs://dlq-$&#123;BUCKET_NAME&#125; Dataflow 실행에 필요한 서비스 계정을 만들고 계정을 Dataflow Worker, Dataflow Admin, Pub/Sub Admin, BigQuery Data Editor, BigQuery Job User, Datastream Admin 역할에 할당 1234567891011121314151617181920212223242526272829gcloud iam service-accounts create df-tutorialgcloud projects add-iam-policy-binding $&#123;PROJECT_ID&#125; \\--member=&quot;serviceAccount:df-tutorial@$&#123;PROJECT_ID&#125;.iam.gserviceaccount.com&quot; \\--role=&quot;roles/dataflow.admin&quot;gcloud projects add-iam-policy-binding $&#123;PROJECT_ID&#125; \\--member=&quot;serviceAccount:df-tutorial@$&#123;PROJECT_ID&#125;.iam.gserviceaccount.com&quot; \\--role=&quot;roles/dataflow.worker&quot;gcloud projects add-iam-policy-binding $&#123;PROJECT_ID&#125; \\--member=&quot;serviceAccount:df-tutorial@$&#123;PROJECT_ID&#125;.iam.gserviceaccount.com&quot; \\--role=&quot;roles/pubsub.admin&quot;gcloud projects add-iam-policy-binding $&#123;PROJECT_ID&#125; \\--member=&quot;serviceAccount:df-tutorial@$&#123;PROJECT_ID&#125;.iam.gserviceaccount.com&quot; \\--role=&quot;roles/bigquery.dataEditor&quot;gcloud projects add-iam-policy-binding $&#123;PROJECT_ID&#125; \\--member=&quot;serviceAccount:df-tutorial@$&#123;PROJECT_ID&#125;.iam.gserviceaccount.com&quot; \\--role=&quot;roles/bigquery.jobUser&quot;gcloud projects add-iam-policy-binding $&#123;PROJECT_ID&#125; \\--member=&quot;serviceAccount:df-tutorial@$&#123;PROJECT_ID&#125;.iam.gserviceaccount.com&quot; \\--role=&quot;roles/datastream.admin&quot;gcloud projects add-iam-policy-binding $&#123;PROJECT_ID&#125; \\--member=&quot;serviceAccount:df-tutorial@$&#123;PROJECT_ID&#125;.iam.gserviceaccount.com&quot; \\--role=&quot;roles/storage.admin&quot; 자동 확장이 사용 설정되면 Dataflow VM이 TCP 포트 12345 및 12346에서 네트워크 트래픽과 통신하고 전송 및 수신할 수 있도록 방화벽 이그레스 규칙을 만든다: VM끼리 통신이 가능하게끔 하는 작업 12345678gcloud compute firewall-rules create fw-allow-inter-dataflow-comm \\--action=allow \\--direction=ingress \\--network=GCP_NETWORK_NAME \\--target-tags=dataflow \\--source-tags=dataflow \\--priority=0 \\--rules tcp:12345-12346 Dataflow 작업을 만들고 실행 → Dataflow 콘솔을 확인하여 새 스트리밍 작업이 시작되었는지 확인 123456789101112131415export REGION=us-central1gcloud dataflow flex-template run orders-cdc-template --region $&#123;REGION&#125; \\--template-file-gcs-location &quot;gs://dataflow-templates/latest/flex/Cloud_Datastream_to_BigQuery&quot; \\--service-account-email &quot;df-tutorial@$&#123;PROJECT_ID&#125;.iam.gserviceaccount.com&quot; \\--parameters \\inputFilePattern=&quot;gs://$&#123;BUCKET_NAME&#125;/&quot;,\\gcsPubSubSubscription=&quot;projects/$&#123;PROJECT_ID&#125;/subscriptions/oracle_retail_sub&quot;,\\inputFileFormat=&quot;json&quot;,\\outputStagingDatasetTemplate=&quot;retail&quot;,\\outputDatasetTemplate=&quot;retail&quot;,\\deadLetterQueueDirectory=&quot;gs://dlq-$&#123;BUCKET_NAME&#125;&quot;,\\autoscalingAlgorithm=&quot;THROUGHPUT_BASED&quot;,\\mergeFrequencyMinutes=1,\\javascriptTextTransformGcsPath=&quot;gs://js-$&#123;BUCKET_NAME&#125;/utils/retail_transform.js&quot;,\\javascriptTextTransformFunctionName=&quot;process&quot; Cloud Shell에서 다음 명령어를 실행하여 Datastream 스트림을 시작 12gcloud datastream streams update oracle-cdc \\--location=us-central1 --state=RUNNING --update-mask=state DataStream 스트림 상태를 확인 12gcloud datastream streams list \\--location=us-central1 상태가 Running으로 표시되는지 확인. 새 상태값이 반영되기까지 몇 초 정도 걸릴 수 있음. Datastream 콘솔을 확인하여 ORDERS 테이블 백필 진행 상황을 확인 이 task는 초기 로드 이므로 Datastream은 ORDERS 객체에서 읽음. 스트림 생성 중에 지정한 Cloud Storage 버킷에 있는 JSON 파일에 모든 레코드를 쓴다. 백필 태스크가 완료되는데 약 10분 정도 걸림. BigQuery에서 데이터 분석 데이터 세트에서 다음 새 테이블 두 개가 Dataflow 작업으로 생성 ORDERS: 이 출력 테이블은 Oracle 테이블 복제본이며 Dataflow 템플릿의 일부로 데이터에 적용된 변환을 포함 ORDERS_log: 이 스테이징 테이블은 Oracle 소스의 모든 변경사항을 기록. 테이블은 파티션으로 나눠지고 변경사항이 업데이트, 삽입 또는 삭제인지 여부와 같은 일부 메타데이터 변경 정보와 함께 업데이트된 레코드를 저장 BigQuery ML에서 수요 예측 모델 빌드 BigQuery ML은 ARIMA_PLUS 알고리즘을 사용하여 수요 예측 모델을 빌드하고 배포하는 데 사용될 수 있음. 이 섹션에서는 BigQuery ML을 사용하여 매장 내 제품 수요를 예측하는 모델을 빌드. 학습 데이터 준비 백필한 데이터의 샘플을 사용하여 모델을 학습 이 경우 1년 동안의 데이터를 사용합니다. 학습 데이터에서는 다음을 보여줍니다. 제품 이름(product_name) 판매된 각 제품의 단위 수량(total_sold) 시간당 판매된 제품 수(hourly_timestamp) BigQuery에서 다음 SQL을 실행하여 학습 데이터를 만들고 training_data라는 새 테이블에 저장 1234567891011CREATE OR REPLACE TABLE `retail.training_data`AS SELECT TIMESTAMP_TRUNC(time_of_sale, HOUR) as hourly_timestamp, product_name, SUM(quantity) AS total_sold FROM `retail.ORDERS` GROUP BY hourly_timestamp, product_name HAVING hourly_timestamp BETWEEN TIMESTAMP_TRUNC(&#x27;2021-11-22&#x27;, HOUR) ANDTIMESTAMP_TRUNC(&#x27;2021-11-28&#x27;, HOUR)ORDER BY hourly_timestamp 예측 수요 BigQuery에서 다음 SQL을 실행하여 ARIMA_PLUS 알고리즘을 사용하는 시계열 모델을 생성 12345678910111213CREATE OR REPLACE MODEL `retail.arima_plus_model` OPTIONS( MODEL_TYPE=&#x27;ARIMA_PLUS&#x27;, TIME_SERIES_TIMESTAMP_COL=&#x27;hourly_timestamp&#x27;, TIME_SERIES_DATA_COL=&#x27;total_sold&#x27;, TIME_SERIES_ID_COL=&#x27;product_name&#x27; ) ASSELECT hourly_timestamp, product_name, total_soldFROM `retail.training_data` ML.FORECAST함수는 n시간 범위에 걸쳐 예상되는 수요를 예측하는 데 사용됨 다음 SQL을 실행하여 향후 30일 동안의 유기농 바나나 수요를 예측 1SELECT * FROM ML.FORECAST(MODEL `retail.arima_plus_model`, STRUCT(720 AS horizon)) 학습 데이터는 시간 단위이므로 범위 값은 예측 시 동일한 시간 단위(시간)를 사용. 720시간 범위 값은 다음 30일 동안의 예측 결과를 반환 이 튜토리얼에서는 소량의 샘플 데이터 세트를 사용하므로 모델의 정확성에 대한 자세한 조사는 이 튜토리얼에서 다루지 않음. 시각화하기 BigQuery에서 다음 SQL 쿼리를 실행하여 유기농 바나나의 실제 판매량과 예상 판매량을 통합하는 뷰를 생성 12345678910111213141516171819202122232425262728CREATE OR REPLACE VIEW `retail.orders_forecast` AS (SELECTtimestamp,product_name,SUM(forecast_value) AS forecast,SUM(actual_value) AS actualfrom(SELECT TIMESTAMP_TRUNC(TIME_OF_SALE, HOUR) AS timestamp, product_name, SUM(QUANTITY) as actual_value, NULL AS forecast_value FROM `retail.ORDERS` GROUP BY timestamp, product_nameUNION ALLSELECT forecast_timestamp AS timestamp, product_name, NULL AS actual_value, forecast_value, FROM ML.FORECAST(MODEL `retail.arima_plus_model`, STRUCT(720 AS horizon)) ORDER BY timestamp)GROUP BY timestamp, product_nameORDER BY timestamp) 이 뷰를 사용하면 실제 데이터와 예측 데이터를 탐색할 때 Looker에서 관련 데이터를 쿼리할 수 있음 다음 SQL을 실행하여 뷰를 검증 1234SELECT * FROM `retail.orders_forecast`WHERE PRODUCT_NAME=&#x27;Bag of Organic Bananas&#x27;AND TIMESTAMP_TRUNC(timestamp, HOUR) BETWEEN TIMESTAMP_TRUNC(&#x27;2021-11-28&#x27;, HOUR) AND TIMESTAMP_TRUNC(&#x27;2021-11-30&#x27;, HOUR)LIMIT 100; 이전 포스팅의 ‘Big Query를 사용하여 SQL로 데이터를 정제한 후 Looker Studio에 SQL로 차트 만들기’ 처럼 sql쿼리로 데이터 셋을 생성 12SELECT * FROM `retail.orders_forecast`WHERE actual IS NOT NULL 결과 페이지 드롭다운 목록의 날짜 부분은 함수를 걸어 date형태로 조회할수 있게 하였음. Reference https://cloud.google.com/architecture/build-visualize-demand-forecast-prediction-datastream-dataflow-bigqueryml-looker?hl=ko#create-a-dataflow-job https://cloud.google.com/dataflow?hl=ko https://cloud.google.com/dataflow/pricing?hl=ko#shuffle-pricing-details https:&#x2F;&#x2F;velog.io&#x2F;@usaindream&#x2F;Dead-Letter-QueueDLQ https://wookiist.dev/175","categories":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}],"tags":[{"name":"Big Query","slug":"Big-Query","permalink":"https://jmj3047.github.io/tags/Big-Query/"},{"name":"BigQueryML","slug":"BigQueryML","permalink":"https://jmj3047.github.io/tags/BigQueryML/"},{"name":"Looker Studio","slug":"Looker-Studio","permalink":"https://jmj3047.github.io/tags/Looker-Studio/"},{"name":"DataFlow","slug":"DataFlow","permalink":"https://jmj3047.github.io/tags/DataFlow/"}],"keywords":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}]},{"title":"K-Means Clustering(2)","slug":"K-Means_Clustering_2","date":"2023-03-10T15:00:00.000Z","updated":"2023-03-13T11:06:45.805Z","comments":true,"path":"2023/03/11/K-Means_Clustering_2/","link":"","permalink":"https://jmj3047.github.io/2023/03/11/K-Means_Clustering_2/","excerpt":"","text":"개요 이전 포스팅에 이어 Big Query ML에서 K-Means Clustering에서 k값을 결정하는 방법을 알아보자 이 포스팅 하단부에 대한 설명 Elbow Method 사용하고자 하는 클러스터 범위를 지정한다. 각 클러스터를 WCSS방법으로 계산을 합니다. WCSS값과 클러스터 K 갯수에 대한 커브선을 그립니다. 뾰족하게 구부러진 부분이나 특정 지점이 팔처럼 굽어지는 부분을 K로 지정합니다. **Within Cluster Sum of Squares(**WCSS) 클러스터 내 제곱합(WCSS) 클러스터 내 모든 포인트에서 클러스터 중심까지의 제곱 평균 거리를 측정 WCSS를 계산하려면 먼저 지정된 포인트와 해당 포인트가 할당된 중심 사이의 유클리드 거리(아래 그림 참조)를 찾는다 클러스터의 모든 포인트에 대해 이 프로세스를 반복한 다음 클러스터의 값을 합산하고 포인트 수로 나눈다. 마지막으로 모든 클러스터의 평균을 계산합니다. 이렇게 하면 평균 WCSS가 계산됩니다. Davies Bouldin Index(DBI) n &#x3D; cluster 개수 $c_x$ &#x3D; cluster $x$의 중심점 $\\sigma_x$ &#x3D; cluster $x$내의 모든 데이터 오브젝트로 부터 중심점 $c_x$까지 거리의 평균값 $d(c_i,c_j)$ &#x3D; 중심점 $c_i$와 중심점 $c_j$간의 거리 높은 클러스터 내 유사도를 가지고 낮은 클러스터간 유사도를 가지는 클러스터를 생성하는 클러스터링 알고리즘은 낮은 DBI값을 갖게 됨. 이 지표가 낮은 클러스터링 알고리즘이 좋은 클러스터링 알고리즘으로 평가 됨 간단한 예제를 통해 이해해 보자 다음과 같이 점 4개가 주어지고 점(1,1)과 점 (1,3)을 중심으로 할때 → 각 그룹의 중심은 (1,3)과 (3,3)으로 나타낼 수 있음. 반면 점 (1,1)과 점(1,5)가 중심이 되었을 때 → 각 그룹의 중심은 (1,2)와 (5,2)로 나타남 위에서 구한 두 가지 경우들에 대해 각각 DBI값을 구한다면 그룹의 중심이 (1,3)과 (3,3)인 경우: 2 그룹의 중심이 (1,2)와 (5,2)인 경우: 0.5 이 결과를 보았을때 값이 작은 후자의 경우가 cluster를 자세하 구분했다고 말할 수 있음. Reference https://odsc.medium.com/unsupervised-learning-evaluating-clusters-bd47eed175ce https://nicola-ml.tistory.com/66 https://ko.wikipedia.org/wiki/K-평균_알고리즘 https://elecs.tistory.com/303","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}],"tags":[{"name":"BigQueryML","slug":"BigQueryML","permalink":"https://jmj3047.github.io/tags/BigQueryML/"},{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"Clustering","slug":"Clustering","permalink":"https://jmj3047.github.io/tags/Clustering/"},{"name":"K-Means Clustering","slug":"K-Means-Clustering","permalink":"https://jmj3047.github.io/tags/K-Means-Clustering/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}]},{"title":"BQML을 이용한 고객 분류","slug":"BQML_Classification","date":"2023-03-09T15:00:00.000Z","updated":"2023-03-19T07:02:05.304Z","comments":true,"path":"2023/03/10/BQML_Classification/","link":"","permalink":"https://jmj3047.github.io/2023/03/10/BQML_Classification/","excerpt":"","text":"개요 K means clustering을 빅쿼리 ML(BQML)을 사용하여 고객을 세분화 하기 GA360의 데이터를 빅쿼리에 적재해 ML학습하기 파이썬을 사용하여 빅쿼리와 연동하고 관련 그래프 시각화하기 목표 구글 브랜드 상품을 판매하는 실제 이커머스 스토어인 구글 머천다이스 스토어의 난독화된 GA360 12개월(2016년 8월~2017년 8월)의 데이터를 가지고 고객을 분류 환경 구축 GCP와 주피터 노트북을 연결 config파일이 필요(이전포스트 설명)PIP install packages and dependencies 123456!pip install google-cloud-bigquery!pip install google-cloud-bigquery-storage!pip install pandas-gbq# Reservation package needed to setup flex slots for flat-rate pricing!pip install google-cloud-bigquery-reservation 123456789101112131415161718192021222324252627282930Requirement already satisfied: google-cloud-bigquery in /opt/homebrew/lib/python3.10/site-packages (3.6.0)Requirement already satisfied: google-cloud-core&lt;3.0.0dev,&gt;=1.6.0 in /opt/homebrew/lib/python3.10/site-packages (from google-cloud-bigquery) (2.3.2)Requirement already satisfied: requests&lt;3.0.0dev,&gt;=2.21.0 in /opt/homebrew/lib/python3.10/site-packages (from google-cloud-bigquery) (2.28.2)Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,&lt;5.0.0dev,&gt;=3.19.5 in /opt/homebrew/lib/python3.10/site-packages (from google-cloud-bigquery) (4.22.1)Requirement already satisfied: grpcio&lt;2.0dev,&gt;=1.47.0 in /opt/homebrew/lib/python3.10/site-packages (from google-cloud-bigquery) (1.51.3)Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,&lt;3.0.0dev,&gt;=1.31.5 in /opt/homebrew/lib/python3.10/site-packages (from google-cloud-bigquery) (2.11.0)Requirement already satisfied: google-resumable-media&lt;3.0dev,&gt;=0.6.0 in /opt/homebrew/lib/python3.10/site-packages (from google-cloud-bigquery) (2.4.1)Requirement already satisfied: packaging&gt;=20.0.0 in /opt/homebrew/lib/python3.10/site-packages (from google-cloud-bigquery) (23.0)Requirement already satisfied: python-dateutil&lt;3.0dev,&gt;=2.7.2 in /opt/homebrew/lib/python3.10/site-packages (from google-cloud-bigquery) (2.8.2)Requirement already satisfied: proto-plus&lt;2.0.0dev,&gt;=1.15.0 in /opt/homebrew/lib/python3.10/site-packages (from google-cloud-bigquery) (1.22.2)Requirement already satisfied: google-auth&lt;3.0dev,&gt;=2.14.1 in /opt/homebrew/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,&lt;3.0.0dev,&gt;=1.31.5-&gt;google-cloud-bigquery) (2.16.1)Requirement already satisfied: googleapis-common-protos&lt;2.0dev,&gt;=1.56.2 in /opt/homebrew/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,&lt;3.0.0dev,&gt;=1.31.5-&gt;google-cloud-bigquery) (1.58.0)Requirement already satisfied: grpcio-status&lt;2.0dev,&gt;=1.33.2 in /opt/homebrew/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,&lt;3.0.0dev,&gt;=1.31.5-&gt;google-cloud-bigquery) (1.51.3)Requirement already satisfied: google-crc32c&lt;2.0dev,&gt;=1.0 in /opt/homebrew/lib/python3.10/site-packages (from google-resumable-media&lt;3.0dev,&gt;=0.6.0-&gt;google-cloud-bigquery) (1.5.0)Requirement already satisfied: six&gt;=1.5 in /opt/homebrew/lib/python3.10/site-packages (from python-dateutil&lt;3.0dev,&gt;=2.7.2-&gt;google-cloud-bigquery) (1.16.0)Requirement already satisfied: idna&lt;4,&gt;=2.5 in /opt/homebrew/lib/python3.10/site-packages (from requests&lt;3.0.0dev,&gt;=2.21.0-&gt;google-cloud-bigquery) (3.4)Requirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /opt/homebrew/lib/python3.10/site-packages (from requests&lt;3.0.0dev,&gt;=2.21.0-&gt;google-cloud-bigquery) (3.0.1)Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /opt/homebrew/lib/python3.10/site-packages (from requests&lt;3.0.0dev,&gt;=2.21.0-&gt;google-cloud-bigquery) (1.26.14)Requirement already satisfied: certifi&gt;=2017.4.17 in /opt/homebrew/lib/python3.10/site-packages (from requests&lt;3.0.0dev,&gt;=2.21.0-&gt;google-cloud-bigquery) (2022.12.7)Requirement already satisfied: pyasn1-modules&gt;=0.2.1 in /opt/homebrew/lib/python3.10/site-packages (from google-auth&lt;3.0dev,&gt;=2.14.1-&gt;google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,&lt;3.0.0dev,&gt;=1.31.5-&gt;google-cloud-bigquery) (0.2.8)Requirement already satisfied: rsa&lt;5,&gt;=3.1.4 in /opt/homebrew/lib/python3.10/site-packages (from google-auth&lt;3.0dev,&gt;=2.14.1-&gt;google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,&lt;3.0.0dev,&gt;=1.31.5-&gt;google-cloud-bigquery) (4.9)Requirement already satisfied: cachetools&lt;6.0,&gt;=2.0.0 in /opt/homebrew/lib/python3.10/site-packages (from google-auth&lt;3.0dev,&gt;=2.14.1-&gt;google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,&lt;3.0.0dev,&gt;=1.31.5-&gt;google-cloud-bigquery) (5.3.0)Requirement already satisfied: pyasn1&lt;0.5.0,&gt;=0.4.6 in /opt/homebrew/lib/python3.10/site-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;3.0dev,&gt;=2.14.1-&gt;google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,&lt;3.0.0dev,&gt;=1.31.5-&gt;google-cloud-bigquery) (0.4.8)Requirement already satisfied: google-cloud-bigquery-storage in /opt/homebrew/lib/python3.10/site-packages (2.19.0)Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,&lt;5.0.0dev,&gt;=3.19.5 in /opt/homebrew/lib/python3.10/site-packages (from google-cloud-bigquery-storage) (4.22.1)...Requirement already satisfied: certifi&gt;=2017.4.17 in /opt/homebrew/lib/python3.10/site-packages (from requests&lt;3.0.0dev,&gt;=2.18.0-&gt;google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,&lt;3.0.0dev,&gt;=1.34.0-&gt;google-cloud-bigquery-reservation) (2022.12.7)Requirement already satisfied: pyasn1&lt;0.5.0,&gt;=0.4.6 in /opt/homebrew/lib/python3.10/site-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;3.0dev,&gt;=2.14.1-&gt;google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,&lt;3.0.0dev,&gt;=1.34.0-&gt;google-cloud-bigquery-reservation) (0.4.8)Installing collected packages: google-cloud-bigquery-reservationSuccessfully installed google-cloud-bigquery-reservation-1.10.0 설치 후 커널 다시 시작 1234# Automatically restart kernel after installsimport IPythonapp = IPython.Application.instance()app.kernel.do_shutdown(True) Project ID와 인증 12345678PROJECT_ID = &quot;your-project-id&quot; REGION = &#x27;US&#x27;DATA_SET_ID = &#x27;bqml_kmeans&#x27; # Ensure you first create a data set in BigQuery# If you have not built the Data Set, the following command will build it for you!bq mk --location=$REGION --dataset $PROJECT_ID:$DATA_SET_ID !gcloud config set project $PROJECT_ID Import libraries and define constants 1234567891011121314151617181920import globfrom google.cloud import bigqueryfrom google.oauth2 import service_accountfrom google.cloud import bigqueryimport numpy as npimport pandas as pdimport pandas_gbqimport matplotlib.pyplot as plt# 서비스 계정 키 JSON 파일 경로key_path = glob.glob(&quot;./config/*.json&quot;)[0]# Credentials 객체 생성credentials = service_account.Credentials.from_service_account_file(key_path)project_id=&quot;your-project-id&quot;# GCP 클라이언트 객체 생성pd.set_option(&#x27;display.float_format&#x27;, lambda x: &#x27;%.3f&#x27; % x) # used to display float formatclient = bigquery.Client(credentials = credentials, project = credentials.project_id) 데이터모델을 구축하기 전에 일반적으로 모델링을 위해 의미 있는 방식으로 데이터 집합을 정리, 탐색 및 집계하는 데 상당한 시간을 투자해야 함. 이 포스트 목적상 이 단계는 BigQuery ML에서 k-평균을 사용한 클러스터링을 우선적으로 보여주기 위해 표시하지 않음. GA360, GA4차이 GA360 GA의 유료 버전으로 무료 버전과 가장 큰 차이점은 ‘데이터 소유권’ 데이터 소유권이 구글인 무료 버전에 비해 GA360은 데이터 소유권이 사용자 때문에 데이터 샘플링이 없고 빅쿼리를 통해 Raw Data를 이용할 수 있음. 다만 유료인만큼 연간 1.5억의 사용료를 지불해야 함. GA4 2019년에 새로 생긴 구글 애널리틱스로 WEB과 APP을 심리스하게 보기 위한 GA 360처럼 유료 버전을 쓰지 않아도 빅쿼리로 데이터를 보내주기 때문에 Raw Data를 쿼리 비용만 내고 사용할수 있음. GA4 UI에선 속도가 매우 빠르지만 GAUI에 비해 GA4 UI에서는 많은 것으르 보옂쥐 않고 데이터 분석을 위해 제대로 사용하기 위해서는 빅쿼리에 익숙애햐 함. 합성 데이터 구축 최종 목표는 온라인(GA360) 및 오프라인(CRM) 데이터를 모두 사용하는 것. 자체 CRM 데이터를 사용할 수도 있지만, 이 경우에는 보여줄 CRM 데이터가 없으므로 대신 합성 데이터를 생성: 예상 가구 소득(House Hold income, hhi)과 성별 이를 위해 전체 방문자 ID를 해시하고 해시의 마지막 숫자를 기반으로 간단한 규칙을 구축합니다. hash: 임의의 길이를 갖는 임의의 데이터를 고정된 길이의 데이터로 매핑하는 것 자체 데이터로 이 프로세스를 실행하면 여러 차원으로 CRM 데이터를 조인할 수 있음. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960# We start with GA360 data, and will eventually build synthetic CRM as an example. # This block is the first step, just working with GA360ga360_only_view = &#x27;GA360_View&#x27;shared_dataset_ref = client.dataset(DATA_SET_ID)ga360_view_ref = shared_dataset_ref.table(ga360_only_view)ga360_view = bigquery.Table(ga360_view_ref)ga360_query = &#x27;&#x27;&#x27;SELECT fullVisitorID, ABS(farm_fingerprint(fullVisitorID)) AS Hashed_fullVisitorID, # This will be used to generate random data. MAX(device.operatingSystem) AS OS, # We can aggregate this because an OS is tied to a fullVisitorID. SUM (CASE WHEN REGEXP_EXTRACT (v2ProductCategory, r&#x27;^(?:(?:.*?)Home/)(.*?)/&#x27;) = &#x27;Apparel&#x27; THEN 1 ELSE 0 END) AS Apparel, SUM (CASE WHEN REGEXP_EXTRACT (v2ProductCategory, r&#x27;^(?:(?:.*?)Home/)(.*?)/&#x27;) = &#x27;Office&#x27; THEN 1 ELSE 0 END) AS Office, SUM (CASE WHEN REGEXP_EXTRACT (v2ProductCategory, r&#x27;^(?:(?:.*?)Home/)(.*?)/&#x27;) = &#x27;Electronics&#x27; THEN 1 ELSE 0 END) AS Electronics, SUM (CASE WHEN REGEXP_EXTRACT (v2ProductCategory, r&#x27;^(?:(?:.*?)Home/)(.*?)/&#x27;) = &#x27;Limited Supply&#x27; THEN 1 ELSE 0 END) AS LimitedSupply, SUM (CASE WHEN REGEXP_EXTRACT (v2ProductCategory, r&#x27;^(?:(?:.*?)Home/)(.*?)/&#x27;) = &#x27;Accessories&#x27; THEN 1 ELSE 0 END) AS Accessories, SUM (CASE WHEN REGEXP_EXTRACT (v2ProductCategory, r&#x27;^(?:(?:.*?)Home/)(.*?)/&#x27;) = &#x27;Shop by Brand&#x27; THEN 1 ELSE 0 END) AS ShopByBrand, SUM (CASE WHEN REGEXP_EXTRACT (v2ProductCategory, r&#x27;^(?:(?:.*?)Home/)(.*?)/&#x27;) = &#x27;Bags&#x27; THEN 1 ELSE 0 END) AS Bags, ROUND (SUM (productPrice/1000000),2) AS productPrice_USDFROM `bigquery-public-data.google_analytics_sample.ga_sessions_*`, UNNEST(hits) AS hits, UNNEST(hits.product) AS hits_productWHERE _TABLE_SUFFIX BETWEEN &#x27;20160801&#x27; AND &#x27;20160831&#x27; AND geoNetwork.country = &#x27;United States&#x27; AND type = &#x27;EVENT&#x27;GROUP BY 1, 2&#x27;&#x27;&#x27;ga360_view.view_query = ga360_query.format(PROJECT_ID)ga360_view = client.create_table(ga360_view) # API requestprint(f&quot;Successfully created view at &#123;ga360_view.full_table_id&#125;&quot;) 데이터 확인 1234567891011121314# Show a sample of GA360 dataga360_query_df = f&#x27;&#x27;&#x27;SELECT * FROM &#123;ga360_view.full_table_id.replace(&quot;:&quot;, &quot;.&quot;)&#125; LIMIT 5&#x27;&#x27;&#x27;job_config = bigquery.QueryJobConfig()# Start the queryquery_job = client.query(ga360_query_df, job_config=job_config) #API Requestdf_ga360 = query_job.result()df_ga360 = df_ga360.to_dataframe()df_ga360 CRM data 추출하여 합성데이터 만들기 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# Create synthetic CRM data in SQLCRM_only_view = &#x27;CRM_View&#x27;shared_dataset_ref = client.dataset(DATA_SET_ID)CRM_view_ref = shared_dataset_ref.table(CRM_only_view)CRM_view = bigquery.Table(CRM_view_ref)# Query below works by hashing the fullVisitorID, which creates a random distribution. # We use modulo to artificially split gender and hhi distribution.CRM_query = &#x27;&#x27;&#x27;SELECT fullVisitorID,IF (MOD(Hashed_fullVisitorID,2) = 0, &quot;M&quot;, &quot;F&quot;) AS gender, CASE WHEN MOD(Hashed_fullVisitorID,10) = 0 THEN 55000 WHEN MOD(Hashed_fullVisitorID,10) &lt; 3 THEN 65000 WHEN MOD(Hashed_fullVisitorID,10) &lt; 7 THEN 75000 WHEN MOD(Hashed_fullVisitorID,10) &lt; 9 THEN 85000 WHEN MOD(Hashed_fullVisitorID,10) = 9 THEN 95000 ELSE Hashed_fullVisitorIDEND AS hhiFROM ( SELECT fullVisitorID, ABS(farm_fingerprint(fullVisitorID)) AS Hashed_fullVisitorID, FROM `bigquery-public-data.google_analytics_sample.ga_sessions_*`, UNNEST(hits) AS hits, UNNEST(hits.product) AS hits_product WHERE _TABLE_SUFFIX BETWEEN &#x27;20160801&#x27; AND &#x27;20160831&#x27; AND geoNetwork.country = &#x27;United States&#x27; AND type = &#x27;EVENT&#x27; GROUP BY 1, 2)&#x27;&#x27;&#x27;CRM_view.view_query = CRM_query.format(PROJECT_ID)CRM_view = client.create_table(CRM_view) # API requestprint(f&quot;Successfully created view at &#123;CRM_view.full_table_id&#125;&quot;) 데이터 확인 1234567891011121314# See an output of the synthetic CRM dataCRM_query_df = f&#x27;&#x27;&#x27;SELECT * FROM &#123;CRM_view.full_table_id.replace(&quot;:&quot;, &quot;.&quot;)&#125; LIMIT 5&#x27;&#x27;&#x27;job_config = bigquery.QueryJobConfig()# Start the queryquery_job = client.query(CRM_query_df, job_config=job_config) #API Requestdf_CRM = query_job.result()df_CRM = df_CRM.to_dataframe()df_CRM 클러스터링을 위한 학습 데이터로 사용할 최종뷰 작성1234567891011121314151617181920# Build a final view, which joins GA360 data with CRM datafinal_data_view = &#x27;Final_View&#x27;shared_dataset_ref = client.dataset(DATA_SET_ID)final_view_ref = shared_dataset_ref.table(final_data_view)final_view = bigquery.Table(final_view_ref)final_data_query = f&#x27;&#x27;&#x27;SELECT g.*, c.* EXCEPT(fullVisitorId)FROM &#123;ga360_view.full_table_id.replace(&quot;:&quot;, &quot;.&quot;)&#125; gJOIN &#123;CRM_view.full_table_id.replace(&quot;:&quot;, &quot;.&quot;)&#125; cON g.fullVisitorId = c.fullVisitorId&#x27;&#x27;&#x27;final_view.view_query = final_data_query.format(PROJECT_ID)final_view = client.create_table(final_view) # API requestprint(f&quot;Successfully created view at &#123;final_view.full_table_id&#125;&quot;) 데이터 시각화 1234567891011121314# Show final data used prior to modelingsql_demo = f&#x27;&#x27;&#x27;SELECT * FROM &#123;final_view.full_table_id.replace(&quot;:&quot;, &quot;.&quot;)&#125; LIMIT 5&#x27;&#x27;&#x27;job_config = bigquery.QueryJobConfig()# Start the queryquery_job = client.query(sql_demo, job_config=job_config) #API Requestdf_demo = query_job.result()df_demo = df_demo.to_dataframe()df_demo K-Means Model초기 모델 만들기 초기 k-means model을 구축 아직 최적의 k 또는 다른 하이퍼 파라미터에는 초점을 맞추지 않겠습니다. 몇 가지 추가 사항 클러스터링을 위한 피처로 fullVisitorID가 필요하지 않기 때문에 해당 수준에서 그룹화되어 있더라도 fullVisitorID를 입력에서 제거. 전체 방문자 ID를 피처로 사용해서는 안됨. 범주형 피처와 숫자 피처가 모두 존재 숫자 피처를 정규화할 필요가 없는데, 이는 BigQuery ML이 자동으로 수행하기 때문 12345678910111213141516171819202122def makeModel (n_Clusters, Model_Name): sql =f&#x27;&#x27;&#x27; CREATE OR REPLACE MODEL `&#123;PROJECT_ID&#125;.&#123;DATA_SET_ID&#125;.&#123;Model_Name&#125;` OPTIONS(model_type=&#x27;kmeans&#x27;, kmeans_init_method = &#x27;KMEANS++&#x27;, num_clusters=&#123;n_Clusters&#125;) AS SELECT * except(fullVisitorID, Hashed_fullVisitorID) FROM `&#123;final_view.full_table_id.replace(&quot;:&quot;, &quot;.&quot;)&#125;` &#x27;&#x27;&#x27; job_config = bigquery.QueryJobConfig() client.query(sql, job_config=job_config) # Make an API request.# Let&#x27;s start with a simple test to ensure everything works. # After running makeModel(), allow a few minutes for training to complete.model_test_name = &quot;test&quot;makeModel(3, model_test_name)# After training is completed, you can either check in the UI, or you can interact with it using list_models(). for model in client.list_models(DATA_SET_ID): print(model) 더 나은 모델을 만들기 위한 작업 올바른 k 값을 결정하는 것은 전적으로 사용 사례에 따라 달라짐.ex) 손으로 쓴 숫자를 사전 처리 → k &#x3D; 10 비즈니스 이해관계자가 세 개의 서로 다른 마케팅 캠페인만 제공하고자 하고 세 개의 고객 클러스터를 식별해야 하는 경우 → k&#x3D;3 그러나 현업에서 위의 예시처럼 딱 떨어지는 사용 사례는 거의 없기 때문에 보통 k의 범위를 지정하고 그 안에서 결과값이 좋은 k를 선택하기도 함. k 값을 결정하기 위한 수단으로 엘보우 방법을 수행한후 데이비스-볼딘 점수로 평가함. DBI가 작을수록 cluster를 자세히 구분했다고 말할수 있음(관련 포스트) 아래에서는 엘보 방법을 모두 수행하고 데이비스-볼딘 점수를 얻기 위한 몇 가지 모델을 생성함. low_k, high_k: 하이퍼 파라미터, 두 값 사이의 모델을 생성. 12345678910111213# Define upper and lower bound for k, then build individual models for each. # After running this loop, look at the UI to see several model objects that exist. low_k = 3high_k = 15model_prefix_name = &#x27;kmeans_clusters_&#x27;lst = list(range (low_k, high_k+1)) #build list to iterate through k valuesfor k in lst: model_name = model_prefix_name + str(k) makeModel(k, model_name) print(f&quot;Model started: &#123;model_name&#125;&quot;) 123456# list all current modelsmodels = client.list_models(DATA_SET_ID) # Make an API request.print(&quot;Listing current models:&quot;)for model in models: full_model_id = f&quot;&#123;model.dataset_id&#125;.&#123;model.model_id&#125;&quot; print(full_model_id) 12345# Remove our sample model from BigQuery, so we only have remaining models from our previous loopmodel_id = DATA_SET_ID+&quot;.&quot;+model_test_nameclient.delete_model(model_id) # Make an API request.print(f&quot;Deleted model &#x27;&#123;model_id&#125;&#x27;&quot;) 123456789101112131415161718192021# This will create a dataframe with each model name, the Davies Bouldin Index, and Loss. # It will be used for the elbow method and to help determine optimal Kdf = pd.DataFrame(columns=[&#x27;davies_bouldin_index&#x27;, &#x27;mean_squared_distance&#x27;])models = client.list_models(DATA_SET_ID) # Make an API request.for model in models: full_model_id = f&quot;&#123;model.dataset_id&#125;.&#123;model.model_id&#125;&quot; sql =f&#x27;&#x27;&#x27; SELECT davies_bouldin_index, mean_squared_distance FROM ML.EVALUATE(MODEL `&#123;full_model_id&#125;`) &#x27;&#x27;&#x27; job_config = bigquery.QueryJobConfig() # Start the query, passing in the extra configuration. query_job = client.query(sql, job_config=job_config) # Make an API request. df_temp = query_job.to_dataframe() # Wait for the job to complete. df_temp[&#x27;model_name&#x27;] = model.model_id df = pd.concat([df, df_temp], axis=0) 아래 코드는 원래 이 노트북에서 만든 명명 규칙을 사용했으며, 두 번째 밑줄 뒤에 k 값이 있다고 가정. model_prefix_name 변수를 변경한 경우, 이 코드가 깨질 수 있음. 123456# This will modify the dataframe above, produce a new field with &#x27;n_clusters&#x27;, and will sort for graphingdf[&#x27;n_clusters&#x27;] = df[&#x27;model_name&#x27;].str.split(&#x27;_&#x27;).map(lambda x: x[2])df[&#x27;n_clusters&#x27;] = df[&#x27;n_clusters&#x27;].apply(pd.to_numeric)df = df.sort_values(by=&#x27;n_clusters&#x27;, ascending=True)df 1df.plot.line(x=&#x27;n_clusters&#x27;, y=[&#x27;davies_bouldin_index&#x27;, &#x27;mean_squared_distance&#x27;]) 참고 - 이 노트북을 실행하면 무작위 클러스터 초기화로 인해 다른 결과를 얻을 수 있음. 도달 범위 실행에 대해 일관되게 동일한 클러스터를 반환하려면 하이퍼파라미터 선택을 통해 초기화를 명시적으로 선택가능. k 선택하기: 최적의 k 값을 결정할 때 완벽한 접근 방식이나 프로세스는 정해져 있지 않음. 비즈니스 규칙이나 요구 사항에 따라 결정되는 경우가 많음. 이 예에서는 간단한 요구 사항이 없으므로 다음과 같은 고려 사항을 따를 수도 있음: 항상 그런 것은 아니지만, 증분 클러스터가 손실을 크게 줄이지 못하는 자연스러운 ‘엘보 방법’이 있는 경우가 있음. 이 특정 예에서는, 그리고 종종 발견할 수 있듯이, 안타깝게도 자연스러운 ‘엘보’가 존재하지 않으므로 프로세스를 계속 진행해야 함. 다음으로 데이비스-볼딘과 k를 차트로 표. 이 점수는 각 클러스터가 얼마나 ‘다른지’를 알려주며, 최적 점수는 0. 클러스터가 5개인 경우 점수는 약 1.4이며, k가 9를 초과하는 경우에만 더 나은 값을 볼 수 있음. 마지막으로 각 모델의 차이를 해석하기 시작. 다양한 모델에 대한 평가 모듈을 검토하여 기능의 분포를 이해할 수 있음. 데이터를 통해 성별, 가구 소득, 쇼핑 습관에 따른 패턴을 찾을 수 있음. 최종 클러스터 분석 모델의 특성을 이해하는 데는 두 가지 옵션이 있습니다. BigQuery UI를 살펴보거나 모델 개체와 프로그래밍 방식 아래에서 후자의 옵션에 대한 간단한 예제를 찾을 수 있습니다. 123456789101112131415161718192021model_to_use = &#x27;kmeans_clusters_5&#x27; # User can edit thisfinal_model = DATA_SET_ID+&#x27;.&#x27;+model_to_usesql_get_attributes = f&#x27;&#x27;&#x27;SELECT centroid_id, feature, categorical_valueFROM ML.CENTROIDS(MODEL &#123;final_model&#125;)WHERE feature IN (&#x27;OS&#x27;,&#x27;gender&#x27;)&#x27;&#x27;&#x27;job_config = bigquery.QueryJobConfig()# Start the queryquery_job = client.query(sql_get_attributes, job_config=job_config) #API Requestdf_attributes = query_job.result()df_attributes = df_attributes.to_dataframe()df_attributes.head() 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556# get numerical information about clusterssql_get_numerical_attributes = f&#x27;&#x27;&#x27;WITH T AS (SELECT centroid_id, ARRAY_AGG(STRUCT(feature AS name, ROUND(numerical_value,1) AS value) ORDER BY centroid_id) AS clusterFROM ML.CENTROIDS(MODEL &#123;final_model&#125;)GROUP BY centroid_id),Users AS(SELECT centroid_id, COUNT(*) AS Total_UsersFROM(SELECT * EXCEPT(nearest_centroids_distance)FROM ML.PREDICT(MODEL &#123;final_model&#125;, ( SELECT * FROM &#123;final_view.full_table_id.replace(&quot;:&quot;, &quot;.&quot;)&#125; )))GROUP BY centroid_id)SELECT centroid_id, Total_Users, (SELECT value from unnest(cluster) WHERE name = &#x27;Apparel&#x27;) AS Apparel, (SELECT value from unnest(cluster) WHERE name = &#x27;Office&#x27;) AS Office, (SELECT value from unnest(cluster) WHERE name = &#x27;Electronics&#x27;) AS Electronics, (SELECT value from unnest(cluster) WHERE name = &#x27;LimitedSupply&#x27;) AS LimitedSupply, (SELECT value from unnest(cluster) WHERE name = &#x27;Accessories&#x27;) AS Accessories, (SELECT value from unnest(cluster) WHERE name = &#x27;ShopByBrand&#x27;) AS ShopByBrand, (SELECT value from unnest(cluster) WHERE name = &#x27;Bags&#x27;) AS Bags, (SELECT value from unnest(cluster) WHERE name = &#x27;productPrice_USD&#x27;) AS productPrice_USD, (SELECT value from unnest(cluster) WHERE name = &#x27;hhi&#x27;) AS hhiFROM T LEFT JOIN Users USING(centroid_id)ORDER BY centroid_id ASC&#x27;&#x27;&#x27;job_config = bigquery.QueryJobConfig()# Start the queryquery_job = client.query(sql_get_numerical_attributes, job_config=job_config) #API Requestdf_numerical_attributes = query_job.result()df_numerical_attributes = df_numerical_attributes.to_dataframe()df_numerical_attributes.head() 위의 결과를 분석해보면 1번 클러스터는 유저수가 두번째로 많고, 제일 유저수가 많은 클러스터 2번보다 구매율이 높은걸 알수 있음 2번 클러스터는 가장 인구가 많지만 구매 횟수가 적고 평균 지출액이 적음. 브랜드 충성도가 높다기 보다는 일회성 구매자 3번 클러스터는 의류에 관심이 많고 평균 구매 가격이 제일 높음. 브랜드별로 소비하진 않지만 가치가 가장 높은 고객 4번 클러스터는 브랜드 별로 소비를 많이 하는 고객들이 몰려 있음. 5번 클러스터는 사무용품에 가장 돈을 많이 사용하는 고객 Export to GA360 모델을 완성한 후에는 이를 추론에 사용 아래 코드는 사용자를 점수화하거나 클러스터에 할당하는 방법을 간략하게 설명 이 코드에는 CENTROID_ID라는 레이블이 붙습니다. 이 코드 자체도 도움이 되지만, 이 점수를 다시 GA360으로 수집하는 프로세스를 권장 BigQuery 테이블에서 Google 애널리틱스 360으로 BigQuery ML 예측을 내보내는 가장 쉬운 방법은 MoDeM(마케팅을 위한 모델 배포) 참조 구현을 사용하는 것 MoDeM은 Google 광고, 디스플레이 및 동영상 360, 검색 광고 360에서 최종적으로 활성화할 수 있도록 데이터를 Google 애널리틱스에 로드하는 데 도움이 됨 1234567891011121314151617181920sql_score = f&#x27;&#x27;&#x27;SELECT * EXCEPT(nearest_centroids_distance)FROM ML.PREDICT(MODEL &#123;final_model&#125;, ( SELECT * FROM &#123;final_view.full_table_id.replace(&quot;:&quot;, &quot;.&quot;)&#125; -- LIMIT 1 ))&#x27;&#x27;&#x27;job_config = bigquery.QueryJobConfig()# Start the queryquery_job = client.query(sql_score, job_config=job_config) #API Requestdf_score = query_job.result()df_score = df_score.to_dataframe()df_score Reference http://googleanalytics360.com/board/view.php?bo_table&#x3D;googleanalytics&amp;wr_id&#x3D;34 https://dev-kani.tistory.com/2","categories":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}],"tags":[{"name":"Big Query","slug":"Big-Query","permalink":"https://jmj3047.github.io/tags/Big-Query/"},{"name":"BigQueryML","slug":"BigQueryML","permalink":"https://jmj3047.github.io/tags/BigQueryML/"}],"keywords":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}]},{"title":"Mac VScode GCP 인증 관련 오류","slug":"Mac_GCP_Error","date":"2023-03-08T15:00:00.000Z","updated":"2023-03-19T07:03:36.636Z","comments":true,"path":"2023/03/09/Mac_GCP_Error/","link":"","permalink":"https://jmj3047.github.io/2023/03/09/Mac_GCP_Error/","excerpt":"","text":"gcp내에 있는 예제들을 실행 시킬때면 주피터 노트북으로 gcp를 사용할때 사용자를 인증해야 하는 이슈가 생김 1Error google.auth.exceptions.DefaultCredentialsError: Could not automatically determine credentials. Please set GOOGLE_APPLICATION_CREDENTIALS or explicitly create credentials and re-run the application. mac에서 방법을 찾다가 오류를 해결함 이 노트북의 아래 코드(!gcloud~)를 실행하다가 오류가 남 이 게시물에서 ‘GCP에 데이터 세트 만들고 서비스 계정 생성하기’ 항목의 서비스 계정을 생성하고 json파일을 다운 받아 주었다. 그리고 코드를 수정하여 이렇게 작성했더니 주피터 노트북에서도 bq명령어나 gcloud명령어가 잘 돌아갔다..! 문제는 m1 때문인거 같은데 이거 때문에 vscode까지 지우고 다시 깔았다. 아마 구글이 사용자 인증하는 서비스를 macOS에서는 제공하지 않아서 그런거 같은데.. 물리적으로 json 파일을 저장하지 않고도 credential 사용해서 바로 연결 할수 있는 방법을 찾았다면 추후에 포스팅 하겠다.","categories":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}],"tags":[{"name":"GCP","slug":"GCP","permalink":"https://jmj3047.github.io/tags/GCP/"},{"name":"Auth","slug":"Auth","permalink":"https://jmj3047.github.io/tags/Auth/"},{"name":"Error","slug":"Error","permalink":"https://jmj3047.github.io/tags/Error/"}],"keywords":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}]},{"title":"BQML을 이용한 게임유저 경향 모델링","slug":"Game_Modeling","date":"2023-03-07T15:00:00.000Z","updated":"2023-03-19T07:02:35.325Z","comments":true,"path":"2023/03/08/Game_Modeling/","link":"","permalink":"https://jmj3047.github.io/2023/03/08/Game_Modeling/","excerpt":"","text":"개요 빅쿼리 ML을 사용하여 다양한 머신러닝 모델을 돌리기 GA4와 빅쿼리 연동 시 추출되는 데이터들을 정제해서 머신러닝 훈련데이터로 만들기 각 모델의 평가, 파라미터들을 알아보고 조정해보기 목표 앱 설치 후 첫 24시간 동안의 사용자 활동을 기반으로 하는 “Flood It!” 데이터 세트를 사용하여 다양한 분류 모델을 시도하여 이탈 성향(1) 또는 이탈하지 않을 성향(0)을 예측 비용 BigQuery 가격 분석 가격 책정: SQL 쿼리, 사용자 정의 함수, 스크립트, 테이블을 스캔하는 DML(데이터 조작 언어) 및 DDL(데이터 정의 언어)문을 포함한 쿼리를 처리할 때 발생하는 비용 주문분석형 가격책정 (주문형)쿼리: 5$ per TB, 매월 1TB까지는 무료 정액제 월간 정액제: 슬롯 100개당 2000$ 연간 정액제: 슬롯 100개당 1700$(달마다) 스토리지 가격 책정: BigQuery에 로드한 데이터를 저장하는 데 드는 비용 BigQueyrML 가격 무료사용량 한도 스토리지: 매월 10GB 쿼리(분석): 매월 처리되는 쿼리 데이터중 최초 1TB는 무료 BigQuery Storage Write API: 매월 처음 2TB는 무료 BigQuery ML CREATE MODEL쿼리: 매월 10GB까지는 CREATE MODEL 문이 포함된 쿼리 데이터가 무료로 처리 주문형 가격 데이터 학습 데이터로 정제 하기 앱으로 복귀할 가능성이 없는 사용자를 필터링합니다. 사용자 인구통계 데이터에 대한 특성을 만듭니다. 사용자 행동 데이터에 대한 특성을 만듭니다. 인구통계 데이터 및 행동 데이터를 결합하면 더 효과적인 예측 모델을 만드는 데 도움이 된다. 처리 후 학습 데이터의 각 행은 user_pseudo_id 열로 식별된 순 사용자의 데이터를 나타낸다. 전체 데이터 조회→ GA4에서 넘어온 데이터 형식(스키마 및 각 열에 대한 세부 정보) 12345SELECT *FROM `firebase-public-project.analytics_153293282.events_*` TABLESAMPLE SYSTEM (1 PERCENT) 총 15000명의 유저와 5.7만개의 이벤트가 있는걸 볼수 있음 12345SELECT COUNT(DISTINCT user_pseudo_id) as count_distinct_users, COUNT(event_timestamp) as count_eventsFROM `firebase-public-project.analytics_153293282.events_*` 학습 데이터로 정제하기 User ID: 고유한 사용자 ID User demographic data: 인구 통계 학적 데이터 User behavioral data: 행동 데이터 Churned: 예측하고자 하는 실제 라벨(1&#x3D;이탈, 0&#x3D;귀환) STEP 1: 각 유저에 대한 라벨 식별 사용자 이탈을 정의하는 방법에는 여러 가지가 있지만, 이 노트북에서는 사용자가 앱을 처음 사용한 후 24시간이 지나도 다시 앱을 사용하지 않는 사용자를 1일 이탈로 예측 → 사용자가 앱에 처음 참여한 후 24시간이 지난 후를 기준으로 합니다: 사용자가 그 이후 이벤트 데이터를 표시하지 않는 경우, 해당 사용자는 탈퇴한 것으로 간주됩니다. 사용자가 그 이후 이벤트 데이터 포인트가 하나 이상 있으면, 해당 사용자는 귀환한 것으로 간주됩니다. 앱에서 단 몇 분만 사용한 후 다시 돌아올 가능성이 낮은 사용자를 제거하고자 할 수도 있는데, 이를 “bouncing”이라고도 합니다. 예를 들어, 앱을 최소 10분 이상 사용한 사용자(이탈하지 않은 사용자)를 대상으로만 모델을 구축하고자 한다고 가정해 보겠습니다. 따라서 이 노트북의 이탈한 사용자에 대한 업데이트된 정의 “앱에서 10분 이상 시간을 보냈지만 앱을 처음 사용한 후 24시간이 지난 후 다시는 앱을 사용하지 않은 모든 사용자” 1234567891011121314151617181920212223242526272829303132333435363738394041424344CREATE OR REPLACE VIEW `bqmlga4.returningusers` AS ( WITH firstlasttouch AS ( SELECT user_pseudo_id, MIN(event_timestamp) AS user_first_engagement, MAX(event_timestamp) AS user_last_engagement FROM `firebase-public-project.analytics_153293282.events_*` WHERE event_name=&quot;user_engagement&quot; GROUP BY user_pseudo_id ) SELECT user_pseudo_id, user_first_engagement, user_last_engagement, EXTRACT(MONTH from TIMESTAMP_MICROS(user_first_engagement)) as month, EXTRACT(DAYOFYEAR from TIMESTAMP_MICROS(user_first_engagement)) as julianday, EXTRACT(DAYOFWEEK from TIMESTAMP_MICROS(user_first_engagement)) as dayofweek, #add 24 hr to user&#x27;s first touch (user_first_engagement + 86400000000) AS ts_24hr_after_first_engagement,#churned = 1 if last_touch within 24 hr of app installation, else 0IF (user_last_engagement &lt; (user_first_engagement + 86400000000), 1, 0 ) AS churned,#bounced = 1 if last_touch within 10 min, else 0IF (user_last_engagement &lt;= (user_first_engagement + 600000000), 1, 0 ) AS bounced, FROM firstlasttouch GROUP BY 1,2,3 );SELECT * FROM `bqmlga4.returningusers` Churned 열의 경우 첫 24시간이 지난 후에 액션을 수행하면 Churned &#x3D;0이 되고 그렇지 않으면 마지막 액션이 첫 24시간 이내에만 이루어진 경우 Churned&#x3D;1이 된다. Bounced 열의 경우 사용자의 마지막 동작이 앱을 처음 터치한 후 10분 이내인 경우 bounced &#x3D; 1, 그렇지 않으면 bounced&#x3D;0 이 15000명의 사용자 중 몇명이 이탈했다가 다시 돌아왔는지 확인 12345678SELECT bounced, churned, COUNT(churned) as count_usersFROM bqmlga4.returningusersGROUP BY 1,2ORDER BY bounced 15000명의 사용자를 기준으로 5,557명(41%)의 사용자가 앱을 처음 사용한 후 10분 이내에 이탈했지만 나머지 8,031명의 사용자 중 1,883(23%)이 24시간 후에 이탈한 것을 확인 학습 데이터의 경우 bounce&#x3D;0인 데이터만 사용함 → 이미 귀환한 유저에 대해서는 할 필요가 없기 때문 12345SELECT COUNTIF(churned=1)/COUNT(churned) as churn_rateFROM bqmlga4.returningusersWHERE bounced = 0 STEP 2: 각 사용자에 대한 인구 통계 데이터 추출 앱정보, 기기, 이커머스, 이벤트 파라미터, 지역 등 사용자에 대한 인구통계학적 데이터 추출 자체 데이터 세트를 사용하고 있고 조인 가능한 first-party data가 있는 경우 이 섹션은 GA4에서 쉽게 사용할 수 없는 각 사용자에 대한 추가 속성을 추가할 수 있는 좋은 기회 인구통계는 변경가능성이 있음. 간단하게 설명하기 위해 사용자가 앱에 처음접속 했을때 구글 애널리틱스 4가 제공하는 인구통계학적 정보를 MIN(event_timestamp)로 표시된대로만 사용 123456789101112131415161718192021CREATE OR REPLACE VIEW bqmlga4.user_demographics AS ( WITH first_values AS ( SELECT user_pseudo_id, geo.country as country, device.operating_system as operating_system, device.language as language, ROW_NUMBER() OVER (PARTITION BY user_pseudo_id ORDER BY event_timestamp DESC) AS row_num FROM `firebase-public-project.analytics_153293282.events_*` WHERE event_name=&quot;user_engagement&quot; ) SELECT * EXCEPT (row_num) FROM first_values WHERE row_num = 1 );SELECT *FROM bqmlga4.user_demographics STEP 3: 각 사용자에 대한 행동 데이터 추출 첫 24시간 동안의 사용자 활동을 기반으로 해당 사용자의 이탈 또는 재방문 여부를 예측 → 첫 24시간 행동 데이터만 학습 시켜야 함 ‘user_first_engagement’에서 첫 인게이지먼트의 월 또는 일과 같은 추가 시간 관련 기능을 추출할 수도 있음 우선 event_name을 기준으로 데이터 세트에 존재하는 모든 고유 이벤트를 탐색 → 총 37개의 이벤트 12345678SELECT event_name, COUNT(event_name) as event_countFROM `firebase-public-project.analytics_153293282.events_*`GROUP BY 1ORDER BY event_count DESC 여기서 user_engagement, level_start_quickplay, level_end_quickplay, level_complete_quickplay, level_reset_quickplay, post_score, spend_virtual_currency, ad_reward, challenge_a_friend, completed_5_levels, use_extra_steps → 이 피처들만 가지고 각각 유저가 몇번이나 이 이벤트를 발생시켰는지 확인 자체 데이터 세트를 사용하는 경우 집계 및 추출할 수 있는 이벤트 유형이 다를 수 있음. 앱이 GA에 매우 다른 event_names를 전송할수 있으므로 시나리오 가장 적합한 이벤트를 사용해야 함 1234567891011121314151617181920212223242526272829303132333435363738CREATE OR REPLACE VIEW bqmlga4.user_aggregate_behavior AS (WITH events_first24hr AS ( #select user data only from first 24 hr of using the app SELECT e.* FROM `firebase-public-project.analytics_153293282.events_*` e JOIN bqmlga4.returningusers r ON e.user_pseudo_id = r.user_pseudo_id WHERE e.event_timestamp &lt;= r.ts_24hr_after_first_engagement )SELECT user_pseudo_id, SUM(IF(event_name = &#x27;user_engagement&#x27;, 1, 0)) AS cnt_user_engagement, SUM(IF(event_name = &#x27;level_start_quickplay&#x27;, 1, 0)) AS cnt_level_start_quickplay, SUM(IF(event_name = &#x27;level_end_quickplay&#x27;, 1, 0)) AS cnt_level_end_quickplay, SUM(IF(event_name = &#x27;level_complete_quickplay&#x27;, 1, 0)) AS cnt_level_complete_quickplay, SUM(IF(event_name = &#x27;level_reset_quickplay&#x27;, 1, 0)) AS cnt_level_reset_quickplay, SUM(IF(event_name = &#x27;post_score&#x27;, 1, 0)) AS cnt_post_score, SUM(IF(event_name = &#x27;spend_virtual_currency&#x27;, 1, 0)) AS cnt_spend_virtual_currency, SUM(IF(event_name = &#x27;ad_reward&#x27;, 1, 0)) AS cnt_ad_reward, SUM(IF(event_name = &#x27;challenge_a_friend&#x27;, 1, 0)) AS cnt_challenge_a_friend, SUM(IF(event_name = &#x27;completed_5_levels&#x27;, 1, 0)) AS cnt_completed_5_levels, SUM(IF(event_name = &#x27;use_extra_steps&#x27;, 1, 0)) AS cnt_use_extra_steps,FROM events_first24hrGROUP BY 1 );SELECT *FROM bqmlga4.user_aggregate_behavior 이 단계는 동작 수행 빈도 외에도 사용자가 사용한 게임 내 화폐의 총액이나 앱과 더 관련이 있을수 있는 특정 앱별 마일스톤(예: 특정 임계값의 경험치 획득 또는 5회 이상 레벨업)에 도달했는지 여부와 같은 다른 행동 특징을 포함할수 있다는 점에 유의. STEP 4: 세 데이터 결합하여 학습데이터 구축 최종 학습 데이터 베이스 구축: 여기어세 bounce&#x3D;0을 지정하여 앱 사용후 처음 10분 이내에 bounce하지 않은 사용자로만 학습 데이터를 제할할 수도 있음. 12345678910111213141516171819202122232425262728293031323334353637CREATE OR REPLACE VIEW bqmlga4.train AS ( SELECT dem.*, IFNULL(beh.cnt_user_engagement, 0) AS cnt_user_engagement, IFNULL(beh.cnt_level_start_quickplay, 0) AS cnt_level_start_quickplay, IFNULL(beh.cnt_level_end_quickplay, 0) AS cnt_level_end_quickplay, IFNULL(beh.cnt_level_complete_quickplay, 0) AS cnt_level_complete_quickplay, IFNULL(beh.cnt_level_reset_quickplay, 0) AS cnt_level_reset_quickplay, IFNULL(beh.cnt_post_score, 0) AS cnt_post_score, IFNULL(beh.cnt_spend_virtual_currency, 0) AS cnt_spend_virtual_currency, IFNULL(beh.cnt_ad_reward, 0) AS cnt_ad_reward, IFNULL(beh.cnt_challenge_a_friend, 0) AS cnt_challenge_a_friend, IFNULL(beh.cnt_completed_5_levels, 0) AS cnt_completed_5_levels, IFNULL(beh.cnt_use_extra_steps, 0) AS cnt_use_extra_steps, ret.user_first_engagement, ret.month, ret.julianday, ret.dayofweek, ret.churned FROM bqmlga4.returningusers ret LEFT OUTER JOIN bqmlga4.user_demographics dem ON ret.user_pseudo_id = dem.user_pseudo_id LEFT OUTER JOIN bqmlga4.user_aggregate_behavior beh ON ret.user_pseudo_id = beh.user_pseudo_id WHERE ret.bounced = 0 );SELECT *FROM bqmlga4.train 학습데이터로 빅쿼리 ML 학습하기 이진분류 작업이므로 간단하게 logistic regression으로 시작할수 있지만 다른 모델도 사용 가능하다. M o d e l Advantage Disadvantage Logistic Regression LOGISTIC_REG 다른 타입에 비해 학습 시간이 빠름 모델 성능이 조금 떨어짐 XGBoost BOOSTED_TREE_CLASSIFIER 높은 모델 수행 능력 feature importance를 검사할수 있다, LOGISTIC_REG 에 비해 학습 시간이 느림 Deep Neural Networks DNN_CLASSIFIER 높은 모델 수행 능력 LOGISTIC_REG 에 비해 학습 시간이 느림 AutoML AUTOML_CLASSIFIER 매우 높은 모델 수행 능력 적어도 몇시간 정도 훈련시간이 걸리고 모델이 어떻게 작동하는지 설명하기 쉽지 않음 훈련, 테스트 셋으로 분할하지 않아도 CREATE MODEL문을 실행하면 BigQuery ML이 자동적으로 학습하기 때문에 학습 후 바로 모델을 평가 할수 있음 하이퍼 파라미터 튜닝: 각 모델에 대한 하이퍼파라미터를 튜닝할수 도 있음(link) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354CREATE OR REPLACE MODEL bqmlga4.churn_logregOPTIONS( MODEL_TYPE=&quot;LOGISTIC_REG&quot;, INPUT_LABEL_COLS=[&quot;churned&quot;]) ASSELECT *FROM bqmlga4.train---CREATE OR REPLACE MODEL bqmlga4.churn_xgbOPTIONS( MODEL_TYPE=&quot;BOOSTED_TREE_CLASSIFIER&quot;, INPUT_LABEL_COLS=[&quot;churned&quot;]) ASSELECT * EXCEPT(user_pseudo_id)FROM bqmlga4.train---CREATE OR REPLACE MODEL bqmlga4.churn_dnnOPTIONS( MODEL_TYPE=&quot;DNN_CLASSIFIER&quot;, INPUT_LABEL_COLS=[&quot;churned&quot;]) ASSELECT * EXCEPT(user_pseudo_id)FROM bqmlga4.train---CREATE OR REPLACE MODEL bqmlga4.churn_automlOPTIONS( MODEL_TYPE=&quot;AUTOML_CLASSIFIER&quot;, INPUT_LABEL_COLS=[&quot;churned&quot;], BUDGET_HOURS=1.0) ASSELECT * EXCEPT(user_pseudo_id)FROM bqmlga4.train AutoML AutoML Tables를 사용하면 구조화된 데이터에 대한 최신 머신러닝 모델을 속도와 규모를 대폭 향상 시켜 자동으로 구축할 수 있음. AutoML Tables는 구글의 모델 집단에서 구조화된 데이터를 자동으로 검색하여 간단한 데이터 집합을 위한 선형&#x2F;로지스틱 휘귀모델부터 더 크고 복잡한 데이터 집합을 위한 고급 심층, 앙상블 및 아키텍쳐 검색 방법에 이르기까지 사용자의 요구에 가장 적합한 모델을 찾아줌. BUDGET_HOURS 매개변수는 AutoML 테이블 학습을 위한 것으로 시간 단위로 지정됨. 기본값은 1.0이며 1.0에서 72.0사이여야 함. 모델 평가Logistic Regression1234SELECT *FROM ML.EVALUATE(MODEL bqmlga4.churn_logreg) XGBoost1234SELECT *FROM ML.EVALUATE(MODEL bqmlga4.churn_xgb) DNN1234SELECT *FROM ML.EVALUATE(MODEL bqmlga4.churn_dnn) AutoML1234SELECT *FROM ML.EVALUATE(MODEL bqmlga4.churn_automl) 모델 예측 이탈 성향에 대한 예측을 할 수 있음 12345SELECT *FROM ML.PREDICT(MODEL bqmlga4.churn_logreg, (SELECT * FROM bqmlga4.train)) #can be replaced with a test dataset 성향 모델링에서 가장 중요한 출력은 행동이 발생할 확률. 밑의 코드는 사용자가 24시간 후에 재방문할 확률을 반환 → 확률이 높고 1에 가까울수록 사용자가 이탈할 가능성이 높고, 0에 가까울수록 사용자가 재방문할 가능성이 높음 123456789SELECT user_pseudo_id, churned, predicted_churned, predicted_churned_probs[OFFSET(0)].prob as probability_churned FROM ML.PREDICT(MODEL bqmlga4.churn_logreg, (SELECT * FROM bqmlga4.train)) #can be replaced with a proper test dataset 빅쿼리 밖으로 예측 결과 export Bigquery Storage API를 사용하여 데이터를 Pandas 데이터 프레임으로 내보낼수 있음(문서 및 코드 샘플) 다른 BigQuery 클라이언트 라이브러리를 사용할수도 있음. 별도의 서비스에서 사용할 수 있도록 예측 테이블을 Google 클라우드 스토리지(GCS)로 직접 보낼수도 있음 → 가장 쉬운 방법은 SQL을 사용하여 GCS로 직접 보내는 것 123456789101112EXPORT DATA OPTIONS (uri=&quot;gs://mybucket/myfile/churnpredictions.csv&quot;, format=CSV) AS SELECT user_pseudo_id, churned, predicted_churned, predicted_churned_probs[OFFSET(0)].prob as probability_churnedFROM ML.PREDICT(MODEL bqmlga4.churn_logreg, (SELECT * FROM bqmlga4.train)) #can be replaced with a proper test dataset Reference https://cloud.google.com/architecture/propensity-modeling-gaming?hl=ko https://github.com/GoogleCloudPlatform/analytics-componentized-patterns/tree/master/gaming/propensity-model/bqml","categories":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}],"tags":[{"name":"Big Query","slug":"Big-Query","permalink":"https://jmj3047.github.io/tags/Big-Query/"},{"name":"BigQueryML","slug":"BigQueryML","permalink":"https://jmj3047.github.io/tags/BigQueryML/"},{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"ML Process","slug":"ML-Process","permalink":"https://jmj3047.github.io/tags/ML-Process/"}],"keywords":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}]},{"title":"로지스틱 회귀 모델 평가시 나오는 용어들 정리","slug":"Logisitic_Eval_words","date":"2023-03-05T15:00:00.000Z","updated":"2023-03-19T07:03:43.987Z","comments":true,"path":"2023/03/06/Logisitic_Eval_words/","link":"","permalink":"https://jmj3047.github.io/2023/03/06/Logisitic_Eval_words/","excerpt":"","text":"개요 이전 포스트의 5단계에서 ML 모델을 평가할때 나왔던 지표들에 대한 소개 바이너리 로지스틱 회귀 모델을 사용했을 때 모델의 성능을 평가하는 지표들을 소개 알아야 할 개념True&#x2F;False &amp; Positive&#x2F;Negative 임계값(Threshold) 로지스틱 회귀 값을 이진 카테고리에 매핑하려면 분류 임계값(결정 임계값)을 정의 해야 함 일반적으로 0.5로 잡지만, 데이터나 상황에 따라서 달라질수 있으므로 조정해야 하는 값. 정밀도(Precision) &#x3D; $\\frac{TP}{TP+FP}$ 거짓 양성(FP)이 생성되지 않는 모델의 정밀도는 1.0 재현율(Recall) &#x3D; $\\frac{TP}{TP+FN}$ 거짓 음성(FN)을 생성하지 않는 모델의 재현율은 1.0 실제 양성 중 정확히 양성이라고 식별된 사례의 비율 정확도(Accuracy) &#x3D; $\\frac{올바른 예측값}{전체 예측 값}$ 이진분류에서 정확도 &#x3D; $\\frac{TP+TN}{TP+TN+FP+FN}$ 비 공식적으로 모델이 올바르게 예측한 비율 클래스간 데이터 차이가 있다면 정확하지 않을 가능성이 높음 ROC curve 모든 분류 임계값에서 분류 모델의 성능을 보여주는 그래프 참 양성률(TPR)과 거짓양성률(FPR)을 매개변수로 표시한다. 분류 임계값을 낮추면 더 많은 항목이 양수로 분류 되므로 거짓양성과 참 양성이 모두 증가한다. ROC 곡선의 포인트를 계사나기 위해 분류 입곗값이 다른 여러차례의 로지스틱 회귀모델을 평가했지만 이는 비효율적 → 이러한 정보제공을 할수 있는 효율적인 정렬기반 알고리즘인 AUC를 사용하는 이유 AUC: ROC 곡선 아래의 영역 ROC 곡선의 적분값 0부터 1사의 값 절댓값이 아닌 예측의 순위를 측정하므로 규모 불변 선택한 분유 임곗값과 관계 없이 모델의 예측 품질을 측정하기에 분류 기준 불변 주의 사항 확장 불변이 항상 바람직한것은 아님 → 잘 보정된 확률 출력이 필요한 경우가 있는데 AUC는 이를 알려주지 않음 분류 임계값 불변이 항상 바람직한 것은 아님 → 거짓음성 대비 거짓양성의 비용이 큰 경우 한 자기 유형의 분류 오류를 최소화 하는 것이 중요함. 예를 들어 스팸 감지를 실행할때 거짓 양성을 최소화하는 것이 중요할 수 있음.(이렇게 하면 거짓 음성이 크게 증가하더라도 상관 없음) AUC는 이 유형의 최적화에 유용한 측정항목이 아님. GCP 공식 설명 Aggregate Metrics For binary classification models, all metrics reflect the values computed when threshold is set to 0.5. 이진 분류 모델의 경우 모든 지표는 임계값을 0.5로 설정했을 때 계산된 값을 반영합니다. Threshold : For binary classification models, this is the positive class threshold. 이진 분류 모델의 경우 이는 양성 클래스의 임계값을 뜻합니다. Precision : Fraction of predicted positives that were actual positives. 예측된 양성 반응 중 실제 양성 반응으로 판명된 비율입니다. → 학습 모델이 판단한 양성 비율 Recall : Fraction of actual positives that were predicted positives. 예측된 양성 반응 중 실제 양성 반응의 비율입니다. → 실제 양성이라고 나온 비율 Accuracy : Fraction of predictions given the correct label. 올바른 레이블이 주어진 예측의 비율입니다. → precision과 recall 을 비교했을 때 학습 모델이 맞춘 비율 F1 score : Harmonic mean of precision and recall. 정확도 및 회수율의 조화 평균입니다. Precision 과 Recall은 서로 Trade-off 관계를 가지면서 접근하는 방식도 Precision은 모델의 예측, Recall은 정답 데이터 기준이므로 서로 상이 합니다. 하지만 두 지표 모두 모델의 성능을 확인하는 데 중요하므로 둘 다 사용되어야 합니다. 따라서 두 지표를 평균값으로 통해 하나의 값으로 나타내는 방법으로 F1 score를 사용합니다. 극단적으로 Precision과 Recall 중에 한쪽이 1에 가깝고 한쪽이 0에 가까운 경우 산술 평균과 같이 0.5가 아니라 0에 가깝도록 만들어 줍니다. 따라서 F1 score를 높이려면 Precision, Recall이 균일한 값이 필요 하기 때문에 두 지표 성능을 모두 높일 수 있도록 해야 합니다. $F_1 &#x3D; 2*\\frac{precision X recall}{precision + recall}$ Log loss : A measure for model performance between 0 (perfect) and 1. The greater the log-loss, the greater the predicted probability diverges from the actual label. 0(완벽)에서 1 사이의 모델 성능 측정값입니다. 로그 손실이 클수록 예측 확률이 실제 레이블과 차이가 커집니다. ROC AUC : Area under the receiver operating characteristic curve. Score threshold Positive class threshold: Predictions above the threshold are classified as positive. 임계값을 초과하는 예측은 양수로 분류됩니다. Positive class &gt;50K Negative class &lt;&#x3D;50K Precision : Fraction of predicted positives that were actual positives. 예측된 양성 반응 중 실제 양성 반응으로 판명된 비율입니다. Recall: Fraction of actual positives that were predicted positives. 예측된 양성 반응 중 실제 양성 반응의 비율입니다. Accuracy : Fraction of actual positives that were predicted positives. 올바른 레이블이 주어진 예측의 비율입니다. F1 score : Harmonic mean of precision and recall. 정밀도 및 회수율의 조화 평균입니다. Precision-recall by threshold: Shows how your model performs on the top-scored label along the full range of confidence threshold values. A higher confidence threshold produces fewer false positives, which increases precision. A lower confidence threshold produces fewer false negatives, which increases recall. 전체 신뢰도 임계값 범위에서 가장 높은 점수를 받은 레이블에 대한 모델의 성능을 표시합니다. 신뢰도 임계값이 높을수록 false positive 값이 줄어들어 정밀도가 높아집니다. 신뢰도 임계값이 낮을수록 false negative 값이 줄어들어 회수율이 높아집니다. Precision-recall curve: Shows the trade-off between precision and recall at different confidence thresholds. A lower threshold results in higher recall but typically lower precision, while a higher threshold results in lower recall but typically with higher precision. 서로 다른 신뢰 임계값에서 정밀도와 리콜 간의 균형을 표시합니다. 임계값이 낮을수록 회수율은 높지만 일반적으로 정밀도는 낮으며 임계값이 높을수록 회수율은 낮아지지만 일반적으로 정밀도가 높습니다. ROC curve: The receiver operating characteristic (ROC) curve shows the trade-off between true positive rate and false positive rate. A lower threshold results in a higher true positive rate (and a higher false positive rate), while a higher threshold results in a lower true positive rate (and a lower false positive rate) receiver operating characteristic(ROC) 곡선은 true positive의 비율과 false positive의 비율 사이의 균형을 보여줍니다. 임계값이 낮을수록 true positive의 비율(및 false positive의 비율)이 높아지고, 임계값이 높을수록 true positive의 비율(및 false positive의 비율)이 낮아집니다 Confusion matrix This table shows how often the model classified each label correctly (in blue), and which labels were most often confused for that label (in gray). 이 표는 모델이 각 레이블을 올바르게 분류한 빈도(파란색)와 해당 레이블과 가장 자주 혼동되는 레이블(회색)을 보여줍니다. Reference https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall?hl=ko https://developers.google.com/machine-learning/crash-course/classification/true-false-positive-negative?hl=ko https://gaussian37.github.io/ml-concept-ml-evaluation/#f1-score-1","categories":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}],"tags":[{"name":"Big Query","slug":"Big-Query","permalink":"https://jmj3047.github.io/tags/Big-Query/"},{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"}],"keywords":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}]},{"title":"Python으로 kaggle 데이터 GCP에 적재","slug":"Kaggle_GCP","date":"2023-03-02T15:00:00.000Z","updated":"2023-03-19T07:03:08.028Z","comments":true,"path":"2023/03/03/Kaggle_GCP/","link":"","permalink":"https://jmj3047.github.io/2023/03/03/Kaggle_GCP/","excerpt":"","text":"요약 Kaggle 데이터 다운로드 GCP에 데이터 세트 만들고 서비스 계정 생성하기 Python-BigQuery 연결 후 데이터 조회 데이터 적재 하기 Kaggle 데이터 다운로드 kaggle을 설치한다 1!pip install kaggle kaggle의 key를 받아온다 123!mkdir ~/.kaggle!echo &#x27;&#123;&quot;username&quot;:&quot;your_id&quot;,&quot;key&quot;:&quot;your_key&quot;&#125;&#x27; &gt; ~/.kaggle/kaggle.json!chmod 600 ~/.kaggle/kaggle.json kaggel TOKEN 받아오기 kaggle에 접속한다음 프로필을 선택하고 Account를 누른다 화면을 내리면 API 탭을 찾을수 있다. Create New API Token을 누르고 다운 kaggle.json 파일을 받아줍니다. 한번 발급 받으면 이전의 것은 알려주지 않기 때문에 잊어버렸다면 Expire API TOKEN 으로 모두 지고 새로운 토큰을 받는다. kaggle에서 원하는 데이터를 다운 받아준다 → competitions에서 원하는 competition을 선택하고 data 탭으로 가면 다운 받을 수 있는 api가 있다. GCP에 데이터 세트 만들고 서비스 계정 생성하기 GCP에 프로젝트를 만들고 새로운 데이터 세트를 만들어 준다 중간에 Default table expiration 을 클릭하면 테이블이 며칠 후에 만료되는지도 설정 가능하다 서비스 계정 생성 GCP 좌측 상단 ‘탐색 메뉴’ 클릭후 ‘IAM 및 관리자’의 ‘서비스 계정’으로 이동 ‘+ 서비스 계정 만들기’ 클릭 후 서비스 계정 만들기 진행 서비스 계정 키 생성 후 JSON 파일 추출 Email에 생성된 서비스 계정 클릭 → 페이지 상단에 KEYS → ADD KEY → Create new key config 폴더 생성 후 해당 경로에 json 파일 다운로드 서비스 계정에 빅쿼리 관련 역할 추가 GCP 좌측 상단 ‘탐색 메뉴’ 클릭후 ‘IAM 및 관리자’의 ‘IAM’으로 이동 ‘추가’ 클릭 - 생성된 서비스 계정 이메일 추가 및 ‘BigQuery 관리자’ 역할 선택 후 저장 Python-BigQuery 연결 후 데이터 조회 구글 클라우드 빅쿼리 클라이언트 설치 1!pip install google-cloud-bigquery 서비스 계정 키 설정 → 빅쿼리 클라이언트 정의 → 데이터 조회 쿼리 실행 데이터 적재 하기 데이터를 불러온다 1234567import pandas as pdBASE_DIR = &quot;./&quot;train = pd.read_csv(BASE_DIR + &#x27;train.csv&#x27;)test = pd.read_csv(BASE_DIR + &#x27;test.csv&#x27;)train.shape, test.shape 1((3000888, 6), (28512, 5)) 데이터를 적재 한다 12345678910from google.oauth2 import service_accountimport pandas_gbqcd = service_account.Credentials.from_service_account_file(&quot;./config/my-project-230227-c3691227da1d.json&quot;)project_id = &#x27;my-project-230227&#x27;train_table = &#x27;kaggle_data.train&#x27;test_table = &#x27;kaggle_data.test&#x27;train.to_gbq(train_table,project_id,if_exists=&#x27;replace&#x27;,credentials=cd) test.to_gbq(test_table,project_id,if_exists=&#x27;replace&#x27;,credentials=cd) print(&#x27;migration complete&#x27;) GCP에 들어가서 확인해 본다 이렇게 테이블을 gcp에 적재하면 데이터들을 다양한 방면으로 사용이 가능하다: Looker Studio를 이용한 시각화 Reference https://dschloe.github.io/gcp/bigquery/01_settings&#x2F;python_bigquery&#x2F; https:&#x2F;&#x2F;velog.io&#x2F;@skyepodium&#x2F;Kaggle-API-사용법 https://wooiljeong.github.io/python/python-bigquery/","categories":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}],"tags":[{"name":"Big Query","slug":"Big-Query","permalink":"https://jmj3047.github.io/tags/Big-Query/"},{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/tags/Python/"},{"name":"kaggle","slug":"kaggle","permalink":"https://jmj3047.github.io/tags/kaggle/"}],"keywords":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}]},{"title":"GCP - Looker Studio 연결하여 대시보드 작성","slug":"GCP_LookerStudio","date":"2023-03-01T15:00:00.000Z","updated":"2023-03-19T07:02:38.982Z","comments":true,"path":"2023/03/02/GCP_LookerStudio/","link":"","permalink":"https://jmj3047.github.io/2023/03/02/GCP_LookerStudio/","excerpt":"","text":"개요 GCP - Looker Studio 연결해서 대시보드 작성하기 bigquery-public-data.ml_datasets.census_adult_income 데이터 사용 목표 대시보드로 데이터를 시각화 하여 인사이트를 도출해본다. 개인의 연간 소등이 50,000달러 이상인지 예측하기 를 위해 지표들의 상관관계를 확인해본다. public-dataset 가져오기 이전 포스트의 [0단계]https://jmj3047.github.io/2023/02/28/BQML_Pred/)와 동일하다. 데이터 확인 및 EDA Looker Studio를 사용하여 데이터를 확인하고 EDA를 하는 방법 전체 데이터 가져와서 차트 만들기 Big Query를 사용하여 SQL로 데이터를 정제한 후 Looker Studio에 SQL로 차트 만들기 전체 데이터를 가져와서 차트 만들기 Big Query를 사용할 필요가 없다. 버튼으로 간단하게 조작이 가능하다. 지표당 간단한 합계, 평균, 집계, 최솟값, 최대값, 중앙값, 표준편차, 분산 까지의 연산은 버튼식으로 수정할수 있으며, 함수 또한 필드추가의 항목으로 만드는 것이 가능하다. GCP에 데이터 세트가 업로드 되어 있다면, Looker Studio를 바로 열어서 데이터 추가 버튼을 누른다. Big Query를 누르고 GCP 데이터 세트를 찾는다. public data set을 사용하는 경우라면 공개 데이터 집합으로 들어가야 한다. 추가 버튼을 누르면 화면 왼쪽에 데이터 세트의 이름과 그 안의 스키마 정보들이 뜬다. 원하는 대시보드를 만들기 위해 차트나 표를 추가하여 자유롭게 대시보드를 구성하면 된다. 예시: https://lookerstudio.google.com/reporting/10e2c716-6289-4e07-a2de-da6cdac415b6 Big Query를 사용하여 SQL로 데이터를 정제한 후 Looker Studio에 SQL로 차트 만들기 표에 있는 지표를 가공하여 보고 싶을때 유용하다. 버튼으로 조작하는 것보다 더 많은 지표 표현이 가능하다. 유지 보수의 어려움이 있다. 사용할 데이터의 지표들을 확인해보면 아래처럼 나타낼 수 있다. age(나이): 개인의 나이를 연단위로 나타냅니다 workclass(노동 계급): 개인의 고용형태 Private, ?, Local-gov , Self-emp-inc, Federal-gov, State-gov, Self-emp-not-inc, Never-worked, Witout-pay functional_weight: 일련의 관측결과를 바탕으로 인구조사국이 부여하는 개인의 가중치 education: 개인의 최종학력 education_num: 교육수준을 숫자로 범주화 하여 열거 합니다. 숫자가 높을수록 개인의 교육수준이 높습니다. 11: Assoc_voc: 전문학교 준학사 13: Bachelors: 학사 9: HS-grad: 고등학교 졸업 marital_status: 개인의 결혼 여부 입니다. Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse occupation: 개인의 직업입니다. relationship: 가정 내 각 개인의 관계입니다. Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried race: 인종을 나타냅니다 White, Asian-Pac-Islander, Amer-Indian-Eskimo, Black, Other sex: 개인의 성별입니다. Female, Male capital_gain: 개인의 자본 이익을 미국 달러로 표기 합니다. capital_loss: 개인의 자본 손실을 미국 달러로 표기 합니다. hours_per_week: 주당 근무시간입니다. native_country: 개인의 출신 국가 입니다. ?,Cambodia,Canada,China,Columbia,Cuba,Dominican-Republic,Ecuador,El-Salvador,England,France,Germany,Greece,Guatemala,Haiti,Holand-Netherlands,Honduras,Hong,Hungary,India,Iran,Ireland,Italy,Jamaica,Japan,Laos,Mexico,Nicaragua,Outlying-US(Guam-USVI-etc),Peru,Philippines,Poland,Portugal,Puerto-Rico,Scotland,South,Taiwan,Thailand,Trinadad&amp;Tobago,United-States,Vietnam,Yugoslavia income_bracket: 개인의 연간 소득이 미화 50,000달러가 넘는지 여부를 나타냅니다 예측을 위한 가설들을 세운다 예시: native_country를 기준으로 평균 주당 근무시간과 개인의 자본 현황의 평균을 보고 싶다. 이러한 데이터는 Looker studio의 기능으로 조회가 불가능하다. → GCP Big Query로 조회하기 실행 쿼리 123456SELECT DISTINCT native_country, AVG(hours_per_week) as avg_hours_per_week, AVG(capital_gain - capital_loss) as avg_capitalFROM `bigquery-public-data.ml_datasets.census_adult_income`GROUP BY 1ORDER BY 1 위 데이터를 Looker Studio로 옮겨서 그래프를 작성해 보자 데이터 추가 → 빅쿼리 → 맞춤 검색어 → 프로젝트를 선택한 후 SQL을 입력해 준다. 시간 데이터가 있을 때는 기간 매개변수 사용 설정을 체크 하면 기간 컨트롤에 대한 부분을 조작할 수 있다. → 하지만 이 데이터에는 기간에 대한 내용은 없으니 추후 작성하는 것으로 한다. 데이터를 추가 하고 차트를 추가하면 대시보드가 완성 된다. Reference https://notebook.community/google/eng-edu/ml/cc/exercises/estimators/ko/intro_to_fairness","categories":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}],"tags":[{"name":"Big Query","slug":"Big-Query","permalink":"https://jmj3047.github.io/tags/Big-Query/"},{"name":"Looker Studio","slug":"Looker-Studio","permalink":"https://jmj3047.github.io/tags/Looker-Studio/"}],"keywords":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}]},{"title":"BQML을 이용한 개인 소득 예측","slug":"BQML_Pred","date":"2023-02-27T15:00:00.000Z","updated":"2023-03-19T07:02:10.360Z","comments":true,"path":"2023/02/28/BQML_Pred/","link":"","permalink":"https://jmj3047.github.io/2023/02/28/BQML_Pred/","excerpt":"","text":"개인 연간 소득이 5만 달러 이상인지 예측하기개요 GCP에서 BQML 사용하기 BQML의 로지스틱 회귀 모델 유형으로 supervised learning을 지원하는 기능 사용 바이너리&#x2F;멀티 로지스틱 회귀 모형을 사용하면 값이 두&#x2F;여러 범주 중 하나에 속할지 예측할 수 있다. 데이터를 둘 이상의 범주로 분류하려는 문제 bigquery-public-data.ml_datasets.census_adult_income 데이터 사용 목표 로지스틱 회귀 모델을 만들고 평가한다. 로지스틱 회귀 모델을 사용하여 예측한다. 비용 BigQuery 가격정책 BigQuery ML 가격정책 0단계: public-dataset 가져오기 GCP BigQuery로 이동하여 프로젝트를 만들고, 프로젝트 밑에 +ADD DATA 를 클릭한다 Public Datasets 를 선택한다 데이터 셋중 아무거나 선택 한 후 VIEW DATASET 을 선택하면 새 창이 뜨면서 구글 퍼블릭데이터 셋이 연결 된다. 이번 포스팅에서 다룰 데이터는 bigquery-public-data.ml_datasets.census_adult_income 이다. 아래처럼 창이 뜨면 빅쿼리SQL을 사용할 준비가 다 되었다. 1단계: 데이터 세트 만들기 내 프로젝트를 선택하고 옆에 점세개를 누르고 Create dataset 을 눌러서 데이터 세트를 만든다. 데이터 세트 ID에 census 를 입력한다 리전을 미국(US)로 선택한다 → public dataset이 US멀티 리전에 있기 때문. 같은 리전에 두어야 혼선을 막을수 있다. 만들어진 데이터 셋을 확인할 수 있다. 2단계: 데이터 확인 및 EDA 데이터 확인 age(나이): 개인의 나이를 연단위로 나타냅니다 workclass(노동 계급): 개인의 고용형태 functional_weight: 일련의 관측결과를 바탕으로 인구조사국이 부여하는 개인의 가중치 education: 개인의 최종학력 education_num: 교육수준을 숫자로 범주화 하여 열거 합니다. 숫자가 높을수록 개인의 교육수준이 높습니다. marital_status: 개인의 결혼 여부 입니다. occupation: 개인의 직업입니다. relationship: 가정 내 각 개인의 관계입니다. race: 인종을 나타냅니다 sex: 개인의 성별입니다. capital_gain: 개인의 자본 이익을 미국 달러로 표기 합니다. capital_loss: 개인의 자본 손실을 미국 달러로 표기 합니다. hours_per_week: 주당 근무시간입니다. native_country: 개인의 출신 국가 입니다. income_bracket: 개인의 연간 소득이 미화 50,000달러가 넘는지 여부를 나타냅니다 예측 작업: 개인의 연간 소득이 50,000달러 이상인지 확인해보기 쿼리 실행 123456789101112SELECT age, workclass, marital_status, education_num, occupation, hours_per_week, income_bracketFROM `bigquery-public-data.ml_datasets.census_adult_income`LIMIT 100; 쿼리 실행 결과 분석 income_bracket : &lt;=50K 또는 &gt;50K 값중 하나만 있음 education , education_num → 동일한 데이터가 서로 다른 형식으로 표기되어 있음 functional_weight : 인구 조사 기관에서 특정행이 대표한다고 판단하는 개인의 수 → 우리가 원하는 income 예측과는 관련이 없음. 3단계: 학습 데이터 선택 학습에 필요한 데이터 선택 age: 연령 workclass: 고용형태 marital_status: 결혼여부 education_num: 교육수준 occupation: 직업 hours_per_week: 주당 근무시간 학습 데이터를 컴파일 하는 뷰를 만든다. 1234567891011121314151617CREATE OR REPLACE VIEW `census.input_view` ASSELECT age, workclass, marital_status, education_num, occupation, hours_per_week, income_bracket, CASE WHEN MOD(functional_weight, 10) &lt; 8 THEN &#x27;training&#x27; WHEN MOD(functional_weight, 10) = 8 THEN &#x27;evaluation&#x27; WHEN MOD(functional_weight, 10) = 9 THEN &#x27;prediction&#x27; END AS dataframeFROM `bigquery-public-data.ml_datasets.census_adult_income` education, education_num 등과 같은 중복되는 카테고리를 제외한다. functional_weight 는 income_bracket과 상관이 없는 컬럼이므로 라벨링으로 사용한다. 80%training, 10% 평가, 10% 예측 MOD(X,Y): X를 Y로 나눴을때의 나머지 위 쿼리를 수행시키면 1단계에서 만든 census에 input_view가 만들어졌음을 알수 있다. 4단계: 로지스틱 회귀모델 만들기 Create Model 문을 LOGISTIC_REG 옵션과 함께 사용하면 로지스틱 회귀 모델을 만들고 학습 시킬수 있다. 12345678910111213CREATE OR REPLACE MODEL `census.census_model`OPTIONS ( model_type=&#x27;LOGISTIC_REG&#x27;, auto_class_weights=TRUE, input_label_cols=[&#x27;income_bracket&#x27;] ) ASSELECT * EXCEPT(dataframe)FROM `census.input_view`WHERE dataframe = &#x27;training&#x27; CREATE MODEL문은 SELECT 문의 학습 데이터를 사용하여 모델을 학습 시킨다. OPTIONS 절은 모델 유형과 학습 옵션을 지정한다. 여기서 LOGISTIC_REG 옵션은 로지스틱 회귀 모델 유형을 지정한다. 바이너리 로지스틱 회귀모델과 멀티클래스 로지스틱 회귀모델을 구분하여 지정할 필요는 없다. BigQuery ML은 라벨열 고유 값 수 기반으로 학습 대상을 결정할 수 있다. → 고유 값 수에 따라서 자동으로 선택하여 학습함 input_label_cols 옵션은 SELECT 문에서 라벨 열로 사용할 열을 지정한다. 여기서 라벨 열은 income_bracket 이므로 모델은 각 행에 있는 다른 값을 기반으로 income_bracket의 두 값 중 가장 가능성이 높은 값을 학습 한다. SELECT 문은 2단계의 뷰를 사용한다. 이 뷰에는 모델 학습용 특성 데이터가 포함된 열만 포함된다. WHERE 절은 학습데이터 프레임에 속하는 행만 학습 데이터에 포함되도록 input_view의 행을 필터링 한다. CREATE MODEL 쿼리 실행1234567891011121314CREATE OR REPLACE MODEL `census.census_model`OPTIONS ( model_type=&#x27;LOGISTIC_REG&#x27;, auto_class_weights=TRUE, data_split_method=&#x27;NO_SPLIT&#x27;, input_label_cols=[&#x27;income_bracket&#x27;], max_iterations=15) ASSELECT * EXCEPT(dataframe)FROM `census.input_view`WHERE dataframe = &#x27;training&#x27; SCHEMA 탭은 BigQuery ML이 로지스틱 회귀를 수행하는데 사용한 속성을 나열한다. 5단계: ML.EVALUATE 함수를 사용하여 모델 평가 4단계의 CREATE MODEL 문을 수행했을때 모델의 결과 창에 EVALUATION 탭으로 확인할 수 있다. ML.EVALUATE 함수는 실제 데이터를 기준으로 예측 값을 평가한다. 12345678910111213SELECT *FROM ML.EVALUATE (MODEL `census.census_model`, ( SELECT * FROM `census.input_view` WHERE dataframe = &#x27;evaluation&#x27; ) ) 앞에서 학습시킨 모델과 SELECT 서브 쿼리에서 반환된 평가 데이터를 받아들인다. 이 함수는 모델에 대한 단일행의 통계를 반환한다. input_view 의 데이터를 평가 데이터로 사용한다. 실행 결과 로지스틱 회귀를 수행했으므로 결과에 precision, recall, accuracy, f1_score, log_loss, roc_auc 열이 포함된다. 1번과 2번의 차이 ML.EVALUATE는 학습 과정에서 계산된 평가 측정값을 가져오는데, 이를 위해 자동으로 예약된 평가 데이터셋을 사용합니다. data_split_method 학습 옵션에 NO_SPLIT 가 지정된 1번 방법의 경우 전체 입력 데이터 세트가 학습과 평가에 모두 사용된다. 평가 데이터와 훈련 데이터를 구분하지 않고 ML.EVALUATE 를 호출 하면 학습 데이터 세트에서 임의의 평가 데이터 세트가 측정되고 이러한 평가 효과는 모델 학습 데이터와 별도로 유지된 데이터 세트에 대한 평가 실행보다 적다. → 따라서 구분해주는게 좋다. 6단계: ML.PREDICT 함수를 사용하여 소득 계층 예측 특정 응답자가 속한 소득 계층을 식별하려면 ML.PREDICT 함수를 사용한다. 12345678910111213SELECT *FROM ML.PREDICT (MODEL `census.census_model`, ( SELECT * FROM `census.input_view` WHERE dataframe = &#x27;prediction&#x27; ) ) prediction 데이터 프레임에 있는 모든 응답자의 소득 계층을 예측한다. 실행 결과: predicted_income_bracket은 income_bracket의 예측값 7단계: Explainable AI 메서드로 예측 결과 설명 모델에서 이러한 예측 결과를 생성하는 이유를 알아 보려면 ML.EXPLAIN_PREDICT 함수를 사용하면 된다. ML.EXPLAIN_PREDICT 는 ML.PREDICT 의 확장된 버전 예측 결과 뿐만 아니라 예측결과를 설명하는 추가 열을 출력한다. BigQueryML의 Shapley 값과 Explainable AI서비스에 대한 자세한 내용 12345678910111213#standardSQLSELECT*FROMML.EXPLAIN_PREDICT(MODEL `census.census_model`, ( SELECT * FROM `census.input_view` WHERE dataframe = &#x27;evaluation&#x27;), STRUCT(3 as top_k_features)) 실행 결과 로지스틱 회귀모델에서 Shapley 값은 머신 러닝 모델의 예측 결과에 각 특성이 기여하는 정도를 평가하기 위해 사용된다. 이 값을 통해 모델의 예측 결과를 해석하고, 모델의 예측 결과를 개선하기 위해 어떤 특성을 수정해야 하는지를 결정하는 데 도움을 준다. top_k_features 가 3으로 설정되었기 때문에 제공된 테이블의 행당 특성 기여 학목 3개를 출력함. 기여항목은 절댓값 기준으로 내림차순으로 정렬된다. 44번행의 결과 값은 education_num이 가장 큰 기여를 하였다면, 47번행의 결과는 age가 가장 큰 기여를 했음을 알수 있다. 8단계: 모델을 전연적으로 설명 일반적으로 소득계층을 셜정하는데 가장 중요한 특성이 무엇인지 확인하려면 ML.GLOABL_EXPLAIN 함수를 사용하면 된다. 이를 사용하려면 모델을 학습 시킬때 ENABLE_GLOBAL_EXPLAIN &#x3D; TRUE 옵션을 사용하여야 한다. sklearn의 feature importance와 같은 기능을 하는 함수 학습 쿼리 1234567891011121314CREATE OR REPLACE MODEL census.census_modelOPTIONS ( model_type=&#x27;LOGISTIC_REG&#x27;, auto_class_weights=TRUE, enable_global_explain=TRUE, input_label_cols=[&#x27;income_bracket&#x27;] ) ASSELECT * EXCEPT(dataframe)FROM census.input_viewWHERE dataframe = &#x27;training&#x27; 전역 설명에 Access하는 쿼리 12345#standardSQLSELECT *FROM ML.GLOBAL_EXPLAIN(MODEL `census.census_model`) 실행 결과 Reference https://notebook.community/google/eng-edu/ml/cc/exercises/estimators/ko/intro_to_fairness https://cloud.google.com/bigquery-ml/docs/logistic-regression-prediction?hl=ko","categories":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}],"tags":[{"name":"Big Query","slug":"Big-Query","permalink":"https://jmj3047.github.io/tags/Big-Query/"},{"name":"BigQueryML","slug":"BigQueryML","permalink":"https://jmj3047.github.io/tags/BigQueryML/"}],"keywords":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}]},{"title":"Mac bash 파일로 hexo, git 명령어 자동화","slug":"Bash_Automation","date":"2023-02-26T15:00:00.000Z","updated":"2023-02-28T07:50:29.954Z","comments":true,"path":"2023/02/27/Bash_Automation/","link":"","permalink":"https://jmj3047.github.io/2023/02/27/Bash_Automation/","excerpt":"","text":"bash 파일로 hexo, git 명령어 자동화 필자가 블로그글을 작성하는데 hexo, git 명령어 자동화의 필요성을 느껴 이 글을 작성한다. mac에서 자동화 하는 경우 윈도우와 달리 batch 파일이 아니라 bash 파일로 실행해야 한다. 우선 메모장에 자동화를 원하는 코드를 작성한다. bash 파일 작성시에는 #!/bin/bash 를 꼭 작성해주어야 실행이 가능하다. 실행파일 이름을 정하고(필자는 submit 으로 설정하였음) 저장한 뒤 확장자를 없애주어야 한다. 저장한 submit 파일을 블로그 로컬 폴더 안에 넣는다 터미널에 chmod +x submit 를 입력해서 권한을 부여해준다 실행시키려면 sh submit 을 입력해주면 잘 돌아가는 것을 볼 수 있다.","categories":[{"name":"Setting","slug":"Setting","permalink":"https://jmj3047.github.io/categories/Setting/"}],"tags":[{"name":"Git","slug":"Git","permalink":"https://jmj3047.github.io/tags/Git/"},{"name":"Hexo","slug":"Hexo","permalink":"https://jmj3047.github.io/tags/Hexo/"},{"name":"Automation","slug":"Automation","permalink":"https://jmj3047.github.io/tags/Automation/"},{"name":"Bash","slug":"Bash","permalink":"https://jmj3047.github.io/tags/Bash/"}],"keywords":[{"name":"Setting","slug":"Setting","permalink":"https://jmj3047.github.io/categories/Setting/"}]},{"title":"Basic ML Process","slug":"ML_Process","date":"2023-02-23T15:00:00.000Z","updated":"2023-02-24T04:50:59.756Z","comments":true,"path":"2023/02/24/ML_Process/","link":"","permalink":"https://jmj3047.github.io/2023/02/24/ML_Process/","excerpt":"","text":"Basic ML Process 이 포스트에서는 필자가 생각한 기본 프로세스를 소개 한다. 머신러닝을 접해보지 않은 사람들에게 대략적인 개념을 보여주는 포스트 이다. 자세한 내용은 추후 추가 예정 가설 수립 → 데이터 확인 및 전처리 → 모델 학습&#x2F; 모델 검증 → 예측하기 → 결과 확인 가설 수립(회귀&#x2F;분류 여부 확인) → 잠재 고객 분류, 매출 예측, 리텐션 예측 등등.. 잠재 고객 분류: 특정 고객군은 추후 유료 서비스를 사용할 것이다. 매출 예측: 다음달 매출 예상액은 전월 대비 ?? 일것이다. 리텐션 예측: 다음주 리텐션 비율이 ?? 일 것이다, ??일 동안 들어온 고객은 장기 고객이 될 것이다 예시 가설: 이틀 이상 접속 &amp; 1 경기 이상 플레이 경험 유저 대상으로 귀환 유저 비율 예측 → 회귀문제 데이터 확인 및 전처리 데이터 feature 선택 예시 raw data(Big Query에서 추출한 부분) A그룹은 2021-12-07 ~ 2021-12-21 B그룹은 2022-02-14 ~ 2022-02-28 전체: 접속시간, 레벨, 게임횟수, 로그인횟수, 평균순위, 최대순위, 총킬횟수, 유저킬횟수, 헤드샷킬수, 신고 횟수, 솔로&#x2F;듀오&#x2F;스쿼드 참여 횟수, 오직 솔로모드만 이용 가입첫날: 총 게임수, 평균순위, 최대순위, 총킬횟수, 유저킬횟수, 헤드샷킬수, 신고 횟수, 솔로&#x2F;듀오&#x2F;스쿼드 참여 횟수, 오직 솔로모드만 이용 가입 다음날: 총 게임수, 평균순위, 최대순위, 총킬횟수, 유저킬횟수, 헤드샷킬수, 신고 횟수, 솔로&#x2F;듀오&#x2F;스쿼드 참여 횟수, 오직 솔로모드만 이용 EDA 유저 킬수, 최대순위, 로그인횟수, 헤드샷킬수, 유저레벨, 플레이 시간, 로그아웃 날짜, 총 게임 횟수, 총 킬수, 평균 순위 데이터 샘플링 데이터를 일부 정리해서 최적의 입력 데이터로 만드는 과정 확률적 샘플링 단순 랜덤 샘플링 2단계 샘플링: 전체 n개의 데이터를 m개의 모집단으로 나누고 m개의 모집단 중에 N개의 데이터를 단순랜덤 샘플링 층별 샘플링: 모집단을 여러개 층으로 구분함으로써 각 층에서 n개씩 랜덤하게 데이터 추출 군집&#x2F;집락 샘플링: 모집단이 여러개의 군집으로 구성되어 있는 경우 군집 중 하나 or 여러개의 군집을 선정해서 선정된 군집의 전체 데이터를 사용하는 방법. Ex) 한국의 시,도 데이터 계통 샘플링: 1에서 n까지 모든 데이터에 번호를 매겨서 일정 간격마다 하나씩 데이터를 추출하는 방법. 대표적으로 시계열에서 사용됨 비확률적 샘플링 편의 샘플링: 데이터르르 수집하기 좋은 시점이나 위치를 선정하여 샘플링 판단 샘플링: 목적에 가장 적합한 대상이라고 생각하느느 대상을 선택 할당 샘플링 참고: https://jmj3047.github.io/2022/09/07/Data_Sampling&#x2F; feature 축소 or 확대 ‘유저 킬수, 최대순위, 로그인횟수, 헤드샷킬수, 유저레벨, 플레이 시간, 로그아웃 날짜, 총 게임 횟수, 총 킬수, 평균 순위’ 의 지표를 EDA 과정을 거쳐 ‘접속시간, 레벨, 게임횟수, 로그인횟수, 평균순위, 최대순위, 총킬횟수, 유저킬횟수, 헤드샷킬수, 신고 횟수, 솔로&#x2F;듀오&#x2F;스쿼드 참여 횟수, 오직 솔로모드만 이용’ 으로 지표를 확대 시킴 → 약 64만행의 데이터 확보 모델 학습&#x2F; 모델 검증 모델 선택 및 파라미터 조정 → 모델 학습 → 모델 검증 대부분 한줄의 코드로 모델 선택이 가능함 파라미터 조정 같은 경우 GridSearchCV로 조정 할수 있음 n_estimators&#x3D;500, learning_rate&#x3D;0.05, gamma&#x3D;0, subsample&#x3D;0.75, → 이 네가지의 파라미터를 최적값으로 조정하고 싶다면, 각각의 범위를 선택해주면 범위 내의 모든 확률을 다 확인한다: 시간도 오래 걸리고 굉장히 무겁게 돌아감. feature importance로 어떤 파라미터가 모델 학습에 기여를 많이 했는지 알수 있음 학습 시 train 데이터를 validation set을 떼어두고 학습 해야 모델 검증이 가능 함. 모델 검증 → 모델과 데이터 간의 적합도 확인 하는 과정 f1 score, $r^2$ score, confusion matrix 등등 예측하기 및 결과 확인 모델 저장 후 test 데이터로 실행해봄 결과의 신뢰성이 어느정도 인지 확인하는 방법 → 미래의 사건이라서 100프로 확인은 안되지만 오류에 대한 부분을 확인하면 유추 가능","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"ML Process","slug":"ML-Process","permalink":"https://jmj3047.github.io/tags/ML-Process/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}]},{"title":"Auto-correlation Function, Partial Auto-correlation Function","slug":"ACF_PACF","date":"2023-02-22T15:00:00.000Z","updated":"2023-02-23T01:50:54.354Z","comments":true,"path":"2023/02/23/ACF_PACF/","link":"","permalink":"https://jmj3047.github.io/2023/02/23/ACF_PACF/","excerpt":"","text":"자기 상관 함수와 부분 자기 상관 함수Autocorrelation Function, 자기 상관 함수 자기 상관 함수(Auto-correlation Function) 어떤 신호의 시간이동 된 자기 자신과의 ‘상관성(Correlation)’ 척도 주요 특징 결정 신호(주기 신호&#x2F;비주기 신호)이든, 랜덤 신호 이든 모든 신호에 대해 적용 가능 특히 랜덤 과정인 경우에 자기상관함수를 이용하여 굳이 시간신호에 대한 푸리에 변환을 구할 필요 없이 주파수상에 분포된 전력(전력밀도스펙트럼)을 취급할 수 있으므로 이를 사용하게 됨 [참고] 서로 다른 신호 간의 상관성 척도에 대해서는 상호상관 참조 상관데 대한 보다 정확한 이해를 위해서는 상관성 참조 상관성 개념의 종합화&#x2F;일반화는 비교(같음&#x2F;닮음&#x2F;다름) 참조 확정적 신호(Deterministic signal)에서, 자기 상관 함수 에너지신호의 자기상관함수: 컨볼루션(*)에 의해 정의돔 x(t)가 실수 신호이면, $Rx(τ)&#x3D;∫^∞ _{−∞} x(t)x(t+τ)dt&#x3D;x(τ)∗x(−τ)$ x(t)가 복소수 신호이면, $Rx(τ)&#x3D;∫^∞_{−∞}x(t)x^∗(t+τ)dt&#x3D;x(τ)∗x^∗(−τ)$ 전력신호의 자기 상관 함수: 시간평균(&lt;&gt;)에 의해 정의됨 실수 신호 복소수 신호 랜덤 과정(Random Prodcess)에서, 자기상관 함수 정의 통계적 평균에 의한 자기상관함수 정의: $R_x(t_1,t_2) &#x3D; E[X(t_1)X(t_2)]$ 결합 PDF(결합 확률밀도함수)에 의한 자기 상관함수 정의: $R_x(t_1,t_2) &#x3D; ∫^∞_{−∞}∫^∞_{−∞}x_1x_2fx_1x_2(x_1,t_1,x_2,t_2)dx_1dx_2$ 만일 랜덤 과정이 광의의 정상과정이면, 시간 t의 함수가 아니라 시간천이 $t-t&#x3D;τ$의 함수가 됨 $R_x(t_1,t_2) &#x3D; R_x(t,t+τ)&#x3D;R_x(τ) &#x3D; E[X(t)X(t+τ)]$ 이때 시간 영역 자기상관과 주파수영역 스펙트럼밀도 간에 푸리에 변환 쌍 관계가 있음. $R(τ)$ ← 푸리에변환 쌍 관계 → $S(f)$ 만일, 랜덤과정이 에르고딕과정이라면, 통계적 평균 및 시간 평균이 상호 호환이 가능함 $R_x(τ) &#x3D; E[X(t)X(t+τ)] &#x3D;&lt; X(t)X(t+τ)&gt;$ 따라서 이 경우에는 R(τ)는 시간 평균이나 통계적 평균 어느 것으로도 구할 수 있음. 자기상관 함수의 성질&#x2F;특징 신호의 ‘시변(time-variant)’ 특성이 어떤가를 보여줌 그 신호가 갖는 스펙트럼의 특성 정보를 나타냄 시간적인(시변) 상관성 척도임 분산이 확률변수가 통계적으로 불규칙하게 분포되는 정도를 나타내는 척도라면 자기 상관은 분산과 유사하게 확률과정이 시간적으로 상관 또는 분산되는 척도를 나타냄 직관적으로 자기 자신과의 시간천이(τ)가 작을수록 상관성이 커짐 따라서 τ &#x3D; 0 에서 최대 상관성 값을 갖음 $|R_x(τ)|≤R_x(0)$식 τ &#x3D; 0 일때 물리적 의미로는 에너지 신호: τ &#x3D; 0 에서의 최대값이 전체 신호에너지와 같음 $R_x(0) &#x3D; ∫^∞_{−∞}|x(t)|^2dt&#x3D; E_x$ 전력신호: τ &#x3D; 0 에서의 최대값이 평균 전력과 같음 $R_x(0) &#x3D;&lt; x^2(t) &gt;&#x3D; ∫^∞_{−∞}S_x(f)df &#x3D; P_{av}$ WSS 랜덤과정: τ &#x3D; 0에서의 최대값이 평균전력과 같음 $R_x(0) &#x3D; E[X^2(t)] &#x3D; ∫^∞_{−∞}S_x(f)df &#x3D; P_{av}$ 시간 영역 자기 상관과 주파수 영역 스펙트럼 밀도 간에 푸리에 변환 쌍 관계가 있음 자기 상관 ← 푸리에변환 쌍 관계 → 스펙트럼 밀도 : 위너킨친정리 참조 Partial Autocorrelation Function, 부분 자기 상관 함수 편 자기 상관 함수(부분자기상관함수)는 다른 모든 짧은 시차 항에 따라 조정한 후 k 시간 단위로 구분된 시계열($y_{t-1},y_{t-2},…,y_{t-k-1}$)의 관측치 ($y_t 및 y_{t-k}$) 간의 상관의 측도임 해석 ARIMA 모형에서 자기 회귀 차수를 식별하는 용도로 사용 됨. 편 자기 상관 함수에서 다음과 같은 패턴을 찾음. 각 시차에서 큰 값을 조사하여 유의한지 확인함. 유의한 큰 값은 유의 한계를 벗어나면, 이는 해당 시차에 대한 상관이 0이 아니라는 것을 나타냄 이 그림에서는 시차 1에 유의한 상관이 있고 그 뒤에는 유의하지 않은 상관이 있음. 이 패턴은 1차 자기회귀 항을 나타냄. Reference http://www.ktword.co.kr/test/view/view.php?m_temp1&#x3D;3547 https://support.minitab.com/ko-kr/minitab/20/help-and-how-to/statistical-modeling/time-series/how-to/partial-autocorrelation/interpret-the-results/partial-autocorrelation-function-pacf/ https://zephyrus1111.tistory.com/135","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"ACF","slug":"ACF","permalink":"https://jmj3047.github.io/tags/ACF/"},{"name":"PACF","slug":"PACF","permalink":"https://jmj3047.github.io/tags/PACF/"},{"name":"Time Series","slug":"Time-Series","permalink":"https://jmj3047.github.io/tags/Time-Series/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}]},{"title":"Probability Distribution Function & Probability Density Function","slug":"Probability_Distribution_Function","date":"2022-11-19T15:00:00.000Z","updated":"2023-02-23T01:38:48.226Z","comments":true,"path":"2022/11/20/Probability_Distribution_Function/","link":"","permalink":"https://jmj3047.github.io/2022/11/20/Probability_Distribution_Function/","excerpt":"","text":"확률 분포 함수와 확률 밀도 함수확률 분포 함수(probability distribution function)와 확률 밀도 함수(probability density function)는 확률 변수의 분포 즉, 확률 분포를 수학적으로 정의하기 위한 수식이다. 연속 확률 분포우선 확률 밀도 함수에 대해 먼저 알아보자. 확률 밀도 함수를 이해하면 확률 분포 함수를 이해하는 것은 쉽다. 확률 밀도 함수는 연속 확률 변수(continuous random variable)를 정의하는데 필요하다. 연속 확률 변수의 값은 실수(real number) 집합처럼 연속적이고 무한개의 경우의 수를 가진다. 연속 확률 변수의 분포를 연속 확률 분포라고 한다. 시계 바늘을 예로 들어보자. 다음과 같은 아날로그 시계의 시계 바늘을 눈을 감고 임의로 돌렸다고 하면 시계 바늘이 정각 12시(각도 0도)를 가리킬 확률은 얼마일까? 만약 이 확률 변수의 확률 분포가 0 이상 360 미만의 구간내에서 균일 분포(uniform distribution) 모형을 가진다고 가정하면 답은 0(zero)이다.시계 바늘이 가리키는 각도의 값은 0도 이상 360도 미만의 모든 실수 값을 가질 수 있는데, 이 경우 수가 무한대이므로 각각의 경우에 대한 확률은 0이 되어야 하기 때문이다.사실 각도가 0도가 아니라 어떤 특정한 각도를 지정하더라도 같은 이유로 그 각도를 가리킬 확률은 0이다. 그럼 도대체 어떤 방법으로 확률 분포를 설명해야 할까? 이렇게 경우의 수가 무한대인 연속 확률 변수의 분포를 설명하려면 특정한 값이 아니라 구간을 지정하여 확률을 설명해야 한다. 예를 들어 위와 같은 시계바늘의 예에서는 다음과 같은 분포의 묘사가 가능하다. 시계 바늘이 12시와 1시 사이에 있을 확률은 1&#x2F;12 시계 바늘이 1시와 3시 사이에 있을 확률은 2&#x2F;12 &#x3D; 1&#x2F;6 시계 바늘이 6시와 9시 사이에 있을 확률은 3&#x2F;12 &#x3D; 1&#x2F;4 이 방법의 단점 중 하나는 분포를 설명하는데 범위를 지정하는 두 개의 숫자가 필요하다는 점이다. 예를 들어 ‘1시와 3시 사이’ 라는 범위를 지정하는데는 1과 3이라는 숫자가 필요하다.그럼 하나의 숫자로 확률 변수의 범위를 지정하는 방법은 없을까? 가능한 방법 중의 하나는 범위를 지정하는 두 개의 숫자 중 작은 숫자 즉, 범위가 시작하는 숫자를 미리 가장 작은 숫자로 고정하는 방법이다. 이 방법을 쓰면 다음과 같이 하나의 숫자로 랜덤 변수의 범위와 해당 확률을 서술할 수 있다. 숫자&#x3D;1 -&gt; 범위&#x3D;12시부터 1시까지 -&gt; 확률 1&#x2F;12 숫자&#x3D;2 -&gt; 범위&#x3D;12시부터 2시까지 -&gt; 확률 2&#x2F;12 숫자&#x3D;5 -&gt; 범위&#x3D;12시부터 5시까지 -&gt; 확률 5&#x2F;12 누적 확률 분포위와 같은 방법으로 서술된 확률 분포를 누적 확률 밀도 함수 (cumulative probability density function) 또는 누적 확률 분포라고 하고 약자로 cdf라고 쓴다. 일반적으로 cdf는 대문자를 사용하여 F(x)와 같은 기호로 표시하며 이 때 독립 변수 x는 범위의 끝을 뜻한다. 범위의 시작은 일반적으로 음의 무한대(negative infinity, -∞)값을 사용한다.몇가지 누적 확률 분포 표시의 예를 들면 다음과 같다. F(-1) : 확률 변수가 -∞ 이상 -1 미만인 구간 내에 존재할 확률 F(10) : 확률 변수가 -∞ 이상 10 미만인 구간 내에 존재할 확률 확률 변수 X에 대한 누적 확률 분포 F(x)의 수학적 정의는 다음과 같다. $$F(x) &#x3D; P(X &lt; x) &#x3D; P(X&lt;x)$$ 일례로 표준 정규 분포의 누적 확률을 그리면 아래와 같다. 123x = np.linspace(-4,4)y = sp.stats.norm.cdf(x)plt.plot(x,y) 누적 밀도 함수 즉, cdf는 다음과 같은 특징을 가진다. F(-∞) &#x3D; 0 F(∞) &#x3D; 1 F(x) ≥ F(y) if x &gt; y 확률 밀도 함수누적 밀도 함수의 단점 중의 하나는 어떤 값이 더 자주 나오든가 혹은 더 가능성이 높은지에 대한 정보를 알기 힘들다는 점이다. 이를 알기 위해서는 확률 변수가 나올 수 있는 전체 구간 (-∞ ~ ∞)을 아주 작은 폭을 가지는 구간들로 나눈 다음 각 구간의 확률을 살펴보는 것이 편리하다. 다만 이렇게 되면 구간의 폭을 얼마로 정해야 하는지에 대한 의문이 생긴다. 123x = np.linspace(-4,4,20)y = sp.stats.norm.cdf(x)z = np.insert(np.diff(y), 0, None) 12w = (4-(-4))/20plt.bar(x-w, z/w, width = w) 이 때 사용할 수 있는 수학적 방법이 바로 미분(differentiation)이다. 미분은 함수의 구간을 무한대 갯수로 쪼개어 각 구간 변화의 정도 즉, 기울기를 계산하는 방법이다. 누적 밀도 함수를 미분하여 나온 도함수(derivative)를 확률 밀도 함수(probability density function)라고 한다. 누적 밀도 함수는 보통 f(x)와 같이 소문자 함수 기호를 사용하여 표기한다. $$f(x)&#x3D;\\frac{dF(x)}{dx} \\text 또는 F(x) &#x3D; \\int_{-\\infty}^{x}f(u)du$$ 확률 밀도 함수는 다음과 같은 특징을 가진다. -∞ 부터 ∞까지 적분하면 그 값은 1이 된다. Reference https:&#x2F;&#x2F;velog.io&#x2F;@groovallstar&#x2F;확률-분포-함수와-확률-밀도-함수의-의미","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"Probability Distribution Function","slug":"Probability-Distribution-Function","permalink":"https://jmj3047.github.io/tags/Probability-Distribution-Function/"},{"name":"Probability Density Function","slug":"Probability-Density-Function","permalink":"https://jmj3047.github.io/tags/Probability-Density-Function/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}]},{"title":"Difference between Normal Distribution & Standard Normal Distribution","slug":"Normal_Distribution","date":"2022-11-10T15:00:00.000Z","updated":"2023-02-23T01:38:44.907Z","comments":true,"path":"2022/11/11/Normal_Distribution/","link":"","permalink":"https://jmj3047.github.io/2022/11/11/Normal_Distribution/","excerpt":"","text":"정규분표와 표준정규분포함수의 차이본 포스팅에서는 정규분포(Normal distribution)와 표준 정규 분포(Standard normal distribution)에 대해 다루도록 한다. 정규 분포의 확률밀도 함수와 예상치(평균), 분산 그리고 증명에 대해 다루며 표준정규분포에 대해서는 확률밀도함수, 누적분포함수, 그리고 표준정규분포를 이용한 정규분포의 확률계산 등의 내용이 다뤄진다. 1. 정규분포(Normal distribution)**정규 분포(Normal distribution)**는 연속확률분포 중 하나이며 광범위하게 사용된다. 확률 분포 중 가장 유명하며 가장 중요하게 다루는 확률 분포이다. **오류 분포(Error distribution)**와 다른 많은 자연현상을 직접 모델링 하기 위한 확률 분포이다. 중심극한정리(Central Limit Theorm)로 인해 매우 유용하며 단순하고 정확한 근사가 가능하다. 정규 분포는 가우스 분포(Gaussian distribution)라고도 불린다. 비율과 개별적인 확률을 모델링하는데도 유용하다. 정규분포는 일반적으로 다음과 같이 표현된다. : X~N(μ, σ2) 아래 그림은 정규 분포의 확률밀도 함수를 보여준다 확률밀도함수(PDF)정규분포의 확률 밀도 함수는 다음과 같다 예상치(Expectation)와 분산(Variance) 정규분포의 적률생성함수(MGF) 예상치와 분산의 증명 본 증명에서는 적률생성함수(MGF)를 이용하여 증명을 수행해보도록 하겠다. 적률생성함수는 다음과 같다 적률생성함수의 미분값과 그 미분값에서 t&#x3D;0인 경우는 다음과 같다 적률생성함수의 2계 미분값과 그 미분값에서 t&#x3D;0인 경우는 다음과 같다 따라서 분산은 다음과 같이 계산된다 표준정규분포(Standard Normal distribution)정규 분포에서는 μ는 0으로 σ2은 1로 설정하여 표준화를 수행한 정규분포를 표준정규분포라고 한다. 즉, 예상치(평균)는 0, 분산은 1이다. 아래 그림은 표준 정규 분포의 확률밀도함수와 누적분포함수를 보여준다. 확률밀도함수(PDF) 표준정규분포의 확률밀도함수는 다음과 같다. 누적분포함수(CDF) 표준정규분포의 누적분포함수는 다음과 같다. 정규분포의 적률생성함수(MGF) 표준정규분포를 이용한 정규분포의 확률계산확률분포가 정규분포인 경우 표준정규분포로 치환하여 특정범위의 확률을 계산할수 있다. 치환은 다음과 같이 수행 하면 된다. 표준정규분포의 누적확률분포(CDF) 값은 비교적 쉽게 계산이 가능하며 다음과 같은 방식으로 계산할 수 있다. 표준 정규 분포의 누적확률 분포(CDF) 표 Excel, R, Python 모듈 등을 활용한 데이터 취득 다음은 정규분포의 특정 범위에서에 대한 확률을 표준정규분포로 치환하는 과정을 보여준다. 정규분포는 중앙값에서 좌우 대칭이다. 따라서 중앙값에서 다음과 같이 특정 범위의 확률을 표현하는 것이 가능하다. 또한 표준 편차를 이용하여 표준화 시켜 특정 범위를 표현할 수 있다. Reference https://color-change.tistory.com/m/61 https://kongdols-room.tistory.com/145","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"Normal Distribution","slug":"Normal-Distribution","permalink":"https://jmj3047.github.io/tags/Normal-Distribution/"},{"name":"Standard Normal Distribution","slug":"Standard-Normal-Distribution","permalink":"https://jmj3047.github.io/tags/Standard-Normal-Distribution/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}]},{"title":"SpeakerGAN, Speaker identification with conditional generative adversarial network","slug":"SpeakerGAN","date":"2022-11-04T15:00:00.000Z","updated":"2022-11-10T15:27:17.845Z","comments":true,"path":"2022/11/05/SpeakerGAN/","link":"","permalink":"https://jmj3047.github.io/2022/11/05/SpeakerGAN/","excerpt":"","text":"Journal&#x2F;Conference : NeurocomputingYear(published year): 2020Author: Liyang Chen, Yifeng Liu , Wendong Xiao, Yingxue Wang, Haiyong XieSubject: Speaker GAN, Generative Adversarial Network SpeakerGAN: Speaker identification with conditional generative adversarial network Summary This paper proposes a novel approach, SpeakerGAN, for speaker identification with the conditional generative adversarial network (CGAN). We configure the generator and the discriminator in SpeakerGAN with the gated convolutional neural network (CNN) and the modified residual network (ResNet) to obtain generated samples of high diversity as well as increase the network capacity. Under the scenario of limited training data, SpeakerGAN obtains significant improvement over the baselines. IntroductionThe x-vector in [13,14] is proposed as a strong contender for the speaker representation and is considered to supplant the i-vector system by many researchers. We evaluate our approach on the dataset of Librispeech-100 for the text-independent SI task. The baselines include i-vector, xvector, CNN and LSTM. Generative Adversarial NetworksConditional GANThe CGAN is a variant of GAN, which aims to let the generator $G$ produce $G(c,z)$ from the condition $c$ and random noise $z$. In this paper, the real samples x are directly utilized as the condition c and the random noise z is abandoned for showing no effectiveness in the experiments. SpeakerGAN for speaker identificationBasic principle of SpeakerGANWe investigate the CGAN as a classifier by enabling the network to learn on additional unlabeled examples. The SpeakerGAN combines the discriminator and classifier by letting the classifier take samples from the generator as inputs and have $N+1$ output units, where $l_{N+1}$corresponds to the probability $P_{model}(y&#x3D;N+1|x)$ that the inputs are real. To stabilize the training and overcome the problem of vanishing gradients, the least square GAN (LSGAN) [40] is adopted. $L_{adv}$ is then split into the losses for the discriminator and generator. Network architecture of SpeakerGAN Fig. 1. Framework of SpeakerGAN. The front extraction part extracts FBanks $x$ from the real speech samples. The generator takes the real FBanks x as inputs and produces fake samples $G(x)$. The discriminator is then fed with the real and generated FBanks to predict class labels and distinguish between the real and fake samples. The adversarial loss in Multiple loss actually denotes the formulation of LSGAN [40]. The dashed lines with arrows denote calculating loss between two objects, and the solid lines denote the flow of information. The generator takes real sequences as the condition for inputs, and produces the fake samples of the same size after passing through a series of convolutional and shuffler layers that progressively downsample and upsample. The discriminator takes the generated samples and real acoustic features from the corpus as inputs, and outputs the discrimination of real&#x2F;fake along with the $N$ classes. Generator design These generators only capture relationships among feature dimension and the generated samples are in lack of consistency. An effective way to solve this problem would be to introduce the RNN, but it is timeconsuming due to the difficulty of parallel computing. For these reasons, we configure the generator using gated CNNs [43], which allow for both sequential structure and faster convergence. This idea was previously explored in [25], and achieved competitive performance. Discriminator design As for the discriminator, we prefer deeper networks to classify speakers. However, training deep neural networks is computationally expensive and difficult. This paper modifies the ResNet [45] toaccelerate the training. ResNets have been applied in many SI systems [16,17] and are known for good classification performance on image data. To reduce parameters and improve calculation efficiency, the variant ResBlock is adopted, which comprises a stack of three convolutional layers. Four ResBlocks are stacked and a softmax classifier is used to predict the speaker identity in the final layer. ExperimentsDataset and basic setupTo evaluate the performance of the proposed approach, we conduct experiments on the Librispeech [46] dataset. We use the train-clean-100 subset in it that contains 251 speakers of 125 females and 126males. Each speaker is provided with an average of 113 utterances lasting 1–15 s. The same acoustic features are used for the baselines of CNN, LSTM and GAN classifier. Speech samples are shuffled before training. Sixty percentage of all utterances are randomly selected as training data and the rest are used as test data. Conclustions and future workIt directly utilizes the discriminator as a classifier using the fake samples produced by the generator as the additional class. The Hybrid loss function includes the adversarial loss of the regular GAN, thecross-entropy loss of the labeled data and the Huber loss between the real samples and generation. Experimental results demonstrate that SpeakerGAN can achieve higher identification accuracy than other state-of-the-art DL based methods and the traditional i-vector and x-vector systems. Link: SpeakerGAN, Speaker identification with conditional generative adversarial network","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Generative Model","slug":"Paper/Generative-Model","permalink":"https://jmj3047.github.io/categories/Paper/Generative-Model/"}],"tags":[{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Speaker GAN","slug":"Speaker-GAN","permalink":"https://jmj3047.github.io/tags/Speaker-GAN/"},{"name":"Speaker Identification","slug":"Speaker-Identification","permalink":"https://jmj3047.github.io/tags/Speaker-Identification/"},{"name":"Generative Adversarial Network","slug":"Generative-Adversarial-Network","permalink":"https://jmj3047.github.io/tags/Generative-Adversarial-Network/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Generative Model","slug":"Paper/Generative-Model","permalink":"https://jmj3047.github.io/categories/Paper/Generative-Model/"}]},{"title":"Multi-Task Learning for Voice Trigger Detection","slug":"MTL_for_VTD","date":"2022-11-01T15:00:00.000Z","updated":"2022-11-10T15:18:26.813Z","comments":true,"path":"2022/11/02/MTL_for_VTD/","link":"","permalink":"https://jmj3047.github.io/2022/11/02/MTL_for_VTD/","excerpt":"","text":"Journal&#x2F;Conference : ICASSP IEEEYear(published year): 2020Author: Siddharth Sigtia, Pascal Clark, Rob Haynes, Hywel Richards, John BridleSubject: Multi-Task Learning Multi-Task Learning for Voice Trigger Detection Summary We start by training a general acoustic model that produces phonetic transcriptions given a large labelled training dataset. 우리는 레이블이 지정된 대규모 훈련 데이터 세트가 주어지면 phonetic transcriptions를 생성하는 일반적인 음향 모델을 훈련하는 것으로 시작한다. Next, we collect a much smaller dataset of examples that are challenging for the baseline system. 다음으로, 우리는 기준 시스템에 도전하는 훨씬 더 작은 예제의 데이터 세트를 수집한다. We then use multi-task learning to train a model to simultaneously produce accurate phonetic transcriptions on the larger dataset and discriminate between true and easily confusable examples using the smaller dataset. 그런 다음 다중 작업 학습을 사용하여 모델을 훈련시켜 더 큰 데이터 세트에서 정확한 phonetic transcriptions를 생성하고 더 작은 데이터 세트를 사용하여 실제 예제와 쉽게 혼동할 수 있는 예제를 구별한다. IntroductionSignificant challenge is that unlike automatic speech recognition (ASR) systems, collecting training examples for a specific keyword or phrase in a variety of conditions is a difficult problem. 중요한 과제는 자동 음성 인식(ASR) 시스템과 달리 다양한 조건에서 특정 키워드 또는 구문에 대한 훈련 예제를 수집하는 것은 어려운 문제라는 것이다. In the literature, the problem of detecting a speech trigger phrase is interchangeably referred to as voice trigger detection [3], keyword spotting [4], wake-up word detection [5] or hotword detection [6]. In the rest of this paper, we refer to this problem as voice trigger detection. 문헌에서 음성 트리거 구문을 검출하는 문제는 voice trigger detection [3], keyword spotting [4], wake-up word detection [5] 또는 hotword detection [6]으로 상호 교환적으로 언급된다. 이 논문의 나머지 부분에서는 이 문제를 voice trigger detection라고 합니다. In the multi-stage approach (Figure 1), the first stage comprises a low-power DNN-HMM system that is always on [3]. 그림1에 보면, 첫 번째 단계는 항상 [3]에 있는 저전력 DNN-HMM 시스템을 포함한다. In this design, it is the second stage that determines the final accuracy of the system and the models used in this stage are the subject of this paper. 이 설계에서 시스템의 최종 정확도를 결정하는 것은 두 번째 단계이며, 이 단계에서 사용되는 모델이 이 논문의 주제이다. Our main contribution is to propose a multi-task learning strategy where a single model is trained to optimise 2 objectives simultaneously. 우리의 주요 기여는 단일 모델이 두 가지 목표를 동시에 최적화하도록 훈련되는 다중 작업 학습 전략을 제안하는 것이다. The first objective is to assign the highest score to the correct sequence of phonetic labels given a speech recording. 첫 번째 목표는 주어진 음성 녹음의 음성 레이블이 올바른 순서로 되어 있다면 가장 높은 점수를 할당하는 것이다. This objective is optimised on a large labelled training dataset which is also used for training the main speech recogniser and is therefore easy to obtain. 이 목표는 주요 음성 인식기를 훈련시키는 데 사용되므로 쉽게 얻을 수 있는 대규모 레이블링된 훈련 데이터 세트에 최적화된다. The second objective is to discriminate between utterances that contain the trigger phrase and those that are phonetically similar and easily confusable. 두 번째 목표는 trigger phrase를 포함하는 발화와 음성학적으로 유사하고 쉽게 혼동되는 발화를 구별하는 것이다. BaselineThe baseline model architecture comprises an acoustic model (AM) with four bidirectional LSTM layers with 256 units each, followed by an output affine transformation + softmax layer over context independent (CI) phonemes, word and sentence boundaries, resulting in 53 output symbols (Figure 2). Firstly, the fact that the second-pass model is used for re-scoring and not in a continuous streaming setting allows us to use bidirectional LSTM layers. 첫째, 2차 통과 모델이 연속 스트리밍 설정이 아닌 재 득점에 사용된다는 사실은 양방향 LSTM 레이어를 사용할 수 있게 합니다. Secondly, using context-independent phones as targets allows us to share training data with the main ASR. 둘째, context-independent phones 를 target으로 사용하면 주요 ASR과 training 데이터를 공유할수 있습니다. This is particularly important since in many cases it is not possible to obtain a large number of training utterances with the trigger phrase, for example when developing a trigger detector for a new language. 이는 특히 중요하다. 많은 경우에 가령 새로운 언어로 트리거 감지를 개발할때 training 발화를 trigger phrase로 다량의 데이터를 얻는것이 불가능하기 때문이다. Furthermore, having CI phones as targets results in a flexible model that can be used for detecting any keyword. 더 나아가서 유연한 모델에서 CI 음소들을 타겟 결과로 갖는 것은 어느 키워드를 감지하는데 사용될수 있다. Given an audio segment x from the first pass, we are interested in calculating the probability of the phone sequence in the trigger phrase, $P(TriggerPhrasePhoneSeq|x)$. segment x 음성이 1차에서 주어졌을때, 우리는 trigger phrase의 음성 시퀀스, $P(TriggerPhrasePhoneSeq|x)$,의 확률을 계산하는 것에 관심이 있다. Multi-Task LearningThe question we really want to answer is, “given an audio segment from the first pass, does it contain the trigger phrase or not?” 우리가 실제로 답하고 싶은 질문은 “1차에서 통과된 주어진 음성 segment가 trigger phrase를 포함하고 있는가 아닌가?” 이다. We would like the second-pass model to be a binary classifier which determines the presence or absence of the trigger phrase. 우리는 2차 모델이 trigger phrase를 포함하는지 하지 않는지를 결정하는 이진 분류기였으면 한다. However the issue with this design is that collecting a large number of training examples that result in false detections by the baseline system is a difficult problem (c.f. Section 4). 그러나, 이 디자인의 이슈는 baseline 시스템에 의해 다량의 training 예시를 수집하는 것이 어려운 문제라는 것이다. Furthermore, the second pass models have millions of parameters, so they can easily overfit a small training set resulting in poor generalisation. 더 나아가 2차 모델은 수백만개의 파라미터가 있다. 그래서 그들은 작은 training set에도 쉽게 과적합 되며 안좋은 일반화를 결과로 낸다. Therefore, we are faced with the choice between a more general phonetic AM that can be trained on a large, readily available dataset but is optimised for the wrong criterion or a trigger phrase specific detector that is trained on the correct criterion but with a significantly smaller training dataset. 따라서, 우리는 크고 손쉽게 사용 가능한 데이터셋으로 train된 하지만 잘못된 기준으로 최적화된 일반화된 음성 AM과 올바른 기준으로 train됐지만 매우 적은 training 데이터로 학습된 특정 trigger phrase 감지기, 둘 중 하나를 선택해야 했다. One solution to this problem is to use multi-task learning (MTL) [19] 우리의 해결책은 multi-task learning을 사용하는 것이었다. Note that predicting the sequence of phonetic labels in an utterance and deciding whether an utterance contains a specific trigger phrase or not, are related tasks. 발화 속에서 phonetic label의 시퀀스를 예측하는 것 그리고 발화가 trigger phrase를 포함하고 있는지 아닌지 결정하는 것은 서로 연관성이 있는 일이다. We train a single network with a stack of shared&#x2F;tied biLSTM layers with two seperate output layers (one for each task) and train the network jointly on both sets of training data (Figure 2). 우리는 두개의 출력 레이어(각각 하나의 task씩)를 갖고 있고 공유하는&#x2F;묶인 biLSTM 레이어들의 묶음으로 이루어진 하나의 네트워크를 훈련했고 두 세트의 training data(Figure 2)에 합동으로 엮인 네트워크를 훈련했다. We hypothesise that the joint network is able to learn useful features from both tasks: a) the network can be trained to predict phone labels on a large labelled dataset of general speech which covers a wide distribution of complex acoustic conditions, b) the same network can also learn to discriminate between examples of true triggers and confusable examples on a relatively smaller dataset. 우리는 합동 네트워크가 두 task의 유용한 feature들을 학습이 가능하다고 가정했다: a) 네트워크는 복잡한 음향 조건의 광범위한 분포를 다루는 일반적인 음성의 큰 labelled된 데이터 셋을 기반으로 phone label들을 예측하게끔 훈련될수 있다. b) 같은 네트워크는 또한 상대적으로 적은 데이터 양으로 실제의 trigger phrase와 헷갈리는 예시들을 구분하는 법을 배울수 있다. An alternative view of this process is that the phonetic transcription task with a significantly larger training set acts as a regulariser for the trigger phrase discrimination task with a much smaller dataset. 이 process에 대한 다른 시각은 엄청나게 많은 training set를 사용하는 phonetic transcription task가 regulariser로 훨씬 적은 데이터를 가지고 trigger pharse를 구분할때 사용된다. The objective function for the phrase specific&#x2F;discriminative output layer is defined as follows: the softmax output layer contains two output units, one for the trigger phrase and the other one for the blank symbol used by the CTC loss function [8, 16] phrase를 특정화&#x2F;차별화 하는 output layer의 목적함수는 다음과 같이 정의된다: softmax output layer은 두개의 output unit을 포함하는데 하나는 trigger phrase를 위함이고 또 다른 하나는 CTC loss function에서 사용되는 blanck symbol을 위함이다. EvaluationThere were 100 subjects, approximately balanced between male and female adults. Distances from the device were controlled, ranging from 8 to 15 feet away. 대략적으로 반반의 성비로 100명의 참가자들이 참여했다. 기기와의 거리도 8-15피트 정도로 통제되었다. There are over 13K utterances overall, evenly divided between four acoustic conditions: (a) quiet room, (b) external noise from a TV or kitchen appliance in the room, (c) music playback from the recording device at medium volume, and (d) music playback from the recording device at loud volume. 전반적으로 만 3천개의 발화들이 있는데 네가지 컨디션으로 동일하게 나누어졌다: (a) 조용한 방, (b) 방의 TV 또는 주방기기의 외부 소음. (c) 중간 볼륨의 녹음 장치에서 음악 재생, (d) 큰 볼륨의 녹음 장치에서 음악 재생 These examples are used to measure the proportion of false rejections (FRs). 이 예시들은 FR 비율을 측정하기 위해 사용되었다. In addition to these recordings, this test set also consists of almost 2,000 hours of continuous audio recordings from TV, radio, and podcasts. This allows the measurement of the false-alarm (FA) rate in terms of FAs per hour of active external audio. 그 녹음들에 관해 덧붙이자면, 이 test set 또한 TV, 라디오, 팟캐스트로부터 2천 시간의 연속적인 음성 녹음을 포함하고 있다. 이를 통해 active한 외부 음성의 시간당 FA, FA비율을 측정할 수 있다. The second test set is an unstructured data collection at home by our employees, designed to be more representative of realistic, spontaneous usage of the smart speaker. 두번째 test set은 사원들의 집에서 나는 정제되지 않은 데이터 모음이다. 이는 조금더 현실적이고 즉흥적인 스마트폰 스피커의 사용을 대표한다. With this data, it is possible to measure nearly unbiased false-reject and false-alarm rates for realistic in-home scenarios similar to customer usage. 이 데이터로 현실적인 집 내부의 시나리오들(소비자의 사용과 비슷한)을 위한 편향되지 않은 FA와 FA 비율 측정이 가능하다. We use detection error trade-off (DET) curves to compare the accuracy between models. Each curve displays the FA rate and the proportion of FRs associated with sweeping the trigger threshold for aparticular model. 우리는 DET 곡선을 두 모델의 정확성을 비교하기 위해 사용한다. 각 곡선은 특정 모델에 대한 트리거 임계값을 스윕하는 것과 연관된 FA 비율 및 FR 비율을 표시한다. In practice, we compare the shapes of the DET curves for different models in the vicinity of viable operating points. 실제로 우리는 실행가능한 작동지점 근처에서 서로 다른 모델들의 DET 곡선의 모양을 비교했다. We compare five models: the baseline phonetic CTC model trained on the ASR dataset (blue), the baseline phrase specific model trained on the much smaller training set with randomly initialised weights (red), the same phrase specific model but with weights initialised with the learned weights from the baseline phonetic CTC model (yellow), the phonetic (purple) and phrase specific (green) branches of the proposed MTL model. 우리는 다섯개의 모델을 비교했다 파랑: ASR 데이터로 훈련한 baseline phonetic CTC 모델 빨강: 무작위로 초기화된 가중치를 갖는 훨씬 더 작은 training 세트 상에서 훈련된 baseline phrase 특정 모델 노랑: 동일한 phrase의 특정 모델이지만 기본 음성 CTC 모델의 학습된 가중치를 초기화된 가중치를 갖는 모델 보라: 제안된 MTL모델 중 phonetic branches 초록: 제안된 MTL 모델 중 phrase specific 모델 Note that the phrase specific model with weight initialisation from the baseline phonetic model (yellow) is effectively trained using both datasets. baseline phonetic model의 초기화된 가중치를 가지는 phrase specific 모델은 효과적으로 두 dataset을 사용하면서 훈련되었다. In both test sets, the MTL phonetic (purple) and phrase-specific (green) models outperform the baseline phonetic CTC (blue), reducing the FR rate by almost half at many points along the curve. 양쪽 test set에서 보라색 그리고 초록색 모델을은 파란색보다 FR비율을 다른 곡선에 비해 반절이나 감소시키면서 결과가 좋았다. The non-MTL phrase specific models (red and yellow) yield significantly worse accuracies in comparison, which is unsurprising given that the training dataset is two orders of magnitude smaller compared to the phonetic baseline (blue). MTL을 사용하지 않은 specific 모델(빨강, 노랑) 부분은 심각하게 안좋은 정확성을 상대적으로 보였으며 이것은 training dataset이 phonetic baseline(파랑)에 비해 두자릿수 더 작다는 점을 감안했을때 당연하다. Comparing the structured data evaluation (left) and the take-home data evaluation (right), it is also striking how the error rates are generally much higher for the latter. 구조화된 데이터 evaluation(왼쪽)과 take-home data evaluation(오른쪽)을 비교해보면 일반적으로 후자의 경우 오류율이 더 높은것도 눈에 띈다. ConclusionsWe trained the model to simultaneously produce phonetic transcriptions on a large ASR dataset and to discriminate between difficult examples on a much smaller trigger phrase specific training set. 우리는 큰 ASR dataset에서 phonetic transcription을 제공하고 동시에 훨씬 더 작은 trigger phrase training set에서 어려운 샘플들을 구별하는 모델을 훈련했다. We evaluate the proposed model on two challenging test sets and find the proposed method is able to almost halve errors and does not require any extra model parameters. 우리는 제안된 모델의 두 test set을 평가했고 제안된 방법이 오류를 반감하기 위해 더 많은 파라미터를 필요로 하지 않은다는 것을 발견했다. Link: Multi-Task Learning for Voice Trigger Detection","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Multi-Task Learning","slug":"Paper/Multi-Task-Learning","permalink":"https://jmj3047.github.io/categories/Paper/Multi-Task-Learning/"}],"tags":[{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Multi-Task Learning","slug":"Multi-Task-Learning","permalink":"https://jmj3047.github.io/tags/Multi-Task-Learning/"},{"name":"Voice Trigger Detection","slug":"Voice-Trigger-Detection","permalink":"https://jmj3047.github.io/tags/Voice-Trigger-Detection/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Multi-Task Learning","slug":"Paper/Multi-Task-Learning","permalink":"https://jmj3047.github.io/categories/Paper/Multi-Task-Learning/"}]},{"title":"Multi-Task Learning for Speaker Verification and Voice Trigger Detection","slug":"MTL_for_SV&VTD","date":"2022-10-30T15:00:00.000Z","updated":"2023-05-17T11:50:35.488Z","comments":true,"path":"2022/10/31/MTL_for_SV&VTD/","link":"","permalink":"https://jmj3047.github.io/2022/10/31/MTL_for_SV&VTD/","excerpt":"","text":"Journal&#x2F;Conference : ICASSP IEEEYear(published year): 2020Author: Siddharth Sigtia, Erik Marchi, Sachin Kajarekar, Devang Naik, John BridleSubject: Multi-Task Learning Multi-Task Learning for Speaker Verification and Voice Trigger Detection Summary In this study, we investigate training a single network to perform automatic speech transcription and speaker recognition, both tasks jointly. 본 연구에서는 단일 네트워크를 훈련하여 automatic speech transcription와 speaker recognition의 두 가지 작업을 공동으로 수행하는 방법을 연구합니다. We train the network in a supervised multi-task learning setup, where the speech transcription branch of the network is trained to minimise a phonetic connectionist temporal classification (CTC) loss while the speaker recognition branch of the network is trained to label the input sequence with the correct label for the speaker. 우리는 네트워크의 speech transcription 브랜치가 음성 CTC 손실을 최소화하도록 훈련되는 감독 된 멀티 태스킹 학습 설정에서 네트워크를 훈련시키는 반면, 네트워크의 화자 인식 브랜치는 입력 시퀀스를 화자에 대한 올바른 라벨로 라벨링하도록 훈련된다. Results demonstrate that the network is able to encode both phonetic and speaker information in its learnt representations while yielding accuracies at least as good as the baseline models for each task, with the same number of parameters as the independent models. 결과는 네트워크가 학습된 표현에서 음성 및 화자 정보를 모두 인코딩할 수 있으며 독립 모델과 동일한 수의 매개 변수를 사용하여 각 작업의 기본 모델만큼 정확도를 산출할 수 있음을 보여줍니다. IntroductionVoice trigger detection, which is interchangeably known as keyword spotting [4], wake-up word detection [5], or hotword detection [6], is treated as an acoustic modeling problem. keyword spotting [4], wake-up word detection [5] 또는 hotword detection [6]로 상호 교환 가능하게 알려진 음성 트리거 검출은 음향 모델링 문제에 속한다 Their primary aim is to recognise the phonetic content (or the trigger phrase directly) in the input audio, with no regard for the identity of the speaker. 그들의 주된 목표는 화자의 신원을 고려하지 않고 입력 오디오의 음성 내용(또는 트리거 문구)을 인식하는 것이다. On the other hand, speaker verification systems aim to confirm the identity of the speaker by comparing an input utterance with a set of enrolment utterances which are collected when a user sets up their device. 한편, 스피커 검증 시스템은 사용자가 장치를 설정할 때 수집되는 등록 발화 집합과 입력 발화를 비교하여 화자의 신원을 확인하는 것을 목표로 한다. Speaker verification algorithms can be characterised based on whether the phonetic content in the inputs is limited, which is known as text-dependent speaker verification [9]. 화자 검증 알고리즘은 입력의 음성 콘텐츠가 제한되어 있는지 여부에 따라 특성화할 수 있는데, 이를 text-dependent speaker verification[9]이라고 한다. We believe that knowledge of the speaker would help determine the phonetic content in the acoustic signal and vice versa, therefore estimating both properties is similar to solving simultaneous equations. 우리는 화자에 대한 지식이 음향 신호의 음성 내용을 결정하는 데 도움이 되고 그 반대의 경우도 마찬가지라고 생각합니다. 따라서 두 속성을 모두 추정하는 것은 연립 방정식을 푸는 것과 유사합니다. In this study, the main research question we try to answer is “can a single network efficiently represent both phonetic and speaker specific information?”. 이 연구에서 우리가 대답하려고하는 주요 연구 질문은 “하나의 네트워크가 음성 및 화자 특정 정보를 효율적으로 표현할 수 있습니까?”입니다. From a practical standpoint, being able to share computation between the two tasks can save on-device memory, computation time or latency and the amount of power&#x2F;battery consumed. 실용적인 관점에서 두 작업간에 계산을 공유할 수 있으면 장치 메모리, 계산 시간 또는 대기 시간 및 소비되는 전력 &#x2F; 배터리 양을 절약할 수 있습니다. More generally, we are interested in studying whether a single model can perform multiple speech understanding tasks rather than designing a separate model for each task. 보다 일반적으로, 우리는 각 작업에 대해 별도의 모델을 설계하기보다는 단일 모델이 여러 개의 음성 이해 작업을 수행할 수 있는지 연구하는 데 관심이 있습니다. We train a joint network to perform a phonetic labelling task and a speaker recognition task. 우리는 음성 라벨링 작업과 화자 인식 작업을 수행하기 위해 공동 네트워크를 훈련시켰습니다. We evaluate the 2 branches of the model on a voice trigger detection task and a speaker verification task, respectively. 우리는 음성 트리거 검출 작업과 화자 검증 작업에서 모델의 두 가지를 각각 평가합니다. It is possible for a single network to encode both speaker and phonetic information and yield similar accuracies as the baseline models without requiring any additional parameters. 단일 네트워크가 스피커 및 음성 정보를 모두 인코딩하고 추가 매개 변수를 필요로하지 않고 기준 모델과 유사한 정확도를 산출할 수 있습니다. Voice Trigger Detection BaselineWe extract 40-dimensional log-filterbanks from the audio at 100 frame-per-second (FPS). At every step, 7 frames are spliced together to form symmetric windows and finally this sequence of windows is sub-sampled by a factor of 3, yielding a 280-dimensional input vector to the model at a rate of 33 FPS. The features are input to a stack of 4 bidirectional LSTM layers with 256 units in each layer (Figure 1). This is followed by a fully connected layer and an output softmax layer over context-independent phonemes and additional sentence and word boundary symbols, resulting in a total of 53 output symbols and 6 million model parameters. This model is then trained by minimising the CTC loss function [16]. The training data for this model is 5000 hours of anonymised audio data that is manually transcribed, where all of the recordings are sampled from intentional voice assistant invocations and are assumed to be near-field. Fig. 1. The left branch of the model represents the voice trigger detector, the right branch is the speaker verification model. Solid horizontal arrows represent layers with tied weights, dashed arrows represent layers with weights that may or may not be tied. 모델의 왼쪽 분기는 음성 트리거 검출기를 나타내고, 오른쪽 분기는 화자 검증 모델이다. 실선 화살표는 묶인 가중치가 있는 레이어를 나타내고 점선 화살표는 묶일 수도 있고 묶이지 않을 수도 있는 가중치가 있는 레이어를 나타냅니다. Speaker Verification BaselineWe use a simple location-based attention mechanism [18] to summarise the encoder activations as a fixed-dimensional vector. 우리는 인코더 활성화를 고정 차원 벡터로 요약하기 위해 간단한 위치 기반주의 메커니즘 [18]을 사용합니다. We found the attention mechanism to be particularly effective in the text-independent setting. 우리는 attention 메커니즘이 텍스트 독립적인 환경에서 특히 효과적이라는 것을 발견했다. During inference, given a test utterance x, the speaker embedding is obtained by removing the final softmax layer and using the 128-dimensional activations of the previous layer. 추론 중에 테스트 발화 x가 주어지면 스피커 삽입은 최종 소프트 맥스 레이어를 제거하고 이전 레이어의 128 차원 활성화를 사용하여 얻어집니다. Each training utterance is of the form “Trigger phrase, payload” for e.g.“Hey Siri (HS), play me something I’d like”. For every training example, we generate 3 segments: the trigger phrase, the payload and the whole utterance. We found that breaking the utterances up this way results in models that generalise significantly better. Evaluation ConclustionsOur results demonstrate that sharing the first two layers of the model between the speaker and phonetic tasks gives accuracies that are as good as the individual baselines. 우리의 결과는 화자와 음성 작업 사이에 모델의 처음 두 레이어를 공유하면 개별 기준선만큼 정확도가 높다는 것을 보여줍니다. This result indicates that it is possible to share some of the lowlevel computation between speech processing tasks without hurting accuracies. 이 결과는 정확도를 해치지 않으면 서 음성 처리 작업간에 저수준 계산의 일부를 공유할 수 있음을 나타냅니다. Link: Multi-Task Learning for Speaker Verification and Voice Trigger Detection","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Multi-Task Learning","slug":"Paper/Multi-Task-Learning","permalink":"https://jmj3047.github.io/categories/Paper/Multi-Task-Learning/"}],"tags":[{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Multi-Task Learning","slug":"Multi-Task-Learning","permalink":"https://jmj3047.github.io/tags/Multi-Task-Learning/"},{"name":"Voice Trigger Detection","slug":"Voice-Trigger-Detection","permalink":"https://jmj3047.github.io/tags/Voice-Trigger-Detection/"},{"name":"Speaker Verification","slug":"Speaker-Verification","permalink":"https://jmj3047.github.io/tags/Speaker-Verification/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Multi-Task Learning","slug":"Paper/Multi-Task-Learning","permalink":"https://jmj3047.github.io/categories/Paper/Multi-Task-Learning/"}]},{"title":"Hexo Blog 생성 및 재연결","slug":"Hexo_Create","date":"2022-10-05T15:00:00.000Z","updated":"2023-02-28T07:50:08.127Z","comments":true,"path":"2022/10/06/Hexo_Create/","link":"","permalink":"https://jmj3047.github.io/2022/10/06/Hexo_Create/","excerpt":"","text":"Hexo Blog 생성 간단하게 Hexo 블로그를 만들어 본다. I. 필수 파일 설치 1단계: nodejs.org 다운로드 설치가 완료 되었다면 간단하게 확인해본다. 1$ node -v 2단계: git-scm.com 다운로드 설치가 완료 되었다면 간단하게 확인해본다. 1$ git --version 3단계: hexo 설치 hexo는 npm을 통해서 설치가 가능하다. 1$ npm install -g hexo-cli II. 깃허브 설정 두개의 깃허브 Repo를 생성한다. 포스트 버전관리 (name: myblog) 포스트 배포용 관리 (name: rain0430.github.io) rain0430 대신에 각자의 username을 입력하면 된다. 이 때, myblog repo를 git clone을 통해 적당한 경로로 내려 받는다. $ git clone your_git_repo_address.git III. 블로그 만들기 (옵션) 적당한 곳에 경로를 지정한 다음 다음과 같이 폴더를 만든다. 12$ mkdir makeBlog # 만약 Powershell 이라면 mkdir 대신에 md를 쓴다. $ cd makeBlog 임의의 블로그 파일명을 만든다. 12345$ hexo init myblog$ cd myblog$ npm install$ npm install hexo-server --save$ npm install hexo-deployer-git --save ERROR Deployer not found: git hexo-deployer-git을 설치 하지 않으면 deploy시 위와 같은 ERROR가 발생합니다. _config.yml 파일 설정 싸이트 정보 수정 1234title: 제목을 지어주세요subtitle: 부제목을 지어주세요description: description을 지어주세요author: YourName 블로그 URL 정보 설정 1234url: https://rain0430.github.ioroot: /permalink: :year/:month/:day/:title/permalink_defaults: 깃허브 연동 12345# Deploymentdeploy: type: git repo: https://github.com/rain0430/rain0430.github.io.git branch: main IV. 깃허브에 배포하기 배포 전, 터미널에서 localhost:4000 접속을 통해 화면이 뜨는지 확인해본다. 1234$ hexo generate$ hexo serverINFO Start processingINFO Hexo is running at http://localhost:4000 . Press Ctrl+C to stop. 화면 확인이 된 이후에는 깃허브에 배포한다. 사전에, gitignore 파일에서 아래와 같이 설정을 진행한다. 1234567.DS_StoreThumbs.dbdb.json*.lognode_modules/public/.deploy*/ 최종적으로 배포를 진행한다. 1$ hexo deploy 배포가 완료가 되면 브라우저에서 USERNAME.github.io로 접속해 정상적으로 배포가 되었는지 확인한다. Hexo Blog 재연결 기존 블로그 폴더 파일 압축해서 백업한 후 진행해야 한다. → theme 같은 경우 받아오는거부터 다시 해야 하기 때문 재연결보다는 재생성이라고 말하는게 더 적합하다. 다른 로컬에서 블로그를 재연결해서 사용할 경우 아래와 같이 순차적으로 진행하면 된다. 1234$ hexo init your_blog_repo # 여기는 각자 소스 레포 확인$ cd myblog$ git init $ git remote add origin https://github.com/your_name/your_blog_repo.git # 각자 소스 레포 주소 아래 명령어에서 에러가 발생이 있다. 1$ git pull --set-upstream origin main # 에러 발생 그런 경우, 아래 명령어를 추가한다. 기존의 디렉토리와 파일을 모두 삭제한다는 뜻이다. 1$ git clean -d -f 그리고 에러가 발생했던 명령어를 다시 실행한다. 이 때에는 이제 정상적으로 실행되는 것을 확인할 수 있다. 1$ git pull --set-upstream origin main # 에러 발생 안함 / 소스 확인 이제 정상적으로 환경 세팅은 된 것이다. 순차적으로 아래와 같이 진행하도록 한다. 이 때, theme 폴더에 본인의 테마 소스코드가 잘 있는지 확인을 하도록 한다. 1234$ npm install $ hexo clean$ hexo generate$ hexo server reference 생성 재연결","categories":[{"name":"Setting","slug":"Setting","permalink":"https://jmj3047.github.io/categories/Setting/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://jmj3047.github.io/tags/Hexo/"}],"keywords":[{"name":"Setting","slug":"Setting","permalink":"https://jmj3047.github.io/categories/Setting/"}]},{"title":"Big Query","slug":"Big_Query","date":"2022-09-28T15:00:00.000Z","updated":"2023-03-19T07:01:57.700Z","comments":true,"path":"2022/09/29/Big_Query/","link":"","permalink":"https://jmj3047.github.io/2022/09/29/Big_Query/","excerpt":"","text":"1. 쿼리 실행순서FROM → WHERE → GROUP BY, Aggregation → HAVING → WINDOW → QUALIFY → DISTINCT → ORDER BY → LIMIT 2. JOIN 3. WINDOW 함수 4. DECLARE 변수를 선언 혹은 초기화할 때 사용 DECLARE variable_name[, ...] [variable_type] [ DEFAULT expression]; 1234567-- 아래 쿼리를 실행하면 값이 모두 1로 나오는걸 확인 할 수 있음-- DEFAULT를 안주면 NULL로 지정 됨DECLARE x, y, z INT64 DEFAULT 1;SELECT x,y,z-- 현재 날짜로 d할당DECLARE d DATE DEFAULT CURRENT_DATE(); 위 예시 말고 쿼리의 결과를 사용해 변수를 초기화 할 수도 있음 5. SET DECLARE와 같이 사용 되어짐. DECLARE에서 변수 타입을 지정하고 SET으로 값 할당이 가능, DECLARE에서 두 과정 모두 할 수 있지만 SET은 쿼리 내 어느 위치에서나 사용 가능 함 6. UDF - User Define Function 영구 UDF는 여러 쿼리에서 재사용 할 수 있음CREATE TEMP FUNCTION → 임시 UDF 생성CREATE FUNCTION → 영구 UDF 생성CREATE OR REPLACE FUNCTION → 영구 UDF 생성 및 수정 12345678910111213141516CREATE OR REPLACE FUNCTION ps-datateam.cbt_global_ceo.함수명(변수명 변수타입)RETURNS INT64 --리턴 타입LANGUAGE js -- 작성 언어(임시는 SQL로도 가능한 듯)AS &quot;&quot;&quot;if (mode == &#x27;solo&#x27;) &#123;return 1;&#125; else if (mode == &#x27;duo&#x27;) &#123;return 2;&#125; else if (mode == &#x27;trio&#x27;)&#123;return 3;&#125; else if (mode == &#x27;squad&#x27;)&#123;return 4;&#125; else &#123;return 0;&#125;&quot;&quot;&quot;; -- 함수 7. 파이썬에서 빅쿼리 데이터 사용12345678910111213141516171819from google.cloud import bigqueryfrom google.oauth2 import service_accountkey_path = &quot;키파일 경로&quot;credentials = service_account.Credentials.from_service_account_file(key_path, scopes=[&quot;https://www.googleapis.com/auth/cloud-platform&quot;])client = bigquery.Client(credentials=credentials, project=&#x27;프로젝트명&#x27;)# 쿼리 결과를 CSV파일로 -----------------------------------------------------------------query = client.query(&quot;&quot;&quot;QUERY&quot;&quot;&quot;)# 쿼리 -&gt; DataFramedf_result = query.to_dataframe ()# DataFrame -&gt; csvdf_result.to_csv(&quot;filename.csv&quot;, index=False, encoding=&#x27;euc-kr&#x27;)# 프로젝트&gt;데이터세트 내의 테이블 목록 조회------------------------------------------------dataset_ref = client.dataset(&quot;kr_dict&quot;) # kr_dict라는 데이터 세트의 정보를 조회dataset = client.get_dataset(dataset_ref)list(client.list_tables(dataset)) # 데이터 세트의 테이블 목록을 리스트로 가져옴 Reference1 Reference2","categories":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}],"tags":[{"name":"Big Query","slug":"Big-Query","permalink":"https://jmj3047.github.io/tags/Big-Query/"}],"keywords":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}]},{"title":"Pandas Dataframe 사용법 정리","slug":"Pandas","date":"2022-09-22T15:00:00.000Z","updated":"2022-10-18T15:05:07.992Z","comments":true,"path":"2022/09/23/Pandas/","link":"","permalink":"https://jmj3047.github.io/2022/09/23/Pandas/","excerpt":"","text":"데이터 합치기 https://yganalyst.github.io/data_handling&#x2F;Pd_12&#x2F; https://seong6496.tistory.com/122 https://datascienceschool.net/01 python/04.06 데이터프레임 합성.html https://hyunmin1906.tistory.com/132 on에 대한 설명: https://wikidocs.net/153875 특정 칼럼의 데이터 종류별로 평균&#x2F;합 구하는 방법 https://www.dinolabs.ai/72 행&#x2F;열 순서 및 이름 변경 https://k-glory.tistory.com/21 시리즈, 데이터 프레임으로 만들기 https://hyefungi.tistory.com/68 리스트, 딕셔너리를 데이터프레임, 시리즈로 바꾸는 법 https://jimmy-ai.tistory.com/89 행열 전환 https://computer-science-student.tistory.com/158 데이터 프레임 index 변경하기 https://cosmosproject.tistory.com/337 데이터 프레임 index 조작 https://datascienceschool.net/01 python/04.05 데이터프레임 인덱스 조작.html 데이터 프레임 필요한 열 추출 https://zephyrus1111.tistory.com/43 데이터 프레임 sum 함수 https://www.delftstack.com/ko/api/python-pandas/pandas-dataframe-dataframe.sum-function/ 행추가, 열 삭제&#x2F;추가 방법 https://jimmy-ai.tistory.com/210 https://ldgeao99.tistory.com/8 https://blog.naver.com/rising_n_falling&#x2F;221631637822 데이터프레임에 리스트를 행 추가 https://emilkwak.github.io/dataframe-list-row-append-ignore-index 랜덤 샘플링 https://rfriend.tistory.com/602 특정 조건에 맞는 데이터 추출 https://computer-science-student.tistory.com/375","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Python","slug":"Python/Python","permalink":"https://jmj3047.github.io/categories/Python/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/tags/Python/"},{"name":"Pandas Dataframe","slug":"Pandas-Dataframe","permalink":"https://jmj3047.github.io/tags/Pandas-Dataframe/"}],"keywords":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Python","slug":"Python/Python","permalink":"https://jmj3047.github.io/categories/Python/Python/"}]},{"title":"임계치 조절","slug":"Threshold","date":"2022-09-19T15:00:00.000Z","updated":"2023-02-23T01:38:08.678Z","comments":true,"path":"2022/09/20/Threshold/","link":"","permalink":"https://jmj3047.github.io/2022/09/20/Threshold/","excerpt":"","text":"&lt; 분류에서 사용하는 성능지표 &gt; 1. Confusion Matrix 분류에서 가장 많이 사용되는 오분류표이다. 행렬의 배치는 그리는 사람에 따라 달라질 수 있으며, Scikit learn에 기반한 confusion matrix는 다음과 같다. FP: 예측은 참이나 실제는 거짓, 제 1종 오류FN: 실제는 참이나 예측은 거짓, 제 2종 오류 정밀도에서는 FP를 줄이는 것, 재현율에서는 FN을 줄이는 것이 중요하다.즉 FP, FN이 커지면 정밀도, 재현율 각각 작아진다. 정확도(Accuracy): 전체 데이터 중에서 예측한 값이 실제 값과 부합하는 비율 정밀도, 재현율, 특이도 정밀도(precision): 예측이 참인 값 중 실제 참인 값재현율(recall,Sensitivity,TPR): 실제 참인 값 중 예측도 참인 값특이도(specificity): 예측이 거짓인 값 중 실제 거짓인 값 2. Precision-Recall Trade-off로지스틱 회귀 모형에서 특정 이메일이 스팸일 확률이 0.9이면 스팸일 가능성이 매우 높다고 예측할 수 있다. 이와 반대로 0.03이라면 스팸이 아닐 가능성이 높다. 그렇다면 0.6인 이메일의 경우 어떻게 분류해야 할까? 이때 분류할 기준이 필요한데, 이 기준을 임계값(Threshold)이라 한다. 어떤 분류기의 임계값에 따른 정밀도와 재현율을 그래프로 나타내면 위과 같으며, Precision과 Recall이 만나는 점이 최적의 임계값이다. 임계값을 높이면(Positive로 판별하는 기준을 빡빡하게 잡으면) 정밀도는 올라가고 재현율은 낮아진다. 반대로 임계값을 낮추면(기준을 널널하게 잡으면) 정밀도는 낮아지고 재현율은 높아진다. 이를 정밀도-재현율 트레이드 오프(Precision-Recall Trade-off)라 한다. 어떤 것을 분류하느냐에 따라 정밀도가 더 중요할 때가 있고, 재현율이 더 중요할 때도 있다. 스팸 메일 분류기는 FP를 줄이는 즉 정밀도, 암 환자 분류기는 FN을 줄이는 즉 재현율을 각각 더 중요하게 생각해야 한다. 3. G-mean, F1 measure G-mean, F1 measure 실제 데이터의 특성상 정확도보다는 제1종 오류와 제2종 오류 중 성능이 나쁜 쪽에 더 가중치를 주는 G-mean지표나 정밀도와 재현율만 고려하는 F1 measure가 더 고려해볼 수 있는 지표이다. 둘 다 높으면 높을 수록 좋은 지표이다. (F1 measure가 더 자주 쓰인다.) 4. ROC curve, AUC ROC curve : 양성에 대한 오답&#x2F;정답 비율 시각화 가로축을 1-특이도(FPR), 세로축을 재현율**(TPR)**로 하여 시각화한 그래프이다.FPR과 TPR은 오차 행렬 내 4개의 요소를 사용한 수치이며 다음과 같다. FPR: 실제 Negative 클래스인 인스턴스를 얼마나 잘못 분류했는지를 나타내는 수치.TPR: 실제 Positive인 클래스인 인스턴스를 얼마나 제대로 분류했는 지를 나타내는 수치. 임계값(Threshold)을 변화시키면 FPR이 변하게 된다. 임계값이 높으면(1) 정밀도(Precision)가 높아지며 FP가 낮아지므로(FN이 높아지므로) FPR은 0이다. 반대로, 임계값이 낮으면(0) FP가 높아지고(FN 낮아지므로)TN은 0이므로 FPR은 1이다. 즉 임계값이 낮추면 더 많은 항목이 양성으로 분류되므로 FPR과 TPR이 모두 증가한다. 이렇게 임계값에 따라 FPR을 0~1까지 변화시켜가며 그에 따라 TPR이 어떻게 변화하는지 기록한 것이 ROC curve이다. AUC(area under the curve): ROC곡선 아랫부분 면적: 0~1사이의 값을 가지며, AUC값은 클수록 좋다. 123456from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, precision_score,recall_score,f1_scorefrom sklearn.metrics import precision_recall_curvefrom sklearn.metrics import roc_auc_score,roc_curve#오분류표confusion_matrix(y_test,y_vld) 24는 FN으로 실제는 참이나, 예측은 거짓이다. 즉 생존자를 사망자로 잘못 예측한 경우이다. 반대로 15는 FP이며 사망자를 생존자로 잘못 예측했다. 이 때문에 정확도가 떨어진다. 12# 간략하게 한 번에 보고 싶을 때 사용classification_report(y_test,y_vld) 123456789101112131415161718# 위에(cr)보다 아래 코드를 더 자주 사용한다.accuracy_score(y_test,y_vld)precision_score(y_test,y_vld)recall_score(y_test,y_vld)f1_score(y_test,y_vld)# 예측 확률proba= model.predict_proba(X_test)# precision,recall은 trade off관계, precision_recall_curve( )precision,recall,th = precision_recall_curve(y_test,proba[:,1])plt.xlabel(&#x27;threadhold&#x27;) #임계값plt.ylabel(&#x27;score&#x27;)plt.plot(th,precision[:len(th)],&#x27;red&#x27;,linestyle = &#x27;--&#x27;,label = &#x27;precision&#x27;)plt.plot(th,recall[:len(th)],&#x27;blue&#x27;,label = &#x27;recall&#x27;)plt.legend()plt.show() 12345678# roc_curve( )fpr,tpr,th = roc_curve(y_test,proba[:,1])plt.xlabel(&#x27;FPR-False Positive&#x27;)plt.ylabel(&#x27;TPR-True Positive &#x27;)plt.plot(fpr,tpr,&#x27;red&#x27;,label = &#x27;rate&#x27;)plt.plot([0,1],[0,1],&#x27;blue&#x27;,linestyle = &#x27;--&#x27;,label = &#x27;th 0.5&#x27;)plt.legend()plt.show() 임계값 조정임계값(Threshold)을 조정해 정밀도 또는 재현율의 수치를 높일 수 있다. 하지만 정밀도와 재현율은 trade off관계이기 때문에 한쪽이 올라가면 다른 한쪽이 떨어지기 쉽다. 일반적으로 임계값을 0.5로 정하고 이보다 크면 positive, 작으면 negative이다. predict_proba()를 통해 레이블별 예측확률을 반환한다. Binarizer: threshold 기준값보다 같거나 작으면 0을, 크면 1을 반환한다. &lt;임계값을 0.5로 설정한 경우&gt; 123456789101112131415from sklearn.preprocessing import Binarizer# Binarizer의 threshold 값을 0.5로 설정custom_threshold = 0.5#즉 Positive 클래스의 컬럼 하나만 추출하여 Binarizer를 적용proba_1 = proba[:,1].reshape(-1,1)binarizer=Binarizer(threshold=custom_threshold).fit(proba_1)custom_pred = binarizer.transform(proba_1)recall=recall_score(y_test, custom_pred)acc=accuracy_score(y_test, custom_pred)print(f&quot;재현율:&#123;recall&#125;, 정확도:&#123;acc:.4f&#125;&quot;) &lt;임계값을 0.4로 설정한 경우&gt; 12345678910111213141516from sklearn.preprocessing import Binarizer# Binarizer의 threshold 값을 0.4로 설정custom_threshold = 0.4# predict_proba() 결과 값의 두 번째 컬럼,#즉 Positive 클래스의 컬럼 하나만 추출하여 Binarizer를 적용proba_1 = proba[:,1].reshape(-1,1)binarizer=Binarizer(threshold=custom_threshold).fit(proba_1)custom_pred = binarizer.transform(proba_1)recall=recall_score(y_test, custom_pred)acc=accuracy_score(y_test, custom_pred)print(f&quot;재현율:&#123;recall:.4f&#125;, 정확도:&#123;acc:.4f&#125;&quot;) 임계값을 낮추면 recall이 상승하고, precision이 떨어진다. Reference","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/tags/Python/"},{"name":"Threshold Adjustment","slug":"Threshold-Adjustment","permalink":"https://jmj3047.github.io/tags/Threshold-Adjustment/"},{"name":"Confusion Matrix","slug":"Confusion-Matrix","permalink":"https://jmj3047.github.io/tags/Confusion-Matrix/"},{"name":"Precision-Recall","slug":"Precision-Recall","permalink":"https://jmj3047.github.io/tags/Precision-Recall/"},{"name":"F1 measure","slug":"F1-measure","permalink":"https://jmj3047.github.io/tags/F1-measure/"},{"name":"ROC curve","slug":"ROC-curve","permalink":"https://jmj3047.github.io/tags/ROC-curve/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}]},{"title":"Grid Search CV","slug":"Grid_Search","date":"2022-09-15T15:00:00.000Z","updated":"2023-02-23T01:37:20.820Z","comments":true,"path":"2022/09/16/Grid_Search/","link":"","permalink":"https://jmj3047.github.io/2022/09/16/Grid_Search/","excerpt":"","text":"Grid search finds the optimal parameters; each model has its own parameters, and it compares which combination yields the best score. This time, we will see a combination of two parameters and use decision tree. 123456789101112from sklearn.datasets import load_irisfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.model_selection import GridSearchCV, train_test_splitimport pandas as pd#훈련데이터 검증데이터 분류iris = load_iris()data = iris.datatarget = iris.targetX_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=999) Bring the iris data Data and target are classified into training data and validation data, and the ratio of validation data is set to 20%. Random_state &#x3D; 999 means that you will continue to use random data that you have picked once. It does not matter if it is not 999 or any other number. 123456789101112#그리드서치dtree = DecisionTreeClassifier()grid_parameters = &#123;&quot;max_depth&quot;: [1, 2, 3], &quot;min_samples_split&quot;: [2, 3] &#125;grid_dtree = GridSearchCV(dtree, param_grid=grid_parameters, cv=3, refit=True)grid_dtree.fit(X_train, y_train) Create a decision tree model, save the value of the desired parameter in dictionary form. Cv is the number of sets of training data divided for cross-validation. Refit &#x3D; True means finding the optimal parameter and then learning based on it. It is added another step before fitting 12345678910111213#결과를 데이터 프레임으로 변환scores_df = pd.DataFrame(grid_dtree.cv_results_)print(scores_df)출력mean_fit_time std_fit_time mean_score_time std_score_time param_max_depth param_min_samples_split params split0_test_score split1_test_score split2_test_score mean_test_score std_test_score rank_test_score0 0.001321 4.587829e-04 0.000997 3.893359e-07 1 2 &#123;&#x27;max_depth&#x27;: 1, &#x27;min_samples_split&#x27;: 2&#125; 0.675 0.675 0.700 0.683333 0.011785 51 0.001676 4.799690e-04 0.001196 2.498432e-04 1 3 &#123;&#x27;max_depth&#x27;: 1, &#x27;min_samples_split&#x27;: 3&#125; 0.675 0.675 0.700 0.683333 0.011785 52 0.002052 8.137361e-05 0.000613 4.378794e-04 2 2 &#123;&#x27;max_depth&#x27;: 2, &#x27;min_samples_split&#x27;: 2&#125; 0.925 0.925 0.975 0.941667 0.023570 33 0.000998 8.104673e-07 0.000332 4.694597e-04 2 3 &#123;&#x27;max_depth&#x27;: 2, &#x27;min_samples_split&#x27;: 3&#125; 0.925 0.925 0.975 0.941667 0.023570 34 0.001271 2.113806e-04 0.000563 4.170978e-04 3 2 &#123;&#x27;max_depth&#x27;: 3, &#x27;min_samples_split&#x27;: 2&#125; 0.975 0.950 0.975 0.966667 0.011785 15 0.001044 4.221438e-05 0.000665 4.699172e-04 3 3 &#123;&#x27;max_depth&#x27;: 3, &#x27;min_samples_split&#x27;: 3&#125; 0.975 0.950 0.975 0.966667 0.011785 1 To see the results fitted above, use cv_results_. Then there are a number of indicators. 123456789101112131415161718#최적의 파라미터 출력scores_df = scores_df[[&quot;params&quot;, &quot;mean_test_score&quot;, &quot;rank_test_score&quot;, &quot;split0_test_score&quot;, &quot;split1_test_score&quot;, &quot;split2_test_score&quot;]]print(scores_df)# 최고의 파라미터 저장해줌print(f&quot;최적의 파라미터: &#123;grid_dtree.best_params_&#125;&quot;)print(f&quot;최고 정확도: &#123;grid_dtree.best_score_&#125;&quot;)출력 params mean_test_score rank_test_score split0_test_score split1_test_score split2_test_score0 &#123;&#x27;max_depth&#x27;: 1, &#x27;min_samples_split&#x27;: 2&#125; 0.683333 5 0.675 0.675 0.7001 &#123;&#x27;max_depth&#x27;: 1, &#x27;min_samples_split&#x27;: 3&#125; 0.683333 5 0.675 0.675 0.7002 &#123;&#x27;max_depth&#x27;: 2, &#x27;min_samples_split&#x27;: 2&#125; 0.941667 3 0.925 0.925 0.9753 &#123;&#x27;max_depth&#x27;: 2, &#x27;min_samples_split&#x27;: 3&#125; 0.941667 3 0.925 0.925 0.9754 &#123;&#x27;max_depth&#x27;: 3, &#x27;min_samples_split&#x27;: 2&#125; 0.966667 1 0.975 0.950 0.9755 &#123;&#x27;max_depth&#x27;: 3, &#x27;min_samples_split&#x27;: 3&#125; 0.966667 1 0.975 0.950 0.975최적의 파라미터: &#123;&#x27;max_depth&#x27;: 3, &#x27;min_samples_split&#x27;: 2&#125;최고 정확도: 0.9666666666666667 Print out the optimal parameters which is max_depth:3, min_samples_split:2 and the score is 0.96 In this case, the function that gives the best parameter and the highest score is best_params_, best_score_. Korean reference","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/tags/Python/"},{"name":"Grid Search CV","slug":"Grid-Search-CV","permalink":"https://jmj3047.github.io/tags/Grid-Search-CV/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}]},{"title":"Ensemble Model","slug":"Ensemble","date":"2022-09-14T15:00:00.000Z","updated":"2023-02-23T01:36:55.728Z","comments":true,"path":"2022/09/15/Ensemble/","link":"","permalink":"https://jmj3047.github.io/2022/09/15/Ensemble/","excerpt":"","text":"1. Ensemble Model어떠한 한 현상에 대한 답을 얻는다고 가정해보자, 많은 경우에 한 명의 전문가보다 여려 명의 일반인들의 의견이 더 나은 경우가 있다. 위 예제와 비슷하게, 하나의 좋은 모형(회귀,분류)으로부터 예측을 하는 것보다 여러 개의 모형으로부터 예측을 수집하는 것이 더 좋은 예측을 할 수 있다. 이러한 여러 개의 모형을 앙상블이라고 부르고, 여러 개의 모형을 조화롭게 학습시키는 것을 앙상블 학습이라고 한다. 그리고 6주차에서 배운 결정 트리 모형이 하나가 아니라, 훈련 세트를 무작위로 다른 서브셋으로 만들어서 결정 트리 분류기를 만들고, 많은 모형들 중에서 가장 많은 선택을 받은 클래스를 예측하는 앙상블 모형을 랜덤포레스트라고 한다. 오늘날의 랜덤포레스트 모델은 가장 강력한 머신러닝 알고리즘 하나이다. 그리고 머신러닝 대회에서 우승하는 솔루션들은 대부분 앙상블 방법을 사용하여서 최고 성능을 낸다. 뒤에서 앙상블 방법들 중 배깅을 설명할 것이다. 투표 기반 분류기 하나의 데이터셋을 여러종류의 분류기들로 훈련시켰다고 가정해보자. 위에서 언급한대로 하나의 좋은 모델을 사용하는 것보다, 여러 종류의 분류기들이 가장 많이 예측한 클래스를 예측하는 것이 더 좋은 분류기를 만드는 매우 간단한 방법이다. 이렇게 다수결의 투표로 정해지는 분류기를 hard voting(집접 투표) 분류기라고 한다. 놀랍게도 위 모델 중 가장 성능이 좋은 모델의 정확도보다 다수결을 통해 예측한 앙상블 모델의 성능이 높은 경우가 많다. 이렇게 랜덤 추측보다 조금 더 높은 성능을 내는 weak learner(약한 학습기) 가 충분히 많고 다양하다면 strong learner(강한 학습기)가 될 수 있다. 어떻게 약한 학습기가 강한 학습기가 되어서 더 좋은 성능을 낼 수 있을까?, 이 질문은 &quot;큰 수의 법칙&quot;으로 설명될 수 있다. 먼저, 50:50의 동전이 아니라, 51:49의 불균형하게 앞면과 뒷면이 나오는 동전이 있다고 가정을 해보자. 이 동전을 1,000번을 던진다면 거의 앞면 510번과 뒷면 490번이 나올 것이다. 수학적으로 1,000번을 던졌을 때 앞면이 더 많게 나오는 확률은 거의 75% 정도 된다. 수학적으로 10,000번을 던졌을 때 앞면이 더 많게 나오는 확률은 거의 97% 정도 된다. 위 수학적 계산은 이항분포의 확률 질량 함수로 계산 가능하다. ex) 1-scipy.stats.binom.cdf(499,1000,0.51) &#x3D; 0.747 위의 내용을 기반으로 우리의 약한 분류기(51%) 1,000개로 앙상블 모형을 구축하고, 가장 많은 클래스를 예측으로 삼는다면 75%, 10,000개로 모형을 만들면 97% 정도의 성능을 낼 수 있다. 하지만..! 위의 과정은 모든 분류기가 완벽하게 독립이고, 모델의 예측 오차에 대해서 상관관계가 없을때만 가능하다. 🌞 TIP : 앙상블에서 예측기가 가능한 서로 독립일 때 최고 성능을 발휘한다. 그래서 가능한 다양한 알고리즘을 사용해서 학습을 하면 다양한 종류의 오차를 만들기 때문에 앙상블 모델의 성능을 높일 수 있다. 여러 종류의 알고리즘을 사용해서 투표기반 분류기를 만드는 예제를 해보자. 123456789101112131415161718192021222324252627282930313233343536373839404142from sklearn.datasets import load_irisfrom sklearn.ensemble import RandomForestClassifier,VotingClassifierfrom sklearn.linear_model import LogisticRegressionfrom sklearn.svm import SVCfrom sklearn.metrics import accuracy_scorefrom sklearn.model_selection import train_test_split# 데이터셋 로드iris = load_iris()X = iris.data[:,2:] # 꽃잎의 길이, 너비Y = iris.targetx_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.3,random_state=2021,shuffle=True)# 약한 학습기 구축log_model = LogisticRegression()rnd_model = RandomForestClassifier()svm_model = SVC()# 앙상블 모델 구축# 만약에 모든 모델이 predict_proba() 메서드가 있으면, 예측의 평균을 내어 soft voting(간접 투표)도 할수 있다.# 간접 투표 방식은 확률이 높은 투표에 비중을 두기 때문에 성능이 더 높다. (voting=&#x27;soft&#x27; 사용)# svc는 기본적으로 predict_proba를 제공하지 않아, probability = True 지정 해야 사용 가능# 대신 svc에서 probability = True를 지정하면 교차 검증을 사용해서 확률을 추정하기 때문에 훈련 속도 느려짐# 대신 성능을 올라감voting_model = VotingClassifier( estimators=[(&#x27;lr&#x27;,log_model),(&#x27;rf&#x27;,rnd_model),(&#x27;svc&#x27;,svm_model)], # 3개의 약한 학습기 voting=&#x27;hard&#x27; # 직접 투표(hard voting))# 앙상블 모델 학습voting_model.fit(x_train,y_train)# 모델 비교for model in (log_model,rnd_model,svm_model,voting_model): model.fit(x_train,y_train) y_pred = model.predict(x_test) print(model.__class__.__name__,&quot; : &quot;,accuracy_score(y_test,y_pred))&gt; LogisticRegression : 1.0 RandomForestClassifier : 0.9555555555555556 SVC : 1.0 VotingClassifier : 1.0 2.배깅과 페이스팅 앙상블 모형의 좋은 성능을 내기 위해서는 다양한 종류의 오차를 만들어야 하고, 그러기 위해서는 다양한 알고리즘을 사용해야 한다고 배웠다. 다양한 오차를 만들기위한 다른 하나의 방법으로는 훈련 세트의 서브셋을 무작위로 구성하여 모델을 학습시키는 것이 있다. 이를 배깅과 페이스팅이라고 부른다. 배깅 : 훈련 세트의 중복을 허용하여 샘플링을 하는 방식 (통계학에서는 “부트스트래핑”이라고도 부름) 페이스팅 : 훈련 세트의 중복을 허용하지 않고 샘플링 하는 방식 배깅은 각 예측기가 학습하는 서브셋에 다양성을 증가시키므로 페이스팅보다 편향이 조금 더 높다. 하지만 배깅은 예측기들의 상관관계를 줄이므로 앙상블의 분산을 감소 시킨다. 전반적으로 배깅이 더 나은 모델을 만들지만, 시간과 장비가 좋다면 교차검증으로 배깅과 페이스팅을 둘다 해보면 좋다. 1. 사이킷런의 배깅과 페이스팅 12345678910111213141516171819202122from sklearn.ensemble import BaggingClassifierfrom sklearn.tree import DecisionTreeClassifier# 모델 구축# BaggingClassifier에서 사용한 분류기가 클래스 확률추정(predict_proba)이 가능하면 자동으로 간접 투표 사용 bag_model = BaggingClassifier( DecisionTreeClassifier(), # 약한 학습기(결정 트리) n_estimators=500, # 약한 학습기(결정 트리) 500개 생성 max_samples=0.05, # 0.0~1.0 사이 실수 선택(실수 x 샘플 수) 혹은 샘플수 지정 bootstrap=True, # True : 배깅, False : 페이스팅 n_jobs=-1 # 훈련과 예측에 사용할 CPU 코어 수 (-1 : 가용한 모든 코어 사용))# 모델 학습bag_model.fit(x_train,y_train)# 모델 예측y_pred = bag_model.predict(x_test)# 모델 평가print(bag_model.__class__.__name__,&quot; : &quot;,accuracy_score(y_test,y_pred))&gt; BaggingClassifier : 0.9777777777777777 단일 결정 트리와 배깅을 사용한 결정트리 앙상블의 결정경계를 비교해보면 트리 앙상블이 더욱 일반화가 잘 된것을 확인할 수 있다. 2. oob 평가 배깅(중복 허용 샘플링)을 하다보면 평균적으로 훈련 샘플의 약 63%정도만 추출되고 나머지 약 37%는 추출되지 않고, 이렇게 추출되지 않은 샘플들을 oob(out-of-bag)샘플이라고 부른다. 예측기가 훈련되는 동안에는 oob샘플을 사용하지 않으므로, 검증 세트나 교차 검증을 사용하지 않고 oob샘플만을 가지고 모델 최적화를 위한 평가를 할 수 있다. 앙상블의 평가는 각 예측기의 oob평가의 평균으로 확인한다. 1234567891011121314151617181920# 모델 구축bag_model = BaggingClassifier( base_estimator = DecisionTreeClassifier(), n_estimators = 500, bootstrap = True, n_jobs = -1, oob_score = True # oob평가를 위해 True를 지정한다.)# 모델 학습bag_model.fit(x_train,y_train)# 모델 평가(oob_score_)print(&#x27;oob_score : &#x27;,bag_model.oob_score_)# 모델 평가y_pred = bag_model.predict(x_test)print(&#x27;test_score : &#x27;,accuracy_score(y_test,y_pred))&gt;oob_score : 0.9523809523809523 test_score : 0.9333333333333333 3. 랜덤 포레스트 랜덤포레스트는 일반적으로 배깅방법을 사용한 결정트리 앙상블 모델이다. 그래서 BaggingClassifier에 DecisionTreeClassifier를 넣는 대신, RandomForestClassifier를 사용할 수 있다. 그래서 RandomForestClassifier는 DecisionTreeClassifier와 BaggingClassifier 매개변수 모두 가지고 있다. 랜덤포레스트 모델은 트리의 노드를 분할할 때 전체 특성 중에서 최선의 특성을 찾는 것이 아니라, 무작위로 선택한 특성들 중에서 최선의 특성을 찾는 방식을 채택하여 무작위성을 더 가지게 된다. 이를 통해 약간의 편향은 손해보지만, 더욱 다양한 트리를 만들므로 분산을 전체적으로 낮추어서 더 훌륭한 모델을 만들 수 있다. 123456789101112131415161718from sklearn.ensemble import RandomForestClassifier# 랜덤포레스트 모델 구축rnd_model = RandomForestClassifier( n_estimators = 500, # 예측기 500개 max_leaf_nodes = 16, # 자식노드의 최대 개수 n_jobs = -1 # CPU 코어 구동 개수)# 모델 학습rnd_model.fit(x_train,y_train)# 모델 예측y_pred_rf = rnd_model.predict(x_test)# 모델 평가print(&quot;rnd_model : &quot;,accuracy_score(y_pred_rf,y_test))&gt; rnd_model : 0.9333333333333333 Reference https:&#x2F;&#x2F;velog.io&#x2F;@changhtun1&#x2F;ensemble#-랜덤-포레스트","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"Ensemble Model","slug":"Ensemble-Model","permalink":"https://jmj3047.github.io/tags/Ensemble-Model/"},{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/tags/Python/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}]},{"title":"Decision Tree Classifier","slug":"Decision_Tree","date":"2022-09-12T15:00:00.000Z","updated":"2023-02-23T01:40:32.401Z","comments":true,"path":"2022/09/13/Decision_Tree/","link":"","permalink":"https://jmj3047.github.io/2022/09/13/Decision_Tree/","excerpt":"","text":"1. 의사결정트리 데이터에 있는 규칙을 학습을 통해 자동으로 찾아내 트리 기반의 분류 규칙을 만드는 알고리즘입니다. 조금 더 쉽게 하자면 if else를 자동으로 찾아내 예측을 위한 규칙을 만드는 알고리즘입니다. 하지만 Decision Tree에서 많은 규칙이 있다는 것은 분류 방식이 복잡해진다는 것이고이는 과적합(Overfitting)으로 이어지기 쉽습니다. 트리의 깊이(depth)가 깊어질수록 결정트리는 과적합되기 쉬워 예측 성능이 저하될 수 있습니다. 가능한 적은 규칙노드로 높은 성능을 가지려면 데이터 분류를 할 때 최대한 많은 데이터 세트가 해당 분류에 속할 수 있도록 규칙 노드의 규칙이 정해져야 합니다. 이를 위해 최대한 균일한 데이터 세트가 구성되도록 분할(Split)하는 것이 필요합니다.(분할된 데이터가 특정 속성을 잘 나타내야 한다는 것입니다.) 규칙 노드는 정보균일도가 높은 데이터 세트로 쪼개지도록 조건을 찾아 서브 데이터 세트를 만들고, 이 서브 데이터에서 이런 작업을 반복하며 최종 클래스를 예측하게 됩니다. 사이킷런에서는 기본적으로 지니계수를 이용하여 데이터를 분할합니다. 지니계수 : 경제학에서 불평등지수를 나타낼 때 사용하는 것으로 0일 때 완전 평등, 1일 때 완전 불평등을 의미합니다. 머신러닝에서는 데이터가 다양한 값을 가질수록 평등하며 특정 값으로 쏠릴 때 불평등한 값이 됩니다. 즉, 다양성이 낮을수록 균일도가 높다는 의미로 1로 갈수록 균일도가 높아 지니계수가 높은 속성을 기준으로 분할된다. 2. 파라미터정보 균일도 측정 방법 정보 이득 방식: 엔트로피는 데이터의 혼잡도를 의미한다. 엔트로피가 놓다는 것은 혼잡도가 높다는 것 지니계수: 불평등지수&#x2F; 이 값이 0이면 평등하다는 것을(분류가 잘됐다) 뜻한다. 이 값이 리프노드가 된다 min_samples_split 노드를 분할하기 위한 최소한의 샘플 데이터수 → 과적합을 제어하는데 사용 Default &#x3D; 2 → 작게 설정할 수록 분할 노드가 많아져 과적합 가능성 증가 min_samples_leaf 리프노드가 되기 위해 필요한 최소한의 샘플 데이터수 min_samples_split과 함께 과적합 제어 용도 불균형 데이터의 경우 특정 클래스의 데이터가 극도로 작을 수 있으므로 작게 설정 필요 max_features 주요 파라미터 최적의 분할을 위해 고려할 최대 feature 개수 Default &#x3D; None → 데이터 세트의 모든 피처를 사용 int형으로 지정 →피처 갯수 &#x2F; float형으로 지정 →비중 sqrt 또는 auto : 전체 피처 중 √(피처개수) 만큼 선정 log : 전체 피처 중 log2(전체 피처 개수) 만큼 선정 max_depth 트리의 최대 깊이 default &#x3D; None → 완벽하게 클래스 값이 결정될 때 까지 분할 또는 데이터 개수가 min_samples_split보다 작아질 때까지 분할 깊이가 깊어지면 과적합될 수 있으므로 적절히 제어 필요 max_leaf_nodes 리프노드의 최대 개수 random_state 주로 검색해서 나오는 소스코드에 random_state &#x3D; 42 라고 되어있어서 엄청난 의미를 가진 것 같지만 사실 42라는 random_state에 할당된 숫자 자체에 큰 의미는 없습니다. 중요한건 이 random_state를 None으로 두냐 정수를 넣느냐더라구요. random_status가 None인 경우 한번 Decision tree를 생성할 때 1,3,47,5…번 데이터를 이용했다고 해서 다시 이 Decision tree를 생성할 때 1,3,47…번째 데이터를 이용하지는 않습니다. 또 다른 어떤 난수번째의 데이터를 이용하게 되는거죠. 만약 random_state가 None이면 정말 규칙없는 어떤 데이터를 뽑아서 Decision tree를 생성하게 되지만 random_state에 어떤 값이 있다면 난수 생성에 어떠한 규칙(이건 만든사람 말고는 알 수 없음)을 넣어서 동일한 결과가 나오게 합니다. 예를 들어 1이라는 값을 넣어서 1,3,47,5…번째 데이터를 이용했다면 또다시 1을 넣으면 1,3,47,5…번째 데이터를 사용하게 되는거죠. 이걸 일반적으로는 random seed라고 합니다. random seed 일반적으로 시스템이 난수를 만들 때 말이 난수지 일정한 패턴의 수를 생성합니다. 여기서 인자로 random seed라는걸 넣어서 어떠한 규칙을 만들어주는건데 C에서는 기본적으로 random seed가 정해져있어서 일반적으로 시간을 random seed로 쓰는 반면 파이썬에서는 기본적으로 이 random seed가 없는 경우 완전 랜덤이 되더라구요. 그래서 이 Random Seed라는건 불규칙속에 규칙을 만들어주는 매개변수라고 생각해주시면 됩니다. random_state를 사용하는 이유 만약 Random_state를 None으로 두는 경우 Decisiontreeclassifier 함수를 이용해 Decision tree를 생성하면 그때그때 다른 데이터를 이용하기 때문에 결과가 바뀝니다. 그러나 Random_state에 변수를 입력할 경우 특정한 규칙을 갖게 되고 A라는 사람이 random_state&#x3D;1로 Decision tree를 생성할 때와 B라는 사람이 random_state&#x3D;1로 Decision tree를 생성할 때의 결과가 동일해지도록 하는거죠. 그러니까 random_state가 0인지 1인지 42인지보다는 같은 변수를 이용해 같은 결과를 도출해내는 데에 큰 의미가 있습니다. 3. 장,단점장점 쉽고 직관적입니다. 각 피처의 스케일링과 정규화 같은 전처리 작업의 영향도가 크지 않습니다. 단점 규칙을 추가하며 서브트리를 만들어 나갈수록 모델이 복잡해지고, 과적합에 빠지기 쉽습니다 → 트리의 크기를 사전에 제한하는 튜닝이 필요합니다. 4. 쿼리 구현123456789101112131415161718192021222324252627282930313233343536373839404142from sklearn.tree import DecisionTreeClassifierfrom sklearn.linear_model import LogisticRegressionfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.preprocession import LabelEncoder, OneHotEncoder, StandardScaler, MinMaxScaler, Binarizerfrom sklearn.model_selection import train_test_split, GridSearchCVfrom skelearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_scorefrom sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curveimport pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as snsimport warningswarnings.filterwarnings(&#x27;ignore&#x27;)from sklearn.datasets import load_iris#분류기 생성dtc_iris = DecisionTreeClassifier(random_state=100)#데이터 로드 및 전처리#학습, 테스트 데이터로 분리iris_data = load_iris()X_train, X_test, y_train, y_test, = train_test_split(iris_data.data, iris_data.target, test_size=0.2, random_state=100)#학습dtc_iris.fit(X_train, y_train)from sklearn.tree import export_graphviz#export_graphiz()의 호출 결과로 out_file로 지정된 tree.dot파일을 생성함export_graphiz(dtc_iris, out_file=&quot;tree.dot&quot;, class_names = iris_data.target_names, feature_names = iris_data.feature_names, impurity=True, filled=True)print(&#x27;===============max_depth의 제약이 없는 경우의 Decision Tree 시각화==================&#x27;)import graphviz# 위에서 생성된 tree.dot 파일을 Graphiviz 가 읽어서 시각화with open(&quot;tree.dot&quot;) as f: dot_graph = f.read()graphviz.Source(dot_graph) 12345678910111213import seaborn as snsimport numpy as np%matplotlib inline# feature importance 추출print(&quot;Feature Importances:\\n&#123;0&#125;\\n&quot;.format(np.round(dt_clf.feature_importances_, 3)))# feature 별 feature importance 매핑for name, value in zip(iris_data.feature_names, dt_clf.feature_importances_): print(&#x27;&#123;0&#125;: &#123;1:.3f&#125;&#x27;.format(name, value)) # feature importance 시각화sns.barplot(x=dt_clf.feature_importances_, y=iris_data.feature_names) Reference https://injo.tistory.com/15 https://continuous-development.tistory.com/173 https://jerry-style.tistory.com/entry/Decisiontreeclassifier-함수의-파라미터-randomstate란","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/tags/Python/"},{"name":"Decision Tree Classifier","slug":"Decision-Tree-Classifier","permalink":"https://jmj3047.github.io/tags/Decision-Tree-Classifier/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}]},{"title":"Density-Based Spatial Clustering of Applications with Noise","slug":"DBSCAN","date":"2022-09-11T15:00:00.000Z","updated":"2023-02-23T01:36:29.696Z","comments":true,"path":"2022/09/12/DBSCAN/","link":"","permalink":"https://jmj3047.github.io/2022/09/12/DBSCAN/","excerpt":"","text":"1. What is Density-Based Spatial Clustering of Applications with Noise DBSCAN (Density-based spatial clustering of applications with noise) uses density-based clustering among clustering algorithms. In the case of K-Means or Hierarchical clustering, clustering is performed using the distance between clusters. Density-based clustering is a method of clustering high-density parts because the dots are densely clustered. To put it simply, if there are ‘n’(or more) points within a radius ‘x’ of a certain point, it is recognized as a single cluster. For example, suppose that there is a point ‘p’, and if there are ‘m’(minPts) points within a distance ‘e’(epsilon) from the point ‘p’, it is recognized as a cluster. In this condition, that is, a point ‘p’ having ‘m’ points within a distance ‘e’ is called a core point. To use the DBSCAN algorithm, the distance epsilon value from the reference point and the number of points(minPts) within this radius, should be passed as a factor. In the figure below, if minPts &#x3D; 4, if there are more than four points in the radius of epsilon around the blue point ‘P’, it can be judged as one cluster, and in the figure below ‘P’ becomes a core point because there are five points. In the figure below, since the gray point P2 has three points within the epsilon radius based on the point P2, it does not reach minPts&#x3D;4, so it does not become the core point of the cluster, but it is called a border point because it belongs to the cluster with the previous point P as the core point. In the figure below, P3 becomes a core point because it has four points within the epsilon radius. However, another core point P is included in the radius around P3, and in this case, core point P and P3 are considered to be connected and are grouped into one cluster. Finally, P4 in the figure below is not included in the range that satisfies minPts&#x3D;4 no matter what point is centered. In other words, it becomes an outlier that does not belong to any cluster, which is called noise point. Putting it all together, we get the following picture: In summary, if there are more points than the number of minPts in the epsilon radius around a point, it becomes a cluster around that point, and that point is called a core point. When a core point becomes part of a cluster of different core points, it became one big cluster. A point belonging to a cluster but not a core point by itself is called a border point, and is mainly a point forming the outer edge of the cluster. A point that does not belong to any cluster becomes a noise point. 2. Key Points The advantage of the DBSCAN algorithm is that it does not have to set the number of clusters like K Means, and because clusters are connected to each other according to the density of clusters, clusters with geometric shapes can be found well, and outlier detection is possible through noise point. It is oftenly used for learning but not in the field → If it is small data, you can use it, but because there is a lot of data in the field, use efficiency is low. Example of clustering geometry Sample Code Korean reference","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"Clustering","slug":"Clustering","permalink":"https://jmj3047.github.io/tags/Clustering/"},{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/tags/Python/"},{"name":"DBSCAN","slug":"DBSCAN","permalink":"https://jmj3047.github.io/tags/DBSCAN/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}]},{"title":"Comparison K means & GMM","slug":"Kmeans_VS_GMM","date":"2022-09-10T15:00:00.000Z","updated":"2023-02-23T01:36:04.330Z","comments":true,"path":"2022/09/11/Kmeans_VS_GMM/","link":"","permalink":"https://jmj3047.github.io/2022/09/11/Kmeans_VS_GMM/","excerpt":"","text":"1. K-Means It can be used for easy, concise, and large data. If the number of features becomes too large with distance-based algorithms, the performance of clustering is degraded. Therefore, in some cases, it is applied with reducing dimensions using PCA In addition, since it is an iterative algorithm, it is a model that is quite sensitive to outliers and learning execution time could slow down when the number of iterations increases rapidly. The K-means API in Scikit-learn provides the following key parameters. n_clusters : As the number of clusters defined in advance, it defines how many clusters K-means will cluster into, that is, the “K” value. init : It is a method of initially setting the coordinates of the cluster center point and is usually set in the ‘k-means++’ method. It is used because setting it in a random way can lead to out-of-the-way results. max_iter : It is to set the number of iterations. If clustering is completed before the set number of times is reached, it ends in the middle even if the number of iterations is not filled. The following are attribute values returned by the K-means API provided by Skikit-learn. In other words, these are values returned by K-means after performing all clustering. labels_ : Returns the cluster label to which each individual data point belongs. (However, keep in mind that this label value is not mapped to the same value as the label value of the actual original data!) cluster_centers_ : After clustering into K clusters, the coordinates of the center points of each K cluster are returned. Using this, the center point coordinates can be visualized. 1234567891011121314151617181920212223242526from sklearn.datasets import make_blobs# X에는 데이터, y에는 클러스터링 된 label값 반환X, y = make_blobs(n_samples=200, n_features=2, centers=3, cluster_std=0.8, random_state=1)print(X.shape, y.shape)# y Target값 분포 확인# return_counts=True 추가하면 array요소마다 value_counts()해줌unique, counts = np.unique(y, return_counts=True)# 클러스터링용으로 생성한 데이터 데이터프레임으로 만들기cluster_df = pd.DataFrame(data=X, columns=[&#x27;ftr1&#x27;,&#x27;ftr2&#x27;])cluster_df[&#x27;target&#x27;] = ycluster_df.head()# 생성 데이터포인트들 시각화해보기target_lst = np.unique(y)markers = [&#x27;o&#x27;,&#x27;s&#x27;,&#x27;^&#x27;,&#x27;P&#x27;,&#x27;D&#x27;]for target in target_lst: target_cluster = cluster_df[cluster_df[&#x27;target&#x27;]==target] plt.scatter(x=target_cluster[&#x27;ftr1&#x27;], y=target_cluster[&#x27;ftr2&#x27;], edgecolor=&#x27;k&#x27;, marker=markers[target])plt.show() Let’s look at the distribution of individual data made with the make_blobs function that creates a clustering dataset. Now, let’s apply K-means to the above data to visualize the coordinates of the center points of each cluster. 123456789101112131415161718192021222324252627282930313233# K-means 클러스터링 수행하고 개별 클러스터 중심을 시각화# 1.K-means 할당kmeans = KMeans(n_clusters=3, init=&#x27;k-means++&#x27;, max_iter=200, random_state=12) # X는 cluster_df의 feature array임.cluster_labels = kmeans.fit_predict(X)cluster_df[&#x27;kmeans_label&#x27;] = cluster_labels# 2.K-means속성의 cluster_centers_는 개별 클러스터의 중심 위치 좌표를 반환centers = kmeans.cluster_centers_unique_labels = np.unique(cluster_labels)markers = [&#x27;o&#x27;,&#x27;s&#x27;,&#x27;^&#x27;,&#x27;P&#x27;]# 3. label별로 루프돌아서 개별 클러스터링의 중심 시각화for label in unique_labels: label_cluster = cluster_df[cluster_df[&#x27;kmeans_label&#x27;] == label] # 각 클러스터의 중심 좌표 할당 center_x_y = centers[label] # 각 클러스터 데이터들 시각화 plt.scatter(x=label_cluster[&#x27;ftr1&#x27;], y=label_cluster[&#x27;ftr2&#x27;], marker=markers[label]) # 각 클러스터의 중심 시각화 # 중심 표현하는 모형 설정 plt.scatter(x=center_x_y[0], y=center_x_y[1], s=200, color=&#x27;white&#x27;, alpha=0.9, edgecolor=&#x27;k&#x27;, marker=markers[label]) # 중심 표현하는 글자 설정 plt.scatter(x=center_x_y[0], y=center_x_y[1], s=70, color=&#x27;k&#x27;, edgecolor=&#x27;k&#x27;, marker=&#x27;$%d$&#x27; % label)# label값에 따라 숫자로 표현한다는 의미plt.show() 2. GMM It is a parametric model and is a representative clustering model using EM algorithms. It is to estimate the probability that individual data belongs to a specific normal distribution under the assumption that they belong to a Gaussian distribution. n_components is main parameter of the API provided by Scikit-learn which refers to the number of clustering predefined GMM has a particularly well-applied data distribution, which is mainly easy to apply to elliptical elongated data distributions 1234567891011121314151617181920212223242526from sklearn.datasets import load_irisfrom sklearn.cluster import KMeansimport matplotlib.pyplot as pltimport numpy as npimport pandas as pd%matplotlib inlineiris = load_iris()feature_names = [&#x27;sepal_length&#x27;,&#x27;sepal_width&#x27;,&#x27;petal_length&#x27;,&#x27;petal_width&#x27;]# 보다 편리한 데이타 Handling을 위해 DataFrame으로 변환irisDF = pd.DataFrame(data=iris.data, columns=feature_names)irisDF[&#x27;target&#x27;] = iris.target# GMM 적용from sklearn.mixture import GaussianMixture# n_components로 미리 군집 개수 설정gmm = GaussianMixture(n_components=3, random_state=42)gmm_labels = gmm.fit_predict(iris.data)# GMM 후 클러스터링 레이블을 따로 설정irisDF[&#x27;gmm_cluster&#x27;] = gmm_labels# 실제 레이블과 GMM 클러스터링 후 레이블과 비교해보기(두 레이블 수치가 동일해야 똑같은 레이블 의미 아님!)print(irisDF.groupby(&#x27;target&#x27;)[&#x27;gmm_cluster&#x27;].value_counts()) The result values are as follows, where target is the label of the actual original data, and gmm_cluster is the label of clustering after clustering. To interpret the above results, all of the labels with a target of 0 were clustered into a clustering label of 2. It is 100% well clustered. On the other hand, among the labels with a target of 1, there are 45 clusters with 0 and 5 clusters with 1, so there are 5 incorrectly clustered data. Korean reference","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"Clustering","slug":"Clustering","permalink":"https://jmj3047.github.io/tags/Clustering/"},{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/tags/Python/"},{"name":"Gaussian Mixture Model","slug":"Gaussian-Mixture-Model","permalink":"https://jmj3047.github.io/tags/Gaussian-Mixture-Model/"},{"name":"K-Means Clustering","slug":"K-Means-Clustering","permalink":"https://jmj3047.github.io/tags/K-Means-Clustering/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}]},{"title":"Gaussian Mixture Model","slug":"GMM","date":"2022-09-09T15:00:00.000Z","updated":"2023-02-23T01:36:14.673Z","comments":true,"path":"2022/09/10/GMM/","link":"","permalink":"https://jmj3047.github.io/2022/09/10/GMM/","excerpt":"","text":"1. What is GMM It is one of several models applying the Expectation Maximum (EM) algorithm. What is EM algorithm? EM algorithm is basically an algorithm mainly used for Unsupervised learning. It is also used in clustering. The EM algorithm can be largely divided into two stages: E-step and M-step. In conclusion, it is a flow that finds the optimal parameter value by repeating E-step and M-step. E-step: Calculate the likelihood value that is as close as possible to the likelihood of the initial value of a given arbitrary parameter. M-step: Obtained a new parameter value that maximizes the likelihood calculated in the E-step The above two steps are continuously repeated until the parameter value does not change significantly. The purpose of the EM algorithm is to find parameters that maximize the likelihood. Maximum Likelihood Estimation (MLE) is a Convex or Convex function whose objective function can be differentiated (which can obtain a global optimum) because the optimal parameter must be obtained directly by partial differentiation of the parameter variable. However, EM is a process of finding a value close to the optimal parameter while repeating the E-M step. Similar to the ANN (artificial neural network) model, it does not guarantee to find the Global Optimum, and even if it does, it does not recognize that the point is the real Global Optimum. Therefore, it is very likely that the local minimum is found and the objective function is not composed of Convex or Concave functions. The EM algorithm defines the function of the parameters to be obtained (ex. probability distribution) as a probability variable and optimizes it. It is also used in clustering models and is mainly used in speech recognition modeling. This is a post that Kakao, which develops voice recognition technology using GMM algorithms. Clustering is performed on the assumption that data are generated by datasets with multiple normal distributions. Several normal distribution curves are extracted and individual data are determined to which normal distribution belongs. This process is called parameter estimation in GMM, and two typical estimates are made and the EM method is applied for this parameter estimation. Means and variance of individual normal distributions Probability of which normal distribution each data corresponds 2. Code123456789101112131415161718192021222324252627282930import numpy as npimport pandas as pdimport matplotlib as mplimport matplotlib.pyplot as pltimport seaborn as snsimport warnings%matplotlib inline%config InlineBackend.figure_format = &#x27;retina&#x27;mpl.rc(&#x27;font&#x27;, family=&#x27;NanumGothic&#x27;) # 폰트 설정mpl.rc(&#x27;axes&#x27;, unicode_minus=False) # 유니코드에서 음수 부호 설정# 차트 스타일 설정sns.set(font=&quot;NanumGothic&quot;, rc=&#123;&quot;axes.unicode_minus&quot;:False&#125;, style=&#x27;darkgrid&#x27;)plt.rc(&quot;figure&quot;, figsize=(10,8))warnings.filterwarnings(&quot;ignore&quot;)from sklearn.datasets import load_irisiris = load_iris()feature_names = [&#x27;sepal_length&#x27;,&#x27;sepal_width&#x27;,&#x27;petal_length&#x27;,&#x27;petal_width&#x27;]iris_df = pd.DataFrame(iris.data, columns = feature_names)iris_df[&quot;target&quot;] = iris.targetiris_df.head() 1234567891011from sklearn.mixture import GaussianMixture# GMM: n_components = 모델의 총 수gmm = GaussianMixture(n_components=3, random_state=0)gmm.fit(iris.data)gmm_cluster_labels = gmm.predict(iris.data)# target, gmm_cluster 비교iris_df[&quot;gmm_cluster&quot;] = gmm_cluster_labelsiris_df.groupby([&quot;target&quot;,&quot;gmm_cluster&quot;]).size() 1234567결과target gmm_cluster0 0 501 1 45 2 52 2 50dtype: int64 GMM can be performed using GaussianMixture() in sklearn.mixture() n_components is the total number of models, and like n_clusters in K-Means, determines the number of clusters. Here, when the target of gmm_cluster was 1, only five were mapped differently, and all the rest were well mapped. Korean reference1 Korean reference2","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"Clustering","slug":"Clustering","permalink":"https://jmj3047.github.io/tags/Clustering/"},{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/tags/Python/"},{"name":"Gaussian Mixture Model","slug":"Gaussian-Mixture-Model","permalink":"https://jmj3047.github.io/tags/Gaussian-Mixture-Model/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}]},{"title":"K-Means Clustering","slug":"K-Means_Clustering","date":"2022-09-08T15:00:00.000Z","updated":"2023-02-23T01:35:53.766Z","comments":true,"path":"2022/09/09/K-Means_Clustering/","link":"","permalink":"https://jmj3047.github.io/2022/09/09/K-Means_Clustering/","excerpt":"","text":"1. What is K-means Clustering The K-Means clustering algorithm does not automatically identify and group the number of clusters by looking at the data. The number of clusters should be specified and the initial value should be selected accordingly by user. How to determine the number of clusters use the indicator that quantifies how well clustering has been done. After deciding the candidates of cluster numbers and performing clustering about each cluster number, calculate the index At this time, we select the number of clusterings that optimize the index as the optimal number of clusters. The frequently used indicators include Dunn Index, and Silhouette Index. The algorithm assigning the group determines the initial value which is the initial center point of the group. Each center point determines a group, and individual data is assigned to the same group as the center point close to itself. It does not end at once (depending on the form of the data) but repeats the process of updating the center of the group and assigning groups of individual data again. It belongs to the hard clustering algorithm, which is a clustering that unconditionally assigns a group of data points to a point close to the center. 2. Pros &amp; Cons Advantages The process is intuitive, easy to understand, and the algorithm is simple so that it is easy to implement. It does not require complex calculations, so it can be applied to large-scale data. convergence is guaranteed. Disadvantages The result can be very different depending on the initial value. Can be affected by an outlier: because the distance is based on Euclidean Distance, it can be affected by an outlier in the process of updating the center value. Cannot reflect intra-group dispersion structure: Since the distance is based on Euclidean Distance, it cannot properly reflect the intra-group dispersion structure. As the dimensionality increases, the distance between individual data becomes closer, and the effect of clustering may not be effective. This has the meaning of clustering only when the distance between clusters is kept as far apart as possible, but as the dimensionality increases, the distance between individual data becomes closer, so this effect may not be observed. The number of clusters is not set automatically, but must be set in advance. However, the optimal number of clusters can be determined using indicators such as Dunn Index and Silhouette Index. Because of Euclidean Distance, it cannot be used if there is a categorical variable. In this case, it is possible to use the extended K-Modes Clustering algorithm. 3. Using python: scikit-learn Put 3 in n_clusters and init_center in init, and create a KMeans instance. If you put data(X) into the fit function, clustering is performed. The final label can be obtained through the labels_ field. 1234567import warningswarnings.filterwarnings(&#x27;ignore&#x27;)from sklearn.cluster import KMeans kmeans = KMeans(n_clusters=3, init=init_center)kmeans.fit(X)labels = kmeans.labels_ Visualization 1234567891011121314fig = plt.figure(figsize=(7,7))fig.set_facecolor(&#x27;white&#x27;)for i, label in enumerate(labels): if label == 0: color = &#x27;blue&#x27; elif label ==1: color = &#x27;red&#x27; else: color = &#x27;green&#x27; plt.scatter(X[i,0],X[i,1], color=color) plt.xlabel(&#x27;x1&#x27;)plt.ylabel(&#x27;x2&#x27;)plt.show() Korean reference1 Korean reference2","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"Clustering","slug":"Clustering","permalink":"https://jmj3047.github.io/tags/Clustering/"},{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/tags/Python/"},{"name":"K-Means Clustering","slug":"K-Means-Clustering","permalink":"https://jmj3047.github.io/tags/K-Means-Clustering/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}]},{"title":"Clustering","slug":"Clustering","date":"2022-09-07T15:00:00.000Z","updated":"2023-02-23T01:35:34.556Z","comments":true,"path":"2022/09/08/Clustering/","link":"","permalink":"https://jmj3047.github.io/2022/09/08/Clustering/","excerpt":"","text":"Clustering is an example of unsupervised learning. Without any label, those with close distances in the data are classified into clusters. It is different from classification, which is supervised learning. In other words, it identifies patterns and groups hidden in the data and binds them together. Even if there’s label in data, there is a possibility that some data with same label can be grouped into different clusters. There are K-Means Clustering, Mean Shift, Gaussian Mixture Model, DBSCAN, Agglomerative Clustering in clustering algorithms and they will be covered in the next post. Korean reference","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"Clustering","slug":"Clustering","permalink":"https://jmj3047.github.io/tags/Clustering/"},{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}]},{"title":"Data Sampling","slug":"Data_Sampling","date":"2022-09-06T15:00:00.000Z","updated":"2023-02-23T01:35:23.520Z","comments":true,"path":"2022/09/07/Data_Sampling/","link":"","permalink":"https://jmj3047.github.io/2022/09/07/Data_Sampling/","excerpt":"","text":"1. Reason why you need The more input data you have on machine learning, the slower the processing. Therefore, in order to speed up the processing speed of machine learning, acceleration of learning speed of data would be helpful, which can be done with optimization of machine learning with representative data. Then let’s see how we can reduce the data so that we can only use the data we need. 2. What is data sampling The process of organizing the data and making it the best input data. For example, it can speed up the processing of machine learning by using sales of ‘month’ rather than using sales of ‘day’ units to analyze last year’s profits of a pizza house. This is the work of making optiml data. There are probabilistic sampling, nonprobability sampling in data sampling method. The probabilistic sampling method is a sampling method based on statistics, and the nonprobability sampling is a sampling method in which the subjectivity of a person is involved. Depending on each sampling method, you should select and use the sampling method that matches the data and the situation because there are advantages and disadvantages. 3. Probabilistic sampling Probabilistic sampling is a random sampling method that can be divided into simple random sampling, two-step sampling, stratified sampling, cluster&#x2F;collective sampling, and system sampling. Simple Random Sampling: A method of randomly extracting samples from the entire data. Two-Step Sampling: A sampling method that separates the entire n data into m subpopulations. Selects m subpopulations and provides simple random sampling of N data from m subpopulations. It is an accurate sampling method rather than simple random sampling. Stratified Sampling: By separating the population from several layers, it is a method of randomly extracting data from each layer by n. For example, it is a method of dividing the layers of Korean cities and province and extracting n data from each layer. cluster&#x2F;collective sampling: If the population is composed of multiple clusters, it is a method of selecting one or several clusters and using the entire data of the selected clusters. For example, it is a way to use all of the data by setting the Korea as it’s city and province. system sampling: It is a method of extracting data one by one at a regular interval by numbering all data from 1 to n. This method is mainly used to sample representative values of the time series data 4. Nonprobability sampling This is a method of subjectively extracting the probability of being selected as a sample in advance. The advantage and disadvantage of this sampling is that the subjective intention of the sampling person is involved. The implicit population extracted by nonprobability sampling is a good sampling if it matches the ideal population, the most suitable population for the subject. Nonprobability sampling methods include convenience sampling, purpose sampling, and quota sampling methods. Convenience Sampling: A method of sampling by selecting a point or location where data is good for collecting. The sample surveyed by this sampling method has a disadvantage that it is less representative than the population. It’s not possible to go through the statistical inference process. Statistical inference is that we generalize the sample analysis results to speculation about populations. Purpose Sampling: This is how you select the object you think is the most suitable object for your purpose; you will sample the data that is subjectively appropriate for your purpose.The downside is that it has also low representative for the population. Quota Sampling: Divide the population into segments, assigning each segment a quartar that represents the number of samples. Within segments, the characteristics related to the topic must be similar, and the populations must be distributed differently between segments. This is methodologically similar to layer-by-layer sampling. But the difference is that the sample is not selected by probability but by subjective judgment. 5. Conclusion Comparing probabilistic sampling with nonprobability sampling, probabilistic sampling may look good when judged by just words. However, stochastic sampling is advantageous for data that can be analyzed based on statistics, and non-probability sampling is advantageous for data such as language and music. It is better to choose and use probabilistic and nonprobable sampling as appropriate. Korean Reference","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Probabilistic sampling","slug":"Probabilistic-sampling","permalink":"https://jmj3047.github.io/tags/Probabilistic-sampling/"},{"name":"Nonprobability sampling","slug":"Nonprobability-sampling","permalink":"https://jmj3047.github.io/tags/Nonprobability-sampling/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}]},{"title":"Growth Hacking, AARRR, Funnel, Retention","slug":"Growth_Hacking","date":"2022-08-11T15:00:00.000Z","updated":"2023-02-23T01:35:04.494Z","comments":true,"path":"2022/08/12/Growth_Hacking/","link":"","permalink":"https://jmj3047.github.io/2022/08/12/Growth_Hacking/","excerpt":"","text":"1. Growth Hacking 그로스해킹(Growth Hacking)은 성장(Growth)을 위한 모든 수단(Hacking)이란 뜻으로 공격 대상의 미세한 빈틈을 찾아 해킹을 하듯이 성장을 위해 고객과 유통과정 등의 공략지점을 찾아내고 이를 적극적으로 공략하는 마케팅 방법론 브랜드, 기업, 제품 매출 증가 등을 위한 가설을 수립하고 이를 빠르게 MVP 모델로 출시하여 시장의 평가를 받아 본 후, 소비자와 시장의 반응에 따라 제품 또는 서비스가 시장에서 원하는 (고객들이 원하는) 완벽한 상품으로 도달할 때까지 쉬지 않고 개선해 나가는 방식 성장(Growth)을 위한 모든 수단(Hacking)을 통해 효율성을 극대화하고 제품과 서비스를 빠르게 성장시킨다는 장점이 있습니다. 여기서 수단이란, 데이터 기반의 분석과 시장의 피드백을 받아 제품과 서비스를 개선하고 확장해 나간다는 의미로, 자본과 리소스가 한정적인 스타트 기업에게서 효과적인 마케팅 성과를 얻을 수 있음 구매자의 행동 패턴을 분석하고 이를 바탕으로 사용자의 경험을 최적화하는 방법. 그로스해킹은 생산부터 관리에 이르기까지 소비자의 Wants를 충족시키는 상품을 만드는 것 2. AARRR Analysis Acquisition: 어떻게 처음 우리 서비스를 접하게 되는가(사용자 유치) Activation: 사용자가 처음 서비스를 접할 때에 긍정적인 경험을 제공하고 있는가(사용자 활성화) Retention: 이후의 서비스를 다시 사용하는 정도는 얼마나 되는가(사용자 유지) Revenue: 최종 목적(거시전환)으로 연결되고 있는가(매출) Referral: 사용자가 자발적으로 확산이나 공유를 일으키고 있는가(추천) 3. Funnel Analysis 퍼널 분석은 웹 사이트에서 특정 결과에 도달하는데 필요한 단계와, 각 단계를 통과하는 사용자 수를 파악하기 위한 방법 퍼널 분석을 통해 기업은 방문자가 사이트에 가입하는지, 구매자로 변환이 되는지 등의 정보를 특정한 퍼널에 매핑하게 됨. 이때 사용자의 흐름을 시각화하는 모형의 모습이 부엌이나 차고에서 흔하게 쓰이는 깔때기와 유사한 모습을 띠고 있어 ‘퍼널 분석’이라는 이름이 붙여짐 4. Retention Analysis 리텐션은 앱 서비스 성장에 있어서 매우 중요한 지표 리텐션이란 한번 획득한 유저들이 서비스를 이탈하지 않고 계속 서비스를 이용하는 것을 의미 리텐션이 높은 서비스는 리텐션이 낮은 서비스보다 획득비용에 투자한 비용을 빠르게 회수할 수 있으며 이렇게 회수한 비용으로 다시금 빠르게 획득에 투자할 수 있도록 하여 성장을 촉진합니다. 리텐션이 낮다는 것은 획득 이후 다시 돌아오지 않고 이탈하는 유저가 많은 것인데요, 리텐션이 낮은 서비스는 성장이 더딜 뿐만 아니라 한번 획득했다가 이탈한 유저는 다시 획득하기에도 더 많은 획득 비용과 시간이 소요되므로 악순환을 만들어냅니다. 따라서 리텐션은 사업 성장에 있어서 반드시 지켜보아야 할 지표 중 하나입니다. 잔존율(D+1지표) Reference Retention Funnel How to measure Retention","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}]},{"title":"Deep Embedding Learning for Text-Dependent Speaker Verification","slug":"Deep_Embedding_Learning_for_Text-Dependent_Speaker_Verification","date":"2022-07-04T15:00:00.000Z","updated":"2022-10-15T08:03:47.207Z","comments":true,"path":"2022/07/05/Deep_Embedding_Learning_for_Text-Dependent_Speaker_Verification/","link":"","permalink":"https://jmj3047.github.io/2022/07/05/Deep_Embedding_Learning_for_Text-Dependent_Speaker_Verification/","excerpt":"","text":"Journal&#x2F;Conference: InterspeechYear(published year): 2020Author: Peng Zhang, Peng Hu, Xueliang ZhangSubject: Speaker Verification Deep Embedding Learning for Text-Dependent Speaker Verification Summary 이 논문은 화자 검증 작업을 위한 효과적인 딥 임베딩 학습 아키텍처를 제시한다. 널리 사용되는 잔류 신경망(ResNet) 및 시간 지연 신경망(TDNN) 기반 아키텍처와 비교하여, 두 가지 주요 개선이 제안된다. 우리는 화자의 단기 컨텍스트 정보를 인코딩하기 위해 조밀하게 연결된 컨볼루션 네트워크(DenseNet)를 사용한다. 양방향 주의 풀링 전략이 제안된다. 장기적인 시간적 맥락을 모델링하고 화자 정체성을 반영하는 중요한 프레임을 집계한다. 결과는 제안된 알고리듬이 과제 1과 과제 3의 평가 세트에서 각각 8.06%, 19.70% minDCF 및 9.26%, 16.16% EERs 상대적 감소로 FFSVC2020의 공식 기준선을 능가한다는 것을 보여준다. Introduction딥 러닝 기반 방법은 화자 간 차별에 필수적인 정보를 포함하는 딥 스피커 임베딩 또는 짧게 임베딩과 같은 발화 수준 표현을 얻기 위해 지배적이었다. 본 논문에서는 화자 검증을 위한 효과적인 딥 임베딩 학습 아키텍처를 제안한다. 최근 조밀하게 연결된 컨볼루션 네트워크의 성공에 자극받아 (DenseNet) 이미지 분류 [15], 음악 소스 분리 [16], 화자 분리 [17] 및 화자인식 [18]에서, 우리는 DenseNet을 프레임 레벨 피처 추출기로 채택하여 후속 레이어의 입력의 변화와 훈련 효율성을 증가시킨다. 개념적으로 각 밀도 블록은 작은 CNN 시스템 역할을 한다. 차이점은 레이어의 출력이 채널 차원의 출력 연결에 의해 구현되는 피드 포워드 방식으로 조밀하게 연결된다는 것이다. 또한, 우리는 발화 수준 기능의 표현력을 향상시키기 위해 시간 컨텍스트를 추가로 모델링하기 위한 양방향 주의 풀링 계층을 설계한다. Model ArchitectureDenseNet(프레임 레벨 기능 추출기): 각각 5개의 CNN 레이어가 있는 4개의 밀도 블록(DenseBlock)으로 구성. 프레임 레벨 피처 추출 후, 양방향 주의 풀링 레이어는 프레임 레벨 피처를 고정 차원 벡터로 변환하는 데 사용되며, 이어서 완전히 연결된 두 개의 숨겨진 레이어가 발화 레벨 피처를 형성한다. 출력은 소프트맥스 분류기 계층이며, 각 노드는 화자ID에 대응합니다. Frame level의 특징 추출을 하고, Input과 다음 레이어의 변화에 최적화 시키고, 훈련 효율을 높이기위해서 사용함.개념적으로 각각의 dense block들 이 작은 CNN의 역할을 수행하고 있는 것. 각각의 레이어들이 바로 전에 있는 모든 레이어들과 feature map(CNN에서의 합성곱과 같은 원리)을 통해서 연결이 되어있는 방식.이러한 연결 패턴은 훈련 중 레이어들 사이에서 더 나은 기울기 flow와 각 레이어들에 대한 접근을 모든 feature에게 전달함으로써 일시적인 문맥 정보를 capture 가능. 프레임 레벨 단계의 경우 각 DenseBlock은 5개의 컨볼루션 레이어(Conv2D), 지수 선형 단위(ELU) 및 인스턴스 정규화(IN)로 구성된다. 각 밀도 블록 뒤의 텐서 모양은 featureMaps × timeSteps × 주파수 채널 형식이다. 각 Conv2D 및 Conv2D+IN+ELU는 kernelSize 형식으로 지정됩니다. 시간 × kernelSizeFreq, (보행)시간, 보폭Freq), (패딩시간, 패딩Freq), 맵을 특징으로 한다. 각 밀도 블록(g)은 5개의 Conv2D+증가율이 g인 IN+ELU 블록을 포함합니다. 발화 수준 단계의 경우 숫자는 우리의 구현에서 출력 기능 맵 또는 임베딩 차원의 채널을 나타낸다. DenseBlock[15]에서 처음 제시된 DenseBlock 아키텍처의 주요 아이디어는 CNN의 네트워크 구축 블록에서 각 계층에서 모든 후속 계층에 직접 연결을 도입하는 것이다. 시간 빈도가 긴 컨텍스트 정보를 효율적으로 캡처하도록 설계되었다. 각 계층은 피쳐 맵 연결을 통해 모든 다음 계층에 직접 연결됩니다. 이러한 연결 패턴은 훈련 중에 계층 간에 더 나은 그레이디언트 흐름을 생성하고 각 계층이 이전 계층의 모든 특징 표현에 액세스할 수 있도록 하여 시간적 컨텍스트 정보를 캡처하도록 설계되었다. Bidirectional Attentive Pooling 양방향 주의 풀링(bidirectional attentive pooling, BAP)의 도식입니다. hi는 BAP 입력의 i번째 벡터를 나타내며 wf와 wb는 각각 BGRU의 전방과 후방 숨겨진 상태를 나타낸다. →U 및 ←U는 BGRU 레이어의 양방향 출력입니다. bidirectional gated recurrent unit (BGRU) layer + attentive pooling : utterance level feature 일반적인 average pooling을 사용하지 않은 이유: 평균화 대신 주의 메커니즘[10, 12]은 숨겨진 표현을 적극적으로 선택하고 화자 차별 정보를 강조하기 위한 더 나은 대안이다. 보다 차별적인 고정 차원 발화 수준 표현을 얻고 장기 시퀀스 정보를 캡처하기 위해 [19]는 CNN-BLSTM 모델과 주의 깊은 풀링 레이어를 함께 결합하는 주의 기반 CNN-BLSTM 프레임워크를 제안하였다. BLSTM을 주의 깊은 풀링 계층에 직접 연결하는 [19]와는 달리 주의 깊은 풀링을 사용하여 양방향 반복 신경망에 의해 출력되는 양방향 시간 정보를 캡처한 다음, 양방향 발화 수준 기능을 연결한다. 제안된 풀링 방법인 양방향 주의 풀링(BAP)은 다음과 같이 표현될 수 있다. BAP 계층은 양방향 순차 모델링과 주의 메커니즘을 모두 활용하여 장기적인 시간적 컨텍스트 정보를 캡처한다. Result Dataset: FFSVC2020The first 30 utterances are of fixed content: ‘ni hao mi ya’ in Mandarin Chinese for TD-SV tasks. The remaining utterances are text-independent.In total, training data sets have nearly 1,139,671 utterances and the total duration approximately 950 hours with 374 speakers Conclusion 텍스트 의존적 스피커 검증을 위한 딥 임베딩 학습 아키텍처를 제안 아키텍처는 프레임 수준에서 스피커 ID를 캡처하기 위한 DenseBlock의 스택과 발화 수준에서 스피커 임베딩을 형성하기 위한 양방향 주의 풀링 구조로 구성됨 컨볼루션 레이어의 출력을 조밀하게 연결함으로써, 시간 주파수 컨텍스트 정보의 다양한 측면을 가진 보다 의미 있는 프레임 레벨 표현이 생성됨 양방향 주의 풀링 계층은 BGRU 계층과 주의 풀링의 조합으로 양방향에서 시간 컨텍스트 정보를 추가로 캡처 FFSVC2020에서 점수 제출의 경우, 우리가 제안한 방법은 평가 세트의 과제 1과 과제 3에 대한 최소 DCF와 EER에서 각각 0.52와 4.72%, 0.14%를 달성. 또한 이 결과는 x-벡터 및 ResNet 기준선 시스템에 비해 상당한 성능 향상을 보여줌 Pdf: Deep Embedding Learning for Text-Dependent Speaker Verification","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speaker Verification","slug":"Paper/Speaker-Verification","permalink":"https://jmj3047.github.io/categories/Paper/Speaker-Verification/"}],"tags":[{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"},{"name":"TD-SV","slug":"TD-SV","permalink":"https://jmj3047.github.io/tags/TD-SV/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speaker Verification","slug":"Paper/Speaker-Verification","permalink":"https://jmj3047.github.io/categories/Paper/Speaker-Verification/"}]},{"title":"Attention is all you need","slug":"Attention_is_all_you_need","date":"2022-05-09T15:00:00.000Z","updated":"2022-10-15T08:03:40.527Z","comments":true,"path":"2022/05/10/Attention_is_all_you_need/","link":"","permalink":"https://jmj3047.github.io/2022/05/10/Attention_is_all_you_need/","excerpt":"","text":"Journal&#x2F;Conference: NIPSYear(published year): 2017Author: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia PolosukhinSubject: NLP Attention is all you need Summary Attention 만으로 시퀀셜 데이터를 분석하여 병렬화와 연산 속도 향상을 가능하게 한 새로운 모델 제시 Seq2Seq 과 Attention 을 결합한 모델(Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. CoRR, abs&#x2F;1409.0473, 2014.)에서 한층 더 발전한 모델입니다. Recurrent model(재귀 구조)없이 Self-attention 만으로 구성한 첫번째 모델입니다. 재귀 구조 제거로 모델을 병렬화(Parallelization)하여 자연 언어 처리 학습&#x2F;추론 시간을 획기적으로 단축시켰습니다. Introduction기존의 자연언어 처리 모델은 RNN, LSTM, GLU 모델로 대표되는 재귀 모델(Recurrent Model)을 encoder-decoder 구조로 결합하는 seq2seq 과 같은 모델을 주로 사용하였습니다. 이러한 재귀 모델은 순차 처리로 인해서 병렬화가 어렵다는 약점이 있고, 메모리 크기가 제한되어 긴 문장을 처리하기도 어렵다는 단점이 있습니다. 이를 보완하기 위한 어텐션(attention) 매커니즘이 제안되었습니다. 어텐션은 재귀 과정에서 입력에서 출력까지의 거리가 길어지는 문제를 해결할 수 있어 입출력의 전역 의존성을 높여주었지만, 재귀 모델과 결합해서만 사용되어 왔습니다. 이에 해당 논문에서는, 어텐션만으로 모델을 구성하여 쉽게 병렬화 할 수 있고 자연언어처리 과제의 성능을 높인 Transformer 모델을 제시합니다. 이는 기존의 재귀 모델과 다르게 8대의 P100 GPU로 12시간 정도만 학습했음에도 당시 기준 SOTA를 달성하였습니다. WMT 2014 English-to-German Translation task -&gt; 28.4 BLEU WMT 2014 English-to-French Translation task -&gt; 41.0 BLEU Model ArchitectureTransformer 모델은 seq2seq으로 대표되는 인코더-디코더 구조를 self-attention 으로 쌓은 뒤, fully connected layer 로 출력을 생성합니다. Encoder and Decoder Stacks 인코더(Encoder) $N$(&#x3D;6)개의 동일한 레이어로 구성 각 레이어는 2개의 하위 레이어로 구성 Multi-head self-attention position-wise fully connected feed-forward 하위 레이어를 거칠 때마다 Residual connection(Resnet) 과 layer normalization 을 실행 각 레이어 출력의 크기는 $d_{model}$(&#x3D;512)로 고정 디코더(Decoder) 인코더와 같이 $N$(&#x3D;6)개의 동일한 레이어로 구성 인코더와 동일한 2개의 하위 레이어에 한가지를 더 추가하여 3개의 하위 레이어로 구성 Multi-head self-attention position-wise fully connected feed-forward 인코더의 출력으로 실행하는 multi-head attention 순차적으로 결과를 만들어 낼 수 있도록 Self-attention 레이어에 Masking 을추가 : $i$ 번째 출력을 만들 때, $i$번째보다 앞선 출력($i-1, i-2,\\dots$) 만을 참고하도록 함 Attentionattention은 query와 key-value pair들을 output에 맵핑해주는 함수입니다. 출력은 values들의 weighted sum으로, value에 할당된 weight는 query와 대응되는 key의 compatibility function으로 계산합니다. Scaled Dot-Product Attention 여기서 사용하는 attention은 Scaled Dot-Product Attention(SDPA)라 부르는데, input은 dimension이 $d_k$인 query와 key, dimension이 $d_v$인 value들로 이루어집니다. 모든 query와 모든 key들에 대해 dot product로 계산되는데 각각의 결과에 dk로 나누어진다. 다음 value의 가중치를 얻기 위해 softmax 함수를 적용합니다. attention 함수는 additive attention과 dot-product attention이 사용됩니다. additive attention은 single hidden layer와 함께 feed-forward network에 compatibility function을 계산하는데 사용됩니다. dot-product attention이 좀 더 빠르고 실제로 space-efficient합니다. 왜냐하면 optimized matrix multiplication code를 사용해서 구현되었기 때문입니다. $d_k$가 작은 경우 additive attention이 dot product attention보다 성능이 좋습니다. 그러나 $d_k$가 큰 값인 경우 softmax 함수에서 기울기 변화가 거의 없는 영역으로 이동하기 때문에 dot product를 사용하면서 $d_k$으로 나누어 scaling을 적용했습니다. Multi-Head Attention $d_{model}$ dimension의 query, key, value 들로 하나의 attention을 수행하는 대신, query, key, value들에 각각 학습된 linear projection을 $h$번 수행하는 것이 더 좋습니다. 즉, Q,K,V에 각각 다른 weight를 곱해주는 것입니다. 이때, projection이라 하는 이유는 각각의 값들이 parameter matrix와 곱해졌을 때, $d_k,d_v,d_{model}$차원으로 project되기 때문입니다. query, key, value들을 병렬적으로 attention function을 거쳐 dimension이 $d_v$인 output 값으로 나오게 됩니다. 이후 여러개의 head를 concatenate하고 다시 $W^O$와 projection하여 dimension이 $d_{model}$인 output 값으로 나오게 됩니다. Position-wise Feed-Forward Networks인코더와 디코더는 fully connected feed-forward network를 가지고 있습니다. 또한, 두 번의 linear transformations과 activation function ReLU로 구성되어집니다. 각각의 position마다 같은 $W,b$ 를 사용하지만 layer가 달라지면 다른 parameter를 사용합니다. Positional Encoding모델이 recurrence와 convolution을 사용하지 않기 때문에 문장안에 상대적인 혹은 절대적인 위치의 token들에 대한 정보를 주입해야만 했습니다. 이후 positional encoding이라는 것을 encoder와 decoder stack 밑 input embedding에 더해줬습니다. positional encoding은 $d_{model}$인 dimension을 가지고 있기 때문에 둘을 더할 수 있습니다. $pos$는 position, $i$는 dimension $pos$는 sequence에서 단어의 위치이고 해당 단어는 $i$에 0부터 $2d_{model}$까지 대입해 dimension이 $d_{model}$인 positional encoding vector를 얻을 수 있습니다. Why Self-Attention Self-Attention을 사용하는 첫 번째 이유는 layer마다 total computational complexity가 작기 때문입니다. 두 번째 이유는 computation의 양이 parallelized하기때문에 sequential operation의 minimum으로 측정되기 때문입니다. 세 번째 이유로는 네트워크에서의 long-range dependencies사이의 path length때문입니다. long-range dependencies를 학습하는 것은 많은 문장 번역 분야에서의 key challenge가 됩니다. input sequence와 output sequence의 길이가 길어지면 두 position간의 거리가 멀어져 long-range dependencies를 학습하는데 어려워집니다. 테이블을 보면 Recurrent layer의 경우 Sequential operation에서 $O(n)$이 필요하지만 Self-Attention의 경우 상수시간에 실행될 수 있습니다. 또한 Self-Attention은 interpretable(설명가능한) model인 것이 이점입니다. Traning OptimizerAdam optimizer 에 파라미터로 $\\beta_1&#x3D;0.9, \\beta_2&#x3D;0.98, \\epsilon&#x3D;10^{-9}$ 를 사용했습니다. 학습동안 아래의 공식을 통해 learning rate를 변화시켰습니다. 이는 warmup_step에 따라 linear하게 증가시키고 step number에 따라 square root한 값을 통해 점진적으로 줄여갔습니다. 그리고 warmup_step &#x3D; 4000을 사용했습니다. Residual Dropout각 sub-layer에서 input을 더하는 것과 normalization을 하기전에 output에 dropout을 설정했습니다. 또한 encoder와 decoder stacks에 embedding의 합계와 positional encoding에도 dropout을 설정했습니다. dropout rate $P_{drop}&#x3D;0.1$ 을 사용했습니다. Label Smoothing학습하는 동안 label smoothing value $\\epsilon_{ls}&#x3D;0.1$을 적용했습니다. Result Machine Translation 영어→독일어 번역에서는 기존 모델들보다 높은 점수가 나왔고 영어→프랑스어 번역에서는 single 모델보다 좋고 ensemble 모델들과 비슷한 성능을 내주는 것을 볼 수 있습니다. 여기서 중요한 점은 Training Cost인데 기존 모델들보다 훨씬 적은 Cost가 들어가는 것을 볼 수 있습니다. Model Variations (A)를 보면 single-head attention은 head&#x3D;16일때보다 0.9 BLEU 낮고 head&#x3D;32로 늘렸을 때도 head&#x3D;16일때보다 BLEU가 낮습니다. (B)를 보면 dk를 낮추는 것이 model quality를 낮추게 합니다. (C), (D)를 보면 더 큰 모델일수록 좋고, dropout이 overfitting을 피하는데 도움이 되는 것을 볼 수 있습니다. (E)를 보면 sinusoidal position대신 learned positional embeddings를 넣었을 때의 결과가 base model과 동일한 결과인 것을 볼 수 있습니다. Conclusion 재귀 구조 없이 Multi-headed Self-attention 으로 인코더-디코더를 대체한 Transformer 모델을 제시하였습니다. 재귀구조가 없으므로 recurrent 또는 convolutional 레이어 기반 모델보다 빠르게 학습을 할 수 있습니다. 해당 모델은 WMT 2014 영어→독어, 영어→불어 번역 분야에서 기존 모든 앙상블 모델들을 능가하는 SOTA를 달성했습니다. Link: Attention is all you need","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"NLP","slug":"Paper/NLP","permalink":"https://jmj3047.github.io/categories/Paper/NLP/"}],"tags":[{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"},{"name":"Transformer","slug":"Transformer","permalink":"https://jmj3047.github.io/tags/Transformer/"},{"name":"Attention","slug":"Attention","permalink":"https://jmj3047.github.io/tags/Attention/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"NLP","slug":"Paper/NLP","permalink":"https://jmj3047.github.io/categories/Paper/NLP/"}]},{"title":"Light Gradient Boosting Machine","slug":"LGBM","date":"2022-05-09T15:00:00.000Z","updated":"2023-02-23T01:34:43.368Z","comments":true,"path":"2022/05/10/LGBM/","link":"","permalink":"https://jmj3047.github.io/2022/05/10/LGBM/","excerpt":"","text":"1. DefinitionEnsemble→ 여러 예측기를 수집해서 단일 예측기 보다 더 좋은 예측기를 만드는 것. 일반적으로 앙상블 기법을 사용하면 , 예측기 하나로 훈련하였을때 보다 , 편향은 비슷하지만 분산이 줄어든다고 알려져 있다. 배깅(bagging) 원데이터 집합으로부터 크기가 같은 표본을 여러 번 단순임의 복원추출하여 각 표본(붓스트랩 표본) 에 대해 분류기를 생성한 후 그 결과를 앙상블하는 방법 반복추출 방법을 사용하기 때문에 같은 데이터가 한 표본에 여러 번 추출될 수도 있고, 어떤 데이터는 추출되지 않을수도 있다. 부스팅(boosting) 배깅의 과정과 유사하나 붓스트랩 표본을 구성하는 sampling 과정에서 각 자료에 동일한 확율을 부여하는 것이 아니라, 분류가 잘못된 데이터에- 더 큰 가중을 주어 표본을 추출한다. Bagging 과 Boosting 의 차이 Bagging 은 독립된 예측기를 통해 더 나은 예측기를 얻는다 Boosting 은 앞의 예측기를 보완해 나가면서 더 나은 예측기를 얻는다 랜덤포레스트(random forest) 배깅에 랜덤 과정을 추가한 방법이다. 각 노드마다 모든 예측변수 안에서 최적의 분할을 선택하는 방법 대신 예측변수들을 임의로 추출하고, 추출된 변수 내에서 최적의 분할을 만들어 나가는 방법을 사용한다. 부트스트랩(Bootstrap) 부트스트랩은 평가를 반복한다는 측면에서 교차검증과 유사하나, 훈련용 자료를 반복 재선정한다는 점에서 차이가 있다. 즉 부트스트랩은 관측치를 한번 이상 훈련용 자료로 사용하는 복원추출법에 기반한다. 부트스트랩은 전체 데이터의 양이 크지않은 경우의 모형평가에 가장 적합하다. 2. GBM Gradient Boosting Machine(GBM)은 Ensemble Learning의 일환 Gradient Boosting&#x3D;Gradient Descent+Boosting Gradient Descent 첫번째 데이터에서 잘 못 맞춘 데이터들에 가중치를 주어, 두번째 모델 에서는 더 많은양을 만들어 준다. 또, 두번째 모델에서 잘못 매칭한 데이터들 에게 가중치를 주어서, 세번째 모델을 만들어주는 그러한 방식으로 모델을 반복한다. Fit an addictive model(ensemble) in a foward stage-wise manner bagging 처럼 한번에 딱 학습을 시키는게 아니고 위에 언급한 것 처럼 하나씩 하나씩 더해가면서 모델을 학습 시켜나가는 모델 Adaptive boosting 모델을 거듭할수록, weak leaner가 만들어 지면서 이전에 가졌던 오류에 대해 해결할 수 있는 능력을 만듦 이러한 weak learner 를 만들기 위해 , 앞서 보였던 모델의 shortcoming 을 더 많이 샘플링 하라는 뜻 Gradient Boosting 그라디언트 부스팅은, Regression,Classification,Ranking 을 다 할 수 있다. 이 셋의 다르기는 loss function 에서 차이가 날 뿐, concept 은 동일 💡 Regression 으로 설명하기가 가장 직관적이기 때문에, Regression 모델로써 concept 을 설명하겠다. 얘는 adaptive boosting 과는 달리, 샘플링을 따로 시행하지는 않는다. 잔차를 목푯값 (y) 으로 놓고 계속해서 반복하면서 잔차에 대한 식을 만들어 낸다. 잔차를 목표값으로 잡아두면, 앞선 모델이 맞추지 못한 만큼만 맞추려고 노력을 하기때문에, 앞선 모델의 결과물과 뒷 모델의 결과물을 더하면 정답이 나온다. 경사도를 통해서 Weak learner 를 boosting 시킴 손실함수의 gradient(경사도) 가 0 에 가까울때 까지 미분을 해준다. Gradient 가 0이 아니라면 weight 를 gradinet 의 반대방향으로 움직이되 얼마만큼 움직이냐에 따라서 달라지니 조금씩 움직인다. 처음, decision tree 로 split point 를 잡아, regression 해준 부분의 잔차를 보면 높은것을 알 수 있다. GBM에서는 이 잔차를 기준으로 또 Split point 를 잡아주면서(이러한 과정에서 손실함수가 들어가며 손실함수의 gradient 를 줄여나가는 과정에서 gradient descent 개념이 들어가는 것) 점점 잔차를 줄여 나가는 것을 볼 수 있다. 💡 처음에는 회귀식이 안좋게 나오는데 iteration 이 반복 될수록 회귀식이 좋아지는것으로 볼 수 있다. Overfitting problem in GBM GBM 의 가장큰 문제점은 오차를 기반으로 모델을 형성 하기때문에(애초에 모델 자체가 반복을 거듭 할수록, 전 모델의 오차를 줄여나가는 모델이기 때문에 오버피팅 문제는 필연적) 우리가 어찌 할수 없는 오차까지도 모델에 학습시키어서 오버피팅 문제를 불러 일으킨다 과적합 해결법 Subsampling→각각의 모델을 만들때 샘플링을 랜덤으로 80% 만해서 모델을 만들어준다 Shrinkage- original 알고리즘들은 전에 만들어진 모델들과 뒤로 갈수록 만들어지는 모델들에, 영향력이 동등했는데 , shrinkage 를 쓰면 , 뒤에 만들어지는 모델들에 대해서 , 가중치를 적게 두어 만들어준다. Early Stopping- validation error 가 증가 할 것 같으면 미리 중지를 시키는것 Information Gain:Split point를 통해서 얼마나 혼잡도,불순도가 낮아지는가. Information Gain 을 통해 그 변수의 영향도를 체크 할 수 있다. 3. LightGBMGOSS 모든 피쳐들을 검사하면 시간이 많이 걸리기 때문에 이를 막기위해서, Gradient-based One-side Sampling (GOSS) 를 사용→ Large gradient 는 keep 하고 small gradient 는 드랍 하는 방식으로, 1000개 데이터를 모두 탐색하는 것이 아니라 gradient 가 큰 것 위주로 탐색하는 방식 → 탐색횟수를 줄이는 것 EFB (Exclusive Feature Bundleing) 모든피쳐를 탐색할 필요를 없애는 것 Bundle 을 찾는 방법 Graph coloring problem 으로 해결가능 각각의 노드는 피쳐이고, edge 는 피쳐들간의 conflict → conflict 가 많은 애들은 중복이 많이 들어가서 bundling 이 되면 안됨 conflict 가 없는 애들 끼리는 bundling 을 해도 됨 Greed bundling 계산법 edge 의 강도: 두 변수의 conflict 강도 edge: 동시에 0이아닌 객체의 수. Degree 시작점을 degree의 내림차순으로 정리 해준 다음, degree가 높은것 부터 시작한다. cutoff 는 hyperparameter 인데, cut-off가 0.2라는 말은, N&#x3D;10 이기 때문에 2회 이상 Nonzero value 가 겹치게 되면, bundleing이 안되는 것. cut-off 기준에 맞지 않기 때문에 x5는 고립이 된다. feature merge 를 쉽게 해주기 위해 feature의 위치를 살짝 조정 하여준다. feature 를 merge 하는방법. Add. offset add offset→bundling 을 하기위한 대상이 되는 변수에다가 기준이 되는 변수가 가질 수 있는 최대 값을 더해준다. conflict 가 일어난 부분은 그대로 기준 변수가 가지는 값을 더해준다. Reference: 고려대학교 산업경영공학부 DSBA 연구실","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}],"tags":[{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}]},{"title":"Making Chatbot with doc2vec tutorial(1)","slug":"Making_Chatbot_with_doc2vec_tutorial(1)","date":"2022-05-05T15:00:00.000Z","updated":"2022-10-15T08:07:56.742Z","comments":true,"path":"2022/05/06/Making_Chatbot_with_doc2vec_tutorial(1)/","link":"","permalink":"https://jmj3047.github.io/2022/05/06/Making_Chatbot_with_doc2vec_tutorial(1)/","excerpt":"","text":"모델 만들기데이터 만들기doc2vec을 이용해서 FAQ데이터들의 질문들을 벡터화하는 모델을 만들어 본다. word2vec이 단어를 벡터화 하는 것이라면 doc2vec은 단어가 아니라 문서를 기준으로 (여기서는 문장)벡터를 만드는 라이브러리이다. doc2vec을 사용하면 서로 다른 문서들이 같은 차원의 벡터값을 갖게 된다. 각 문서라 갖는 벡터값을 비교해 같으면 같을 수록 유사한 문서라는 것을 알 수 있다. 따라서 doc2vec을 이용해 FAQ의 질문들을 벡터화 한다면 어떤 질문이 들어왔을 때 동일 모델로 질문을 벡터화 한다음, 저장돼 있는 질문들의 벡터와 비교해서 가장 유사한 질문을 찾을 수 있다. 가장 유사한 질문을 찾은 다음 그 질문의 답을 출력하면 FAQ챗봇을 만들 수 있다. **GPU사용 필수..! 1234567891011import osimport warningsfrom gensim.models import doc2vecfrom gensim.models.doc2vec import TaggedDocumentfaqs = [[&quot;1&quot;, &quot;당해년도 납입액은 수정 가능 한가요?&quot;, &quot;네, 당해년도 납입액은 12464 화면 등록전까지 수정 가능합니다.&quot;], [&quot;2&quot;, &quot;대리인통보 대상계좌 기준은 어떻게 되나요?&quot;, &quot;모계좌 기준 가장 최근에 개설된 계좌의 관리점에서 조회 됩니다. 의원폐쇄된 자계좌는 조회대상 계좌에서 제외됩니다. 계좌주 계좌가 사절원 계좌가 아닌 경우만 조회됩니다&quot;], [&quot;3&quot;, &quot;등록가능 단말기수는 어떻게 되나요?&quot;, &quot;5대까지 등록 가능입니다.&quot;], [&quot;4&quot;, &quot;모바일계좌개설 가능한 시간은 어떻게 되나요?&quot;, &quot;08:00 ~ 20:00(영업일만 가능&quot;], [&quot;5&quot;, &quot;미국인일때 미국납세자등록번호 작성 방법은 어떻게 되나요?&quot;, &quot;계좌주가 미국인일 때 계좌주의 미국납세자등록번호(사회보장번호(Social Security Number), 고용주식별번호(Employer Identification Number), 개인납세자번호(Individual Taxpayer Identification Number))를 기재합니다..&quot;]] 위와 같이 5개의 FAQ데이터를 임의로 만들었다. 이제 여기에 5개의 질문을 벡터화 할건데 사실 벡터화 할 때 데이터는 많을 수록 좋다. 적으면 서로 분간이 잘 안됨. 형태소 분석doc2vec으로 문장을 벡터화하기 전에 약간의 전처리 과정이 필요하다. 각 문장을 tokenize해야 한다. 토큰화 하는 과정이 영어랑 한국어랑 조금 다른데 한국어의 경우 형태소 분석(pos tagging)을 통해 형태소 단위로 나눈뒤 토큰으로 사용할 형태소를 결정하고 나눈다. 즉 각 문장을 형태소 단위의 배열로 만든다. 한국어 형태소 분석기는 konlpy를 사용한다. 123456789101112#형태소 분석import jpypefrom konlpy.tag import Kkmakkma = Kkma()def tokenize_kkma(doc): jpype.attachThreadToJVM() #자바를 사용하기 위한 소스 코드 token_doc = [&#x27;/&#x27;.join(word) for word in kkma.pos(doc) ] #형태소 분석한 단어와 형태소 명을 &#x27;단어/형태소&#x27;형태로 출력하기 위한 코드 return token_doctokenize_kkma(faqs[0][1]) Kkma를 import 하고 jpype도 import 한다. jpype는 파이썬에서 자바를 사용할 수 있게 해주는 패키지인데 기본적으로 kkma가 자바 베이스라서 꼭 필요하다. Kkma()로 형태소 분석기를 불러온다음 kkma.pos(doc)로 형태소 분석을 한다. 123456789101112출력 결과:[&#x27;당해/NNG&#x27;, &#x27;년도/NNM&#x27;, &#x27;납입/NNG&#x27;, &#x27;액/XSN&#x27;, &#x27;은/JX&#x27;, &#x27;수정/NNG&#x27;, &#x27;가능/NNG&#x27;, &#x27;한/MDN&#x27;, &#x27;가요/NNG&#x27;, &#x27;?/SF&#x27;] 형태소 분석을 하면 문장이 단어&#x2F;형태소 형태의 배열로 출력된다. 1번 문장은 총 10개의 형태소로 나뉘었다. 형태소 분석기종류에 따라 결과가 조금씩 다를수 있다. Doc2Vec 모델 만들기Doc2Vec을 이용해 모델을 만들기 위해서는 토큰화 된 리스트와 태그 값이 필요하다. 여기서 태그는 문장 번호. [문장의 번호, 문장을 토큰화한 배열] 이렇게 두 개의 값을 가진 리스트를 사용해 doc2vec 모델을 만들 수 있다. 실제로 모델을 만드는데 사용하는 건 토큰 값이지만 비슷한 문장이 무엇인지 찾기 위한 인덱스로 태그 값을 사용하게 된다. 1234567# 리스트에서 각 문장부분 토큰화token_faqs = [(tokenize_kkma(row[1]), row[0]) for row in faqs]# Doc2Vec에서 사용하는 태그문서형으로 변경tagged_faqs = [TaggedDocument(d, [c]) for d, c in token_faqs]tagged_faqs 12345[TaggedDocument(words=[&#x27;당해/NNG&#x27;, &#x27;년도/NNM&#x27;, &#x27;납입/NNG&#x27;, &#x27;액/XSN&#x27;, &#x27;은/JX&#x27;, &#x27;수정/NNG&#x27;, &#x27;가능/NNG&#x27;, &#x27;한/MDN&#x27;, &#x27;가요/NNG&#x27;, &#x27;?/SF&#x27;], tags=[&#x27;1&#x27;]), TaggedDocument(words=[&#x27;대리인/NNG&#x27;, &#x27;통보/NNG&#x27;, &#x27;대상/NNG&#x27;, &#x27;계좌/NNG&#x27;, &#x27;기준/NNG&#x27;, &#x27;은/JX&#x27;, &#x27;어떻/VA&#x27;, &#x27;게/ECD&#x27;, &#x27;되/VV&#x27;, &#x27;나요/EFQ&#x27;, &#x27;?/SF&#x27;], tags=[&#x27;2&#x27;]), TaggedDocument(words=[&#x27;등록/NNG&#x27;, &#x27;가능/NNG&#x27;, &#x27;단말/NNG&#x27;, &#x27;기수/NNG&#x27;, &#x27;는/JX&#x27;, &#x27;어떻/VA&#x27;, &#x27;게/ECD&#x27;, &#x27;되/VV&#x27;, &#x27;나요/EFQ&#x27;, &#x27;?/SF&#x27;], tags=[&#x27;3&#x27;]), TaggedDocument(words=[&#x27;모바일/NNG&#x27;, &#x27;계좌/NNG&#x27;, &#x27;개설/NNG&#x27;, &#x27;가능/NNG&#x27;, &#x27;하/XSV&#x27;, &#x27;ㄴ/ETD&#x27;, &#x27;시간/NNG&#x27;, &#x27;은/JX&#x27;, &#x27;어떻/VA&#x27;, &#x27;게/ECD&#x27;, &#x27;되/VV&#x27;, &#x27;나요/EFQ&#x27;, &#x27;?/SF&#x27;], tags=[&#x27;4&#x27;]), TaggedDocument(words=[&#x27;미국인/NNG&#x27;, &#x27;일/NNG&#x27;, &#x27;때/NNG&#x27;, &#x27;미국/NNP&#x27;, &#x27;납세자/NNG&#x27;, &#x27;등록/NNG&#x27;, &#x27;번호/NNG&#x27;, &#x27;작성/NNG&#x27;, &#x27;방법/NNG&#x27;, &#x27;은/JX&#x27;, &#x27;어떻/VA&#x27;, &#x27;게/ECD&#x27;, &#x27;되/VV&#x27;, &#x27;나요/EFQ&#x27;, &#x27;?/SF&#x27;], tags=[&#x27;5&#x27;])] TaggedDocument function을 사용하면 doc2vec에서 사용할 수 있는 태그 된 문서 형식으로 변경한다. 출력해 보면 words배열과 tags값을 갖는 Dic형태의 자료형이 되었음을 확인할 수 있다. 1234567891011121314151617181920212223# make model import multiprocessing cores = multiprocessing.cpu_count() d2v_faqs = doc2vec.Doc2Vec(vector_size=50, alpha=0.025, min_alpha=0.025, hs=1, negative=0, dm=0, dbow_words = 1, min_count = 1, workers = cores, seed=0) d2v_faqs.build_vocab(tagged_faqs) # train document vectors for epoch in range(10): d2v_faqs.train(tagged_faqs, total_examples = d2v_faqs.corpus_count, epochs = d2v_faqs.epochs ) d2v_faqs.alpha -= 0.0025 # decrease the learning rate d2v_faqs.min_alpha = d2v_faqs.alpha # fix the learning rate, no decay 모델을 만들고 학습 시킨다. doc2vec모델을 만들 때 파라미터는 여러가지가 들어가는데 여기서는 vector_size와 min_count정도를 수정했다. vector_size는 만들어지는 벡터 차원의 크기이고, min_count는 최소 몇 번 이상 나온 단어에 대해 학습할지 정하는 파라미터이다. 여기서는 일단 사이즈 50에 최소 횟수는 1회로 정했다. epoch는 10번으로 해서 train했다. 유사 문장 찾기이제 이 모델로 어떤 문장이 들어왔을 때 1~5번 중에 무엇과 비슷한지 알아보자. 먼저 어떤 문장이 들어오면 그 문장을 벡터화 하고 그 벡터가 어떤 문장과 비슷한지 태그 값을 찾아본다. 12predict_vector = d2v_faqs.infer_vector([&quot;당해년도 납입액은 수정 가능 한가요?&quot;])d2v_faqs.docvecs.most_similar([predict_vector], topn=2) 1[(&#x27;2&#x27;, 0.21605531871318817), (&#x27;3&#x27;, 0.10707802325487137)] 제대로 됐는지 확인하기 위해 1번 문장을 그대로 넣었지만 답은 2, 3번이 나왔다. 학습이 제대로 되지 않아서 이런 결과가 나왔다. 테스트할 문장을 벡터화 할 때도 형태소 분석을 해줘야 한다. 왜냐하면 모델을 학습 할 때 문장들을 형태소로 분석해서 넣어줬기 때문이다. 123test_string = &quot;대리인통보 대상계좌 기준은 어떻게 되나요?&quot;tokened_test_string = tokenize_kkma(test_string)tokened_test_string 1234567891011[&#x27;대리인/NNG&#x27;, &#x27;통보/NNG&#x27;, &#x27;대상/NNG&#x27;, &#x27;계좌/NNG&#x27;, &#x27;기준/NNG&#x27;, &#x27;은/JX&#x27;, &#x27;어떻/VA&#x27;, &#x27;게/ECD&#x27;, &#x27;되/VV&#x27;, &#x27;나요/EFQ&#x27;, &#x27;?/SF&#x27;] 12test_vector = d2v_faqs.infer_vector(tokened_test_string)d2v_faqs.docvecs.most_similar([test_vector], topn=2) 1[(&#x27;1&#x27;, 0.1448383331298828), (&#x27;3&#x27;, 0.0218462273478508)] 2번 문장으로 했을 때 결과입니다. doc2vec이라는 모델은 문서단위로 벡터화 하는 것이기 때문에 문서가 많아야 한다. 여기서는 문장이 많아야 한다. 문장이 많으면 많을 수록 문장 간의 거리를 계산해서 더 잘 구분해준다. 간단하게 생각하면 문장이 적으면 적중률이 높을 것 같지만 사실은 그 반대이다. 데이터가 많을수록 그 데이터간의 차이를 구분할 수 있기 때문에 더 잘 예측하게 된다. Local path :C:\\Users\\jmj30\\Dropbox\\카메라 업로드\\Documentation\\2022\\2022 상반기\\휴먼교육센터\\mj_chatbot_prac\\faq_chatbot Reference: https://cholol.tistory.com/466","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Python","slug":"Python/Python","permalink":"https://jmj3047.github.io/categories/Python/Python/"}],"tags":[{"name":"Doc2vec","slug":"Doc2vec","permalink":"https://jmj3047.github.io/tags/Doc2vec/"},{"name":"Chatbot","slug":"Chatbot","permalink":"https://jmj3047.github.io/tags/Chatbot/"},{"name":"NLP","slug":"NLP","permalink":"https://jmj3047.github.io/tags/NLP/"}],"keywords":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Python","slug":"Python/Python","permalink":"https://jmj3047.github.io/categories/Python/Python/"}]},{"title":"Making English Chatbot with doc2vec(3)","slug":"Making_English_Chatbot_with_doc2vec(3)","date":"2022-05-05T15:00:00.000Z","updated":"2022-10-15T08:08:04.571Z","comments":true,"path":"2022/05/06/Making_English_Chatbot_with_doc2vec(3)/","link":"","permalink":"https://jmj3047.github.io/2022/05/06/Making_English_Chatbot_with_doc2vec(3)/","excerpt":"","text":"많은 데이터로 실험해보기데이터 살펴보기더 많은 학습 데이터로 모델을 학습한다. 데이터 원본 링크: https://www.kaggle.com/jiriroz/qa-jokes총 3만 8천개의 문장 데이터 불러오기, 전처리12345678import osimport warningsfrom gensim.models import doc2vecfrom gensim.models.doc2vec import TaggedDocumentimport pandas as pdfaqs = pd.read_csv(os.path.join(&#x27;data&#x27;,&#x27;jokes.csv&#x27;))faqs 한국어와 다르게 영어는 띄어쓰기로 단어가 잘 구분되기 때문에 형태소 분석은 생략한다. 형태소 분석을 하지 않아도 띄어쓰기로 split하면 단어 단위로 잘 짤리기 때문이다. 하지만 영어 단어를 원형으로 만들어 주는 lemmatization이나 the나 a같은 관사를 제거하는 stopword 제거는 해준다. 12345678910from nltk.tokenize import word_tokenizefrom nltk.stem import WordNetLemmatizerfrom nltk.corpus import stopwordsimport nltknltk.download(&#x27;punkt&#x27;)# 토큰화tokened_questions = [word_tokenize(question.lower()) for question in faqs[&#x27;Question&#x27;]]tokened_questions 123456789101112131415[[&#x27;did&#x27;, &#x27;you&#x27;, &#x27;hear&#x27;, &#x27;about&#x27;, &#x27;the&#x27;, &#x27;native&#x27;, &#x27;american&#x27;, &#x27;man&#x27;, &#x27;that&#x27;, &#x27;drank&#x27;, &#x27;200&#x27;, &#x27;cups&#x27;, &#x27;of&#x27;, &#x27;tea&#x27;, &#x27;?&#x27;], 대문자를 모두 소문자로 바꿔주고 토큰화를 한 다음 띄어쓰기로 출력해주었다. 12345678910lemmatizer = WordNetLemmatizer()nltk.download(&#x27;wordnet&#x27;)# lemmatizationlemmed_questions = [[lemmatizer.lemmatize(word) for word in doc] for doc in tokened_questions]lemmed_questionsnltk.download(&#x27;stopwords&#x27;)# stopword 제거 불용어 제거하기stop_words = stopwords.words(&#x27;english&#x27;)questions = [[w for w in doc if not w in stop_words] for doc in lemmed_questions]questions 123456789101112131415**[[&#x27;hear&#x27;, &#x27;native&#x27;, &#x27;american&#x27;, &#x27;man&#x27;, &#x27;drank&#x27;, &#x27;200&#x27;, &#x27;cup&#x27;, &#x27;tea&#x27;, &#x27;?&#x27;], [&quot;&#x27;s&quot;, &#x27;best&#x27;, &#x27;anti&#x27;, &#x27;diarrheal&#x27;, &#x27;prescription&#x27;, &#x27;?&#x27;], [&#x27;call&#x27;, &#x27;person&#x27;, &#x27;outside&#x27;, &#x27;door&#x27;, &#x27;ha&#x27;, &#x27;arm&#x27;, &#x27;leg&#x27;, &#x27;?&#x27;], [&#x27;star&#x27;, &#x27;trek&#x27;, &#x27;character&#x27;, &#x27;member&#x27;, &#x27;magic&#x27;, &#x27;circle&#x27;, &#x27;?&#x27;], [&quot;&#x27;s&quot;, &#x27;difference&#x27;, &#x27;bullet&#x27;, &#x27;human&#x27;, &#x27;?&#x27;], [&#x27;wa&#x27;, &#x27;ethiopian&#x27;, &#x27;baby&#x27;, &#x27;cry&#x27;, &#x27;?&#x27;], [&quot;&#x27;s&quot;, &#x27;difference&#x27;, &#x27;corn&#x27;, &#x27;husker&#x27;, &#x27;epilepsy&#x27;, &#x27;hooker&#x27;, &#x27;dysentery&#x27;, &#x27;?&#x27;], [&#x27;2016&#x27;, &quot;&#x27;s&quot;, &#x27;biggest&#x27;, &#x27;sellout&#x27;, &#x27;?&#x27;],** lemmatization하고 불용어를 제거하고 난 다음의 결과물. 이제 모든 전처리가 끝났으니 TaggedDocument로 변형시키고 나서 doc2vec에 넣어준다. 12345678# 리스트에서 각 문장부분 토큰화index_questions = []for i in range(len(faqs)): index_questions.append([questions[i], i ])# Doc2Vec에서 사용하는 태그문서형으로 변경tagged_questions = [TaggedDocument(d, [int(c)]) for d, c in index_questions] doc2vec 모델화doc2vec을 훈련하기 전에 모델에 변형을 주었다. for문도 빼고 파라미터도 변경해 주었다. 123456789101112131415161718192021222324252627# make modelimport multiprocessingcores = multiprocessing.cpu_count()d2v_faqs = doc2vec.Doc2Vec(vector_size=200, # alpha=0.025, # min_alpha=0.025, hs=1, negative=0, dm=0, dbow_words = 1, min_count = 5, workers = cores, seed=0, epochs=20)d2v_faqs.build_vocab(tagged_questions)d2v_faqs.train(tagged_questions, total_examples = d2v_faqs.corpus_count, epochs = d2v_faqs.epochs ) # # train document vectors # for epoch in range(50): # d2v_faqs.train(tagged_faqs, # total_examples = d2v_faqs.corpus_count, # epochs = d2v_faqs.epochs # ) # d2v_faqs.alpha -= 0.0025 # decrease the learning rate # d2v_faqs.min_alpha = d2v_faqs.alpha # fix the learning rate, no decay 1234567# 테스트하는 문장도 같은 전처리를 해준다.test_string = &quot;What&#x27;s the best anti diarrheal prescription?&quot;tokened_test_string = word_tokenize(test_string)lemmed_test_string = [lemmatizer.lemmatize(word) for word in tokened_test_string]test_string = [w for w in lemmed_test_string if not w in stop_words]test_string 12345678910111213141516# 성능 측정raten = 5found = 0for i in range(len(faqs)): tstr = faqs[&#x27;Question&#x27;][i] tokened_test_string = word_tokenize(tstr) lemmed_test_string = [lemmatizer.lemmatize(word) for word in tokened_test_string] ttok = [w for w in lemmed_test_string if not w in stop_words] tvec = d2v_faqs.infer_vector(ttok) re = d2v_faqs.docvecs.most_similar([tvec], topn = raten) for j in range(raten): if i == re[j][0]: found = found + 1 breakprint(&quot;정확도 = &#123;&#125; % (&#123;&#125;/&#123;&#125; ) &quot;.format(found/len(faqs),found, len(faqs))) 1정확도 = 0.8626303274190598 % (33012/38269 ) Local path :C:\\Users\\jmj30\\Dropbox\\카메라 업로드\\Documentation\\2022\\2022 상반기\\휴먼교육센터\\mj_chatbot_prac\\faq_chatbot Reference: https://cholol.tistory.com/466","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Python","slug":"Python/Python","permalink":"https://jmj3047.github.io/categories/Python/Python/"}],"tags":[{"name":"Doc2vec","slug":"Doc2vec","permalink":"https://jmj3047.github.io/tags/Doc2vec/"},{"name":"Chatbot","slug":"Chatbot","permalink":"https://jmj3047.github.io/tags/Chatbot/"},{"name":"NLP","slug":"NLP","permalink":"https://jmj3047.github.io/tags/NLP/"}],"keywords":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Python","slug":"Python/Python","permalink":"https://jmj3047.github.io/categories/Python/Python/"}]},{"title":"Making Korean Chatbot with doc2vec(2)","slug":"Making_Korean_Chatbot_with_doc2vec(2)","date":"2022-05-05T15:00:00.000Z","updated":"2022-10-15T08:08:14.439Z","comments":true,"path":"2022/05/06/Making_Korean_Chatbot_with_doc2vec(2)/","link":"","permalink":"https://jmj3047.github.io/2022/05/06/Making_Korean_Chatbot_with_doc2vec(2)/","excerpt":"","text":"모델 다듬기FAQ데이터 늘리기더 많은 학습 데이터로 모델을 학습한다. 데이터 원본 링크: https://www.data.go.kr/dataset/3068685/fileData.do 123456789import osimport warningsfrom gensim.models import doc2vecfrom gensim.models.doc2vec import TaggedDocumentimport pandas as pdfaqs = pd.read_csv(os.path.join(&#x27;data&#x27;,&#x27;kor_elec_faq2.csv&#x27;), encoding=&#x27;CP949&#x27;)faqsfaqs[[&#x27;순번&#x27;, &#x27;제목&#x27;, &#x27;내용&#x27;]] pandas를 사용해 csv파일을 바로 읽어준다. utf-8 인코딩 문제로 에러가 나면 cp949로 넣어준다. 전체 필드에서 필요한 index와 질문(여기서는 제목), 답변(여기서는 내용)만 뽑아낸다. 총 351개의 질문과 답변 데이터가 생겼으니 전 게시물과 동일한 방법으로 모델학습을 시킨다. 전 데이터는 pandas데이터가 아니었기 때문에 수정해 준다. 1234567# 리스트에서 각 문장부분 토큰화token_faqs = []for i in range(len(faqs)): token_faqs.append([tokenize_kkma_noun(faqs[&#x27;제목&#x27;][i]), faqs[&#x27;순번&#x27;][i]])# Doc2Vec에서 사용하는 태그문서형으로 변경tagged_faqs = [TaggedDocument(d, [c]) for d, c in token_faqs] 이렇게 하고 모델을 돌려도 성능이 좋지 않음. doc2vec모델의 경우 최소 만단위의 문장이 있어야 제대로 나온다. 튜닝 시도해보기이전까지 데이터 내에 있는 순번을 인덱스로 사용했는데 정상적으로 작동하지 않아 다시 만들어준다. 123456789# 리스트에서 각 문장부분 토큰화token_faqs = []for i in range(len(faqs)): token_faqs.append([tokenize_kkma_noun(faqs[&#x27;제목&#x27;][i]), i ]) # token_faqs.append([tokenize_kkma_noun(faqs[&#x27;제목&#x27;][i]), faqs[&#x27;순번&#x27;][i]])# Doc2Vec에서 사용하는 태그문서형으로 변경tagged_faqs = [TaggedDocument(d, [int(c)]) for d, c in token_faqs]# tagged_faqs = [TaggedDocument(d, [c]) for d, c in token_faqs] 문서 원본을 수정할 필요는 없고 TaggedDocument만들 때만 잘 넣어준면 된다. 기존에는 faqs[’순번’][i]를 태그 값으로 넣어주었는데 그냥 i를 넣는다. 이렇게 하면 좋은 점이 원본의 index와 태그값이 같아지기 때문에 나중에 원문질문을 복원할 때 faqs[’제목’][tag]로 출력이 가능하다. 그리고 이제 가장 먼저 할 거는 전처리를 조금 수정하는 것이다. 형태소 분석을 할 때 필요 없는 데이터를 제외시키는 방법이다. 보통 문장에서는 명사와 동사가 중요하기 때문에 명사 동사 빼고는 다 날려본다. 1234567891011121314151617#튜닝:형태소 필터링kkma = Kkma()filter_kkma = [&#x27;NNG&#x27;, #보통명사 &#x27;NNP&#x27;, #고유명사 &#x27;OL&#x27; , #외국어 &#x27;VA&#x27;,&#x27;VV&#x27;,&#x27;VXV&#x27; ]def tokenize_kkma(doc): jpype.attachThreadToJVM() token_doc = [&#x27;/&#x27;.join(word) for word in kkma.pos(doc)] return token_docdef tokenize_kkma_noun(doc): jpype.attachThreadToJVM() token_doc = [&#x27;/&#x27;.join(word) for word in kkma.pos(doc) if word[1] in filter_kkma] return token_doc 이런식으로 filter_kkma리스트를 하나 만들어 형태소를 분석했을 때 나오는 형태소가 filter_kkma에 포함되어 있을 경우만 학습 대상에 추가한다. tokenize_kkma를 쓰면 전체 형태소 분석, tokenize_kkmk_noun을 쓰면 동사 명사만 추출한다. 가장 결과가 좋게 나온 조합은 명사만 추출, for 문 50번에 epochs&#x3D;100으로 한 결과값. 12345678910111213141516#튜닝:명사만 추출kkma = Kkma()filter_kkma = [&#x27;NNG&#x27;, #보통명사 &#x27;NNP&#x27;, #고유명사 &#x27;OL&#x27; , #외국어 ]def tokenize_kkma(doc): jpype.attachThreadToJVM() token_doc = [&#x27;/&#x27;.join(word) for word in kkma.pos(doc)] return token_docdef tokenize_kkma_noun(doc): jpype.attachThreadToJVM() token_doc = [&#x27;/&#x27;.join(word) for word in kkma.pos(doc) if word[1] in filter_kkma] return token_doc 123456789101112131415161718192021222324252627# make model import multiprocessing import tensorflow as tf with tf.device(&#x27;/device:GPU:0&#x27;): cores = multiprocessing.cpu_count() d2v_faqs = doc2vec.Doc2Vec(vector_size=20, #100 alpha=0.025, min_alpha=0.025, hs=1, negative=0, dm=0, window=3, dbow_words = 1, min_count = 1, workers = cores, seed=0, epochs=100) d2v_faqs.build_vocab(tagged_faqs) # train document vectors for epoch in range(50): d2v_faqs.train(tagged_faqs, total_examples = d2v_faqs.corpus_count, epochs = d2v_faqs.epochs ) d2v_faqs.alpha -= 0.0025 # decrease the learning rate d2v_faqs.min_alpha = d2v_faqs.alpha # fix the learning rate, no decay 123test_string = &quot;변압기공동이용(모자거래)이란 무엇이며, 요금계산은 어떻게 합니까&quot;tokened_test_string = tokenize_kkma_noun(test_string)tokened_test_string 12345678910111213# 성능 측정# raten = 5 #정확도 = 0.5128205128205128 % (180/351 )raten = 1 #정확도 = 0.24216524216524216 % (85/351 ) found = 0for i in range(len(faqs)): tstr = faqs[&#x27;제목&#x27;][i] ttok = tokenize_kkma_noun(tstr) tvec = d2v_faqs.infer_vector(ttok) re = d2v_faqs.docvecs.most_similar([tvec], topn = raten) for j in range(raten): if i == re[j][0]: found = found + 1print(&quot;정확도 = &#123;&#125; % (&#123;&#125;/&#123;&#125; ) &quot;.format(found/len(faqs),found, len(faqs)) 모델 저장, 불러오기1234567891011121314151617# 모델 저장d2v_faqs.save(os.path.join(&#x27;data&#x27;,&#x27;/d2v_faqs_size100_min1_batch50_epoch100_nounonly_dm0.model&#x27;))# 모델 loadd2v_faqs_1 = doc2vec.Doc2Vec.load(os.path.join(&#x27;data&#x27;,&#x27;/d2v_faqs_size100_min1_batch50_epoch100_nounonly_dm0.model&#x27;))#testtest_string = &quot;건물을 새로 지을 때 임시전력은 어떻게 신청하나요&quot;tokened_test_string = tokenize_kkma_noun(test_string)tokened_test_stringtopn = 5# 모델 추측test_vector1 = d2v_faqs_1.infer_vector(tokened_test_string)result1 = d2v_faqs_1.docvecs.most_similar([test_vector1], topn=topn)for i in range(topn): print(&quot;모델 1 &#123;&#125;위. &#123;&#125;, &#123;&#125; &#123;&#125;&quot;.format(i+1, result1[i][1], result1[i][0],faqs[&#x27;제목&#x27;][result1[i][0]] )) Local path :C:\\Users\\jmj30\\Dropbox\\카메라 업로드\\Documentation\\2022\\2022 상반기\\휴먼교육센터\\mj_chatbot_prac\\faq_chatbot Reference: https://cholol.tistory.com/466","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Python","slug":"Python/Python","permalink":"https://jmj3047.github.io/categories/Python/Python/"}],"tags":[{"name":"Doc2vec","slug":"Doc2vec","permalink":"https://jmj3047.github.io/tags/Doc2vec/"},{"name":"Chatbot","slug":"Chatbot","permalink":"https://jmj3047.github.io/tags/Chatbot/"},{"name":"NLP","slug":"NLP","permalink":"https://jmj3047.github.io/tags/NLP/"}],"keywords":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Python","slug":"Python/Python","permalink":"https://jmj3047.github.io/categories/Python/Python/"}]},{"title":"Making English Chatbot with Django(4)","slug":"Making_English_Chatbot_with_Django(4)","date":"2022-05-05T15:00:00.000Z","updated":"2022-05-06T08:28:34.000Z","comments":true,"path":"2022/05/06/Making_English_Chatbot_with_Django(4)/","link":"","permalink":"https://jmj3047.github.io/2022/05/06/Making_English_Chatbot_with_Django(4)/","excerpt":"","text":"실제 서비스 구현해보기**code: https://github.com/jmj3047/faq_chatbot_example.git vs code로 django 설정하기: https://integer-ji.tistory.com/81 채팅창 만들기 html&#x2F;css를 사용해 간단한 채팅화면을 만들었다. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465&lt;!-- //templates/addresses/chat_test.html --&gt;&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;/static/jquery-3.2.1.min.js&quot;&gt;&lt;/script&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt; &lt;link rel=&quot;preconnect&quot; href=&quot;https://fonts.googleapis.com&quot;&gt; &lt;link rel=&quot;preconnect&quot; href=&quot;https://fonts.gstatic.com&quot; crossorigin&gt; &lt;link href=&quot;https://fonts.googleapis.com/css2?family=Inconsolata:wght@400;600&amp;display=swap&quot; rel=&quot;stylesheet&quot;&gt;&lt;/head&gt;&lt;style&gt;* &#123;font-family: &#x27;Inconsolata&#x27;, monospace;&#125;.chat_wrap &#123;display:none;width: 350px;height: 500px;position: fixed;bottom: 30px;right: 95px;background: #a9bdce;&#125;.chat_content &#123;font-size:16pt; position:relative; height: 600px;width: 500px;overflow-y:scroll;padding:10px 15px;background: cornflowerblue&#125;.chat_input &#123;border:solid 0.5px lightgray; padding:2px 5px;&#125;.chat_header &#123;padding: 10px 15px; width: 500px; border-bottom: 1px solid #95a6b4;&#125;.chat_header .close_btn &#123;border: none;background: lightgray;float: right;&#125;.send_btn &#123;border: none; background: #ffeb33;height: 100%; color: #0a0a0a;&#125;.msg_box:after &#123;content: &#x27;&#x27;;display: block;clear:both;&#125;.msg_box &gt; span &#123;padding: 3px 5px;word-break: break-all;display: block;max-width: 300px;margin-bottom: 10px;border-radius: 4px&#125;.msg_box.send &gt; span &#123;background:#ffeb33;float: right;&#125;.msg_box.receive &gt; span &#123;background:#fff;float: left;&#125;&lt;/style&gt;&lt;body&gt;&lt;div class=&quot;chat_header&quot;&gt; &lt;span style=&quot;font-size:20pt;&quot;&gt;EDITH&lt;/span&gt; &lt;button type=&quot;button&quot; id=&quot;close_chat_btn&quot; class=&quot;close_btn&quot;&gt;X&lt;/button&gt;&lt;/div&gt;&lt;div id=&quot;divbox&quot; class=&quot;chat_content&quot;&gt;&lt;/div&gt;&lt;form id=&quot;form&quot; style=&quot;display: inline&quot;&gt; &lt;input type=&quot;text&quot; placeholder=&quot;write message..&quot; name=&quot;input1&quot; class=&quot;chat_input&quot; id=&quot;input1&quot; size=&quot;74&quot; style=&quot;margin:-3px; display: inline; width: 468px; height: 32px; font-size: 16pt;&quot; /&gt; &lt;input type=&quot;button&quot; value=&quot;SEND&quot; id=&quot;btn_submit&quot; class=&quot;send_btn&quot; style=&quot;margin:-5px; display: inline; width: 53px; height: 38px; font-size: 14pt;&quot; /&gt;&lt;/form&gt;&lt;script&gt; $(&#x27;#btn_submit&#x27;).click(function () &#123; send(); &#125;); $(&#x27;#form&#x27;).on(&#x27;submit&#x27;, function(e)&#123; e.preventDefault(); send(); &#125;); $(&#x27;#close_chat_btn&#x27;).on(&#x27;click&#x27;, function()&#123; $(&#x27;#chat_wrap&#x27;).hide().empty(); &#125;); function send()&#123; $(&#x27;#divbox&#x27;).append(&#x27;&lt;div class=&quot;msg_box send&quot;&gt;&lt;span&gt;&#x27;+$(&#x27;#input1&#x27;).val()+&#x27;&lt;span&gt;&lt;/div&gt;&#x27;); $(&quot;#divbox&quot;).scrollTop($(&quot;#divbox&quot;)[0].scrollHeight); console.log(&quot;serial&quot;+$(&#x27;form&#x27;).serialize()) $.ajax(&#123; url: &#x27;http://127.0.0.1:8000/chat_service/&#x27;, //챗봇 api url type: &#x27;post&#x27;, dataType: &#x27;json&#x27;, data: $(&#x27;form&#x27;).serialize(), success: function(data) &#123; &lt;!--$(&#x27;#reponse&#x27;).html(data.reponse);--&gt; $(&#x27;#divbox&#x27;).append(&#x27;&lt;div class=&quot;msg_box receive&quot;&gt;&lt;span&gt;&#x27;+ data.response +&#x27;&lt;span&gt;&lt;/div&gt;&#x27;); $(&quot;#divbox&quot;).scrollTop($(&quot;#divbox&quot;)[0].scrollHeight); &#125; &#125;); $(&#x27;#input1&#x27;).val(&#x27;&#x27;); &#125;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 간단하게 설명하면 Django로 restfulAPI를 구현하기 위한 소스 위에 챗봇을 붙이기 위한 화면과 모델이 들어가 있는 버전이다. 위에 소스는 화면 역할을 하는 chat_test.html 파일이다. jquery 라이브러리를 사용했기 때문에 jquery를 import 해야 한다. jquery file이 static 폴더에 있어야 한다. jquery는 소스 하단부에 있는 script를 위해 필요하다. 채팅에서 전송 버튼을 누르거나 엔터를 누르면 send()라는 함수가 실행되고 이 함수는 ajax로 질문에 대한 답변을 받아오는 API를 호출한다. 여기서는 localhost&#x2F;chat_service를 호출한다. 채팅을 위한 API화면이 만들어졌으면 이제 질문을 받아 답변을 생성하는 API를 만든다. 아직 FAQ데이터를 학습한 모델은 넣지 않았으니 인풋이 들어오면 더미데이터(dummy)를 리턴하는 API를 만든다. 이런 API 동작들은 view.py에서 구현할 수 있다. 1234567891011#faq_chatbot_example/addresses/views.py@csrf_exemptdef chat_service(request): if request.method == &#x27;POST&#x27;: input1 = request.POST[&#x27;input1&#x27;] output = dict() output[&#x27;response&#x27;] = &quot;이건 응답&quot; return HttpResponse(json.dumps(output), status=200) else: return render(request, &#x27;addresses/chat_test.html&#x27;) Django 프로젝트 안에 addresses 앱에 있는 views.py를 보면 chat_service 함수를 만들었다. POST형식으로 콜이 오면 response에 아웃풋 메세지를 담아서 json형태로 리턴한다. views.py에 함수를 만들고 url로 연결하기 위해서 urls.py에 chat_service를 입력한다. 123456789101112###django 3.8.3 버전 맞춰줘야 함#/faq_chatbot_example/restfulapiserver/urls.py# from django.conf.urls import url, includefrom addresses import viewsfrom django.urls import path, re_path, includefrom django.contrib import adminurlpatterns = [ ... path(&#x27;chat_service/&#x27;, views.chat_service), ...] urls.py에서 ~&#x2F;chat_service를 views.chat_service에 연결시킨다. 이제 ~&#x2F;chat_service로 콜하면 views.chat_service가 실행된다. 아까 위에서 만든 채팅페이지에 전송버튼을 누르면 ajax를 이용해 chat_service를 호출했다. 정상적으로 되는지 테스트 해본다. FAQ 모델 넣기addresses 앱 안에 새로운 py 모델을 만들어서 넣기 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374#/faq_chatbot_example/addresses/faq_chatbot.pyfrom gensim.models import doc2vec, Doc2Vecfrom gensim.models.doc2vec import TaggedDocumentimport pandas as pdfrom nltk.tokenize import word_tokenizefrom nltk.stem import WordNetLemmatizerfrom nltk.corpus import stopwordsimport nltk# 파일로부터 모델을 읽는다. 없으면 생성한다.try: d2v_faqs = Doc2Vec.load(&#x27;d2v_faqs_size200_min5_epoch20_jokes.model&#x27;) lemmatizer = WordNetLemmatizer() stop_words = stopwords.words(&#x27;english&#x27;) faqs = pd.read_csv(&#x27;jokes.csv&#x27;)except: faqs = pd.read_csv(&#x27;jokes.csv&#x27;) nltk.download(&#x27;punkt&#x27;) # 토근화 tokened_questions = [word_tokenize(question.lower()) for question in faqs[&#x27;Question&#x27;]] lemmatizer = WordNetLemmatizer() nltk.download(&#x27;wordnet&#x27;) # lemmatization lemmed_questions = [[lemmatizer.lemmatize(word) for word in doc] for doc in tokened_questions] nltk.download(&#x27;stopwords&#x27;) # stopword 제거 불용어 제거하기 stop_words = stopwords.words(&#x27;english&#x27;) questions = [[w for w in doc if not w in stop_words] for doc in lemmed_questions] # 리스트에서 각 문장부분 토큰화 index_questions = [] for i in range(len(faqs)): index_questions.append([questions[i], i ]) # Doc2Vec에서 사용하는 태그문서형으로 변경 tagged_questions = [TaggedDocument(d, [int(c)]) for d, c in index_questions] # make model import multiprocessing cores = multiprocessing.cpu_count() d2v_faqs = doc2vec.Doc2Vec( vector_size=200, hs=1, negative=0, dm=0, dbow_words=1, min_count=5, workers=cores, seed=0, epochs=20 ) d2v_faqs.build_vocab(tagged_questions) d2v_faqs.train(tagged_questions, total_examples=d2v_faqs.corpus_count, epochs=d2v_faqs.epochs) d2v_faqs.save(&#x27;d2v_faqs_size200_min5_epoch20_jokes.model&#x27;)# FAQ 답변def faq_answer(input): # 테스트하는 문장도 같은 전처리를 해준다. tokened_test_string = word_tokenize(input) lemmed_test_string = [lemmatizer.lemmatize(word) for word in tokened_test_string] test_string = [w for w in lemmed_test_string if not w in stop_words] topn = 5 test_vector = d2v_faqs.infer_vector(test_string) result = d2v_faqs.docvecs.most_similar([test_vector], topn=topn) print(result) for i in range(topn): print(&quot;&#123;&#125;위. &#123;&#125;, &#123;&#125; &#123;&#125; &#123;&#125;&quot;.format(i + 1, result[i][1], result[i][0], faqs[&#x27;Question&#x27;][result[i][0]], faqs[&#x27;Answer&#x27;][result[i][0]])) return faqs[&#x27;Answer&#x27;][result[0][0]]faq_answer(&quot;What do you call a person who is outside a door and has no arms nor legs?&quot;) 위 소스에서 상단에 있는 모델을 만드는 코드는 API 서버를 실행하는 시점에서 호출된다. 무조건 호출하는 건 아니고 views.py에서 import를 써 넣으면 최초 1번은 실행되게 된다. 채팅 웹페이지로부터 faq_chatbot.py에 있는 faq_answer를 호출하는 것 까지 flow를 그려보면 chat_test.html→view.py(chat_service)→faq_chatbot.py(faq_answer) 순서이다. 따라서 views.py에서 faq_answer함수를 호출하기 위해 import를 하게 되는데 django는 최초 실행시 views.py를 한번 읽기 때문에 faq_chatbot.py에 적어놓은 소스가 한번 실행되게 된다. 매번서버를 실행할 때마다 모델을 새로 만들게 되면 서버 기동 속도가 느려지고 비효율적이기 때문에 모델을 만들고 나서 패일로 저장하고, 만들어진 파일이 없다면 모델을 생성하도록 try&#x2F;except를 사용했다. 추가적으로 프로젝트상 소스가 실행되기 때문에 파일경로는 root이다. jokes.csv가 있어야 할 곳과 모델이 생성되는 곳의 경로는 프로젝트의 root폴더이다. 자 이제 질문의 답을 찾아주는 함수가 만들어 졌으니 아까 더미 데이터로 리턴해주던 views.py의 함수를 바꿔보자. 123456789101112#faq_chatbot_example/addresses/views.py@csrf_exemptdef chat_service(request): if request.method == &#x27;POST&#x27;: input1 = request.POST[&#x27;input1&#x27;] response = faq_answer(input1) output = dict() output[&#x27;response&#x27;] = response return HttpResponse(json.dumps(output), status=200) else: return render(request, &#x27;addresses/chat_test1.html&#x27;) 이전에는 response에 무조건 더미 응답을 보냈는데 이제는 faq_answer함수를 사용해 해당 질문에 알맞은 정답을 가져온다. faq_answer함수를 사용하기 위해 제일 상단에 from .faq_chatbot import faq_answer를 선언해야 한다. 실행 결과(html 파일 수정) **code: ‣https://github.com/jmj3047/faq_chatbot_example.git Reference: https://cholol.tistory.com/478","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Django","slug":"Python/Django","permalink":"https://jmj3047.github.io/categories/Python/Django/"}],"tags":[{"name":"Doc2vec","slug":"Doc2vec","permalink":"https://jmj3047.github.io/tags/Doc2vec/"},{"name":"Chatbot","slug":"Chatbot","permalink":"https://jmj3047.github.io/tags/Chatbot/"},{"name":"NLP","slug":"NLP","permalink":"https://jmj3047.github.io/tags/NLP/"}],"keywords":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Django","slug":"Python/Django","permalink":"https://jmj3047.github.io/categories/Python/Django/"}]},{"title":"Support Vector Machine","slug":"SVM","date":"2022-05-05T15:00:00.000Z","updated":"2023-02-23T01:34:32.094Z","comments":true,"path":"2022/05/06/SVM/","link":"","permalink":"https://jmj3047.github.io/2022/05/06/SVM/","excerpt":"","text":"1. 분류에 대한 수적 표현 학습 데이터 X(독립변수),Y(종속변수)가 있을 때 (i&#x3D;1,2,3,4,5 ….데이터의 갯수) Y⇒{-1,1} (두 개의 클래스를 의미) ⇒ 경우에 따라서, 클래스를 1과 -1 로 나눔 Y(정답) * F(x)(예측한 정답) &gt;0 라는 것은 제대로 분류된 형태 ( 같은 부호끼리 곱하면 양수인 경우니까) 2. 선형 분할(Linear Classifier) f(x)&#x3D; W transpose X + b (선형조합, 각각의 항들이 더하기로 이루어진 조합.) 선형분할은 직선으로 나누는 것 (2차원이건 3차원이건 그 이상이건 상관 없음) b(bias) Y 절편을 의미 W는 직선의 기울기 3. 초평면 분할 더 나은(최적) 분류를 위한 초평면(Hyperplane)→선 보다 더 큰 차원 좋은 판별선에 대한 기준 최적화: 좋은 것 을 극대화 시키고 나쁜 것 을 극소화 시키는 것 분류에서의 최적화: 잘 안나뉘는것 , 잘 나뉘는 것 나중에 Testing data 를 돌렸을때, 가장 좋게 나뉜 것은 반절로 나뉜 직선이다. Test data 가 어떻게 들어올지 모르는 것 이기때문에 , 과적합 되어 있는 것보다 확실히 절반으로 나누는것이 좋다. 최적의 분할 초평면 찾기 Margin: c는 선형분할의 각 클래스별 거리 각 클래스별 거리를 합친 것 Margin&#x3D;2c를 최대화 하는, w T x +b&#x3D;0 의 직선을 찾아야 하는것 이다. Marign 을 최대화 시키는 초평면이 최적 “Learning Theory” 에 따르면, Marigin을 최대화 시키는 초평면이 일반화 오류가 가장 낮게 나타남(Test data 에서도 좋은 점수가 나온다) Margin:초평면과 가장 근접한 각 클래스 관측치와의 거리의 합. Margin 수식 유도 일반적인 방법 점과 선 사이의 거리 거리 d 가 2개이니까 2&#x2F;||W|| Margin 최대화 (최적화) ||w|| 가 분모에 있기 때문에 결국 ||w|| 를 최소화 해주는것 이 2&#x2F;||w|| 를 최대화 해주는거랑 같다고 할 수 있다 우리는 결국 w 값을 최소화 시켜주는것이 목적이기 때문에 제곱을 취해주든 상수를 곱해주는 상관이 없다 Lagrange Multiplier(수학적 기법) ⇒ 제약조건을 최적화 조건에 녹여버리는 기법. 💡 라그랑쥬를 다 풀고 나면 판별식이 나온다. Xi tranpose X ( 학습데이터와 분류할 데이터의 내적) 4. SVM(Support Vector Machine) 판별식에 서포트벡터만 사용하기 때문에 아웃라이어에 대한 영향을 안 받음(KKT 조건으로 걸러냄) KNN 또한 이웃을 확인하는 개수인 K의 한계가 있어서 어느 elbow point 를 지나치면 정확도가 떨어진다. → 비슷한 원리 ⇒ svm 또한 분류를 유효하게 하기위해서 support verctor 만 이용해준다. 선형으로 완벽히 나눠지지 않는 데이터라면 테스트 데이터에게는 위의 모델 보다 아래 모델이 더 좋을 것 으로 보인다. 하지만 SVM 의 제약조건에는 트레인데이터가 완벽하게 나누어져야 한다는 제약 조건이 걸려있다. 어떻게 하면 좋을까? Slack Variable for “Soft Margin” Soft Margin SVM Non-linear SVM Reference: 한국공학대학교 경영학과 강지훈 교수님 강의","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}],"tags":[{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}]},{"title":"Setting Git & Virtualenv","slug":"Setting_Git_&_Virtualenv","date":"2022-05-05T15:00:00.000Z","updated":"2023-02-28T07:50:54.784Z","comments":true,"path":"2022/05/06/Setting_Git_&_Virtualenv/","link":"","permalink":"https://jmj3047.github.io/2022/05/06/Setting_Git_&_Virtualenv/","excerpt":"","text":"Put Local folder into git repo Make folder ‘example’ and git repo ‘example 123456789101112131415161718192021222324#in local cmd example foldergit init #add remote repogit remote add origin &#x27;repo https&#x27;#bring files in repo to localgit pull origin master #bring local files to git repogit add .git commit -m &#x27;updated&#x27;git push orgin master #check remotegit remote -v#check current statusgit status#error: failed to push some refs to &#x27;https://github.com/jmj3047/.git&#x27;#force to push git push -f origin master Setting virtual env in window&#x2F;linux1234567#****use virtual env no matter what****&gt;python -m venv env_name&gt;source env_name/Scripts/activate #window&gt;source env_name/bin/activate #linux#put all the version of modules in requirements.txt&gt;pip install -r requirements.txt","categories":[{"name":"Setting","slug":"Setting","permalink":"https://jmj3047.github.io/categories/Setting/"}],"tags":[{"name":"Git","slug":"Git","permalink":"https://jmj3047.github.io/tags/Git/"},{"name":"Virtualenv","slug":"Virtualenv","permalink":"https://jmj3047.github.io/tags/Virtualenv/"}],"keywords":[{"name":"Setting","slug":"Setting","permalink":"https://jmj3047.github.io/categories/Setting/"}]},{"title":"What is Transformer","slug":"What_is_Transformer","date":"2022-05-05T15:00:00.000Z","updated":"2022-10-15T08:08:32.440Z","comments":true,"path":"2022/05/06/What_is_Transformer/","link":"","permalink":"https://jmj3047.github.io/2022/05/06/What_is_Transformer/","excerpt":"","text":"Transformer란?트랜스포머(Transformer)는 구글에서 발표한 논문 “Attention is all you need”에 나오는 모델이다. 아래 글은 이 논문 abstract의 일부분이다. The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU … 여기서도 알 수 있듯이, 트랜스포머는 어텐션(Attention) mechanism을 기반으로 여러개의 인코더와 디코더를 연결한 구조를 갖고 있다. 또한 CNN, RNN, LSTM 등의 구조를 사용하지 않았기 때문에 학습 시간이 훨씬 감소된 성능을 내었다고 한다. 그렇다면 그 구조가 무엇인지 더 알아보도록 하자. (1) 트랜스포머의 입력먼저 트랜스포머의 입력부터 알아보자, 단어 벡터 데이터가 트랜스포머의 입력으로 들어가지게 되는데 이 때 각 단어의 위치 정보를 알려주어야 한다. 왜냐하면 트랜스포머에 단어가 입력될 때 순차적으로 받아지지 않기 때문이다. 따라서 순서 정보를 더해주어야 하기 때문에 위치 정보를 각 단어 벡터마다 더해주어야 하는데, 이 과정을 포지셔널 인코딩(positional encoding)이라고 한다. 포지셔널 인코딩 값을 더해주기 위해서는 사인함수와 코사인함수를 사용한 아래 두 함수를 사용한다. 위 식에서 pos는 입력된 데이터의 임베딩 벡터(몇번째 단어인지)를, i는 임베딩 벡터내의 차원의 인덱스(0~512)를 뜻한다. 임베딩 벡터내의 차원이란 트랜스포머 모델의 인코더와 디코더에서 정해진 입력과 출력의 크기를 말한다. 논문상에서 이 차원을 512로 설정했으면 이 차원은 인코더의 값을 디코더로 보낼때 값을 유지하도록 한다. 다시 돌아와서, 위 함수에서 차원이 2i(짝수)인지 2i+1(홀수)인지에 따라서 사용하는 함수가 다르다. 짝수 차원의 경우 사인함수, 홀수차원의 경우 코사인 함수를 사용하게된다. (2)인코더(Encoder)의 구조 위 이미지는 트랜스포머 논문에 함께 실려 있는 이미지로, 트랜스포머의 구조를 나타낸다.여기서 왼쪽 부분이 트랜스포머의 인코더 부분인데, 인코더의 구조는 어떻게 이루어졌을까? 먼저 이미지 왼쪽 아래를 보자. Input data가 들어가게 되면 Input Embedding을 거치게 되는데, 여기서는 문자열인 단어 데이터를 벡터형태로 변환해 준다(단어 길이 X 벡터차원의 행렬). 그리고 나서 위에서 설명한 포지셔널 인코딩을 수행해주게 된다. 그러고 나서 박스로 표현된 인코더에 들어가게 된다. 인코더 안에서는 크게 Multi-Head Attention과 Feed Forward과정이 수행되는데, Multi-Head Attention은 셀프 어텐션이 병렬적으로 사용된 것을 말하며, Feed Forward란 피드포워드 신경망 구조를 의미한다. 한편, 위에서 잠깐 언급했지만 트랜스포머에서는 여러 개의 인코더와 디코더를 쌓은 구조를 갖고 있다. 즉, 인코더가 1개가 아니라는 뜻인데, 논문에서는 6개의 인코더 층을 사용했다고 하니, 6개라고 설정하도록 하겠다. 아무튼, 인코더 과정을 총 6번 반복한다고 생각하면 된다. *셀프 어텐션이란?셀프 어텐션이란 자기 자신에게 어텐션 함수를 수행하는 것을 말하는데, 그렇다면 어텐션이란 무엇일까? 어텐션에서도 다양한 종류가 있는데 간단히 말하자면, 쿼리(Query)가 주어졌을 때, 이 쿼리와 여러개의 키(Key)와의 유사도를 각각 구하고, 구한 유사도를 가중치로 설정하여 각각의 값(value)을 구한 뒤, 이 값(유사도가 반영된 값)들을 모두 가중합하여 반환하는 함수를 말한다. 예를 들어, 한 텍스트 문장이 쿼리로 입력될 때, 각 단어 벡터들과의 유사도를 계산해 이 유사도를 가중합하여 반환된 값이 그 문장의 어텐션 값이 된다. 그렇다면 셀프 어텐션 값을 구하기 위해서 입력된 문장의 단어 벡터(쿼리)에 대해 쿼리(query),키(key), 값(value) 벡터가 정의되어야 할 것이다. 그 과정은 아래 이미지를 통해 쉽게 이해할 수 있다. ‘student’라는 단어 벡터가 입력되었을 때, 각각 쿼리, 키 값의 가중치 행렬을 곱해주어 쿼리, 키, 값 벡터를 얻어낸다. 이렇게 쿼리 벡터, 키 벡터, 값 벡터를 얻어냈다면 쿼리 벡터는 모든 키 벡터에 대해 어텐션 스코어(attention score)를 구하게 되고, 이를 이용하여 모든 값 벡터를 가중합 하여 어텐션 값을 구하게 된다. 한편, 이러한 연산은 각 단어마다가 아닌 문장 전체에 대해서 행렬 연산으로도 일괄적으로 연산이 가능한데, 위와 같이 문장에 대한 쿼리 벡터, 키 벡터의 연산을 통해 값 벡터 행렬을 구할 수 있게 된다. 마지막으로 쿼리벡터와 키 벡터가 연산되어 나온 행렬에 전체적으로 특정 값(key벡터 차원의 제곱근 값)을 나누어 준 뒤, 소프트맥스 함수를 적용해주고, 가중치가 계산된 값 벡터를 곱하게 되면 최종적으로 각 단어의 어텐션 값을 가지는 어텐션 값 행렬이 도출된다. 즉, 요약하자면 어텐션 함수는 쿼리(Query)가 주어졌을 때, 이 쿼리와 여러개의 키(key)와의 유사도를 각각 구하고, 구한 유사도를 가중치로 설정하여 각각의 값(value)을 구한 뒤, 유사도가 반영된 값들을 모두 가중합 하여 반환하는 함수를 말한다. *멀티 헤드 어텐션(Multi-Head Attention)이란?앞에서 트랜스포머의 인코더에서는 어텐션이 병렬적으로 수행되는 멀티 헤드 어텐션이 수행된다고 했다. 논문에서는 512차원의 벡터를 8로 나누어 54차원의 Query, Key, Value 벡터로 바꾸어서 어텐션 함수를 병렬적으로 수행한 것인데, 그렇다면 왜 이렇게 수행한 것일까? 즉, 차원을 나누어서 어텐션 함수를 수행한 뒤, 가중치 행렬을 곱해주고 이를 다시 합치게 되는건데, 논문에 따르면 single attention function을 하는 것보다 병렬적으로 수행하는 것이 모델이 학습하는 데에는 더 효과적이었으며, 모델이 다른 영역(과거시점과 미래시점)에 있는 정보들을 참조할 수 있다고한다. 따라서 출력된 값들은 인코더의 입력 값의 차원과 동일하게 유지된다. *피드 포워드 신경망이란?인코더 안에서 Multi-Head Attention이 수행되고 나면 Feed Forward가 수행된다고 했었는데, Feed Forward는 무엇일까? Feed Forward는 일종의 신경망으로 Feed Forward Neural Network를 줄여서 FFNN이라고 한다. FFNN의 종류도 여러가지가 있는데, 트랜스 포머의 인코더 층에는 포지션 와이즈(Position-wise) FFNN을 사용한다. 포지션 와이즈 FFNN은 Fully-connected FFNN과 같은 기능을 하는데, 아래와 같은 연산을 수행한다. 위 식에서 x의 값은 Multi-Head Attention에서 출력된 행렬 값이다. 반면, 가중치를 의미하는 W1, W2, b1, b2는 가중치 값으로 인코더 마다 다른 값을 가지지만 하나의 인코더 층 안에서는 문장과 단어들마다 동일하게 사용된다고 한다. 이렇게 피드 포워드 신경망까지 거치게 되면 한 인코더의 출력값이 도출 되고, 이 값은 다시 두번째 인코더 입력으로 들어가게 되며 이 과정이 반복된다. *Add &amp; Norm한편 인코더의 구조를 보여준 이밎를 다시 보고 오면 2개의 서브층인 Multi-Head Attention과 Feed Forward가 각각 끝나고 나면 Add &amp; Norm 이라는 단계가 수행된다. 이것은 또 무엇일까? 논문의 일부분을 읽어보면 Add &amp; Norm이란 바로 두개의 서브층을 residual connection 해주고 layer normalization을 해주는 것을 의미한다. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. Residual connection과 layer normalization에 대해 짧게 요약하자면, residual connection은 서브층의 입력과 출력을 더 하는 것이다. 이러한 알고리즘은 RNN, VGG 구조에서도 볼 수 있고, 이러한 연산이 가능한 것은 입력 데이터와 출력 데이터가 동일한 차원을 갖고 있기 때문이라고 한다. 반면, layer normalization은 정규화를 하는 과정으로 출력된 값들에 대해서 평균과 분산을 구해서 정규화를 하는 것을 말한다. 앞에서 입력데이터인 512차원의 벡터를 8로 나누어 어텐션 함수를 병렬적으로 수행하였다고 했는데, 그렇게 출력된 8개의 값들로 layer normalization을 하는 것이다. (3)디코더(Decoder)의 구조 다시 트랜스포머의 구조를 살펴보자. 지금까지 왼쪽에 있는 인코더에 대해 살펴 보았고, 이제 오른쪽에 있는 디코더에 대해 살펴보도록 하겠다. 디코더는 인코더에서 넘겨받은 값에 대해 Multi-Head Attention과 Feed Forward를 수행하기 전 output data에 대해 임베딩과 포지셔널 인코딩을 한 값을 입력 받는다. 그리고 인코더와는 다르게 Masked Multi-Head Attention이라는 것을 해주게 된다. *Masked Multi-Head Attention이란?Masked Multi-Head Attention은 말그대로 Multi-Head Attention에서 Mask기능이 들어간 것이다. 앞에서 Multi-Head Attention은 셀프어텐션을 병렬적으로 수행한 것을 의미했었다. 따라서 다른 영역에 있는, 즉 미래의 시점에 있는 단어의 정보도 알 수 있게 된다고 했었다. 이러한 이유로 트랜스포머의 디코더에는 현재시점보다 미래에 있는 단어를 참고해 예측하지 못하고 이전시점들에 있는 단어들만 참고할 수 있도록 마스킹해줘야 한다. 아마 미래에 있는 단어를 참고해 예측하도록 한다면 학습하는데 도움이 되지 않는가 보다. 답지보고 베끼는 느낌이랄까? 아무튼 마스킹을 하기 위해 lood-ahead mask라는 것을 해주는데, Multi-Head Attention을 통해 나온 행렬 값에 대해 마스킹을 하고자 하는 값에는 1, 마스킹을 하지 않는 값에는 0을 리턴하도록 한다. 그리고 나서 Add &amp; Norm 과정을 수행해준뒤 도출된 결과를 다음 결과로 보내준다. *디코더의 Multi-Head Attention과 Feed Forward디코더에서 Masked Multi-Head Attention이 수행되고 나면 그 다음부터는 인코더와 마찬가지로 Multi-Head Attention과 Feed Forward가 수행된다. 근데 이때 Multi-Head Attention에 입력으로 들어가는 값들을 잘 살펴 봐야 한다. 인코더에서 출력된 값과 디코더 첫번째 서브층에서 출력된 값이 인풋으로 들어가기 때문이다. 두번째 서브층인 Multi-Head Attention에서는 마찬가지로 셀프어텐션 함수를 수행하기 위해 Query, Key, Value 벡터가 입력되어야 한다. 이때 Query는 디코더 첫번째 서브층에서 출력된 값이 해당되고, Key 벡터와 Value 벡터는 마지막 인코더에서 출력된 값으로 입력된다. 그리고 똑같이 Multi-Head Attention을 수행해주게 된다. 이렇게 6개의 디코더마다 Multi-Head Attention의 Query 벡터는 디코더 첫번째 서브층의 output, Key벡터와 Value벡터는 인코더의 output이 입력으로 들어가게 된다. Reference attention 논문: https://arxiv.org/abs/1409.0473 https://wikidocs.net/31379 https:&#x2F;&#x2F;velog.io&#x2F;@seolini43&#x2F;파이썬Transformer로-오피스-챗봇-만들기-이론편","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"NLP","slug":"Paper/NLP","permalink":"https://jmj3047.github.io/categories/Paper/NLP/"}],"tags":[{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"},{"name":"Transformer","slug":"Transformer","permalink":"https://jmj3047.github.io/tags/Transformer/"},{"name":"Attention","slug":"Attention","permalink":"https://jmj3047.github.io/tags/Attention/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"NLP","slug":"Paper/NLP","permalink":"https://jmj3047.github.io/categories/Paper/NLP/"}]},{"title":"HTML with Python_CGI","slug":"HTML_with_Python_CGI","date":"2022-05-03T15:00:00.000Z","updated":"2022-05-04T08:38:19.000Z","comments":true,"path":"2022/05/04/HTML_with_Python_CGI/","link":"","permalink":"https://jmj3047.github.io/2022/05/04/HTML_with_Python_CGI/","excerpt":"","text":"** code: https://github.com/jmj3047/mj_chatbot_prac&#x2F;blob&#x2F;c5bec233b833b24345deeffe7391621415dc1dcb&#x2F;cgi_webpython.py Making Website with CGI Constructing Web Server: Download and Install Apche Official: Installing Apache in window(https://httpd.apache.org/docs/2.4/platform/windows.html) Beginner version: Install Bitnami Wamp Stack push 1 or 2 click this button below it takes some time to install it Constructing Web Server: Bitnami Wamp Stack Start Bitnami Wamp Stack Click Go to Application: If you can see the site like the picture below, success! Start or Stop the server: Click Manage Server If program below is shut down, go to the folder where ‘bitnami wamp stack’ installed and click ‘manager-windows.exe’ Using Python in Web(HTML): Setting Apache Install python and apache find folder where apach installed(‘D:\\wamp\\apache2\\conf’) &gt; ‘conf’ folder &gt; httpd.conf open httpd.conf file and search: 1LoadModule cgid_module modules/mod_cgid.so and if ‘#’ exists in front of code, delete it Find tag in httpd.conf and add some lines 1234567891011121314151617181920212223242526272829303132333435&lt;Directory &quot;/Applications/mampstack-8.0.5-0/apache2/htdocs&quot;&gt; # # Possible values for the Options directive are &quot;None&quot;, &quot;All&quot;, # or any combination of: # Indexes Includes FollowSymLinks SymLinksifOwnerMatch ExecCGI MultiViews # # Note that &quot;MultiViews&quot; must be named *explicitly* --- &quot;Options All&quot; # doesn&#x27;t give it to you. # # The Options directive is both complicated and important. Please see # http://httpd.apache.org/docs/2.4/mod/core.html#options # for more information. # Options Indexes FollowSymLinks # # AllowOverride controls what directives may be placed in .htaccess files. # It can be &quot;All&quot;, &quot;None&quot;, or any combination of the keywords: # AllowOverride FileInfo AuthConfig Limit # AllowOverride None # # Controls who can get stuff from this server. # Require all granted ********** add this code ********** &lt;Files *.py&gt; Options ExecCGI AddHandler cgi-script .py &lt;/Files&gt; *********************************** &lt;/Directory&gt; htdocs 디렉토리 내 확장자가 py인 모든 파일은 CGI기능을 활성시키고 CGI로 실행하라는 의미 Restart Apache Web Server in manager-osx Python file setting index.py 가 있는 htdocs 디렉토리에서 index.py 실행 후 아래같이 입력(다른 파이썬 파일을 만들어도 상관 없음) 123#!/usr/local/bin/python3 &gt;&gt;&gt; python.exe 경로 환경변수에 저장해줬다면 !Python만 해도 됨 print(&quot;Content-Type: text/html&quot;) print() index.html 의 코드 넣기( 다른 html 파일 이어도 됨): index.py가 실행되었을 때 index.html 의 코드가 출력되게 해주는 코드 12345678910111213141516171819202122#!/usr/local/bin/python3print(&quot;Content-Type: text/html&quot;)print()print(&#x27;&#x27;&#x27;&lt;!doctype html&gt; # ---&gt; 줄바꿈을 위해 docsting (&#x27;&#x27;&#x27; &#x27;&#x27;&#x27;) 사용&lt;html&gt;&lt;head&gt; &lt;title&gt;WEB1 - Welcome&lt;/title&gt; &lt;meta charset=&quot;utf-8&quot;&gt;&lt;/head&gt;&lt;body&gt; &lt;h1&gt;&lt;a href=&quot;index.html&quot;&gt;WEB&lt;/a&gt;&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;a href=&quot;qs-1.html&quot;&gt;HTML&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;qs-2.html&quot;&gt;CSS&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;qs-3.html&quot;&gt;JavaScript&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h2&gt;WEB&lt;/h2&gt; &lt;p&gt;The World Wide Web (abbreviated WWW or the Web) is an information space where documents and other web resources are identified by Uniform Resource Locators (URLs), interlinked by hypertext links, and can be accessed via the Internet.[1] English scientist Tim Berners-Lee invented the World Wide Web in 1989. He wrote the first web browser computer program in 1990 while employed at CERN in Switzerland.[2][3] The Web browser was released outside of CERN in 1991, first to other research institutions starting in January 1991 and to the general public on the Internet in August 1991. &lt;/p&gt;&lt;/body&gt;&lt;/html&gt;&#x27;&#x27;&#x27;) 웹 브라우저 주소창에 localhost:8080&#x2F;index.py 입력하고 접속 index.html 파일의 내용이 잘 출력된다면 구현 성공 Internal Server Error 가 확인된다면 에디터에서 apache2&#x2F;logs 디렉토리 내 error_log 파일에 있는 에러 코드 확인 및 구글링 EXAMPLE123456789101112131415161718192021222324252627282930#!C:\\Python310\\python.exe ---&gt;파이썬 경로# 한글이 꺠지지 않으려면 꼭 넣어야 함# -*- coding:utf-8 -*-import sysimport codecssys.stdout =codecs.getwriter(&quot;utf-8&quot;)(sys.stdout.detach())import cgi# cgitb는 CGI 프로그래밍시 디버깅을 위한 모듈로 cgitb.enable()할 경우 런타임 에러를 웹브라우저로 전송함# cgitb.enable() 하지 않은 상태로 실행 중 오류가 발생한 경우 웹서버는 클라이언트에게 HTTP응답 코드 500을 전송함import cgitbcgitb.enable()# HTTP 규격에서 헤더 전송 이후에는 반드시 줄 바꿈을 하게되어 있음으로 마지막에 \\r\\n을 전송# 마지막에 \\r\\n을 전송하지 않으면 브라우저 측에서 오류가 발생print(&quot;Content-type: text/html;charset=utf-8\\r\\n&quot;)print(&quot;&quot;&quot; &lt;!doctype html&gt; &lt;html&gt; &lt;head&gt; &lt;meta charset=&#x27;utf-8&#x27;&gt; &lt;h1&gt;안녕?&lt;/h1&gt; &lt;h2&gt;Thank you so much&lt;/h2&gt; &lt;h3&gt;This page is made by Python&lt;/h3&gt; &quot;&quot;&quot;)a = 3+4+5b = a/3print(&#x27;b는 :&#x27;, b)print(&quot;&lt;/head&gt;&quot;)print(&quot;&lt;/html&gt;&quot;) Result Reference https://daekiry.tistory.com/4?category=928946 https://daekiry.tistory.com/5?category=928946 https://daekiry.tistory.com/6 https:&#x2F;&#x2F;velog.io&#x2F;@ssoulll&#x2F;python-웹-페이지를-CGI로-구현","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"HTML","slug":"Python/HTML","permalink":"https://jmj3047.github.io/categories/Python/HTML/"}],"tags":[{"name":"Web Server","slug":"Web-Server","permalink":"https://jmj3047.github.io/tags/Web-Server/"},{"name":"CGI","slug":"CGI","permalink":"https://jmj3047.github.io/tags/CGI/"}],"keywords":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"HTML","slug":"Python/HTML","permalink":"https://jmj3047.github.io/categories/Python/HTML/"}]},{"title":"HTML with Python_Flask & Brython","slug":"HTML_with_Python_Flask&Brython","date":"2022-05-03T15:00:00.000Z","updated":"2023-05-17T11:50:35.489Z","comments":true,"path":"2022/05/04/HTML_with_Python_Flask&Brython/","link":"","permalink":"https://jmj3047.github.io/2022/05/04/HTML_with_Python_Flask&Brython/","excerpt":"","text":"Flask 파이썬 기반 마이크로 웹 개발 프레임워크 웹 개발의 핵심기능만 간경하게 유지 필요한 기능은 다른 라이브러리나 프레임워크로 손쉽게 확장 신속하게 최소한의 노력으로 웹 애플리케이션 개발 가능 Installation start virtualenv pip install flask Error → note: could not find a version that satisfies the requirement flask → 네트워크 문제로 외부 라이브러리 저장소에 접근하지 못할 경우 나오는 문제, → 직접 https://github.com/mitsuhiko/flask 위치로 가서 소스 받아 설치 해야 함. Strat Flask**code: https://github.com/jmj3047/mj_chatbot_prac&#x2F;blob&#x2F;c5bec233b833b24345deeffe7391621415dc1dcb&#x2F;flask_prac&#x2F;0.flask_hello.py 12345678910from flask import Flaskapp = Flask(__name__)@app.route(&#x27;/&#x27;)def hello_world(): return &#x27;Hello World!&#x27; if __name__ == &#x27;__main__&#x27;: app.debug =True app.run() 소스를 실행하고, terminal 에서 ‘python flask_test.py’ 입력 후 http://127.0.0.1:5000 으로 접근 Process for starting Flask Application 특정 URL 호출(request) : http://127.0.0.1:5000/ 또는 http://localhost:5000 특정 URL 매핑 검색 : @app.route(‘&#x2F;‘) 특정 URL에 매칭된 함수(def 함수) 실행 : def hello_world() 비즈니스 로직 실행 : result 결과 응답으로 전송(response): return result HTML 로 화면에 출력 쿠키(Cookie), 세션(Session), 로깅(logging) 등 제공 Routing**code: https://github.com/jmj3047/mj_chatbot_prac&#x2F;blob&#x2F;c5bec233b833b24345deeffe7391621415dc1dcb&#x2F;flask_prac&#x2F;1.flask_login.py URL을 통해 처리할 핸들러를 찾는 것 플라스크는 복잡한 URI를 함수로 연결하는 방법을 제공 URI 를 연결하는 route() 데코레이터 함수 제공 &#x2F; 접속 시 root_world() 가 호출 됨 &#x2F;hello 접속 시 hello_world() 가 호출 됨 12345678910111213141516from flask import Flask, redirect, url_forapp = Flask(__name__)@app.route(&#x27;/&#x27;) #127.0.0.1:5000에 가면 함수 실행def root_world(): result = &#x27;root world&#x27; return result@app.route(&#x27;/hello&#x27;) #127.0.0.1:5000/hello 를 가면 실행def hello_world(): result = &#x27;hello world&#x27; return resultif __name__ == &#x27;__main__&#x27;: app.debug =True app.run() app.debug는 개발의 편의를 위해 존재 True값을 경우 코드를 변경하면 자동으로 서버가 재 실행 됨 또한, 웹상에서 파이썬 코드를 수행할 수 있게 되므로, 운영환경에서 사용을 유의해야 함. 현재 접근은 개발 소스가 존재하는 로컬에서만 접근 가능 외부에서도 접근을 가능하게 하려면 app.run(host&#x3D;’0.0.0.0’)로 서버 실행 부를 변경해야 함 1234567891011121314151617181920212223242526272829303132from flask import Flask, redirect, url_forapp = Flask(__name__)@app.route(&#x27;/users/&lt;user_id&gt;&#x27;) #동적 변수를 사용하여 URI 접속# &lt;동적변수&gt;를 뷰함수의 인자로 사용# &lt;동적 변수&gt; 다음에 /를 넣으면 안됨def user_id(userid): result = &#x27;user_id = &#x27; + userid return result@app.route(&#x27;/admin&#x27;)def hello_admin(): return &#x27;Hello Admin&#x27;@app.route(&#x27;/guest/&lt;guest&gt;&#x27;)def hello_guest(guest): return &#x27;Hello %s as Guest&#x27; % guest@app.route(&#x27;/user/&lt;name&gt;&#x27;)def hello_user(name): if name == &#x27;admin&#x27;: return redirect(url_for(&#x27;hello_admin&#x27;)) else: return redirect(url_for(&#x27;hello_guest&#x27;, guest=name))# url_for(): 함수를 호출하는 URI를 반환# redirect(): 다른 route 경로 이동(다른 페이지 이동)if __name__ == &#x27;__main__&#x27;: app.debug = True app.run() Flask GET 방식으로 값 전송 및 처리**code: https://github.com/jmj3047/mj_chatbot_prac&#x2F;blob&#x2F;c5bec233b833b24345deeffe7391621415dc1dcb&#x2F;flask_prac&#x2F;2.flask_app.py https://github.com/jmj3047/mj_chatbot_prac&#x2F;blob&#x2F;c5bec233b833b24345deeffe7391621415dc1dcb&#x2F;flask_prac&#x2F;templates&#x2F;login&#x2F;login_form_get.html mkdir templates 폴더 생성 login_form_get.html 파일 작성 get 방식 지정 : method&#x3D;”get” 1234567891011121314151617181920from flask import Flask, request, session, render_templateapp = Flask(__name__)@app.route(&#x27;/login_form_get&#x27;) def login_form_get(): return render_template(&#x27;login/login_form_get.html&#x27;)@app.route(&#x27;/login_get_proc&#x27;, methods=[&#x27;GET&#x27;]) def login_get_proc(): user_id = request.args.get(&#x27;user_id&#x27;) user_pwd = request.args.get(&#x27;user_pwd&#x27;) if len(user_id) == 0 or len(user_pwd) == 0: return &#x27;no &#123;&#125; or &#123;&#125;&#x27;.format(user_id, user_pwd) return &#x27;welcome &#123;&#125;&#x27;.format(user_id)if __name__ == &#x27;__main__&#x27;: app.debug = True app.run() 12345678910111213141516&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;meta charset = &quot;UTF-8&quot;&gt; &lt;title&gt;login_form_get.html&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt; &lt;form action=&quot;/login_get_proc&quot; method=&quot;get&quot;&gt; ID: &lt;input type=&quot;text&quot;, name=&quot;user_id&quot;&gt;&lt;br&gt; PW: &lt;input type=&quot;password&quot;, name=&quot;user_pwd&quot;&gt;&lt;br&gt; &lt;input type=&quot;submit&quot;, value=&quot;Click&quot;&gt; &lt;/form&gt; &lt;/h1&gt; &lt;/body&gt;&lt;/html&gt; Brython**code:https://github.com/jmj3047/mj_chatbot_prac&#x2F;blob&#x2F;c5bec233b833b24345deeffe7391621415dc1dcb&#x2F;flask_prac&#x2F;templates&#x2F;brython_test.html python을 HTML 코드에 삽입해서 사용 12345678910111213141516171819202122&lt;html&gt; &lt;head&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;/path/to/brython.js&quot;&gt;&lt;/script&gt; &lt;/head&gt; &lt;body onload=&quot;brython()&quot;&gt; &lt;script type=&quot;text/python&quot;&gt; from browser import document, alert def echo(event): alert(document[&quot;zone&quot;].value) document[&quot;mybutton&quot;].bind(&quot;click&quot;, echo) &lt;/script&gt; &lt;input id=&quot;zone&quot;&gt;&lt;button id=&quot;mybutton&quot;&gt;click !&lt;/button&gt; &lt;/body&gt;&lt;/html&gt; Reference: https://essim92.tistory.com/8 https://code-examples.net/ko/q/dc0356 https://github.com/brython-dev/brython #Test Brython online(DEMO)","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"HTML","slug":"Python/HTML","permalink":"https://jmj3047.github.io/categories/Python/HTML/"}],"tags":[{"name":"Web Server","slug":"Web-Server","permalink":"https://jmj3047.github.io/tags/Web-Server/"},{"name":"Flask","slug":"Flask","permalink":"https://jmj3047.github.io/tags/Flask/"},{"name":"Brython","slug":"Brython","permalink":"https://jmj3047.github.io/tags/Brython/"}],"keywords":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"HTML","slug":"Python/HTML","permalink":"https://jmj3047.github.io/categories/Python/HTML/"}]},{"title":"Making Office Chatbot with Transformer","slug":"Making_Office_Chatbot_with_Transformer","date":"2022-05-03T15:00:00.000Z","updated":"2022-10-15T08:08:18.515Z","comments":true,"path":"2022/05/04/Making_Office_Chatbot_with_Transformer/","link":"","permalink":"https://jmj3047.github.io/2022/05/04/Making_Office_Chatbot_with_Transformer/","excerpt":"","text":"일상 대화와 오피스 대화 데이터를 Transformer 모델로 학습시켜, 질문에 대한 적절한 답변을 하는 챗봇 Data: 한국어대화데이터셋(오피스데이터) 사용 (AIHUB에 ‘개방데이터-인식기술 언어지능-한국어대화데이터셋’에서 로그인 후 다운로드) GPU 사용 그 외 환경 123batch size = 64buffer size = 20000epochs = 50 1. Environments1!pip install tensorflow_datasets 12345678import pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport reimport urllib.requestimport timeimport tensorflow_datasets as tfdsimport tensorflow as tf 12import osos.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;3&quot; 1234567with tf.device(&#x27;/device:GPU:3&#x27;): # 텐서 생성 a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]) b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]) c = tf.matmul(a, b) print(c) tf.Tensor( [[22. 28.] [49. 64.]], shape=(2, 2), dtype=float32) 2. 데이터 전처리12data = pd.read_csv(&#x27;./ChatbotData.csv&#x27;)data = data[0:5290] 1data[:10] 123456789f = open(r&#x27;./conversation_office.txt&#x27;,&quot;r&quot;)lines = f.readlines()Q = []A = []for i in range(len(lines)) : if i%2 == 0 : Q.append(lines[i][2:-1]) A.append(lines[i+1][2:-1]) 123456import pandas as pddf = pd.DataFrame()df[&#x27;Q&#x27;] = Qdf[&#x27;A&#x27;] = Adf[&#x27;label&#x27;] = 1 1df[:10] 1234#두 데이터를 concat() 함수를 이용하여 합쳐 하나의 데이터프레임으로 나타내주도록 함train_data = pd.concat([data, df],ignore_index=True)train_data = train_data.sample(frac=1).reset_index(drop=True) #데이터를 랜덤으로 섞어주는 코드 3. 단어 집합 생성123456789101112131415# 문장 그대로 학습 모델에 넣으면 모델이 인식을 할 수 없기 때문에 단어 집합을 만들어 줘야 함.# 정수 인코딩과 패딩을 해주는 작업을 해주어야 함#특수기호 띄어쓰기questions = []for sentence in train_data[&#x27;Q&#x27;]: sentence = re.sub(r&quot;([?.!,])&quot;, r&quot; \\1 &quot;, sentence) sentence = sentence.strip() questions.append(sentence) answers = []for sentence in train_data[&#x27;A&#x27;]: sentence = re.sub(r&quot;([?.!,])&quot;, r&quot; \\1 &quot;, sentence) sentence = sentence.strip() answers.append(sentence) 123456789# 서브워드텍스트인코더를 사용하여 질문, 답변 데이터로부터 단어 집합(Vocabulary) 생성tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus( questions + answers, target_vocab_size=2**13) # 시작 토큰과 종료 토큰에 대한 정수 부여START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]# 시작 토큰과 종료 토큰을 고려하여 단어 집합의 크기를 + 2VOCAB_SIZE = tokenizer.vocab_size + 2 123456789101112131415161718192021222324252627#정수 인코딩과 패딩# 서브워드텍스트인코더 토크나이저의 .encode()를 사용하여 텍스트 시퀀스를 정수 시퀀스로 변환.print(&#x27;임의의 질문 샘플을 정수 인코딩 : &#123;&#125;&#x27;.format(tokenizer.encode(questions[20])))#출력 : 임의의 질문 샘플을 정수 인코딩 : [8656, 331]# 최대 길이를 40으로 정의MAX_LENGTH = 40# 토큰화 / 정수 인코딩 / 시작 토큰과 종료 토큰 추가 / 패딩def tokenize_and_filter(inputs, outputs): tokenized_inputs, tokenized_outputs = [], [] for (sentence1, sentence2) in zip(inputs, outputs): # encode(토큰화 + 정수 인코딩), 시작 토큰과 종료 토큰 추가 sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN tokenized_inputs.append(sentence1) tokenized_outputs.append(sentence2) # 패딩 tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences( tokenized_inputs, maxlen=MAX_LENGTH, padding=&#x27;post&#x27;) tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences( tokenized_outputs, maxlen=MAX_LENGTH, padding=&#x27;post&#x27;) return tokenized_inputs, tokenized_outputs 임의의 질문 샘플을 정수 인코딩 : [2704, 1081, 13, 542] 1questions, answers = tokenize_and_filter(questions, answers) 1234#sample# 정수 인코딩과 패딩이 된 결과가 출력print(questions[0])print(answers[0]) [10023 31 121 4282 10024 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [10023 3607 213 13 21 1 10024 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] 12from tensorflow.python.client import device_libdevice_lib.list_local_devices() 1234567891011121314151617181920212223#인코더와 디코더의 입력 데이터가 되도록 배치 크기로 데이터를 묶어줌 with tf.device(&#x27;/device:GPU:3&#x27;): BATCH_SIZE = 64 BUFFER_SIZE = 20000 dataset = tf.data.Dataset.from_tensor_slices(( &#123; &#x27;inputs&#x27;: questions, &#x27;dec_inputs&#x27;: answers[:, :-1] # 디코더의 입력 / 마지막 패딩 토큰 제거 &#125;, &#123; &#x27;outputs&#x27;: answers[:, 1:] # 맨 처음 토큰이 제거 = 시작 토큰 제거 &#125;, )) dataset = dataset.cache() dataset = dataset.shuffle(BUFFER_SIZE) dataset = dataset.batch(BATCH_SIZE) dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE) 4. 트랜스포머 모델 만들기123456789101112131415161718192021222324252627282930313233343536373839def transformer(vocab_size, num_layers, dff, d_model, num_heads, dropout, name=&quot;transformer&quot;): # 인코더의 입력 inputs = tf.keras.Input(shape=(None,), name=&quot;inputs&quot;) # 디코더의 입력 dec_inputs = tf.keras.Input(shape=(None,), name=&quot;dec_inputs&quot;) # 인코더의 패딩 마스크 enc_padding_mask = tf.keras.layers.Lambda( create_padding_mask, output_shape=(1, 1, None), name=&#x27;enc_padding_mask&#x27;)(inputs) # 디코더의 룩어헤드 마스크(첫번째 서브층) look_ahead_mask = tf.keras.layers.Lambda( create_look_ahead_mask, output_shape=(1, None, None), name=&#x27;look_ahead_mask&#x27;)(dec_inputs) # 디코더의 패딩 마스크(두번째 서브층) dec_padding_mask = tf.keras.layers.Lambda( create_padding_mask, output_shape=(1, 1, None), name=&#x27;dec_padding_mask&#x27;)(inputs) # 인코더의 출력은 enc_outputs. 디코더로 전달된다. enc_outputs = encoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff, d_model=d_model, num_heads=num_heads, dropout=dropout, )(inputs=[inputs, enc_padding_mask]) # 인코더의 입력은 입력 문장과 패딩 마스크 # 디코더의 출력은 dec_outputs. 출력층으로 전달된다. dec_outputs = decoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff, d_model=d_model, num_heads=num_heads, dropout=dropout, )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask]) # 다음 단어 예측을 위한 출력층 outputs = tf.keras.layers.Dense(units=vocab_size, name=&quot;outputs&quot;)(dec_outputs) return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name) 1234567891011121314151617181920212223242526272829303132class PositionalEncoding(tf.keras.layers.Layer): def __init__(self, position, d_model): super(PositionalEncoding, self).__init__() self.pos_encoding = self.positional_encoding(position, d_model) def get_angles(self, position, i, d_model): angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32)) return position * angles def positional_encoding(self, position, d_model): angle_rads = self.get_angles( position=tf.range(position, dtype=tf.float32)[:, tf.newaxis], i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :], d_model=d_model) # 배열의 짝수 인덱스(2i)에는 사인 함수 적용 sines = tf.math.sin(angle_rads[:, 0::2]) # 배열의 홀수 인덱스(2i+1)에는 코사인 함수 적용 cosines = tf.math.cos(angle_rads[:, 1::2]) angle_rads = np.zeros(angle_rads.shape) angle_rads[:, 0::2] = sines angle_rads[:, 1::2] = cosines pos_encoding = tf.constant(angle_rads) pos_encoding = pos_encoding[tf.newaxis, ...] print(pos_encoding.shape) return tf.cast(pos_encoding, tf.float32) def call(self, inputs): return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :] 1234567891011121314151617181920212223242526272829303132333435def create_padding_mask(x): mask = tf.cast(tf.math.equal(x, 0), tf.float32) # (batch_size, 1, 1, key의 문장 길이) return mask[:, tf.newaxis, tf.newaxis, :]# 디코더의 첫번째 서브층(sublayer)에서 미래 토큰을 Mask하는 함수def create_look_ahead_mask(x): seq_len = tf.shape(x)[1] look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0) padding_mask = create_padding_mask(x) # 패딩 마스크도 포함 return tf.maximum(look_ahead_mask, padding_mask)#encoderdef encoder(vocab_size, num_layers, dff, d_model, num_heads, dropout, name=&quot;encoder&quot;): inputs = tf.keras.Input(shape=(None,), name=&quot;inputs&quot;) # 인코더는 패딩 마스크 사용 padding_mask = tf.keras.Input(shape=(1, 1, None), name=&quot;padding_mask&quot;) # 포지셔널 인코딩 + 드롭아웃 embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs) embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32)) embeddings = PositionalEncoding(vocab_size, d_model)(embeddings) outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings) # 인코더를 num_layers개 쌓기 for i in range(num_layers): outputs = encoder_layer(dff=dff, d_model=d_model, num_heads=num_heads, dropout=dropout, name=&quot;encoder_layer_&#123;&#125;&quot;.format(i), )([outputs, padding_mask]) return tf.keras.Model( inputs=[inputs, padding_mask], outputs=outputs, name=name) 1234567891011121314151617181920212223242526272829def encoder_layer(dff, d_model, num_heads, dropout, name=&quot;encoder_layer&quot;): inputs = tf.keras.Input(shape=(None, d_model), name=&quot;inputs&quot;) # 인코더는 패딩 마스크 사용 padding_mask = tf.keras.Input(shape=(1, 1, None), name=&quot;padding_mask&quot;) # 멀티-헤드 어텐션 (첫번째 서브층 / 셀프 어텐션) attention = MultiHeadAttention( d_model, num_heads, name=&quot;attention&quot;)(&#123; &#x27;query&#x27;: inputs, &#x27;key&#x27;: inputs, &#x27;value&#x27;: inputs, # Q = K = V &#x27;mask&#x27;: padding_mask # 패딩 마스크 사용 &#125;) # 드롭아웃 + 잔차 연결과 층 정규화 attention = tf.keras.layers.Dropout(rate=dropout)(attention) attention = tf.keras.layers.LayerNormalization( epsilon=1e-6)(inputs + attention) # 포지션 와이즈 피드 포워드 신경망 (두번째 서브층) outputs = tf.keras.layers.Dense(units=dff, activation=&#x27;relu&#x27;)(attention) outputs = tf.keras.layers.Dense(units=d_model)(outputs) # 드롭아웃 + 잔차 연결과 층 정규화 outputs = tf.keras.layers.Dropout(rate=dropout)(outputs) outputs = tf.keras.layers.LayerNormalization( epsilon=1e-6)(attention + outputs) return tf.keras.Model( inputs=[inputs, padding_mask], outputs=outputs, name=name) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960class MultiHeadAttention(tf.keras.layers.Layer): def __init__(self, d_model, num_heads, name=&quot;multi_head_attention&quot;): super(MultiHeadAttention, self).__init__(name=name) self.num_heads = num_heads self.d_model = d_model assert d_model % self.num_heads == 0 # d_model을 num_heads로 나눈 값. # 논문 기준 : 64 self.depth = d_model // self.num_heads # WQ, WK, WV에 해당하는 밀집층 정의 self.query_dense = tf.keras.layers.Dense(units=d_model) self.key_dense = tf.keras.layers.Dense(units=d_model) self.value_dense = tf.keras.layers.Dense(units=d_model) # WO에 해당하는 밀집층 정의 self.dense = tf.keras.layers.Dense(units=d_model) # num_heads 개수만큼 q, k, v를 split하는 함수 def split_heads(self, inputs, batch_size): inputs = tf.reshape( inputs, shape=(batch_size, -1, self.num_heads, self.depth)) return tf.transpose(inputs, perm=[0, 2, 1, 3]) def call(self, inputs): query, key, value, mask = inputs[&#x27;query&#x27;], inputs[&#x27;key&#x27;], inputs[ &#x27;value&#x27;], inputs[&#x27;mask&#x27;] batch_size = tf.shape(query)[0] query = self.query_dense(query) key = self.key_dense(key) value = self.value_dense(value) # 2. 헤드 나누기 # q : (batch_size, num_heads, query의 문장 길이, d_model/num_heads) # k : (batch_size, num_heads, key의 문장 길이, d_model/num_heads) # v : (batch_size, num_heads, value의 문장 길이, d_model/num_heads) query = self.split_heads(query, batch_size) key = self.split_heads(key, batch_size) value = self.split_heads(value, batch_size) # 3. 스케일드 닷 프로덕트 어텐션. 앞서 구현한 함수 사용. # (batch_size, num_heads, query의 문장 길이, d_model/num_heads) scaled_attention, _ = scaled_dot_product_attention(query, key, value, mask) # (batch_size, query의 문장 길이, num_heads, d_model/num_heads) scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3]) # 4. 헤드 연결(concatenate)하기 # (batch_size, query의 문장 길이, d_model) concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model)) # 5. WO에 해당하는 밀집층 지나기 # (batch_size, query의 문장 길이, d_model) outputs = self.dense(concat_attention) return outputs 123456789101112131415161718192021222324252627def scaled_dot_product_attention(query, key, value, mask): # query 크기 : (batch_size, num_heads, query의 문장 길이, d_model/num_heads) # key 크기 : (batch_size, num_heads, key의 문장 길이, d_model/num_heads) # value 크기 : (batch_size, num_heads, value의 문장 길이, d_model/num_heads) # padding_mask : (batch_size, 1, 1, key의 문장 길이) # Q와 K의 곱. 어텐션 스코어 행렬. matmul_qk = tf.matmul(query, key, transpose_b=True) # 스케일링 # dk의 루트값으로 나눠준다. depth = tf.cast(tf.shape(key)[-1], tf.float32) logits = matmul_qk / tf.math.sqrt(depth) # 마스킹. 어텐션 스코어 행렬의 마스킹 할 위치에 매우 작은 음수값을 넣는다. # 매우 작은 값이므로 소프트맥스 함수를 지나면 행렬의 해당 위치의 값은 0이 된다. if mask is not None: logits += (mask * -1e9) # 소프트맥스 함수는 마지막 차원인 key의 문장 길이 방향으로 수행된다. # attention weight : (batch_size, num_heads, query의 문장 길이, key의 문장 길이) attention_weights = tf.nn.softmax(logits, axis=-1) # output : (batch_size, num_heads, query의 문장 길이, d_model/num_heads) output = tf.matmul(attention_weights, value) return output, attention_weights 123456789101112131415161718192021222324252627def decoder(vocab_size, num_layers, dff, d_model, num_heads, dropout, name=&#x27;decoder&#x27;): inputs = tf.keras.Input(shape=(None,), name=&#x27;inputs&#x27;) enc_outputs = tf.keras.Input(shape=(None, d_model), name=&#x27;encoder_outputs&#x27;) # 디코더는 룩어헤드 마스크(첫번째 서브층)와 패딩 마스크(두번째 서브층) 둘 다 사용. look_ahead_mask = tf.keras.Input( shape=(1, None, None), name=&#x27;look_ahead_mask&#x27;) padding_mask = tf.keras.Input(shape=(1, 1, None), name=&#x27;padding_mask&#x27;) # 포지셔널 인코딩 + 드롭아웃 embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs) embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32)) embeddings = PositionalEncoding(vocab_size, d_model)(embeddings) outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings) # 디코더를 num_layers개 쌓기 for i in range(num_layers): outputs = decoder_layer(dff=dff, d_model=d_model, num_heads=num_heads, dropout=dropout, name=&#x27;decoder_layer_&#123;&#125;&#x27;.format(i), )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask]) return tf.keras.Model( inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask], outputs=outputs, name=name) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def decoder_layer(dff, d_model, num_heads, dropout, name=&quot;decoder_layer&quot;): inputs = tf.keras.Input(shape=(None, d_model), name=&quot;inputs&quot;) enc_outputs = tf.keras.Input(shape=(None, d_model), name=&quot;encoder_outputs&quot;) # 룩어헤드 마스크(첫번째 서브층) look_ahead_mask = tf.keras.Input( shape=(1, None, None), name=&quot;look_ahead_mask&quot;) # 패딩 마스크(두번째 서브층) padding_mask = tf.keras.Input(shape=(1, 1, None), name=&#x27;padding_mask&#x27;) # 멀티-헤드 어텐션 (첫번째 서브층 / 마스크드 셀프 어텐션) attention1 = MultiHeadAttention( d_model, num_heads, name=&quot;attention_1&quot;)(inputs=&#123; &#x27;query&#x27;: inputs, &#x27;key&#x27;: inputs, &#x27;value&#x27;: inputs, # Q = K = V &#x27;mask&#x27;: look_ahead_mask # 룩어헤드 마스크 &#125;) # 잔차 연결과 층 정규화 attention1 = tf.keras.layers.LayerNormalization( epsilon=1e-6)(attention1 + inputs) # 멀티-헤드 어텐션 (두번째 서브층 / 디코더-인코더 어텐션) attention2 = MultiHeadAttention( d_model, num_heads, name=&quot;attention_2&quot;)(inputs=&#123; &#x27;query&#x27;: attention1, &#x27;key&#x27;: enc_outputs, &#x27;value&#x27;: enc_outputs, # Q != K = V &#x27;mask&#x27;: padding_mask # 패딩 마스크 &#125;) # 드롭아웃 + 잔차 연결과 층 정규화 attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2) attention2 = tf.keras.layers.LayerNormalization( epsilon=1e-6)(attention2 + attention1) # 포지션 와이즈 피드 포워드 신경망 (세번째 서브층) outputs = tf.keras.layers.Dense(units=dff, activation=&#x27;relu&#x27;)(attention2) outputs = tf.keras.layers.Dense(units=d_model)(outputs) # 드롭아웃 + 잔차 연결과 층 정규화 outputs = tf.keras.layers.Dropout(rate=dropout)(outputs) outputs = tf.keras.layers.LayerNormalization( epsilon=1e-6)(outputs + attention2) return tf.keras.Model( inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask], outputs=outputs, name=name) 1234567891011121314151617tf.keras.backend.clear_session()# Hyper-parametersD_MODEL = 256NUM_LAYERS = 2NUM_HEADS = 8DFF = 512DROPOUT = 0.1model = transformer( vocab_size=VOCAB_SIZE, num_layers=NUM_LAYERS, dff=DFF, d_model=D_MODEL, num_heads=NUM_HEADS, dropout=DROPOUT) (1, 10025, 256) (1, 10025, 256) 123456789101112131415161718192021222324class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule): def __init__(self, d_model, warmup_steps=4000): super(CustomSchedule, self).__init__() self.d_model = d_model self.d_model = tf.cast(self.d_model, tf.float32) self.warmup_steps = warmup_steps def __call__(self, step): arg1 = tf.math.rsqrt(step) arg2 = step * (self.warmup_steps**-1.5) return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)def loss_function(y_true, y_pred): y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1)) loss = tf.keras.losses.SparseCategoricalCrossentropy( from_logits=True, reduction=&#x27;none&#x27;)(y_true, y_pred) mask = tf.cast(tf.not_equal(y_true, 0), tf.float32) loss = tf.multiply(loss, mask) return tf.reduce_mean(loss) 1234567891011learning_rate = CustomSchedule(D_MODEL)optimizer = tf.keras.optimizers.Adam( learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)def accuracy(y_true, y_pred): # 레이블의 크기는 (batch_size, MAX_LENGTH - 1) y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1)) return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy]) 123## 모델 학습EPOCHS = 50model.fit(dataset, epochs=EPOCHS) Epoch 1/50 104/104 [==============================] - 12s 54ms/step - loss: 1.2164 - accuracy: 0.0149 Epoch 2/50 104/104 [==============================] - 6s 54ms/step - loss: 1.0725 - accuracy: 0.0285 Epoch 3/50 104/104 [==============================] - 6s 54ms/step - loss: 0.9082 - accuracy: 0.0472 Epoch 4/50 104/104 [==============================] - 6s 54ms/step - loss: 0.7714 - accuracy: 0.0482 ... 104/104 [==============================] - 6s 54ms/step - loss: 0.0160 - accuracy: 0.1321 Epoch 46/50 104/104 [==============================] - 6s 56ms/step - loss: 0.0158 - accuracy: 0.1320 Epoch 47/50 104/104 [==============================] - 6s 55ms/step - loss: 0.0152 - accuracy: 0.1320 Epoch 48/50 104/104 [==============================] - 6s 54ms/step - loss: 0.0151 - accuracy: 0.1322 Epoch 49/50 104/104 [==============================] - 6s 55ms/step - loss: 0.0149 - accuracy: 0.1321 Epoch 50/50 104/104 [==============================] - 6s 53ms/step - loss: 0.0148 - accuracy: 0.1321 &lt;keras.callbacks.History at 0x7f794c0f0880&gt; 5. 챗봇 평가하기 학습시킨 챗봇에 새로운 문장을 넣어서 평가 12345678910111213141516171819202122232425262728293031323334# 새로운 문장도 인코더 입력 형식으로 변형하는 코드def preprocess_sentence(sentence): # 단어와 구두점 사이에 공백 추가. # ex) 12시 땡! -&gt; 12시 땡 ! sentence = re.sub(r&quot;([?.!,])&quot;, r&quot; \\1 &quot;, sentence) sentence = sentence.strip() return sentencedef evaluate(sentence): sentence = preprocess_sentence(sentence) sentence = tf.expand_dims( START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0) output = tf.expand_dims(START_TOKEN, 0) # 디코더의 예측 시작 for i in range(MAX_LENGTH): predictions = model(inputs=[sentence, output], training=False) # 현재(마지막) 시점의 예측 단어를 받아온다. predictions = predictions[:, -1:, :] predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32) # 만약 마지막 시점의 예측 단어가 종료 토큰이라면 예측을 중단 if tf.equal(predicted_id, END_TOKEN[0]): break # 마지막 시점의 예측 단어를 출력에 연결한다. # 이는 for문을 통해서 디코더의 입력으로 사용될 예정이다. output = tf.concat([output, predicted_id], axis=-1) return tf.squeeze(output, axis=0) 123456789101112def predict(sentence): prediction = evaluate(sentence) predicted_sentence = tokenizer.decode( [i for i in prediction if i &lt; tokenizer.vocab_size]) print(&#x27;Master: &#123;&#125;&#x27;.format(sentence)) # print(&#x27;Output: &#123;&#125;&#x27;.format(predicted_sentence)) print(&#x27;Chatbot: &#123;&#125;&#x27;.format(predicted_sentence)) return predicted_sentence 12#predict() 함수에 문장을 입력하면 해당 문장에 대한 결과가 출력됨output = predict(&quot;굿모닝&quot;) Input: 굿모닝 Output: 좋은 아침이에요 . 1output = predict(&quot;오늘 날씨&quot;) Input: 오늘 날씨 Output: 충분히 아름다워요 . 1output = predict(&quot;오늘 날씨 어때?&quot;) Input: 오늘 날씨 어때? Output: 오전엔 화창하지만 오후에는 비가 올 것입니다 . 1output = predict(&quot;집중력&quot;) Input: 집중력 Output: 병원 가보세요 . 1output = predict(&quot;퇴근&quot;) Input: 퇴근 Output: 인생은 채워나가는거죠 . 1output = predict(&quot;야근 싫어&quot;) Input: 야근 싫어 Output: 얼른 집에 가서 쉬시길 바랄게요 . 123output = str(input(&quot;오피스 챗봇입니다. 무엇을 도와드릴까요?:&quot;))output = predict(output) Master: 일하기 싫어 Chatbot: 저도요 ! ! Reference https:&#x2F;&#x2F;velog.io&#x2F;@seolini43&#x2F;파이썬Transformer로-오피스-챗봇-만들기-코드 **code: https://github.com/jmj3047/mj_chatbot_prac&#x2F;blob&#x2F;c5bec233b833b24345deeffe7391621415dc1dcb&#x2F;chatbot_backend.py","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Python","slug":"Python/Python","permalink":"https://jmj3047.github.io/categories/Python/Python/"}],"tags":[{"name":"Transformer","slug":"Transformer","permalink":"https://jmj3047.github.io/tags/Transformer/"},{"name":"Chatbot","slug":"Chatbot","permalink":"https://jmj3047.github.io/tags/Chatbot/"},{"name":"NLP","slug":"NLP","permalink":"https://jmj3047.github.io/tags/NLP/"}],"keywords":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Python","slug":"Python/Python","permalink":"https://jmj3047.github.io/categories/Python/Python/"}]},{"title":"MongoDB Update Operator","slug":"MongoDB_update","date":"2022-04-27T15:00:00.000Z","updated":"2023-03-19T07:04:36.264Z","comments":true,"path":"2022/04/28/MongoDB_update/","link":"","permalink":"https://jmj3047.github.io/2022/04/28/MongoDB_update/","excerpt":"","text":"$set: 필드값을 설정하고 필드가 존재하지 않으면 새 필드가 생성됨. 스키마를 갱신하거나 사용자 정의 키를 추가 할때 편리함. $unset: 키와 값을 모두 제거함 1234567891011121314&gt; db.users.insertOne(&#123;&quot;name&quot;:&quot;joe&quot;&#125;)&gt; db.users.updateOne(&#123;&quot;_id&quot; : ObjectId(&quot;6269e32b7b15b7097fbad433&quot;)&#125;,&#123;&quot;$set&quot;:&#123;&quot;favorite book&quot; : &quot;War and Peace&quot;&#125;&#125;)&#123; &quot;acknowledged&quot; : true, &quot;matchedCount&quot; : 1, &quot;modifiedCount&quot; : 1 &#125;&gt; db.users.findOne()&#123; &quot;_id&quot; : ObjectId(&quot;6269e32b7b15b7097fbad433&quot;), &quot;name&quot; : &quot;joe&quot;, &quot;favorite book&quot; : &quot;War and Peace&quot;&#125;&gt; db.users.updateOne(&#123;&quot;name&quot; : &quot;joe&quot;&#125;,&#123;&quot;$unset&quot;:&#123;&quot;favorite book&quot;:1&#125;&#125;)&#123; &quot;acknowledged&quot; : true, &quot;matchedCount&quot; : 1, &quot;modifiedCount&quot; : 1 &#125;&gt; db.users.find()&#123; &quot;_id&quot; : ObjectId(&quot;6269e32b7b15b7097fbad433&quot;), &quot;name&quot; : &quot;joe&quot; &#125; $inc: $set과 비슷하지만, 숫자를 증감하기 위해 사용. int, long, double, decimal 타입 값에만 사용 가능 12345678910111213141516171819202122&gt;db.games.insertOne(&#123;&quot;game&quot; :&quot;pinball&quot;,&quot;user&quot; :&quot;joe&quot;&#125;)&#123; &quot;acknowledged&quot; : true, &quot;insertedId&quot; : ObjectId(&quot;6269e4be7b15b7097fbad436&quot;)&#125;&gt; db.games.find()&#123; &quot;_id&quot; : ObjectId(&quot;6269e4be7b15b7097fbad436&quot;), &quot;game&quot; : &quot;pinball&quot;, &quot;user&quot; : &quot;joe&quot; &#125;&gt; db.games.updateOne(&#123;&quot;game&quot; :&quot;pinball&quot;,&quot;user&quot; :&quot;joe&quot;&#125;,&#123;&quot;$inc&quot;:&#123;&quot;score&quot;:50&#125;&#125;)&#123; &quot;acknowledged&quot; : true, &quot;matchedCount&quot; : 1, &quot;modifiedCount&quot; : 1 &#125;&gt; db.games.find()&#123; &quot;_id&quot; : ObjectId(&quot;6269e4be7b15b7097fbad436&quot;), &quot;game&quot; : &quot;pinball&quot;, &quot;user&quot; : &quot;joe&quot;, &quot;score&quot; : 50 &#125;&gt; db.games.updateOne(&#123;&quot;game&quot; :&quot;pinball&quot;,&quot;user&quot; :&quot;joe&quot;&#125;,&#123;&quot;$inc&quot;:&#123;&quot;score&quot;:10000&#125;&#125;)&#123; &quot;acknowledged&quot; : true, &quot;matchedCount&quot; : 1, &quot;modifiedCount&quot; : 1 &#125;&gt; db.games.find()&#123; &quot;_id&quot; : ObjectId(&quot;6269e4be7b15b7097fbad436&quot;), &quot;game&quot; : &quot;pinball&quot;, &quot;user&quot; : &quot;joe&quot;, &quot;score&quot; : 10050 &#125;&gt; $push: 배열이 이미 존재하지만 배열 끝에 요소를 추가하고, 존재하지 않으면 새로운 배열을 생성함. $each: $push에 $each제한자를 사용하면 작업 한 번으로 값을 여러개 추가할 수 있음. 12345678910111213141516171819202122&gt; db.blog.posts.findOne() &#123; &quot;_id&quot; : ObjectId(&quot;4b2d75476cc613d5ee930164&quot;), &quot;title&quot; : &quot;A blog post&quot;, &quot;content&quot; : &quot;...&quot;&#125; &gt; db.blog.posts.updateOne(&#123;&quot;title&quot; : &quot;A blog post&quot;&#125;, &#123;&quot;$push&quot; : &#123;&quot;comments&quot; : &#123;&quot;name&quot; : &quot;joe&quot;, &quot;email&quot; : &quot;joe@example.com&quot;, &quot;content&quot; : &quot;nice post.&quot;&#125;&#125;&#125;) &#123; &quot;acknowledged&quot; : true, &quot;matchedCount&quot; : 1, &quot;modifiedCount&quot; : 1 &#125; &gt; db.blog.posts.findOne() &#123; &quot;_id&quot; : ObjectId(&quot;4b2d75476cc613d5ee930164&quot;), &quot;title&quot; : &quot;A blog post&quot;, &quot;content&quot; : &quot;...&quot;, &quot;comments&quot; : [ &#123;&quot;name&quot; : &quot;joe&quot;, &quot;email&quot; : &quot;joe@example.com&quot;, &quot;content&quot; : &quot;nice post.&quot; &#125; ] &#125; $ne: 배열이 존재하지 않을 때 해당 값을 추가하면서 배열을 집합처럼 처리할 때 사용. $addToSet: 다른주소를 추가할 때 중복을 피할 수 있음 고유한 값을 여러개 추가하려면 $addToSet&#x2F;$each조합을 활용해야 함. $ne&#x2F;$push조합으로는 할 수 없음. 123456789101112&gt; db.users.updateOne(&#123;&quot;_id&quot; : ObjectId(&quot;4b2d75476cc613d5ee930164&quot;)&#125;, &#123;&quot;$addToSet&quot; : &#123;&quot;emails&quot; : &#123;&quot;$each&quot; : [&quot;joe@php.net&quot;, &quot;joe@example.com&quot;, &quot;joe@python.org&quot;]&#125;&#125;&#125;) &#123; &quot;acknowledged&quot; : true, &quot;matchedCount&quot; : 1, &quot;modifiedCount&quot; : 1 &#125; &gt; db.users.findOne(&#123;&quot;_id&quot; : ObjectId(&quot;4b2d75476cc613d5ee930164&quot;)&#125;) &#123; &quot;_id&quot; : ObjectId(&quot;4b2d75476cc613d5ee930164&quot;), &quot;username&quot; : &quot;joe&quot;, &quot;emails&quot; : [ &quot;joe@example.com&quot;, &quot;joe@gmail.com&quot;, &quot;joe@yahoo.com&quot;, &quot;joe@hotmail.com&quot; &quot;joe@php.net&quot; &quot;joe@python.org&quot; ] &#125; Reference 몽고DB 완벽 가이드: 실전 예제로 배우는 NoSQL 데이터베이스 기초부터 활용까지","categories":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"MongoDB","slug":"Data-Platform-Base/MongoDB","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/MongoDB/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://jmj3047.github.io/tags/MongoDB/"}],"keywords":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"MongoDB","slug":"Data-Platform-Base/MongoDB","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/MongoDB/"}]},{"title":"MongoDB Install & Basic Command","slug":"MongoDB_Install","date":"2022-04-24T15:00:00.000Z","updated":"2023-03-19T07:04:11.321Z","comments":true,"path":"2022/04/25/MongoDB_Install/","link":"","permalink":"https://jmj3047.github.io/2022/04/25/MongoDB_Install/","excerpt":"","text":"Link: www.mongodb.com/try/download/enterprise Download proper version of Mongodb Install이 완료된 후에는 MongoDB 환경변수 설정을 위해 시스템 환경 변수 편집을 진행하여 줍니다. 환경변수 편집을 위해 환경변수 &gt;시스템 변수 Path 설정을 선택하여 줍니다. 설치된 MongoDB의 bin폴더 경로를 입력하여 줍니다.(C:\\Program Files\\MongoDB\\Server\\5.0\\bin) 저장 후 cmd창에서 mongdb –version을 통해 정상 설치를 확인하여 줍니다. cmd 창에 mongodb 실행 1&gt;mongo 명령어 두줄로 잘 실행되는지 간단히 확인 12&gt; db.world.insert(&#123; &quot;speech&quot; : &quot;Hello World!&quot; &#125;);&gt; cur = db.world.find();x=cur.next();print(x[&quot;speech&quot;]); Basic Command사용 가능한 모든 데이터베이스 표시 : 1show dbs; 액세스 할 특정 데이터베이스를 선택 (Ex: mydb . 이미 존재하지 않으면 mydb 가 생성됩니다 : 1use mydb; 데이터베이스에 모든 콜렉션을 표시. 먼저 콜렉션을 선택하십시오 (위 참조). 1show collections; 데이터베이스와 함께 사용할 수있는 모든 기능 표시 : 1db.mydb.help(); 현재 선택한 데이터베이스를 확인 12&gt; dbmydb db.dropDatabase() 명령은 기존 데이터베이스를 삭제하는 데 사용됩니다. 1db.dropDatabase() Reference https://khj93.tistory.com/entry/MongoDB-Window에-MongoDB-설치하기 https://learntutorials.net/ko/mongodb/topic/691/mongodb-시작하기","categories":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"MongoDB","slug":"Data-Platform-Base/MongoDB","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/MongoDB/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://jmj3047.github.io/tags/MongoDB/"}],"keywords":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"MongoDB","slug":"Data-Platform-Base/MongoDB","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/MongoDB/"}]},{"title":"MongoDB CRUD","slug":"MongoDB_CRUD","date":"2022-04-24T15:00:00.000Z","updated":"2023-03-19T07:04:10.157Z","comments":true,"path":"2022/04/25/MongoDB_CRUD/","link":"","permalink":"https://jmj3047.github.io/2022/04/25/MongoDB_CRUD/","excerpt":"","text":"Initial setting 12345678&gt;dbtest&gt;use videoswitched to db video # if video doesnt exist, created&gt;dbvideo&gt;db.movies # created movies collectionvideo.movies Create: insertOne 함수 1234567891011121314151617181920&gt;movie = &#123;&quot;title&quot; : &quot;Star Wars: Episode IV - A New Hope&quot;, &quot;director&quot; : &quot;George Lucas&quot;, &quot;year&quot; : 1977&#125;&#123; &quot;title&quot; : &quot;Star Wars: Episode IV - A New Hope&quot;, &quot;director&quot; : &quot;George Lucas&quot;, &quot;year&quot; : 1977&#125;&gt;db.movies.insertOne(movie) #영화가 데이터 베이스에 저장됨&#123; &quot;acknowledged&quot; : true, &quot;insertedId&quot; : ObjectId(&quot;6266057e4619361339f0c881&quot;)&#125;#Find 함수로 호출&gt;db.movies.find().pretty()&#123; &quot;_id&quot; : ObjectId(&quot;6266057e4619361339f0c881&quot;), &quot;title&quot; : &quot;Star Wars: Episode IV - A New Hope&quot;, &quot;director&quot; : &quot;George Lucas&quot;, &quot;year&quot; : 1977&#125; Read: find, findOne 함수 1234567&gt; db.movies.findOne(movie) &#123; &quot;_id&quot; : ObjectId(&quot;6266057e4619361339f0c881&quot;), &quot;title&quot; : &quot;Star Wars: Episode IV - A New Hope&quot;, &quot;director&quot; : &quot;George Lucas&quot;, &quot;year&quot; : 1977&#125; Update : updateOne 함수 1234567891011&gt; db.movies.updateOne(&#123;title : &quot;Star Wars: Episode IV - A New Hope&quot;&#125;, &#123;$set : &#123;reviews: []&#125;&#125;) &#123; &quot;acknowledged&quot; : true, &quot;matchedCount&quot; : 1, &quot;modifiedCount&quot; : 1 &#125;&gt; db.movies.find().pretty()&#123; &quot;_id&quot; : ObjectId(&quot;6266057e4619361339f0c881&quot;), &quot;title&quot; : &quot;Star Wars: Episode IV - A New Hope&quot;, &quot;director&quot; : &quot;George Lucas&quot;, &quot;year&quot; : 1977, &quot;reviews&quot; : [ ]&#125; Delete: deleteOne, deleteMany 함수 12&gt;db.movies.deleteOne(&#123;title : &quot;Star Wars: Episode IV - A New Hope&quot;&#125;)&#123; &quot;acknowledged&quot; : true, &quot;deletedCount&quot; : 1 &#125; Reference 몽고DB 완벽 가이드: 실전 예제로 배우는 NoSQL 데이터베이스 기초부터 활용까지","categories":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"MongoDB","slug":"Data-Platform-Base/MongoDB","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/MongoDB/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://jmj3047.github.io/tags/MongoDB/"}],"keywords":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"MongoDB","slug":"Data-Platform-Base/MongoDB","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/MongoDB/"}]},{"title":"BeautifulSoup Quick Start","slug":"BeautifulSoup_QuickStart","date":"2022-04-21T15:00:00.000Z","updated":"2022-04-22T02:53:21.000Z","comments":true,"path":"2022/04/22/BeautifulSoup_QuickStart/","link":"","permalink":"https://jmj3047.github.io/2022/04/22/BeautifulSoup_QuickStart/","excerpt":"","text":"Index_prac.html123456789101112131415161718192021222324252627&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;title&gt; The Dormouse&#x27;s story &lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class=&quot;title&quot;&gt;&lt;b&gt;The Dormouse&#x27;s story&lt;/b&gt;&lt;/p&gt; &lt;p class=&quot;story&quot;&gt; Once upon a time there were three little sisters; and their names were &lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;, &lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and &lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class=&quot;story&quot;&gt;To Be Continued...&lt;/p&gt; &lt;/body&gt;&lt;/html&gt; Quick Start.py123456789101112131415161718192021222324252627282930313233343536373839404142434445from bs4 import BeautifulSoupsoup = BeautifulSoup(open(&quot;index_prac.html&quot;), &#x27;html.parser&#x27;)# print allprint(soup.prettify())# navigate that data structureprint(soup.title)# &lt;title&gt;The Dormouse&#x27;s story&lt;/title&gt;print(soup.title.name)# u&#x27;title&#x27;print(soup.title.string)# u&#x27;The Dormouse&#x27;s story&#x27;print(soup.title.parent.name)# u&#x27;head&#x27;print(soup.p)# &lt;p class=&quot;title&quot;&gt;&lt;b&gt;The Dormouse&#x27;s story&lt;/b&gt;&lt;/p&gt;print(soup.p[&#x27;class&#x27;])# u&#x27;title&#x27;print(soup.a)# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;print(soup.find_all(&#x27;a&#x27;))# [&lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;,# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;,# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;]print(soup.find(id=&quot;link3&quot;))# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;## extracting all the URLsfor link in soup.find_all(&#x27;a&#x27;): print(link.get(&#x27;href&#x27;))# http://example.com/elsie# http://example.com/lacie# http://example.com/tillie## extracting all the text from a pageprint(soup.get_text()) Reference: https://www.crummy.com/software/BeautifulSoup/bs4/doc/#quick-start","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"HTML","slug":"Python/HTML","permalink":"https://jmj3047.github.io/categories/Python/HTML/"}],"tags":[{"name":"HTML","slug":"HTML","permalink":"https://jmj3047.github.io/tags/HTML/"},{"name":"Web Crawling","slug":"Web-Crawling","permalink":"https://jmj3047.github.io/tags/Web-Crawling/"},{"name":"BeautifulSoup","slug":"BeautifulSoup","permalink":"https://jmj3047.github.io/tags/BeautifulSoup/"}],"keywords":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"HTML","slug":"Python/HTML","permalink":"https://jmj3047.github.io/categories/Python/HTML/"}]},{"title":"Improved Training of Wasserstein GANs","slug":"WGAN","date":"2022-04-21T15:00:00.000Z","updated":"2022-10-15T08:05:10.371Z","comments":true,"path":"2022/04/22/WGAN/","link":"","permalink":"https://jmj3047.github.io/2022/04/22/WGAN/","excerpt":"","text":"Journal&#x2F;Conference: NIPSYear(published year): 2017Author: Ishaan Gulrajani, Faruk Ahmed, Martín Arjovsky, Vincent Dumoulin, Aaron C. CourvilleSubject: DCGAN, Generative Model Improved Training of Wasserstein GANs Summary 기존의 Wasserstein-GAN 모델의 weight clipping 을 대체할 수 있는 gradient penalty 방법을 제시 hyperparameter tuning 없이도 안정적인 학습이 가능해졌음을 제시 IntroductionGAN 모델을 안정적으로 학습하기 위한 많은 방법들이 존재해왔습니다. 특히, 가치함수가 수렴하는 성질을 분석하여 Discriminator(이후 Critic)가 1-Lipschitz function 공간에 있도록 하는 Wasserstein GAN(WGAN) 이 제시된 바 있습니다. 논문은 WGAN 의 단점을 개선한 WGAN-GP 모델을 제시합니다. Toy datasets에 대해 critic의 weight clipping이 undesired behavior를 유발할 수 있음을 증명 “Gradient penalty”(WGAN-GP) 기법으로 제안 다양한 GAN 구조에대해 안정적인 학습을 수행할 수 있고, 고품질 이미지 생성을 수행하며, 개별 샘플링이 필필요하지 않는 문자수준 언어 모델을 제시 BackgroundGenerative adversarial networks일반적인 GAN 구조에 대해 다시 한번 개념을 되짚습니다. Wasserstein GANsWGAN 은 GAN 의 목적함수인 JSD 가 parameter 에 연속적이지 않에 학습에 문제가 발생함을 지적하였습니다. 이에, Earth-Mover distance 로 모든 구간에서 연속적이고 대부분의 구간에서 미분 가능하게 하여 문제를 해결하였습니다. 이외에도 WGAN 의 특징에 대해 서술하며, 가장 중요한 특징으로 Lipschitz 조건을 만족하기 위해 시행하는 weight clipping 을 언급합니다. Properties of the optimal WGAN critic최적의 WGAN critic 을 가정했을 때, ****weight clipping이 WGAN critic에서 문제를 발생시킴을 언급하고 증명한 결과를 제시합니다. Difficulties with weight constraintsWGAN의 weight clipping이 최적화에 문제를 발생시킬 수 있고, 최적화가 잘 되더라도 critic이 pathological value surface 을 가질 수 있음을 증명하였던 내용을 확인하기 위한 실험을 진행합니다. 논문은 기존 WGAN 이 제시하였던 hard clipping 방식 이외에도, L2 norm clipping&#x2F;weight normalization&#x2F;L1 and L2 weight decay 등의 weight constraint 를 가정하였을 때 모두 비슷한 문제가 발생하였음을 언급합니다. Capacity underuse &amp; Exploding and vanishing gradientsk-Lipshitz 조건을 달성하기 위해 weight clipping 을 수행하였을 때, critic은 더욱 단순한 형태의 함수를 취하게 됩니다. 논문은 이를 증명하기 위해 Generator 의 분포를 toy distribution + unit-variance 가우시안-노이즈에 고정한뒤, weight clipping 과 함께 WGAN critic 을 학습한 결과를 제시합니다. 왼쪽 그림에서 Weight clipping 수행한 경우의 value surface 모양이 단순해졌음을 확인할 수 있습니다. 또한, 우측 그림과 같이, Gradient penalty 를 수행한 경우에 gradient vanishing 이나 exploding 이 발생하지 않았음을 제시합니다. Gradient penaltyWeight Clipping 을 사용하지 않고 Lipschitz constraint 를 유지할 수 있는 방법을 설명합니다. 입력에 대한 Critic 출력 gradient 의 크기를 직접 제약합니다. 이 때, tractability issue 를 피하기 위해 무작위로 추출한 샘플 $\\hat{x}$ 의 gradient norm 을 사용해 soft 한 제약을 줍니다. 이렇게 새롭게 정의되는 목적함수는 아래와 같습니다. Sampling distribution논문은 데이터 분포와 generator 분포에서 샘플링한 점의 쌍을 이은 뒤, 점을 잇는 선분을 따라 $\\hat{x}$ 를 샘플링하였고, 실험적으로 좋은 성능을 얻었음을 언급합니다. Penalty coefficientgradient penalty 를 가하는 정도를 경정하는 계수로, 논문에서는 모두 $\\lambda&#x3D;10$ 을 사용했음을 언급합니다. No critic batch normalization기존 GAN 모델은 batch normalization 을 모든 곳에서 사용했지만, 이는 discriminator의 단일 입력을 단일 출력으로 매핑하는 문제에서, 입력의 전체 배치로부터 출력의 배치로 매핑하는 문제로 변화시킵니다. 이 때문에 gradient penalty 를 수행하면 batch normalization 이 유효하지 않은 결과가 발생한다고 합니다. 따라서 논문은 critic 에 batch normalization 을 제거하였고 그럼에도 적절한 성능을 보였음을 언급합니다. Two-sided penaltygradient penalty 는 norm이 1 아래에 머무르지 않고(one-sided penalty), 1로 향하기(two-sided penalty)는 것을 촉진한다는 점을 제시합니다. ExperimentsTraining random architectures within a set 일반적인 DCGAN 구조에서 위의 표의 설정을 랜덤하게 설정하여 모델을 구성합니다. 이렇게 무작위로 200개의 모델을 구성한뒤, 32x32 ImageNet 에 대해 WGAN-GP, standard GAN을 합니다. 구성한 모델의 inception_score 가 min_score 보다 큰 경우 성공으로 분류합니다. WGAN-GP 는 많은 구조를 학습하는데 성공했다는 결과를 제시합니다. Training varied architectures on LSUN bedrooms아래와 같이 6개의 모델을 기본 모델로 사용합니다. 여기에 DCGAN, LSGAN, WGAN, WGAN-GP 를 각각 적용하였을 때의 성능을 비교합니다. 단, WGAN-GP는 discriminator 에서 Batch normalization 을 사용할 수 없기에 layer normalization 을 사용합니다. WGAN-GP 를 제외한 모든 모델에서 불안정하거나 mode collapse 에 빠진 모습을 보입니다. Improved performance over weight clippingWGAN-GP 가 weight clipping 에 비해 더 빠른 학습 속도와 샘플 효율을 보인다는 점을 증명하기 위한 실험 결과를 제시합니다. 이를 위해, WGAN 과 WGAN-GP 모델을 CIFAR-10 으로 학습하여 Inception Score 를 계산합니다. 왼쪽은 iteration에 따른 Inception Score이며, 오른쪽은 시간에 따른 Inception Score입니다. WGAN-GP는 weight clipping 보다 항상 더 좋은 성능을 보입니다. 이는 같은 optimizer 를 사용했을 때도 마찬가지이며, 비록 DCGAN 보다는 느리지만 수렴에 있어서 안정적인 점수를 보일 수 있음을 제시합니다. Sample quality on CIFAR-10 and LSUN bedrooms CIFAR-10으로 학습한 모델의 Inception score 를 계산하여 다양한 구조의 GAN을 비교한 표를 제시합니다. WGAN-GP 는 Supervised 의 경우 SGAN 을 제외했을 때 가장 좋은 성능을 보입니다. 또한, WGAN-GP 로 deep ResNet 모델을 사용하여 128X128 LSUN 침대 이미지를 생성하여 위와같은 결과를 제시합니다. Modeling discrete data with a continuous generator Generator 는 연속적인 분포의 함수를 가정합니다. 따라서언어 모델은 비연속적인 분포를 모델링 해야하므로 GAN 으로 학습하기에 부적절할 수 있습니다. 위는 Google Billion Word 데이터셋을 사용해 문자 수준 언어 모델을 WGAN-GP 로 학습한 결과입니다. 모델이 빈번하게 철자를 틀리지만, 언어의 통계에 대해서는 어느정도 학습을 수행하였음을 볼 수 있습니다. Meaningful loss curves and detecting overfitting기존의 weight clipping 은 loss 가 sample quality 와 연관되어 최소값으로 수렴할 수 있다는 점입니다. WGAN-GP 가 해당 특성을 유지하는지 확인하기 위한 테스크를 진행한 결과를 제시합니다. (a)에서 LSUN 침대 데이터셋을 학습하고 critic 의 negative loss 를 그렸을 때, Gnerator 가 학습됨에 따라 값이 줄어드는 것을 확인할 수 있습니다. 이 경우, WGAN-GP가 critic에서의 과적합을 완화했다고 볼 수 있습니다. 또한, MNIST 무작위 숫자 1000개로 학습한 결과는, 적은 데이터셋을 사용한 만큼 과적합이 발생하기 쉽습니다. 때문에, critic이 generator보다 더 빨리 과적합되어 training loss를 점차 증가시키고 validation loss를 감소시켰음을 확인할 수 있습니다. ConclusionWGAN에 Gradient penalty를 적용하여 기존의 weight clipping 을 적용함으로 인해 발생하는 문제를 해결할 수 있음을 제시하였습니다. Summarize GAN의 가장 큰 문제는 학습환경이 매우 불안정하다는 것이다. 생성자와 구분자 둘 중에 하나가 실력이 월등이 좋아진다면 밸런스가 붕괴되고 모델이 정확히 학습되지 않고 학습이 완료된 후에도 mode dropping 이 생기는데 이는 구분자가 그 역할을 충분히 하지 못해 모델이 최적점까지 학습이 안 된 것이다. 따라서 이 문제를 해결하기 위해 본 논문에서는 WGAN 방법을 도입했다. 간단히 설명하면 GAN의 discriminator보다 선생님 역할을 잘 할 수 있는 critic을 사용함으로써 gradient를 잘 전달시키고 critic과 generator를 최적점까지 학습할 수 있다는 것이다. 그렇다면 이를 적용하면 학습시킬 때 생성자와 구분자의 밸런스가 잘 맞는지 주의깊게 보지 않아도 되고 학습한 이후에 발생하는 mode droppin이 해결 가능하다. 식을 해석해보면 생성자가 Lipschitz 함수 조건을 만족하는가 하지않는가에 대한 기준이 하나 더 생기는것 이다. Link: Improved Training of Wasserstein GANs","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Generative Model","slug":"Paper/Generative-Model","permalink":"https://jmj3047.github.io/categories/Paper/Generative-Model/"}],"tags":[{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"},{"name":"Image Classification","slug":"Image-Classification","permalink":"https://jmj3047.github.io/tags/Image-Classification/"},{"name":"WGAN-GP","slug":"WGAN-GP","permalink":"https://jmj3047.github.io/tags/WGAN-GP/"},{"name":"WGAN","slug":"WGAN","permalink":"https://jmj3047.github.io/tags/WGAN/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Generative Model","slug":"Paper/Generative-Model","permalink":"https://jmj3047.github.io/categories/Paper/Generative-Model/"}]},{"title":"Basic Web Crawling","slug":"Web_Crawling_Basic","date":"2022-04-21T15:00:00.000Z","updated":"2022-04-22T07:30:27.000Z","comments":true,"path":"2022/04/22/Web_Crawling_Basic/","link":"","permalink":"https://jmj3047.github.io/2022/04/22/Web_Crawling_Basic/","excerpt":"","text":"Crawling Tools -Beautifulsoup: 파이썬에서 가장 일반적인 수집도구(CSS 통해서 수집) -Scrapy (CSS, XPATH 형태로 수집) -Selenium (CSS, XPATH 통해서 데이터 수집 + Java Script) →자바 필요 + 몇개 설치 도구 필요 웹사이트 만드는 3대 조건 +1 :HTML, CSS, JavaScript, Ajax(비동기처리) 웹사이트 구동방식 :GET &#x2F; POST Create virtual env(git bash)123pip install virtualenvpython -m virtualenv venvsource venv/Scripts/activate Installing Library123pip install beautifulsoup4pip install numpy pandas matplotlib seabornpip install requests Reference: https://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-beautiful-soup","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"HTML","slug":"Python/HTML","permalink":"https://jmj3047.github.io/categories/Python/HTML/"}],"tags":[{"name":"HTML","slug":"HTML","permalink":"https://jmj3047.github.io/tags/HTML/"},{"name":"Web Crawling","slug":"Web-Crawling","permalink":"https://jmj3047.github.io/tags/Web-Crawling/"},{"name":"BeautifulSoup","slug":"BeautifulSoup","permalink":"https://jmj3047.github.io/tags/BeautifulSoup/"}],"keywords":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"HTML","slug":"Python/HTML","permalink":"https://jmj3047.github.io/categories/Python/HTML/"}]},{"title":"Web Crawling Practice","slug":"Web_Crawling_Headline","date":"2022-04-21T15:00:00.000Z","updated":"2022-04-22T08:17:11.000Z","comments":true,"path":"2022/04/22/Web_Crawling_Headline/","link":"","permalink":"https://jmj3047.github.io/2022/04/22/Web_Crawling_Headline/","excerpt":"","text":"1. Crawling Headline news from Naver12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import requestsfrom bs4 import BeautifulSoupimport pandas as pddef crawling(soup): div = soup.find(&quot;div&quot;, class_=&#x27;list_issue&#x27;) # print(type(div)) print(div.find_all(&#x27;a&#x27;)) #list형태 result = [] urls = [] for a in div.find_all(&quot;a&quot;): # print(a.get_text()) urls.append(a[&#x27;href&#x27;]) result.append(a.get_text()) # print(result) #save as csv file df = pd.DataFrame(&#123;&#x27;news_title&#x27;: result, &quot;url&quot;: urls&#125;) print(df) df.to_csv(&quot;newscrawling.csv&quot;) print(&quot;Crawling Success!&quot;) def main(): CUSTOM_HEADER = &#123; &#x27;referer&#x27; : &#x27;https://www.naver.com/&#x27;, &#x27;user-agent&#x27; : &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36&#x27; &#125; url = &#x27;https://www.naver.com/&#x27; req = requests.get(url = url, headers=CUSTOM_HEADER) print(req.status_code) #200: 정상적으로 사이트 돌아가고 있음 #404: 주소 오류 #503: 서버 내려진 상태 #print(req.text)이거를 beautifulsoup객체로 바꿔줌 soup = BeautifulSoup(req.text, &#x27;html.parser&#x27;) print(type(soup)) # print(soup.find(&quot;strong&quot;, class_=&#x27;new&#x27;)) # print(soup.find_all(&quot;span&quot;, class_=&#x27;blind&#x27;)) #print(soup.find(&quot;a&quot;, class_=&#x27;link_newsstand&#x27;)) crawling(soup)if __name__ == &quot;__main__&quot;: main() 2. Crawling Product List from ACBF1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import requestsfrom bs4 import BeautifulSoupimport pandas as pddef crawling(soup): div = soup.find_all(&quot;div&quot;, class_=&#x27;name&#x27;) print(type(div)) #&lt;class &#x27;bs4.element.ResultSet&#x27;&gt; # print(div) product_name = [] # urls =[] for a in div: # print(a.get_text()) # urls.append(a.get(&#x27;href&#x27;)) product_name.append(a.get_text()) print(product_name) # df = pd.DataFrame(&#123;&#x27;news_title&#x27;: product_name&#125;) # print(df) # df.to_csv(&quot;suit_product.csv&quot;) print(&quot;Crawling Success!&quot;) def main(): CUSTOM_HEADER = &#123; &#x27;referer&#x27; : &#x27;https://anyonecanbeafuse.com/category/%EC%88%98%ED%8A%B8/89/&#x27;, &#x27;user-agent&#x27; : &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36&#x27; &#125; url = &#x27;https://anyonecanbeafuse.com/category/%EC%88%98%ED%8A%B8/89/&#x27; req = requests.get(url = url, headers=CUSTOM_HEADER) print(req.status_code) #200: 정상적으로 사이트 돌아가고 있음 #404: 주소 오류 #503: 서버 내려진 상태 #print(req.text)이거를 beautifulsoup객체로 바꿔줌 soup = BeautifulSoup(req.text, &#x27;html.parser&#x27;) #print(type(soup)) # print(soup.find_all(&quot;div&quot;, class_=&#x27;name&#x27;)) crawling(soup)if __name__ == &quot;__main__&quot;: main() 3. Crawling Music Title from Chart123456789101112131415161718192021222324252627282930313233343536373839404142import requestsfrom bs4 import BeautifulSoupdef crawling(soup): tbody_df = soup.find(&quot;tbody&quot;) # print(tbody_df) result = [] for a in tbody_df.find_all(&#x27;p&#x27;, class_=&#x27;title&#x27;): # print(a.get_text()) # print(type(a.get_text())) result.append(a.get_text().strip(&quot;\\n&quot;)) print(result) print(&quot;Crawling Success!&quot;) def main(): CUSTOM_HEADER = &#123; &#x27;referer&#x27; : &#x27;https://music.bugs.co.kr/chart&#x27;, #필수 아님 &#x27;user-agent&#x27; : &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36&#x27; &#125; url = &#x27;https://music.bugs.co.kr/chart&#x27; req = requests.get(url = url, headers=CUSTOM_HEADER) print(req.status_code) #200: 정상적으로 사이트 돌아가고 있음 #404: 주소 오류 #503: 서버 내려진 상태 #print(req.text)이거를 beautifulsoup객체로 바꿔줌 soup = BeautifulSoup(req.text, &#x27;html.parser&#x27;) print(type(soup)) #&lt;class &#x27;bs4.BeautifulSoup&#x27;&gt; # print(soup.find_all(&quot;p&quot;, class_=&#x27;title&#x27;)) # print(soup.find_all(&quot;span&quot;, class_=&#x27;blind&#x27;)) #print(soup.find(&quot;a&quot;, class_=&#x27;link_newsstand&#x27;)) crawling(soup)if __name__ == &quot;__main__&quot;: main()","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"HTML","slug":"Python/HTML","permalink":"https://jmj3047.github.io/categories/Python/HTML/"}],"tags":[{"name":"HTML","slug":"HTML","permalink":"https://jmj3047.github.io/tags/HTML/"},{"name":"Web Crawling","slug":"Web-Crawling","permalink":"https://jmj3047.github.io/tags/Web-Crawling/"},{"name":"BeautifulSoup","slug":"BeautifulSoup","permalink":"https://jmj3047.github.io/tags/BeautifulSoup/"}],"keywords":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"HTML","slug":"Python/HTML","permalink":"https://jmj3047.github.io/categories/Python/HTML/"}]},{"title":"Unsupervised representation learning with deep convolutional generative adversarial networks","slug":"DCGAN","date":"2022-04-20T15:00:00.000Z","updated":"2022-10-15T08:03:41.524Z","comments":true,"path":"2022/04/21/DCGAN/","link":"","permalink":"https://jmj3047.github.io/2022/04/21/DCGAN/","excerpt":"","text":"Journal&#x2F;Conference: ICLRYear(published year): 2016Author: Alec Radford, Luke Metz, Soumith ChintalaSubject: DCGAN, Generative Model Unsupervised representation learning with deep convolutional generative adversarial networks Summary CNN 과 GAN framework 를 결합한 DCGAN 모델을 제시 Introduction논문 당시 GAN 은 불안정한 학습과 Generator 의 오작동으로 인해 제한적으로만 쓰였습니다. 이에 논문은 이미지 생성 모델을 만들기 위한 CNN 기반 GAN framework인 DCGAN(Deep Convolutional GANs) 을 제시합니다. 모델 구조에 제약을 가하여 대부분의 상황에서 안정적으로 학습할 수 있게 함 Discriminator 로 image classification 을 수행하였을 때 기타 비지도 학습 모델과 비슷한 성능을 보임 특정 필터가 특정 object를 그려낸다는 것을 시각화하여 제시함 Generator 에 입력하는 noise 를 제어하여 생성되는 샘플의 다양한 속성이 변화하는 것을 탐구함 Approach and Model Architecture논문 이전에도 GAN에 CNN을 써서 이미지 품질을 높이려는 시도가 있었으나 좋은 성과를 거두지 못하였다고 설명합니다. 이후, 다양한 데이터셋에서 안정적이고 높은 해상도의 이미지를 생성하기 위한 DCGAN 모델 설계 가이드라인을 제시합니다. Details of Adversarial Training3가지 데이터셋을 사용합니다. Large-scale Scene Understanding(LSUN) Imagenet-1k Faces 그외에 학습 디테일을 아래와 같이 제시합니다. pre-processing 제외 batch size 128 가중치는 N(0, 0.02) 로 초기화 Leaky ReLU의 기울기는 0.2로 설정함 AdamOptimizer, $\\beta_1 &#x3D;0.0002, \\beta_2&#x3D;0.9$ Generator 구조의 모식도는 위와 같습니다. LSUN 데이터셋으로 1 epoch 를 학습시킨 후 침실을 생성했을 때의 결과입니다. 이론적으로 모델이 훈련 예시를 기억했을 수도 있으나, 작은 학습률과 미니배치를 사용했음을 감안할 때 가능성이 낮다고 설명합니다. LSUN 데이터셋으로 5 epoch 학습 후 침실을 생성한 결과입니다. 침대 등의 근처에서 오히려 underfitting 이 발생했음을 확인할 수 있습니다. Empirical Validation of DCGANs CapabilitiesUnsupervised representation learning 알고리즘을 평가하는 일반적인 방법은 supervised 데이터셋으로 특징을 추출한 뒤 performance를 측정하는 것입니다. CIFAR-10 데이터셋에 대해 검증한 결과, 다른 방법들(K-means, Exemplar CNN 등)과 비교하여 결과에 큰 차이가 존재하지 않습니다. StreetView House Numbers dataset(SVHN) 데이터셋에서는 state-of-the-art 결과를 얻었음을 제시합니다. Investigating and Visualizing the Internals of the Networks가장 가까운 학습 데이터 이미지를 찾거나, 최근접 픽셀&#x2F;특징을 확인하거나 log-likelihood metric 으로 평가를 하는 방법은 모두 성능이 떨어지는 metric 이기에 사용하지 않았음을 언급합니다. 논문은 대신, 2개의 이미지를 생성할 때 사용한 noise 2개를 interpolation 하고, interpolated z 로 이미지를 생성한 결과를 제시합니다. 한 이미지에서 다른 이미지로 점진적으로 변해가는 모습을 관측할 수 있습니다. 또한 노이즈 벡터 z 의 산술 연산을 통해, vec(웃는 여자) −− vec(무표정 여자) ++ vec(무표정 남자) &#x3D;&#x3D; vec(웃는 남자) 같은 결과를 얻을 수 있었음을 제시합니다. 또한, 랜덤하게 생성한 필터와 학습된 필터의 activation 을 아래와 같이 시각화 하였습니다. 이해할 수 없는 feature 가 아닌 특정 object나 특징을 추출하고 있음을 확인할 수 있습니다. Conclusions and future work논문은 CNN 기반의 안정적인 이미지 생성모델인 DCGAN을 제안하였으며, image representation에 적합한 성능을 보임을 제시합니다. 그러나 여전히, 학습이 길어지는 경우 필터 일부가 요동치는 등의 현상을 관측하기도 하였음을 언급합니다. Link: Unsupervised representation learning with deep convolutional generative adversarial networks","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Generative Model","slug":"Paper/Generative-Model","permalink":"https://jmj3047.github.io/categories/Paper/Generative-Model/"}],"tags":[{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"},{"name":"DCGAN","slug":"DCGAN","permalink":"https://jmj3047.github.io/tags/DCGAN/"},{"name":"Convolutional Neural Networks","slug":"Convolutional-Neural-Networks","permalink":"https://jmj3047.github.io/tags/Convolutional-Neural-Networks/"},{"name":"Image Classification","slug":"Image-Classification","permalink":"https://jmj3047.github.io/tags/Image-Classification/"},{"name":"GAN","slug":"GAN","permalink":"https://jmj3047.github.io/tags/GAN/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Generative Model","slug":"Paper/Generative-Model","permalink":"https://jmj3047.github.io/categories/Paper/Generative-Model/"}]},{"title":"Generative Adversarial Nets","slug":"Generative_Adversarial_Nets","date":"2022-04-20T15:00:00.000Z","updated":"2022-10-15T08:03:52.639Z","comments":true,"path":"2022/04/21/Generative_Adversarial_Nets/","link":"","permalink":"https://jmj3047.github.io/2022/04/21/Generative_Adversarial_Nets/","excerpt":"","text":"Journal&#x2F;Conference: NIPSYear(published year): 2014Author: I. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, Yoshua BengioSubject: GAN, Generative Model Generative Adversarial Nets Summary 적대적으로 동작하는 두개의 네트워크를 사용해 새로운 데이터를 생성할 수 있는 GAN(Generative Adversarial Nets) 구조를 제안 생성자(Generator) 와 감별자(Discriminator) 모두 마르코프 체인 등의 구조없이 back-propagation 으로 학습이 가능한 인공신경망 구조를 사용 이후 등장하는 수많은 GAN 기반 모델의 기원이 되는 논문 Introduction &amp; Related Works분류 문제에 제한되어 사용되던 딥러닝 모델의 용도를 새로운 데이터를 생성하는 문제에도 적용할 수 있는 적대적 생성 신경망(Generative Adversarial Nets)을 최초로 제시한 논문입니다. GAN은 아래와 같은 목표를 가진, 적대적인 두 모델을 학습합니다. 감별자(Discriminator) 모델 데이터가 원본 데이터셋에서 온것인지, 생성자가 만든 것인지를 판별 예시) 경찰이 지폐가 위조되었는지를 판별 생성자(Generator) 모델 감별자가 구분할 수 없는 가짜 데이터를 생성 예시) 위폐범이 경찰이 구분할 수 없는 위조 지폐를 제작함 논문은 해당 방법이 특별한 모델이나 학습 방법을 필요로 하지 않는 방법이라고 하며, MLP(multi-layer perception) 구조를 사용해 학습한 결과를 소개합니다. Adversarial nets적대적 신경망의 가장 직관적인 예시로 MLP 모델을 사용한 경우를 가정하여 설명합니다. 이 때 사용하는 표기법은 다음과 같습니다. $x\\sim p_{data}$ : 실제 데이터로부터 뽑은 샘플 $p_g$ : 생성자가 생성하는 데이터의 분포 $p_z(z)$ : 데이터를 생성하기 위해 사용하는 입력 노이즈 분포 $G(z;\\theta_g)$ : 생성자 모델 $\\theta_g$ : 생성자 모델 파라미터 $D(x;\\theta_d)$ : 감별자 모델 $\\theta_D$ : 감별자 모델 파라미터 D 는 실제 데이터와 생성된 데이터에 정확히 구분할 수 있는 확률을 최대화 하려고 합니다. G 는 D가 실제 데이터로 착각할 만한 데이터를 생성하는 것을 목적으로 $\\log(1-D(G(z))$ 를 최소화 하려고 합니다. 따라서 아래와 같이 가치함수 $V(G,D)$ 가 주어졌을 때, G 는 최소화, D는 최대화를 목적으로 경쟁합니다. 실제 계산에서 V 를 최대로 하는 D 를 구할 때 많은 계산이 필요하고, 데이터셋이 제한된 상황에서 과적합이 발생할 수도 있습니다. 따라서 실제 훈련에서는 D 를 k 번만 학습하고 G 를 학습합니다. 또한, 학습 초기에는 G가 생성하는 데이터의 품질이 낮으므로 D가 판별을 하기 쉬워, $\\log(1−D(G(z)))$ 항이 소실될수 있습니다. 따라서, $\\log D(G(z))$ 를 최대화 하는 문제로 변환하여 초기에 학습이 잘 이뤄질 수 있도록 합니다. 학습 과정의 모식도입니다. 파란 점선은 감별자 D의 분포, 검은 점은 원본 데이터 분포, 초록 실선은 생성자 G의 분포를 나타냅니다. 파란색점선: discriminator 검정색점선: real data에서나온sample 초록색실선: generator Z옆의검정색실선: domain from which z is sampled 화살표: 생성자가 noise를 real data와 얼마나 비슷하게 만들어주는지에 대한 지표 (a) 와 같이 학습이 완료되기 전의 상태에서 시작합니다.(model training 초기상태) (b) 와 같이 D 를 업데이트 할 때, 최적의 D( $D^{*}_G(x)$ ) 는 $\\frac{p_{data}(x)}{p_{data}(x) + p_g(x)}$ 로 수렴합니다.(내부의 알고리즘에 의해서 구분자가 train됨) (c) G를 업데이트하면, D 를 교란할 수 있도록 G 가 생성하는 분포가 실제 데이터 분포에 가까워집니다.(구분자가 학습한 걸 생성자에게 업데이트) (d) 학습 과정을 반복하면 생성자는 데이터 분포와 일치하는 데이터를 생성($p_g &#x3D; p_{data}$) 하며, 감별자는 어떠한 샘플도 구분할 수 없게 됩니다. ($D(x)&#x3D;\\frac{1}{2}$) (real data와f ake data가 같은 모습이 된 단계. 구분자는 fake와 real data를 구분할 수 없게 됨.) Fake data가왜noise인지? 명확한 이유는 명시되어 있지 않음. 대략적인 이유를 추론해보자면 생성자에 편향되지 않은 데이터가 들어가야 실험의 결과가 더 clear하기 때문. 생성자에 넣어서 만들어진 데이터가 m개라면, 그 데이터 m개가 만들어지려면 같은 숫자의 real data가 있어야 함. 따라서 총데이터는 2m개 구분자의 결과값은 fake data일때 0, real data 일 때 1인 하나의 스칼라 값 따라서 가장 이상적인 구분자가 될 때의 값은 0.5 GAN 모델은 markov 모델이 해야하는 훈련 과정과 overfitting에 문제점을 보완하기 위해 이를 한번에 하기 위해 만들어진 네트워크 Theoretical Results적대적 신경망 문제에서 생성자가 원본 데이터와 유사한 분포의 데이터를 생성할 수 있다는 증명을 제시합니다. 또한, 실제 적대적 신경망을 학습하기 위해 설계한 아래 알고리즘 또한 같은 결과에 수렴한다는 증명을 제시합니다. Global Optimality of $p_g &#x3D; p_{data}$먼저 임의의 G 가 주어졌을 때 최적의 D 를 계산하는 과정을 보입니다. 최적의 D 를 이용하여 Equation 1 을 G에 관한 수식으로 표현할 수 있습니다. 이 때 새롭게 정리한 가치함수가, G가 생성하는 데이터의 분포가 실제 분포를 따르는 경우에만 최소화된다는 것을 다음과 같이 증명합니다. Convergence of Algorithm 1G 와 D 가 $p_g$ 충분한 표현력을 갖고 있을 때, 제시한 알고리즘이 $p_g&#x3D;p_{data}$ 로 수렴함을 아래와 같이 증명합니다. 실제로 MLP 를 사용한 G 로는 모든 형태의 $p_g$ 를 표현할 수 없으므로 이론적인 최고 성능을 보장하기 어렵습니다. 논문은 그럼에도 불구하고 GAN이 실제 훈련결과에서 좋은 성능을 보임을 제시합니다. Experiments실험에 사용한 조건은 다음과 같습니다. Dataset : MNIST, Toronto Face Database(TFD), CIFAR-10 사용 Generator : ReLU&#x2F;sigmoid 활성함수를 혼합하여 사용 Discriminator : maxout 활성함수를 사용 D를 학습시킬 때만 Dropout을 사용 G에서 데이터를 생성하는 경우에만 noise를 input 으로 사용 GAN 은 데이터 분포 자체를 구하기 위한 tractable likelihood 를 가정하지 않습니다. 이러한 모델을 평가하기 위해 기존에 제안된 방법은 다음과 같습니다. Generator 에서 생성한 데이터를 Gaussian Parzen window 에 fitting fitting 한 분포가 주어졌을 때 log-likelihood 를 계산 Validation set 으로 교차 검증을 수행해 표준 편차를 계산 논문은 해당 방법의 분산이 크고 높은 차원의 데이터에서 잘 작동하지 않지만, GAN 이 기존 모델에 비해 상대적으로 좋은 결과를 보이고 있음을 제시합니다. 다음으로 GAN 모델로 생성한 데이터를 제시합니다. 가장 우측에는 원본 데이터셋 중 생성된 데이터에 가장 가까운 데이터를 배치하였습니다. 논문은 해당 모델이 기존의 생성 모델보다 낫다고 주장하기는 어렵지만, 비슷한 성과와 응용 가능성을 보여줄 수 있다는 의견을 제시합니다. 또한 위 그림과 같이, Generator 의 Input noise 를 점진적으로 변형시킬 때, 점점 interploation 되어가는 생성 데이터를 확인할 수 있습니다. Advantages and disadvantagesGAN 의 단점을 아래와 같이 정리합니다. Generator 가 생성하는 데이터의 분포가 명시적으로 존재하지 않습니다. Generator 와 Discriminator 의 균형이 깨지는 경우 학습이 원활이 이루어지지 않습니다. 또한, GAN 의 장점을 아래와 같이 정리합니다. 마르코프 체인 같은 구조 없이 역전파 만으로도 학습이 가능합니다. Generator 의 분포로 특별한 모델을 가정하지 않습니다. 더욱 복잡한 데이터 분포를 모사할 수 있어 선명한 데이터를 생성할 수 있습니다. Conclusions and future workGAN 프레임워크를 확장하고 개선할 수 있는 다양한 방법을 제시합니다. 주어진 조건에 따라 데이터를 생성하는 모델로 발전 가능 x가 주어졌을 때 z를 예측하는 보조 네트워크를 학습한다면 생성자의 데이터 분포를 예측할 수 있음 parameters를 공유하는 conditionals model를 학습함으로써 다른 conditionals models을 근사적으로 모델링할 수 있음 Semi-supervised learning에도 활용 가능 : 데이터가 제한된 경우 Discriminator 를 활용하여 classifier의 성능을 향상시킬 수 있음 효율성 개선: G와 D를 균형있게 학습할 수 있는 방법이나 새로운 z 분포를 제시하여 학습 속도 개선 가능 Summarize GAN 모델은 생성자(Generator)와 구분자(Discriminator) 둘의 적대적인 경쟁을 통해서 학습하는 딥러닝 네트워크 실제 우리가 학습시키려는 데이터와 생성자가 만든 Fake 데이터를 구분자에 모두 학습시켜서 구분을 더 잘 짓게 하는 방식으로이루어진네트워크이며, 생성자는랜덤노이즈를학습데이터와유사한패턴으로만들어주는네트워크구조를가진다. 이를 테스트하기 위해서 확인할 지표는 바이너리크로스엔트로피와 손실함수의 값이 구분자가 출력한 확률값이 정답에 가까우면 낮아지기 때문에 이것이 모델 학습의 목표가 된다. 구분자의 손실함수는 그래서 두가지 합인데 하나는 가짜이미지를 입력했을 때의 출력값과 1의차이, 그리고 가짜 이미지를 입력했을 때의 출력값과 0의 차이. 이 둘의 합이 구분자의 손실함수이며 이를 최소화하는 방향으로 구분자의 파라미터가 업데이트 된다. 이 업데이트는 최적화 함수를 통해 이루어진다. 데이터가 어떤 유형인지에 따라서 fake data를 어떤 것을 사용할지도 달라지는데 이 논문에서는 fake data를 데이터 분포를 통해서 샘플을 사용하며 이는 대체적으로 차원이 낮은 랜덤노이즈이다. 최악의 경우(max)를 가정했을 때 손실을 최소화(min)하는 것을 minimax게임이라고 하며 이것이 GAN 기저에 깔려있는 이론이라고 할 수 있다. GAN의 가장 큰문제는 학습환경이 매우 불안정하다는 것이다. 생성자와 구분자 둘 중에 하나가 실력이 월등이 좋아진다면 밸런스가 붕괴되고 모델이 정확히 학습되지 않고 학습이 완료된 후에도 mode dropping 이 생기는데 이는 구분자가 그 역할을 충분히 하지 못해 모델이 최적점까지 학습이 안 된 것이다. 이런 문제를 해결하기 위해 이후 논문에서 다양한 해결 방법이 제시된다. Link: Generative Adversarial Nets","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Generative Model","slug":"Paper/Generative-Model","permalink":"https://jmj3047.github.io/categories/Paper/Generative-Model/"}],"tags":[{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"},{"name":"GAN","slug":"GAN","permalink":"https://jmj3047.github.io/tags/GAN/"},{"name":"Generative Model","slug":"Generative-Model","permalink":"https://jmj3047.github.io/tags/Generative-Model/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Generative Model","slug":"Paper/Generative-Model","permalink":"https://jmj3047.github.io/categories/Paper/Generative-Model/"}]},{"title":"Hexo Hueman Tutorial","slug":"Hexo_Hueman_tutorial","date":"2022-04-20T15:00:00.000Z","updated":"2023-02-28T07:50:13.607Z","comments":true,"path":"2022/04/21/Hexo_Hueman_tutorial/","link":"","permalink":"https://jmj3047.github.io/2022/04/21/Hexo_Hueman_tutorial/","excerpt":"","text":"1.Starting Hexo Blog1234567891011121314151617181920212223242526username@LAPTOP-D1EUIRLS MINGW64 ~/Desktop$ hexo init your_blog_folderusername@LAPTOP-D1EUIRLS MINGW64 ~/Desktop$ cd your_blog_folder/username@LAPTOP-D1EUIRLS MINGW64 ~/Desktop/your_blog_folder$ echo &quot;# your_blog_folder&quot; &gt;&gt; README.mdgit initgit add README.mdgit commit -m &quot;first commit&quot;git branch -M mastergit remote add origin https://github.com/your_id/your_blog_folder.gitgit push -u origin masterusername@LAPTOP-D1EUIRLS MINGW64 ~/Desktop/your_blog_folder (master)$ git add .username@LAPTOP-D1EUIRLS MINGW64 ~/Desktop/your_blog_folder (master)$ git commit -m &quot;updated&quot;username@LAPTOP-D1EUIRLS MINGW64 ~/Desktop/your_blog_folder (master)$ git pushusername@LAPTOP-D1EUIRLS MINGW64 ~/Desktop/your_blog_folder (master)$ code . 2.Applying Hueman Theme3.Basic Hexo Tutorial4.Hexo Tag Plugins5.Add Math Formula(without changing from Notion) Creat File name mathjax.ejs on themes/hueman/layout folder 123456789101112MathJax.Hub.Config(&#123; jax: [&quot;input/TeX&quot;, &quot;output/HTML-CSS&quot;], # mathjax tex2jax: &#123; inlineMath: [ [&#x27;$&#x27;, &#x27;$&#x27;] ], displayMath: [ [&#x27;$$&#x27;, &#x27;$$&#x27;]], processEscapes: true, skipTags: [&#x27;script&#x27;, &#x27;noscript&#x27;, &#x27;style&#x27;, &#x27;textarea&#x27;, &#x27;pre&#x27;, &#x27;code&#x27;] &#125;, messageStyle: &quot;none&quot;, &quot;HTML-CSS&quot;: &#123; preferredFont: &quot;TeX&quot;, availableFonts: [&quot;STIX&quot;,&quot;TeX&quot;] &#125;&#125;); Check #Plugins in themes/hueman/_config.yml file and change mathjax: false to true Add mathjax:true at the header when you post Reference: Math Formula 6.Font Change7.Deleting Posts8.Error in Hueman ThemeTo Be Continued..","categories":[{"name":"Setting","slug":"Setting","permalink":"https://jmj3047.github.io/categories/Setting/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://jmj3047.github.io/tags/Hexo/"},{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Hueman","slug":"Hueman","permalink":"https://jmj3047.github.io/tags/Hueman/"}],"keywords":[{"name":"Setting","slug":"Setting","permalink":"https://jmj3047.github.io/categories/Setting/"}]},{"title":"ImageNet Classification with Deep Convolutional Neural Networks","slug":"ImageNet_Classification","date":"2022-04-20T15:00:00.000Z","updated":"2022-10-15T08:04:25.032Z","comments":true,"path":"2022/04/21/ImageNet_Classification/","link":"","permalink":"https://jmj3047.github.io/2022/04/21/ImageNet_Classification/","excerpt":"","text":"Journal&#x2F;Conference: NIPSYear(published year): 2012Author: Alex Krizhevsky, Ilya Sutskever, Geoffrey E. HintonSubject: AlexNet, Computer Vision ImageNet Classification with Deep Convolutional Neural Networks Summary 기존 머신러닝 모델을 제치고 딥러닝 모델이 더 우수한 성능을 보일 수 있음을 증명한 최초의 모델 ReLU 활성화 함수와 Dropout 의 유용함, Data Augmentation 기법을 제시 2012년 ImageNet 대회 ILSVRC 에서 우승을 차지한 모델 IntroductionAlexNet 이전의 객체 인식 모델은 대부분 고전적인 ML 모델로, 수만개 정도의 작은 데이터셋(NORB, Caltech-101&#x2F;256, CIFAR-10&#x2F;100)을 사용합니다. 그러나 이후, 수십만 개의 완전 분할 된 이미지로 구성된 LabelMe 와 1500 만 개 이상의 고해상도 이미지로 구성된 ImageNet 이 등장합니다. 이런 데이터셋을 처리하기 위해서는 높은 학습 역량을 가진 모델이 필요합니다. 또한, 학습과정에 사용되지 않은 수많은 데이터에 대해서도 추론을 할 수 있는 방대한 사전 지식을 담아내야합니다. 이에 논문은 컨볼루션 신경망(CNN) 모델을 기반으로 하는 AlexNet 을 제시합니다. CNN 은 FFNN(feed-forward NN)에 비해 더 적은 매개 변수를 가지므로 훈련이 용이합니다. 이를 통해 ILSVRC-2010, ILSVRC-2012 대회에 사용된 ImageNet subset에서 최고의 성능을 달성했습니다. 또한, 네트워크 성능 향상과 훈련시간 감소를 위한 여러가지 방법을 제시합니다. ALexNet 은 2개의 GTX 580 3GB GPU에서 5-6 일동안 훈련을 수행하였습니다. The Dataset지금은 대부분의 딥러닝 모델에서 기본적으로 사용하는 ImageNet 에 대한 소개입니다. 22,000 개 범주로 구분되는 1,500 만개 고해상도 이미지 웹에서 수집한 이미지를 Amazon 의 Mechanical Turk 크라우드 소싱 도구로 라벨링 2010 년부터 Pascal Visual Object Challenge의 일환으로 ImageNet 대규모 시각 인식 도전 (ILSVRC)이라는 연례 대회가 열렸습니다. ILSVRC는 1000 개의 카테고리 각각에 약 1000 개의 이미지가있는 ImageNet의 하위 집합을 사용합니다. 이는 약 120 만 개의 훈련 이미지, 50,000 개의 검증 이미지, 150,000 개의 테스트 이미지로 구성됩니다. 대부분의 실험 결과는 테스트 이미지가 공개된 ILSVRC-2010 를 사용합니다. 별도로, AlexNet 이 참가했던 ILSVRC-2012 실험 결과 또한 제시합니다. ImageNet 데이터셋 성능 지표로는 Top-1&#x2F;Top-5 Accuracy 를 사용합니다. 가변 해상도로 구성된 ImageNet 을 처리하기 위해 256 × 256의 고정 해상도로 다운 샘플링을 수행합니다. 직사각형 이미지는 scaling 후 중앙 256x256 패치를 잘라냅니다. 이외의 전처리는 수행하지 않습니다. The ArchitectureReLU Nonlinearity논문 발표 당시 일반적으로 사용된 perceptron 의 activation 함수는 tanh 혹은 sigmoid 입니다. 이들은 출력값이 무한대로 발산하지 않고 특정한 영역으로 제한되는 saturating 함수입니다. 반면 ReLU(Recitified Linear Unit) activation 은 출력값이 0 에서 무한대까지 발산할 수 있는 non-saturating 함수입니다. 논문은 4 layer CNN 으로 CIFAR-10 데이터셋을 사용하여 학습하였을 때, ReLU 가 6배 빠른 학습 속도를 보여주었음을 제시합니다. 이를 통해, non-saturating 한 함수가 gradient 를 더 빠르게 update 할 수 있음을 제시합니다. Training on Multiple GPUsGPU 메모리 제한과 느린 학습 속도를 개선할 수 있는 병렬학습 방법을 제시합니다. 기본 골자는 네트워크를 분할(커널, 뉴런 등)하여 서로 다른 GPU 에서 병렬적으로 연산을 수행하는 것입니다. 이 때, 메모리의 한계 및 병목 현상을 고려하여, 특정한 레이어에서만 연산 결과를 교환합니다. 논문은 이를 통해 half-size kernel 을 사용한 단일 GPU 모델보다 Top-1&#x2F;Top-5 accuracy 를 1.7% &#x2F; 1.2% 감소시켰음을 제시합니다. Local Response NormalizationReLU 활성 함수는 입력을 normalization 하지 않아도 saturation 이 발생하지 않습니다. 그러나 positive value 를 그대로 출력하는 ReLU 함수의 특성으로 인해 CNN 의 일부 구역에서 강한 신호가 생성될 수 있습니다. 이에 논문은 아래와 같은 local response normalization 방법을 제시합니다. 요약하면, CNN 에서 인접한 필터를 사용하여 normalization 을 진행한 것으로, 논문에서는 Top-1&#x2F;Top-5 Accuracy 를 1.4%, 1.2% 개선할 수 있었음을 제시합니다. 또한, CIFAR-10 으로 학습을 수행하였을 때도 2% 의 오차율 감소를 보였음을 제시합니다.(논문 당시에는 Batch Normalization 이 소개되지 않았습니다.) Overlapping PoolingCNN의 풀링 레이어는 같은 채널에 존재하는 인접한 뉴런의 출력을 요약해줍니다. 논문 이전에는 pooling 을 수행하는 영역이 겹치지 않도록 구성하여 사용하는 것이 일반적이었습니다. 논문은 풀링을 수행하는 영역이 이동하는 거리를 조절하여 풀링 영역이 겹치도록 한 결과, Top-1&#x2F;Top-5 Accuracy 를 0.4 %&#x2F;0.3 % 감소했다고 합니다. 또한 이를 통해 모델의 과적합 가능성을 줄일 수 있었다고 합니다. Overall Architecture AlexNet 의 전체 구조도 입니다. 2GPU 로 병렬학습을 수행하기 위해 두 갈래로 나뉘어 표현되어 있습니다. 총 5개의 convolution layer 와 3개의 max pooling layer, 3개의 dense layer 로 구성되어 있으며, 필요한 경우에만 GPU 연산 결과를 공유합니다. 또한 convolution&#x2F;dense layer 의 활성함수는 ReLU 를 사용합니다. Input : 224 x 224 x 3 &#x3D; 150,528 Convolution 1 : 11x11 kernel, 4 stride : 54x54x96 Max pooling 1 : 3x3 kernel, 2 stride : 26x26x96 Convolution 2 : 5x5 kernel, 2 padding : 26x26x256 Max pooling 2 : 3x3 kernel, 2 stride : 12x12x256 Convolution 3 : 3x3 kernel, 1 padding : 12x12x384 Convolution 4 : 3x3 kernel, 1 padding : 12x12x384 Convolution 5 : 3x3 kernel, 1 padding : 12x12x384 Max pooling 3 : 3x3 kernel, 2 stride : 5x5x256 Dense 1 : 4096 Dense 2 : 4096 Dense 3 : 1000 Reducing Overfitting6천만개의 파라미터로 구성된 모델의 과적합을 막기 위해 사용한 방법을 소개합니다. Data Augmentation학습 데이터를 인위적으로 변환하여 훈련 데이터를 증가시키는 방법입니다. 변환된 이미지를 저장하지 않고 GPU 학습시에 CPU에서 계산하도록 하여, 추가적인 계산 비용을 소모하지 않았다고 합니다. 주요 방법은 두가지로 요약됩니다. 256 × 256 이미지에서 224 × 224 패치를 추출하고, 수평 방향으로 뒤짚기 기존 데이터 셋의 2048 배 확장 가능 실제 : 5 개의 224 × 224 패치 (4 개의 코너 패치 및 중앙 패치)와 수평 반사를 수행한 10개의 패치 사용 RGB 채널 강도 조정 학습 데이터셋의 픽셀값으로 PCA 를 수행 PCA eigenvector 에 N(0,0.1) 인 정규분포에 추출한 랜덤값을 곱해 색상을 조정 Top-1 오차율을 1% 감소할 수 있었음 DropoutDense Layer 의 Output 에 Dropout rate 0.5 를 사용한 Dropout layer 를 추가합니다. 학습에 필요한 Epoch 를 2배 정도 늘렸으나, 과적합을 성공적으로 방지했음을 제시합니다. Details of learning모델 학습의 세부내용입니다. Batch size : 128 SGD (momentum 0.9, weight decay 0.0005) weight decay 가 모델을 정규화 할 뿐만 아니라 직접적으로 모델의 학습 오차를 줄였음을 제시합니다. 가중치 업데이트 과정은 아래와 같습니다. 가중치는 평균이 0, 표준 편차가 0.01인 정규 분포를 따르도록 초기화합니다. 2&#x2F;4&#x2F;5 번째 convolution 과 dense layer의 bias 는 1로 초기화하여, 학습을 가속할 수 있었음을 제시합니다. 학습률은 모든 layer 에 대해서 동일하되, 훈련을 수행하면서 메뉴얼하게 조정합니다. 학습률 0.01 에서 시작하여, 학습이 개선되지 않을 때 학습률을 10으로 나누는 방식으로 수행합니다. RESULT ILSVRC-2010 데이터에 대해서 기존 모델의 결과를 압도적으로 상회하는 결과를 제시하였습니다. AlexNet 이 직접 참가했던 ILSVRC-2012 에서도 다른 최고 성능의 모델에 비해 압도적인 결과를 보였음을 확인할 수 있습니다. 또한, CNN Layer 갯수를 추가할 때마다 성능이 상승함을 제시합니다. Qualitative Evaluations CNN kernel 을 시각화한 그림을 제시하면서, 각 커널이 이미지의 다양한 Feature 를 효과적으로 추출해냈음을 제시합니다. AlexNet 은 중앙을 벗어나는 데이터도 효과적으로 분류해냈습니다. 또한, Top-5 예측이 대부분 유사한 범주인 것으로 보아 합리적인 예측을 수행하고 있음을 제시합니다. 또한, 자세가 서로 다른 코끼리의 사례와 같이, Pixel 차원에서 완전히 다른 데이터임에도 유사한 범주로 분류할 수 있는 결과를 보여줍니다. Discussion“깊은” CNN 이 효과적으로 작동하였음을 제시합니다. Convolution layer 를 제거할 때마다 Top-1 Accuracy 가 2%씩 감소하는 점에 미루어, “깊이”의 중요성을 다시 한번 제시합니다. Link: ImageNet Classification with Deep Convolutional Neural Networks","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Computer Vision","slug":"Paper/Computer-Vision","permalink":"https://jmj3047.github.io/categories/Paper/Computer-Vision/"}],"tags":[{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"},{"name":"Convolutional Neural Networks","slug":"Convolutional-Neural-Networks","permalink":"https://jmj3047.github.io/tags/Convolutional-Neural-Networks/"},{"name":"Image Classification","slug":"Image-Classification","permalink":"https://jmj3047.github.io/tags/Image-Classification/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Computer Vision","slug":"Paper/Computer-Vision","permalink":"https://jmj3047.github.io/categories/Paper/Computer-Vision/"}]},{"title":"K-Nearest Neighbor","slug":"KNN","date":"2022-04-20T15:00:00.000Z","updated":"2023-02-23T01:34:13.754Z","comments":true,"path":"2022/04/21/KNN/","link":"","permalink":"https://jmj3047.github.io/2022/04/21/KNN/","excerpt":"","text":"1. Classification 분류나 예측을 진행할때 나랑 가장 가까운 이웃 k개를 고려하겠다. 나랑 가까운 이웃 한명이 검정색이면 검정색으로 판단 파란색의 가장 가까운 이웃을 확인해본 결과 검정색 이므로 파란색도 검정색으로 분류되었다 K&#x3D;3 일 경우 형광색 친구를 분류한다고 하였을때 이웃중 파란색이 2개 검정색이 한개이기 때문에 파란색으로 분류된다. 분류를 원하는 관측치의 주변 N개의 데이터(근접 이웃)을 골라서, 주변대세를 확인 (다수결의 원칙으로) 2. Prediction 인접 K개의 데이터의 수치를 확인해줘서 그 데이터의 평균을 검은점의 예측치로 설정해준다. 3. How to find optimal k?k의 결정 k가 너무 큰 경우, KNN모델이 지나치게 일반화됨 K가 너무 작은 경우,KNN 모델의 예측 결과의 분산이 큼 주로 이것저것 해보고 error이 가장 작은 k를 설정하여준다. 거리 척도의 결정 상황에 맞는 거리척도를 사용하여야 한다. 거리척도의 종류:Minkowski distance , Euclidean distance, Citi block distance, Mahalanobis distance, Correlation distance 등 Reference: 한국공학대학교 강지훈교수님 강의","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}],"tags":[{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}]},{"title":"Pyspark Tutorial(1)","slug":"Pyspark_Tutorial_1","date":"2022-04-20T15:00:00.000Z","updated":"2022-04-22T07:33:10.000Z","comments":true,"path":"2022/04/21/Pyspark_Tutorial_1/","link":"","permalink":"https://jmj3047.github.io/2022/04/21/Pyspark_Tutorial_1/","excerpt":"","text":"Reference: https://spark.apache.org/docs/latest/quick-start.html Get Started01.basic.py1234567891011121314# -*- coding: utf-8 -*-import pysparkprint(pyspark.__version__)from pyspark.sql import SparkSession#스파크 세션 초기화 :spark session이 하나 만들어진것spark = SparkSession.builder.master(&#x27;local[1]&#x27;).appName(&#x27;SampleTutorial&#x27;).getOrCreate()rdd = spark.sparkContext.parallelize([1,2,3,4,5])print(&quot;rdd Count&quot;, rdd.count())spark.stop() 02.rating.py1234567891011121314151617181920212223242526272829303132333435#SparkContext#RDDfrom pyspark import SparkConf, SparkContextimport collectionsprint(&quot;Hello&quot;)def main(): # MasterNode = local # MapReduce conf = SparkConf().setMaster(&quot;local&quot;).setAppName(&#x27;RatingHistogram&#x27;) sc = SparkContext(conf = conf) lines = sc.textFile(&quot;ml-100k/u.logs&quot;) #print(lines) ratings = lines.map(lambda x: x.split()[2]) #print(&quot;ratings:&quot;,ratings) #rdd라는 객체가 만들어진것 result = ratings.countByValue() #print(&quot;result:&quot;,result) #정렬하기 sortedResults = collections.OrderedDict(sorted(result.items())) for key, value in sortedResults.items(): print(&quot;%s %i&quot; % (key, value)) if __name__ == &quot;__main__&quot;: main() ##spark를 쓰는 이유:로그 데이터를 가져와서 규칙을 대입해서 정렬한다음에 정형데이터로 치환하기위해#실제로 의미있는 로그라면 분석도 의미가 있다#분석과 로그데이터를 처리할 수 있는 환경을 지원해줌#과거에는 두개가 따로 있었음 03.dataloading.py1234567891011121314151617181920212223242526272829303132333435363738394041424344#Spark SQL 적용#Spark Sessionfrom pyspark.sql import SparkSession# #스파크 세션 생성# my_spark = SparkSession.builder.getOrCreate()# print(my_spark)# #테이블을 확인하는 코드# print(my_spark.catalog.listDatabases())# #show database# my_spark.sql(&quot;show databases&quot;).show()# #check current DB# my_spark.catalog.currentDatabase()# my_spark.stop()#loading csv filespark = SparkSession.builder.master(&#x27;local[1]&#x27;).appName(&quot;DBTutorial&quot;).getOrCreate()flights = spark.read.option(&#x27;header&#x27;,&#x27;true&#x27;).csv(&#x27;data/flight_small.csv&#x27;)#flights.show(4)#spark.catalog.currentDatabase()#flights 테이블을 default DB에 추가함flights.createOrReplaceTempView(&#x27;flights&#x27;)#print(spark.catalog.listTables(&#x27;default&#x27;))#spark.sql(&#x27;show tables from default&#x27;).show()#쿼리 통해서 데이터 저장query = &quot;FROM flights SELECT * LIMIT 10&quot;query2 = &quot;SELECT * FROM flights LIMIT 10&quot;# 스파크 세션 할당flights10 = spark.sql(query2)#flights10.show()#spark 데이터 프레임을 pandas data frame으로 변환import pandas as pdpd_flights10 = flights10.toPandas()print(pd_flights10.head()) 04.struct_type.py Reference: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.types.StructType.html 1234567891011121314151617181920212223242526272829303132from pyspark.sql import SparkSessionfrom pyspark.sql import functions as func #aliasfrom pyspark.sql.types import StructType, StructField, IntegerType, LongTypeprint(&quot;Hello&quot;)#세션 할당spark = SparkSession.builder.appName(&quot;PopularMovies&quot;).getOrCreate()#스키마 작성(u.logs 데이터)schema = StructType( [ StructField(&quot;userID&quot;,IntegerType(), True) , StructField(&quot;movieID&quot;,IntegerType(), True) , StructField(&quot;rating&quot;, IntegerType(), True) , StructField(&quot;timestamp&quot;, LongType(), True) ])print(&quot;Schema is done&quot;)#데이터 불러오기movies_df = spark.read.option(&quot;sep&quot;,&quot;\\t&quot;).schema(schema).csv(&quot;ml-100k/u.logs&quot;)#내림차순으로 인기 있는 영화 정렬#movieID로 그룹바이, count() 진행, orderbytopMovieIds = movies_df.groupby(&quot;movieID&quot;).count().orderBy(func.desc(&#x27;count&#x27;))print(topMovieIds.show(10))#세션 종료spark.stop() 05.advance_structtype.py1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162from pyspark.sql import SparkSessionfrom pyspark.sql import functions as func #aliasfrom pyspark.sql.types import StructType, StructField, IntegerType, LongTypeimport codecsprint(&quot;Starting Session&quot;)def loadMovieNames(): #u.item에서 영화 이름 가져옴 movieNames = &#123;&#125; with codecs.open(&quot;ml-100k/u.item&quot;,&quot;r&quot;, encoding=&quot;ISO-8859-1&quot;, errors =&quot;ignore&quot;) as f: for line in f: fields = line.split(&quot;|&quot;) movieNames[int(fields[0])] = fields[1] #데이터 추가하는 딕셔너리 문법 return movieNames #세션 할당spark = SparkSession.builder.appName(&quot;PopularMovies&quot;).getOrCreate()#파이썬 딕셔너리 객체를 spark 객체로 변환nameDict = spark.sparkContext.broadcast(loadMovieNames())#스키마 작성(u.logs 데이터)schema = StructType( [ StructField(&quot;userID&quot;,IntegerType(), True) , StructField(&quot;movieID&quot;,IntegerType(), True) , StructField(&quot;rating&quot;, IntegerType(), True) , StructField(&quot;timestamp&quot;, LongType(), True) ])print(&quot;Schema is done&quot;)#데이터 불러오기movies_df = spark.read.option(&quot;sep&quot;,&quot;\\t&quot;).schema(schema).csv(&quot;ml-100k/u.logs&quot;)#내림차순으로 인기 있는 영화 정렬할 필요 없음#movieID로 그룹바이, count() 진행, orderby#topMovieIds = movies_df.groupby(&quot;movieID&quot;).count().orderBy(func.desc(&#x27;count&#x27;))topMovieIds = movies_df.groupby(&quot;movieID&quot;).count()# 딕셔너리 # key-value# 키값을 알면 value자동으로 가져옴(movietitle)def lookupName(movieID): return nameDict.value[movieID]# 사용자 정의 함수 사용할 때 쓰는 spark 문법lookupNameUDF = func.udf(lookupName)# MovieTitle을 기존 topMovieIds 데이터에 추가#컬럼 추가moviesWithNames = topMovieIds.withColumn(&quot;movietitle&quot;,lookupNameUDF(func.col(&quot;movieID&quot;)))#정렬final_df = moviesWithNames.orderBy(func.desc(&quot;count&quot;))print(final_df.show(10))#세션 종료spark.stop()","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Pyspark","slug":"Python/Pyspark","permalink":"https://jmj3047.github.io/categories/Python/Pyspark/"}],"tags":[{"name":"pyspark","slug":"pyspark","permalink":"https://jmj3047.github.io/tags/pyspark/"}],"keywords":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Pyspark","slug":"Python/Pyspark","permalink":"https://jmj3047.github.io/categories/Python/Pyspark/"}]},{"title":"Pyspark Tutorial(2)","slug":"Pyspark_Tutorial_2","date":"2022-04-20T15:00:00.000Z","updated":"2022-04-22T07:33:14.000Z","comments":true,"path":"2022/04/21/Pyspark_Tutorial_2/","link":"","permalink":"https://jmj3047.github.io/2022/04/21/Pyspark_Tutorial_2/","excerpt":"","text":"Data cleansing01.pipeline.py123456789101112131415161718192021222324from pyspark.sql import SparkSessionfrom pyspark.sql import *from pyspark.sql import functions as F#Create Spark Sessionspark = SparkSession.builder.master(&quot;local[1]&quot;).appName(&quot;MLSampleTutorial&quot;).getOrCreate()#Load Datadf = spark.read.csv(&quot;data/AA_DFW_2015_Departures_Short.csv.gz&quot;, header = True)print(&quot;file loaded&quot;)print(df.show())#remove duration = 0df = df.filter(df[3] &gt; 0) #Actual elapsed time (Minutes) 여기 컬럼 값이 0보다 작은건 보여주지 않음# df.show()#ADD ID columndf = df.withColumn(&#x27;id&#x27;,F.monotonically_increasing_id()) #id값을 자동으로 넣어줌df.write.csv(&quot;data/output.csv&quot;, mode = &#x27;overwrite&#x27;)spark.stop() 02.total_spent.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# #라이브러리 불러오기# from pyspark import SparkConf, SparkContext# #사용자 정의 함수# #main함수# def main():# conf = SparkConf.setMaster(&quot;local&quot;).setAppName(&#x27;SpentbyCustomer&#x27;)# sc = SparkContext(conf= conf)# # 파이썬 코드# # 실행코드 작성# if __name__ == &quot;__main__&quot;:# main()########## 이게 spark 기본 세팅 ##########라이브러리 불러오기from pyspark import SparkConf, SparkContext#사용자 정의 함수def extractCusPrice(line): fields = line.split(&quot;,&quot;) return(int(fields[0]), float(fields[2]))#main함수def main(): #스파크 설정 conf = SparkConf().setMaster(&quot;local&quot;).setAppName(&#x27;SpentbyCustomer&#x27;) sc = SparkContext(conf= conf) #데이터 불러오기 input = sc.textFile(&#x27;data/customer-orders.csv&#x27;) #print(&#x27;is data?&#x27;) mappedInput = input.map(extractCusPrice) #튜플 형태로 나옴 totalByCustomer = mappedInput.reduceByKey(lambda x, y : x + y) #정렬 flipped = totalByCustomer.map(lambda x: (x[1], x[0])) totalByCustomerSorted = flipped.sortByKey() results = totalByCustomerSorted.collect() for result in results: print(result) #파이썬 코드 # 실행코드 작성if __name__ == &quot;__main__&quot;: main() 03.friends_by_age.py123456789101112131415161718from pyspark import SparkConf, SparkContextconf = SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;FriendsByAge&quot;)sc = SparkContext(conf = conf)def parseLine(line): fields = line.split(&#x27;,&#x27;) age = int(fields[2]) numFriends = int(fields[3]) return (age, numFriends)lines = sc.textFile(&quot;data/fakefriends.csv&quot;)rdd = lines.map(parseLine)totalsByAge = rdd.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))averagesByAge = totalsByAge.mapValues(lambda x: x[0] / x[1])results = averagesByAge.collect()for result in results: print(result) 04.min_temp.py123456789101112131415161718192021222324252627282930313233#온도를 측정하는 프로그램 만들기from dataclasses import fieldfrom pyspark import SparkConf, SparkContextconf = SparkConf().setMaster(&#x27;local&#x27;).setAppName(&#x27;MinTemperatures&#x27;) #마스터 노드에다가 올린다sc = SparkContext(conf = conf)print(&quot;Start&quot;)def parseLine(line): fields = line.split(&quot;,&quot;) #쉼표로 다 끊어줌 -&gt; 리스트로 반환됨 stationID = fields[0] entryType = fields[2] temperature = float(fields[3]) * 0.1 * (9.0/5.0) + 32.0 return (stationID, entryType, temperature)lines = sc.textFile(&#x27;data/1800.csv&#x27;)#print(lines)parseLines = lines.map(parseLine)#print(parseLine)minTemps = parseLines.filter(lambda x: &quot;TMIN&quot; in x[1])stationTemps = minTemps.map(lambda x: (x[0],x[2]))minTemps = stationTemps.map(lambda x, y: min(x, y))results = minTemps.collect()#print(results)for result in results: print(result[0]+ &quot;\\t&#123;:.2f&#125;F&quot;.format(result[1]))","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Pyspark","slug":"Python/Pyspark","permalink":"https://jmj3047.github.io/categories/Python/Pyspark/"}],"tags":[{"name":"pyspark","slug":"pyspark","permalink":"https://jmj3047.github.io/tags/pyspark/"}],"keywords":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Pyspark","slug":"Python/Pyspark","permalink":"https://jmj3047.github.io/categories/Python/Pyspark/"}]},{"title":"Pyspark Tutorial(3)","slug":"Pyspark_Tutorial_3","date":"2022-04-20T15:00:00.000Z","updated":"2022-04-22T07:33:20.000Z","comments":true,"path":"2022/04/21/Pyspark_Tutorial_3/","link":"","permalink":"https://jmj3047.github.io/2022/04/21/Pyspark_Tutorial_3/","excerpt":"","text":"Machine Learning01.regression.py1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950from pyspark.ml.regression import DecisionTreeRegressorfrom pyspark.sql import SparkSessionfrom pyspark.ml.feature import VectorAssemblerprint(&quot;Starting Session&quot;)#세션 할당spark = SparkSession.builder.appName(&quot;DecisionTree&quot;).getOrCreate()#데이터 불러오기#StructType 과정 생략 가능data = spark.read.option(&quot;header&quot;, &quot;true&quot;).option(&quot;inferSchema&quot;, &quot;true&quot;).csv(&quot;data/realestate.csv&quot;)#print(data.show())#데이터 프레임을 행렬로 변환assembler = VectorAssembler().setInputCols([&quot;HouseAge&quot;, &quot;DistanceToMRT&quot;,&quot;NumberConvenienceStores&quot;]).setOutputCol(&quot;features&quot;) #데이터 컬럼 값 아무거나 넣어도 됨#타겟 데이터 설정df = assembler.transform(data).select(&quot;PriceofUnitArea&quot;,&quot;features&quot;)#데이터 분리trainTest = df.randomSplit([0.5,0.5])trainingDF = trainTest[0]testDF = trainTest[1]#Decision Tree 클래스 정의dtr = DecisionTreeRegressor().setFeaturesCol(&quot;features&quot;).setLabelCol(&quot;PriceofUnitArea&quot;)#모델 학습model = dtr.fit(trainingDF)#print(model)#모델 예측fullPredictions = model.transform(testDF).cache()#예측값과 label확인predictions = fullPredictions.select(&quot;prediction&quot;).rdd.map(lambda x: x[0])#실제데이터labels = fullPredictions.select(&quot;PriceofUnitArea&quot;).rdd.map(lambda x: x[0])#예측값과 label을 zip으로 묶어줌preds_label = predictions.zip(labels).collect()for prediction in preds_label: print(prediction)#세션 종료spark.stop() 02.logistic_regression.py1234567891011121314151617181920212223from pyspark.sql import SparkSessionfrom pyspark.ml.classification import LogisticRegression #Important# 세션 할당spark = SparkSession.builder.appName(&quot;AppName&quot;).getOrCreate()# load Datatraining = spark.read.format(&quot;libsvm&quot;).load(&quot;data/mllib/sample_libsvm_data.txt&quot;)print(&quot;Data loaded&quot;)# model# Scikit-Learn 문법과 비슷mlr = LogisticRegression() # Importantmlr_model = mlr.fit(training) # Important# 로지스텍 회귀, 선형 모델 .. 계수와 상수를 뽑아낼 수 있음print(&quot;Coefficients :&quot; + str(mlr_model.coefficients))print(&quot;Intercept :&quot; + str(mlr_model.intercept))spark.stop() 03.pipeline.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657from tokenize import Tokenfrom pyspark.ml import Pipelinefrom pyspark.ml.classification import LogisticRegressionfrom pyspark.ml.feature import HashingTF, Tokenizerfrom pyspark.sql import SparkSession# 세션 할당 spark = SparkSession.builder.appName(&quot;MLPipeline&quot;).getOrCreate()# 가상의 데이터 만들기training = spark.createDataFrame([ (0, &quot;a b c d e spark&quot;, 1.0), (1, &quot;b d&quot;, 0.0), (2, &quot;spark f g h&quot;, 1.0), (3, &quot;hadoop mapreduce&quot;, 0.0)], [&quot;id&quot;, &quot;text&quot;, &quot;label&quot;])# Feature Engineering# 1. Prparation# step01. 텍스트를 단어로 분리tokenizer = Tokenizer(inputCol=&#x27;text&#x27;, outputCol=&#x27;words&#x27;)# step02. 변환된 텍스트를 숫자로 변환hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=&quot;features&quot;)# step03. 모델을 가져옴lr = LogisticRegression(maxIter=5, regParam=0.01)# 2. Starting pipeplinepipeline = Pipeline(stages=[tokenizer, hashingTF, lr])# 3. Model Trainingmodel = pipeline.fit(training)# 4. Prepare test documents, which are unlabeled (id, text) tuples.test = spark.createDataFrame([ (4, &quot;spark i j k&quot;), (5, &quot;l m n&quot;), (6, &quot;spark hadoop spark&quot;), (7, &quot;apache hadoop&quot;)], [&quot;id&quot;, &quot;text&quot;])# 5. Predictionprediction = model.transform(test)selected = prediction.select(&quot;id&quot;, &quot;text&quot;, &quot;probability&quot;, &quot;prediction&quot;)for row in selected.collect(): row_id, text, prob, prediction = row #튜플 형태로 반환 print( # 문자열 포맷팅 &quot;(%d, %s) -------&gt; probability=%s, prediction=%f&quot; % (row_id, text, str(prob), prediction) )# training.show()# 세션 종료spark.stop() 04.randomforest.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960from cProfile import labelfrom pyspark.sql import SparkSession# 머신러닝 라이브러리from pyspark.ml import Pipelinefrom pyspark.ml.classification import RandomForestClassifierfrom pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexerfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator# 데이터 불러오기 spark = SparkSession.builder.appName(&quot;RandomForest&quot;).getOrCreate()data = spark.read.format(&quot;libsvm&quot;).load(&quot;data/mllib/sample_libsvm_data.txt&quot;)print(type(data))# Feature Engineering# label column labelIndexer = StringIndexer(inputCol=&#x27;label&#x27;, outputCol=&#x27;indexedLabel&#x27;).fit(data)# 범주형 데이터 체크, 인덱스화featureIndexer = VectorIndexer(inputCol=&#x27;features&#x27;, outputCol=&#x27;IndexedFeatures&#x27;, maxCategories=4).fit(data)# 데이터 분리(trainingData, testData) = data.randomSplit([0.7, 0.3])# 모델 rf = RandomForestClassifier(labelCol=&#x27;indexedLabel&#x27;, # 종속변수 featuresCol=&#x27;IndexedFeatures&#x27;, # 독립변수 numTrees=10)# outputCol=&#x27;indexedLabel&#x27; --&gt; original label로 변환labelConvereter = IndexToString(inputCol=&#x27;prediction&#x27;, outputCol=&#x27;predictedLabel&#x27;, labels=labelIndexer.labels)# 파이프라인 구축pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConvereter])# 모델 학습model = pipeline.fit(trainingData)# 모델 예측predictions = model.transform(testData)# 행에 표시할 것 추출 predictions.select(&quot;predictedLabel&quot;, &#x27;label&#x27;, &#x27;features&#x27;).show(5)# 모형 평가evaluator = MulticlassClassificationEvaluator( labelCol=&quot;indexedLabel&quot;, predictionCol=&quot;prediction&quot;, metricName=&quot;accuracy&quot;)accuracy = evaluator.evaluate(predictions)print(&quot;Test Error = %f &quot; % (1.0 - accuracy))spark.stop()","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Pyspark","slug":"Python/Pyspark","permalink":"https://jmj3047.github.io/categories/Python/Pyspark/"}],"tags":[{"name":"pyspark","slug":"pyspark","permalink":"https://jmj3047.github.io/tags/pyspark/"}],"keywords":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Pyspark","slug":"Python/Pyspark","permalink":"https://jmj3047.github.io/categories/Python/Pyspark/"}]},{"title":"How to install PySpark","slug":"install_PySpark","date":"2022-04-18T15:00:00.000Z","updated":"2022-04-22T02:53:55.000Z","comments":true,"path":"2022/04/19/install_PySpark/","link":"","permalink":"https://jmj3047.github.io/2022/04/19/install_PySpark/","excerpt":"","text":"Preparation installing spark need python3 if you are first using python, install anaconda Installing JAVA Installing file: Java SE 8 Archive Downloads (JDK 8u211 and later) Need to login Oracle Run the download file as admin → Click Next button → Changing the path on file (Space between words like Program Files can be problem during installation) Changing Path Same changes to folders in the JAVA runtime environment folder (Click ‘Change’ and modify) Create and save jre folder in the path right after the C dirve Installing Spark Installing site: https://spark.apache.org/downloads.html Download installation file After clicking Download Spark: [spark-3.2.0-bin-hadoop3.2.tgz](https://www.apache.org/dyn/closer.lua/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz), you can download it by clicking the HTTP 하단 page like picture below Installation URL: https://www.apache.org/dyn/closer.lua/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz (2022.01) Download WinRAR Program You need to install WinRAR, to unzip .tgz file. Installation file: https://www.rarlab.com/download.htm Install what fits your computer Create Spark folder and move files Moving files Copy all the file in spark-3.2.0-bin-hadoop3.2 folder After that, create spark folder below C drive and move all of them to it. Modify log4j.properties file • Open the fileconf - [log4j.properties](http://log4j.properties) Open the log file as notebook and change INFO → ERROR just like example below. During the process, all the output values can be removed. 1234567# Set everything to be logged to the console# log4j.rootCategory=INFO, consolelog4j.rootCategory=ERROR, consolelog4j.appender.console=org.apache.log4j.ConsoleAppenderlog4j.appender.console.target=System.errlog4j.appender.console.layout=org.apache.log4j.PatternLayoutlog4j.appender.console.layout.ConversionPattern=%d&#123;yy/MM/dd HH:mm:ss&#125; %p %c&#123;1&#125;: %m%n Installing winutils This time, we need program that makes local computer mistakes Sparks for Hadoop. Installing file: https://github.com/cdarlint/winutils Download winutils programs that fit installation version. I downloaded version 3.2.0 Create winutils&#x2F;bin folder on C drive and save the downloaded file. Ensure this file is authorized to be used so that it can be executed without errors whne running Spark This time, open CMD as admin and run the file If ChangeFileModeByMask error (3) occurs, create tmp\\hive folder below C drive. 12C:\\Windows\\system32&gt;cd c:\\winutils\\binc:\\winutils\\bin&gt; winutils.exe chmod 777 \\tmp\\hive Setting environment variables Set the system environment variable Click the 사용자 변수 - 새로 만들기 button on each user account Set SPARK_HOME variable Set JAVA_HOME variable Set HADOOP_HOME variable Edit PATH variable. Add the code below. Add code below %SPARK_HOME%\\bin %JAVA_HOME%\\bin Testing Spark Open CMD file, set the path as c:\\spark folder if the logo appears when input ‘spark’, success Check whether the code below works 1234&gt;&gt;&gt; rd = sc.textFile(&quot;README.md&quot;)&gt;&gt;&gt; rd.count()109&gt;&gt;&gt;","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Pyspark","slug":"Python/Pyspark","permalink":"https://jmj3047.github.io/categories/Python/Pyspark/"}],"tags":[{"name":"pyspark","slug":"pyspark","permalink":"https://jmj3047.github.io/tags/pyspark/"}],"keywords":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Pyspark","slug":"Python/Pyspark","permalink":"https://jmj3047.github.io/categories/Python/Pyspark/"}]},{"title":"Adversarial Speaker Verification","slug":"Adversaria 2d6a8","date":"2022-04-16T15:00:00.000Z","updated":"2022-10-15T08:03:42.984Z","comments":true,"path":"2022/04/17/Adversaria 2d6a8/","link":"","permalink":"https://jmj3047.github.io/2022/04/17/Adversaria%202d6a8/","excerpt":"","text":"Journal&#x2F;Conference: ICASSP IEEEYear(published year): 2019Author: Zhong Meng, Yong Zhao, Jinyu Li, Yifan GongSubject: Speaker Verification Adversarial Speaker Verification GoalWith ASV, our goal is to learn a condition-invariant and speaker-discriminative deep hidden feature in the background DNN through adversarial multi-task learning such that a noise-robust deep embedding can be obtained from these deep features for an enrolled speaker or a test utterance. Data“Hey Cortana” from the Windows 10 desktop Cortana service logs.CHiME-3: buses (BUS), in cafes (CAF), in pedestrian areas (PED), at street junctions (STR))From the clean Cortana data, we select 6 utterances from each of the 3k speakers as the enrollment data (called “Enroll A”). We select 60k utterances from 3k target speakers and 3k impostors in Cortana dataset and mix them with CHiME-3 real noise to generate the noisy evaluation set. Result Why? In ASV, a speaker classification network and a condition identification network are jointly trained to minimize the speaker classification loss and to mini-maximize the condition loss through adversarial multitask learning.The target labels of the condition network can be categorical (environment types) and continuous (SNR values). With ASV, speaker-discriminative and condition-invariant deep embeddings can be extracted for both enrollment and test speech. 적대적 학습은 [22] 논문에서 먼저 적용되었는데 이 논문과의 차이점은, 두가지 소음 컨디션을 서로 다른 방법으로 막은 것(22 논문에서는 환경 개선 보다는 unlabeled 타겟 도메인 데이터를 훈련하여 적응 시키는 걸 목표로 함), 그리고 본 논문은 네트워크에 직접적으로 음성 피처를 인풋으로 넣어 훈련하는 반면, 22 논문은 i-벡터를 인풋으로 넣었고 이는 computational한 시간과 자원이 더 들어감. ** Train ASV model to adapt to noise ** Experiment Embeddings : 인공신경망에서 원래 차원보다 저차원의 벡터로 만드는 것을 의미원래 차원은 매우 많은 범주형 변수들로 구성되어 있고 이것들이 학습방식을 통해 저차원으로 대응됨. 수천 수만개의 고차원 변수들을 몇백개의 저차원 변수로 만들어 주고, 또한 변형된 저차원 공간에서도 충분히 카테고리형 의미를 내재함.출처: 인공신경망(딥러닝)의 Embedding 이란 무엇일까? - 임베딩의 의미(1&#x2F;3) 훈련단계에서 background DNN을 화자들을 구별하기 위해 훈련 시킴. F &#x3D; {f1 ,…, fT }, ft ∈ Rrf : deep hidden featuresX &#x3D; {x1 ,…, xT}, xt ∈ Rrx , t &#x3D; {1 ,…, T} : input speech frames from training set to intermediate deep hidden featuresΘf: parameters maps input speech framesMf: the hidden layers of the background DNN as a feature extractor network with parameters Θf P(a|ft;Θy), a ∈ A : Speaker posteriors, where A is the set of all speakers in the training setΘy: maps the deep features F to the speaker posteriors.My: the upper layers of the background DNN as a speaker classifier network with parameters Θy Θf and Θy are optimized by Minimizing cross entropy loss of speaker classification. Y &#x3D; {y1 ,…, yT }, yt ∈A : sequence of speaker labels aligned with X1[.]: indicator function equals to 1 if the condition in the bracket is satisfied and 0 other wise. Categorical Condition Classification Loss: to address the conditions that are characterized as a categorical variable additional condition classification network Mc: which predicts the condition posteriors p(b| ft;Θf ); b ∈ B given the deep features F from the training setB : the set of all conditions in the training set With a sequence of condition labels C &#x3D; {c1 ,…, cT} that is aligned with X, compute the condition classification loss through cross-entropy Continuous Condition Regression Loss: an additional condition regression network Mc to predict the frame-level condition value (SNR value) compute the condition regression loss through mean-square error Deep feature F 를 condition invariant 하게 만들려면, 소음들 각각의 환경에서 나오는 피처들의 차이가 최대한 적어야 함.따라서 Mf 와 Mc 는 같이 적대적으로 train 하게 되고, Θf 가 frame-level condition loss, Lcondition 을 최대화 시키고 Θc가 Lcondition을 최소화 시키는 방향으로 감.이 둘의 경쟁은 처음에 Mc에 대한 차별성을 높여주고, speaker invariance 의 deep feature 가 Mf에 의해 만들어짐.결국 Mf가 극단적으로 Mc가 구별하지 못하는 피처를 만드는 지점에 수렴.그와 동시에 논문에서는 화자 차별적인 deep feature 들을 Lspeaker(Eq3)의 speaker classification 손실함수를 최소화 하면서 만듦. 최적의 파라미터를 찾는 식: 여기서 λ가 speaker classification 손실함수와 condition 함수 사이의 균형을 통제.GRL은 forward propagation 에서 identity transform 역할을 하며 back propagation 에서 경사도를 – λ로 곱함. Link: ADVERSARIAL SPEAKER VERIFICATION","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speaker Verification","slug":"Paper/Speaker-Verification","permalink":"https://jmj3047.github.io/categories/Paper/Speaker-Verification/"}],"tags":[{"name":"Adversarial Speaker Verification","slug":"Adversarial-Speaker-Verification","permalink":"https://jmj3047.github.io/tags/Adversarial-Speaker-Verification/"},{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speaker Verification","slug":"Paper/Speaker-Verification","permalink":"https://jmj3047.github.io/categories/Paper/Speaker-Verification/"}]},{"title":"Definition of Distance","slug":"Definition_of_Distance","date":"2022-04-16T15:00:00.000Z","updated":"2023-02-23T01:33:50.993Z","comments":true,"path":"2022/04/17/Definition_of_Distance/","link":"","permalink":"https://jmj3047.github.io/2022/04/17/Definition_of_Distance/","excerpt":"","text":"1. Euclidean distance 가장 흔히 사용하는 거리측도 대응되는 x,y값 간 차이 제곱합의 제곱근으로써, 두 관측치 사이의 직선 거리를 의미함. 다차원 데이터에서도 마찬가지 이다. 2. Manhattan Distance 맨하탄은 블럭이 나누어져 있어 직선으로 갈 수가 없다. 직선거리가 아닌 격자거리. 격자:바둑판처럼 가로세로를 일정한 간격으로 직각이 되게 짠 구조나 물건. 각 좌표의 차이의 절댓값의 합 3. Mahalanobis Distance 변수 내 분산,변수 간 공분산을 모두 반영하여 x,y,간 거리를 계산하는 방식⇒변수간 상관관계를 고려한 거리지표이다. 데이터의 공분산 행렬이 단위행렬인 경우는 유클리디안 거리와 동일함 공분산 행렬의 역행렬을 취했다는 것 → 분산이 분모에 들어간다는 뜻 → 분산이 커지면 거리가 작아지고 , 분산이 작아지면 거리가 길어짐 마할라노비스거리가 제곱근이 취해져 있기 때문에 제곱근을 없앴다. 2차원 행렬로 비유를 했을시 , 쭈욱 대입하면 아래의 식으로 나타난다 y값에 0,0 을주고 대입하면 타원의 방정식이 나온다. 유클리디안 관점에서는 중앙점과 비교했을때, A가 더 멀다. 상관관계를 고려한 마할라노비스 거리로 보면 B가 더 멀다 Reference: 한국공학대학교 강지훈교수님 강의","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}]}]}