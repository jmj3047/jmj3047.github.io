{"meta":{"title":"Jang Minjee","subtitle":"","description":"","author":"Jang Minjee","url":"https://jmj3047.github.io"},"pages":[],"posts":[{"title":"Personalized Adaptation with Pre-trained Speech Encoders for Continuous Emotion Recognition","slug":"PAPT_SER","date":"2024-06-13T15:00:00.000Z","updated":"2024-07-05T07:04:11.745Z","comments":true,"path":"2024/06/14/PAPT_SER/","link":"","permalink":"https://jmj3047.github.io/2024/06/14/PAPT_SER/","excerpt":"","text":"Journal&#x2F;Conference : INTERSPEECHYear(published year): 2023Author: Tran, M., Yin, Y., Soleymani, M.Subject: Speech Emotion Recognition, Personalization, Adaptation Personalized Adaptation with Pre-trained Speech Encoders for Continuous Emotion RecognitionAbstract To achieve unsupervised personalized emotion recognition, we first pre-train an encoder with learnable speaker embeddings in a self-supervised manner to learn robust speech representations conditioned on speakers. We propose an unsupervised method to compensate for the label distribution shifts by finding similar speakers and leveraging their label distributions from the training set. Introduction(1) What happens to the personalization gap as the number of speakers increases for fine-tuned encoders? (2) How do existing personalization methods behave when the input speech features are not fixed? (3) How can we incorporate personalization with pre-trained encoders to boost performance? We first show that as the number of speakers increases, the personalization gap (the performance difference between speaker-dependent and speaker-independent) of fine-tuned models decreases, which motivates the need for methods that adapts the pre-trained weights for personalization prior to fine-tuning. Hence, we propose to continue the pre-training process of the speech encoder jointly with speaker embeddings (see Figure 2 (a)). We also introduce a simple yet effective unsupervised personalized calibration step to adjust label distribution per speaker for better accuracy (see Figure 2 (b)). The proposed methods are unsupervised, requiring no prior knowledge of the test labels. The major contributions of this work are as follows. (1)We propose a method for personalized adaptive pre-training to adjust the existing speech encoders for a fixed set of speakers. (2) We propose an unsupervised personalized post-inference technique to adjust the label distributions. (3)We provide extensive experimental results along with an ablation study to demonstrate the effectiveness of the methods. (4) We further show that our methods can be extended to unseen speakers without the need to re-train any component, achieving superior performance compared to the baselines Related workAdaptive Pre-training In the field of speech emotion recognition, Chen et al. [10] propose a novel pseudo-label generation method in combination with task-adaptive pre-training for wav2vec2.0 [12] to boost emotion recognition accuracy. However, there is no prior work exploring personalized adaptive pre-training. Personalized Speech Emotion Recognition Most relevant to our work is the unsupervised personalized method proposed by Sridhar et al. [14], which is validated on the same dataset (MSP-Podcast) as in this paper. They propose to find speakers in the train set to form the adaptation set whose acoustic patterns closely resemble those of the speakers in the test set. Specifically, they apply Principal Component Analysis (PCA) on the feature set proposed for the computational paralinguistics challenge (ComParE) [19] and fit Gaussian Mixture Models to measure the speaker similarity based on the KL divergence metric. we explore personalization with fine-tuned encoders instead of pre-extracted features, which achieves superior performance compared to the best-performing models. For example, our weakest baseline (HuBERT-large fine-tuning) achieves a two times higher Concordance Correlation Coefficient (CCC) compared to the reported results from Sridhar et al. [14] for valence estimation. More importantly, our method is extensible and remains effective for unseen speakers without the need to re-train any components. Preliminary informationProblem Formulation Our goal is to produce a robust emotion recognition model that performs better than a model exposed to the same amount of data excluding speaker ID information. We further want our method to be extensible to new speakers outside of D. Dataset We use the MSP-Podcast corpus [13] as our dataset D. MSP-Podcast is the largest corpus for speech emotion recognition in English, containing emotionally rich podcast segments retrieved from audio-sharing websites. In this paper, we focus on arousal and valence estimation. The labels range from 1 to 7. The dataset contains pre-defined train, validation, and test sets, namely $D_{tr}, D_{val}, D_{te}$, which are subject independent. We use two versions of the dataset, namely v1.6 and v1.10, for the experiments. To be consistent with prior studies [20, 14, 9], most of our experiments are based on MSP-Podcast v1.6. We remove all the utterances marked with â€œUnknownâ€ speakers in accordance with our problem formulation. Pre-trained Speech Encoder HuBERT consists of two main components, namely a 1D CNN and a Transformer encoder [22]. The 1D CNN takes raw waveforms as inputs and returns low-level feature representations of speech. Therefore, the pre-training loss $L_{pt}$ for HuBERT can be defined as the sum of the cross-entropy loss computed over the masked frames. Personalization Gap The concept of the personalization gap is introduced and investigated. The personalization gap refers to the performance difference between speaker-dependent models (trained on data specific to individual speakers) and speaker-independent models (trained without considering speaker information). The study explores how the personalization gap changes as the number of speakers in the dataset increases. By conducting experiments with subsets of data containing different numbers of speakers, the authors demonstrate that as the dataset becomes more diverse with a larger number of speakers, the personalization gap decreases. Figure 1 demonstrates the inverse relationship between k and the performance gap. It suggests that given sufficiently large and diverse training data, the pre-trained encoders become robust enough to learn both the general emotional patterns and the unique characteristics of different groups of speech expressions such that supervised training of the model on the test speakers leads to marginal gains. Hence, to enhance the performance of the pre-trained encoders for a target speaker, we can: (1) make the input data personalized (pre-processing); (2) modify the weights of the pre-trained encoder for the target speaker; or (3) adjust the label predictions to be more personalized (post-processing). Existing studies on personalized SER, e.g., [5, 8, 14], focus on the first approach. Performance variance across speakers We investigate whether the performance variance is due to the feature shift or the label shift. Specifically, to measure the feature and label shift for each target speaker, we calculate the KL divergence between the feature and label distributions of the target speaker and those of the whole training set. Then we calculate the Pearson correlation coefficient (PCC) between the feature&#x2F;label shift and the speaker performance. For arousal estimation, we find that the PCC between the feature shift and the regression performance is âˆ’0.714 while the PCC between the label shift and performance is âˆ’0.502. The results suggest that both feature and label shifts contribute to the performance variance. Moreover, the correlation between the feature shift and label shift is 0.285, which suggests the potential of using features to detect and remove label shifts. í™”ì ê°„ ì„±ëŠ¥ ì°¨ì´ë¥¼ ë…¼ì˜í•˜ëŠ” ì„¹ì…˜ì—ì„œëŠ” ë°ì´í„° ì„¸íŠ¸ì˜ ì—¬ëŸ¬ í™”ìì—ì„œ ê´€ì°°ë˜ëŠ” ì„±ëŠ¥ì˜ ë³€ë™ì„±ì— ê¸°ì—¬í•˜ëŠ” ìš”ì¸ì„ ì‚´í´ë´…ë‹ˆë‹¤. ì´ ë§¥ë½ì—ì„œ ì–¸ê¸‰ë˜ëŠ” ë‘ ê°€ì§€ í•µì‹¬ ê°œë…ì€ feature shiftì™€ label shiftì…ë‹ˆë‹¤: feature shift: íŠ¹ì§• ì´ë™ì€ ê°œë³„ í™”ì ê°„ì˜ ì…ë ¥ íŠ¹ì§•(ì˜ˆ: ìŒì„±ì˜ ìŒí–¥ì  íŠ¹ì„±) ë¶„í¬ì˜ ì°¨ì´ ë˜ëŠ” ë³€ë™ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ìŒì„± ê°ì • ì¸ì‹ì˜ ë§¥ë½ì—ì„œ í™”ìë§ˆë‹¤ ìŒì„± íŒ¨í„´, ì–µì–‘ ë˜ëŠ” ê¸°íƒ€ ìŒí–¥ì  íŠ¹ì§•ì— ì°¨ì´ê°€ ìˆì„ ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” ëª¨ë¸ì˜ ì •í™•í•œ ê°ì • ì¸ì‹ ëŠ¥ë ¥ì— ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹ì§• í¸ì°¨ëŠ” íŠ¹ì • í™”ìì˜ ì…ë ¥ íŠ¹ì§•ì´ í›ˆë ¨ ì„¸íŠ¸ì˜ ì „ì²´ íŠ¹ì§• ë¶„í¬ì—ì„œ ì–¼ë§ˆë‚˜ ë²—ì–´ë‚˜ëŠ”ì§€ë¥¼ ì •ëŸ‰í™” í•©ë‹ˆë‹¤. label shift: ë°˜ë©´ì— ë ˆì´ë¸” ì´ë™ì€ ì—¬ëŸ¬ í™”ìì— ê±¸ì³ ìŒì„± ìƒ˜í”Œì— í• ë‹¹ëœ ê°ì • ë ˆì´ë¸” ë¶„í¬ì˜ ë¶ˆì¼ì¹˜ ë˜ëŠ” ë³€í™”ì™€ ê´€ë ¨ì´ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ë°ì´í„° ì„¸íŠ¸ì—ì„œ í™”ìë§ˆë‹¤ ê°ì • ì£¼ì„(ë ˆì´ë¸”)ì˜ ë¶„í¬ê°€ ë‹¬ë¼ì§€ëŠ” ê²ƒì„ ë°˜ì˜í•©ë‹ˆë‹¤. ë¼ë²¨ ì´ë™ì€ ê°œì¸ì´ ê°ì •ì„ í‘œí˜„í•˜ê±°ë‚˜ ì¸ì§€í•˜ëŠ” ë°©ì‹ì˜ ì°¨ì´ë¡œ ì¸í•´ ë°œìƒí•  ìˆ˜ ìˆìœ¼ë©°, í™”ì ê°„ ì¼ê´€ëœ ê°ì • ì¸ì‹ì— ë¬¸ì œë¥¼ ì¼ìœ¼í‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ë…¼ë¬¸ì€ ë°ì´í„° ì„¸íŠ¸ì˜ ê° ëŒ€ìƒ í™”ìì— ëŒ€í•œ íŠ¹ì§• ë° ë¼ë²¨ ì´ë™ì„ ë¶„ì„í•˜ì—¬ ì´ëŸ¬í•œ ì´ë™ì´ ìŒì„± ê°ì • ì¸ì‹ ì‘ì—…ì—ì„œ í™”ì ê°„ì— ê´€ì°°ë˜ëŠ” ì„±ëŠ¥ ì°¨ì´ì— ì–´ë–»ê²Œ ê¸°ì—¬í•˜ëŠ”ì§€ ì´í•´í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. íŠ¹ì§• ì´ë™, ë¼ë²¨ ì´ë™, í™”ì ì„±ëŠ¥ ê°„ì˜ ìƒê´€ê´€ê³„ëŠ” ëª¨ë¸ì´ ë‹¤ì–‘í•œ í™”ìì— ëŒ€í•´ ê°ì •ì„ ì •í™•í•˜ê²Œ ì¸ì‹í•˜ëŠ” ëŠ¥ë ¥ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ìš”ì¸ì„ íŒŒì•…í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤. Method Personalized Adaptive Pre-training (PAPT) we propose to perform adaptive pre-training on $D &#x3D; {(u_i, s_i)}^N_{i&#x3D;1}$ along with trainable speaker embeddings in a self-supervised manner. Specifically, in addition to the original speech encoder E, we train a speaker embedding network S to extract the speaker embedding $e_i &#x3D; S(s_i) âˆˆ R^d$, where d is the embedding size for the Transformer. Then, the speaker embedding $e_i$ is summed with the utterance feature $f_i &#x3D; E(u_i)$ to get a personalized feature representation $f^p_i &#x3D; f_i + e_i$. For personalized pre-training, $f^p_i$ is used to compute the pre-training loss (cross-entropy) on pseudo-label prediction for masked frames. $L_(pt) &#x3D; - \\sum\\limits_{i&#x3D;1}^{N_b} \\sum\\limits_{t&#x3D;1}^{M_i}logP(l_{it}|f_{it}^p)$ $N_b$ is the number of utterances in the batch $M_i$ is the number of masked frames for utterance $u_i$, $l_{it}$ denotes the pseudo-label for the $t$-th masked frame in utterance $u_i$. For ER downstream tasks, we reduce the temporal dimension for $f_i^p$ by mean-pooling and feed the output to a fully-connected layer to produce the label predictions. Personalized Label Distribution Calibration (PLDC) we further want to add a personalized post-inference technique to correct the predicted label distributions. Specifically, given the predictions for a target speaker, the main idea is to identify the most similar speakers from the train set based on the feature similarity and use their label distribution statistics (means and standard deviations) to calibrate the predicted label distributions of the target test speaker. In particular, for speaker s in both the train and test set, we extract the features for each utterance of s and average them to form the speaker vector $v_s &#x3D; \\frac{\\sum\\limits_{k&#x3D;1}^{N_s} \\bar{E}_{ft}^p(u_s^k)}{N_s}$ Where $E_{ft}^p$ denotes the ER-fine-tuned model of $E^p$ (the personalized adapted version of E), $\\bar{E}_{ft}^p(u_s^k)$ denotes the mean-pooled vector representation for utterance $u_s^k$ , and $N_s$ is the number of utterances from speaker s. Then, for each speaker in the test set, we retrieve the top-k most similar speakers in the train set based on the cosine similarity between the speaker vectors. Next, we average the label distribution statistics from the retrieved speakers to get an estimation of the mean $\\bar{Î¼}$ and standard deviation $\\bar{Ïƒ}$. Finally, each predicted label y for the target speaker would be shifted as $\\tilde{y} &#x3D; \\frac{y-\\mu}{\\sigma} * \\bar{\\sigma} + \\bar{\\mu}$ Î¼ and Ïƒ are the mean and standard deviation for the predicted label distribution. Experiments and DiscussionsImplementation and Training Details we perform adaptive pre-training for ten epochs using the Adam optimizer with a linear learning rate scheduler (5% warm-up and a maximum learning rate of 1eâˆ’5) on a single NVIDIA Quadro RTX8000 GPU. All other settings are identical to HuBERTâ€™s pre-training configurations. For downstream fine-tuning experiments, we add a light interpreter on top of the HuBERT encoder to process the mean-pooled extracted representations. Following prior work [14], the models are optimized with a CCC loss LCCC &#x3D; 1 âˆ’ CCC for arousal and valence estimation All of our experiments are performed with the HuBERT-large architecture, except for the personalization gap experiments, as the model used to generate the pseudo-labels for HuBERT-base is not publicly available. We report two evaluation metrics, namely the Overall CCC (O-CCC), which concatenates the predictions on all test speakers before computing a single CCC score for the test set, and A-CCC, which denotes the average CCC scores computed for each test speaker. Baselines We compare our method to three baselines: (1) Vanilla-FT in which E is fine-tuned on Dtr. (2) B2 represents the data weighting method proposed by Sridhar et al. [14]. (3) Task-Adaptive Pre-Training (TAPT) in which encoder E is continued pre-training on D for ten epochs. Experimental Results on test-b Compared to the best-performing baselines, our methods achieve superior performance on both arousal and valence estimation, with a gain of 0.023 and 0.009 on arousal and valence A-CCC respectively. Notably, we achieve state-of-the-art results for the task of valence estimation, in which our Overall-CCC score achieves 0.665. We attribute this to the high variance in the number of utterances of each speaker in the test set. Furthermore, Table 2 also demonstrates that PLDC consistently achieves the best performance when we only perform Ïƒ shifting, while Î¼ shifting often reduces both A-CCC and O-CCC. Ablation Study Table 4 shows the experimental results for arousal estimation on test-b of fine-tuned encoders (without PLDC) adaptively pre-trained with different fusion positions of the speaker embeddings. In particular, Last refers to our proposed setting in which the speaker embeddings are added to the output of the Transformer encoder; First refers to speaker embeddings being added to the inputs of the first layer of the Transformer encoder, and Prefix refers to the setting in which the speaker embeddings are concatenated as prefixes to the inputs of the Transformer encoder. None refers to the vanilla HuBERT encoder. We find that Last provides the best results. Conclusion In this paper, we propose two methods to adapt pre-trained speech encoders for personalized speech emotion recognition, namely PAPT, which jointly pre-trains speech encoders with speaker embeddings to produce personalized speech representations, and PLDC, which performs distribution calibration for the predicted labels based on retrieved similar speakers. We validate the effectiveness of the proposed techniques via extensive experiments on the MSP-Podcast dataset, in which our models consistently outperform strong baselines and reach state-of-the-art performance for valence estimation.","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Few Shot Learning","slug":"Paper/Few-Shot-Learning","permalink":"https://jmj3047.github.io/categories/Paper/Few-Shot-Learning/"}],"tags":[{"name":"Speech Emotion Recognition","slug":"Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/tags/Speech-Emotion-Recognition/"},{"name":"Personalization","slug":"Personalization","permalink":"https://jmj3047.github.io/tags/Personalization/"},{"name":"Adaptation","slug":"Adaptation","permalink":"https://jmj3047.github.io/tags/Adaptation/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Few Shot Learning","slug":"Paper/Few-Shot-Learning","permalink":"https://jmj3047.github.io/categories/Paper/Few-Shot-Learning/"}]},{"title":"Few Shot Learning Guided by Emotion Distance for Cross-corpus Speech Emotion Recognition","slug":"EMOFSL_for_cross_corpus_SER","date":"2024-04-07T15:00:00.000Z","updated":"2024-04-22T11:19:18.904Z","comments":true,"path":"2024/04/08/EMOFSL_for_cross_corpus_SER/","link":"","permalink":"https://jmj3047.github.io/2024/04/08/EMOFSL_for_cross_corpus_SER/","excerpt":"","text":"Journal&#x2F;Conference: Asia Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC) (pp. 1008-1012). IEEE.Year(published year): 2023Author: Yue, P., Wu, Y., Qu, L., Zheng, S., Zhao, S., &amp; Li, T.Subject: Few-shot Learning, Speech Emotion Recognition, Metric Learning Few Shot Learning Guided by Emotion Distance for Cross-corpus Speech Emotion RecognitionIntroduction Two main types of emotion classification systems have been widely used in emotion research and applications, namely discrete emotion categories and continuous emotion. Discrete emotion theories propose that there are a number of emotion categories, such as anger, fear, happiness, sadness, disgust and surprise, that are biologically based and universally recognizable by facial expressions and physiological responses [6][7][8]. Dimensional models of emotion suggest that emotions can be described along a few continuous dimensions, such as valence (positive-negative), arousal (high-low) and dominance (active-passive) [9][10][11]. ê°ì • ì—°êµ¬ì™€ ì‘ìš© ë¶„ì•¼ì—ì„œ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ë‘ ê°€ì§€ ì£¼ìš” ìœ í˜•ì˜ ê°ì • ë¶„ë¥˜ ì²´ê³„ëŠ” Discrete ê°ì • ë²”ì£¼ì™€ Dimensional ê°ì •ì…ë‹ˆë‹¤. Discrete ê°ì •ì—ì„œëŠ” ë¶„ë…¸, ê³µí¬, í–‰ë³µ, ìŠ¬í”” í˜ì˜¤, ë†€ë¼ì›€ê³¼ ê°™ì€ ì—¬ëŸ¬ ê°€ì§€ ê°ì • ë²”ì£¼ê°€ ìˆìœ¼ë©°, ì´ëŠ” ìƒë¬¼í•™ì— ê·¼ê±°í•˜ì—¬ ì–¼êµ´ í‘œì •ê³¼ ìƒë¦¬ì  ë°˜ì‘ì— ì˜í•´ ë³´í¸ì ìœ¼ë¡œ ì¸ì‹ë  ìˆ˜ ìˆë‹¤ê³  ì œì•ˆí•©ë‹ˆë‹¤. Dimensional ê°ì • ëª¨ë¸ì€ valence(pos-neg), arousal(high-low), dominance(active-passive)ì™€ ê°™ì€ ëª‡ê°€ì§€ ì—°ì†ì ì¸ ì°¨ì›ì„ ë”°ë¼ ê°ì •ì„ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤ê³  ì œì•ˆí•©ë‹ˆë‹¤. In recent years, there have been articles verifying that discrete emotion labeling and continuous emotion labeling can complement each other, and better emotion recognition performance can be achieved through multi-task learning[12][13][14][15]. The logic behind these articles is that both discrete and continuous emotion annotation information can provide a basic emotion feature extraction capability for emotion recognition models, and the information provided by the two is complementary. Especially, continuous emotion annotations can provide useful information for discrete emotion recognition. ì§€ë‚œ ëª‡ë…„ ë™ì•ˆ ë‘ ê°ì • ì²´ê³„ë“¤ì˜ ë¼ë²¨ë§ì´ ì„œë¡œ ë³´ì™„ëœë‹¤ëŠ” ë…¼ë¬¸ë“¤ì´ ë°œí‘œ ë˜ì—ˆê³ , multi task learningì— ë†’ì€ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ì˜€ìŠµë‹ˆë‹¤. Continuous ê°ì • ì£¼ì„ì€ discrete ê°ì • ì¸ì‹ì— ìœ ìš©í•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤. Another problem with traditional cross-corpus SER methods is their weak ability in speech representation because they use traditional speech features mostly coming from openSMILE toolkit or low-level speech descriptor instead of state-of-the-art unsupervised pre-trained speech features. Inspired by the above series of studies on discrete versus continuous emotions, this paper quantifies the distance between emotion categories through their distribution in continuous emotion space, and uses this as prior knowledge in discrete emotion category learning. For instance, the Euclidean distance between the two emotions of happy and sad is the farthest, because they are relatively far apart in the two dimensions of valence and arousal. According to the distribution, one can divide the distance between pairs of emotion categories into several levels. The emotion distance guides the metric loss construction in fewshot learning to learn more meaningful and generalizable representations of emotions that are consistent across domains. The contributions of this paper are as follows: 1) This paper introduces emotion distance for the first time to the cross corpus SER task. 2) This paper adopts a self-supervised speech feature extractor instead of traditional features for cross-corpus SER. 3) The proposed few-shot learning guided by emotion distance method achieves good cross-corpus SER performance. Method Figure 2 shows the fine-tuning and testing stage of the baseline and emotion distance-guided few-shot learning, from which one can also see how the emotional distance knowledge is used in the construction of the metric loss. Some general notation assumptions of this paper are as follows. x represents a speech sample and F represents the speech emotion feature extractor. F(x) is the emotion embedding of sample x, and cen represents the shot average embedding of a certain emotion in the support set. The parameters of the SER model can be represented as W and the loss function is L(W). A. Baseline transfer learning based on few-shot learning The baseline method consists of three steps: 1) Training a feature extractor on a large source corpus using a self-supervised contrastive learning objective. 2) Fine-tuning a classifier on a few labeled examples from the target corpus using the extracted features. 3) Testing the SER model on the target corpus. During the fine-tuning stage on the target corpus, the training loss of basic transfer learning (BTL) only contains the CE loss related to emotion labels, while the few-shot learning based transfer learning (FSTL) also takes into account the relationship between categories, as shown in the â€˜metric lossâ€™ in Figure 2. The SNN is trained by creating positive and negative pairs of inputs, where positive pairs belong to the same class and negative pairs belong to different classes. The network uses a contrastive loss to minimize the distance between similar inputs and maximize the distance between dissimilar inputs: where Xc is the set of data belonging to class c,Xcâ€² is the set of data belonging to class câ€² different than c, d is the Euclidean distance between sample pair embeddings and Îº determines the trade-off between penalizing dissimilarity between samples belonging to the same class against similarity between samples belonging to different classes. BTLì€ ê¸°ë³¸ì ì¸ FSLê³¼ CE lossë¥¼ ì‚¬ìš©í•˜ì—¬ finetuning, FSTLì€ SNNì€ ì‚¬ìš©í•˜ì—¬ Negative pair, Positive pairë¡œ ë‚˜ëˆˆ inputìœ¼ë¡œ í›ˆë ¨ë¨ â†’ contrasive lossë¥¼ ì‚¬ìš©í•˜ëŠ”ë° ë¹„ìŠ·í•œ inputì€ ì„œë¡œ ë¶™ê³ , ë‹¤ë¥¸ inputì€ ë©€ë¦¬ ë–¨ì–´ëœ¨ë ¤ ë†“ìŒ B. Few-shot learning guided by emotion distance Inspired by the distribution of emotion categories in the valence-arousal continuous emotion space, this study proposes few-shot learning based on a fine-grained emotion relationship metric. ë³¸ ì—°êµ¬ì—ì„œëŠ” valence-arousal ì—°ì† ê°ì • ê³µê°„ì˜ ê°ì • ë²”ì£¼ ë¶„í¬ì—ì„œ ì˜ê°ì„ ì–»ì–´ ì„¸ë¶„í™”ëœ ê°ì • ê´€ê³„ ë©”íŠ¸ë¦­ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ few-shot learning í•™ìŠµì„ ì œì•ˆ As shown in Figure 1, in the continuous emotion space, the {happy, sad} sample pair has the farthest Euclidean distance. In addition, in the learning of speech emotion representation, the representation ability of emotion representation in the arousal dimension is stronger than that of the valence dimension, so when we consider the distance between emotion categories, the distance in the valence dimension is compressed, which leads to the emotion distance between {anger, sadness} sample pair being as far as that between {happy, sad} sample pair. These two types of emotion sample pairs are considered to be pure negative (PN) pairs. {í™”ë‚¨, ìŠ¬í””} ê·¸ë¦¬ê³  {í–‰ë³µ, ìŠ¬í””}ì˜ ë‘ ìŒì„ PN ìŒì´ë¼ê³  ê°„ì£¼ For pairs of samples with the same emotion category, we believe that the distance between them should be as close as possible and they are pure positive (PP) pairs. For other emotion sample pairs, such as {happy, neutral} and {happy, angry}, we consider the distance between them to be at an intermediate level, not as far away as PN pairs, and we call them medium negative (MN) sample pairs. ë™ì¼í•œ ê°ì • ì¹´í…Œê³ ë¦¬ë¥¼ ê°€ì§„ ìƒ˜í”Œ ìŒì˜ ê²½ìš° ê·¸ ì‚¬ì´ ê±°ë¦¬ëŠ” ê°€ëŠ¥í•œí•œ ê°€ê¹Œì›Œì•¼ í•˜ë©° PP ìŒìœ¼ë¡œ ê°„ì£¼. {í–‰ë³µ, ì¤‘ë¦½}, {í–‰ë³µ, ë¶„ë…¸}ì™€ ê°™ì€ ë‹¤ë¥¸ ê°ì • ìƒ˜í”Œ ìŒì˜ ê²½ìš° PN ìŒë§Œí¼ ê±°ë¦¬ê°€ ë©€ì§€ ì•Šë‹¤ê³  ê°„ì£¼í•˜ì—¬ ì´ë¥¼ MN ì´ë¼ê³  ê°„ì£¼. Following the above discussion of emotion distance, we construct a fine-grained metric loss function for few-shot learning guided by emotion distance (EMOFSL): where Î± &gt; Î² and represent the restricted distance margin of the PN pairs and the MN pairs, respectively, and Îº1 and Îº2 determine the trade-off among different parts of the loss. ì—¬ê¸°ì„œ Î±ëŠ” Î²ë³´ë‹¤ í¬ë©°, PNê³¼ MNì˜ ì œí•œëœ ê±°ë¦¬ ë§ˆì§„ì„ ë‚˜íƒ€ëƒ„. ê·¸ë¦¬ê³  Îº1ê³¼ Îº2ëŠ” ì†ì‹¤ì˜ ë‹¤ë¥¸ ë¶€ë¶„ì˜ íŠ¸ë ˆì´ë“œ ì˜¤í”„ë¥¼ ê²°ì •. ì œí•œëœ ê±°ë¦¬ ë§ˆì§„: ì œê³µí•œ ë¬¸ì¥ì˜ ë§¥ë½ì—ì„œ PN ìŒê³¼ MN ìŒì— ëŒ€í•œ â€˜ì œí•œëœ ê±°ë¦¬ ì—¬ë°±â€™ì€ ì„œë¡œ ë‹¤ë¥¸ ê°ì • ì¹´í…Œê³ ë¦¬ì˜ ì„ë² ë”© ê°„ì— í—ˆìš©ë˜ëŠ” ê±°ë¦¬ ë˜ëŠ” ê°„ê²©ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. íŠ¹ì • ì—¬ë°±(PN ìŒì˜ ê²½ìš° Î±, MN ìŒì˜ ê²½ìš° Î²)ì„ ì„¤ì •í•¨ìœ¼ë¡œì¨ ëª¨ë¸ì€ ì„œë¡œ ë‹¤ë¥¸ ê°ì • ì¹´í…Œê³ ë¦¬ì˜ ì„ë² ë”©ì´ íŠ¹ì§• ê³µê°„ì—ì„œ ì–¼ë§ˆë‚˜ ë©€ë¦¬ ë–¨ì–´ì ¸ ìˆì–´ì•¼ í•˜ëŠ”ì§€ì— ëŒ€í•œ ì œì•½ì„ ì ìš©í•©ë‹ˆë‹¤. Î±ì™€ Î²: Î±ì™€ Î²ëŠ” ë‹¤ì–‘í•œ ìœ í˜•ì˜ ê°ì • ìƒ˜í”Œ ìŒì— ëŒ€í•œ ì œí•œëœ ê±°ë¦¬ ë§ˆì§„ì„ ì •ì˜í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ë§¤ê°œë³€ìˆ˜ì…ë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ Î±ëŠ” ìˆœìˆ˜ ìŒìˆ˜(PN) ìŒì˜ ì œí•œëœ ê±°ë¦¬ ë§ˆì§„ì„ ë‚˜íƒ€ë‚´ê³  Î²ëŠ” ì¤‘ê°„ ìŒìˆ˜(MN) ìŒì˜ ì œí•œëœ ê±°ë¦¬ ë§ˆì§„ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. Î± &gt; Î² ì¡°ê±´ì€ PN ìŒì˜ ë§ˆì§„ì´ MN ìŒì˜ ë§ˆì§„ë³´ë‹¤ í¬ë‹¤ëŠ” ê²ƒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. Îº1 ë° Îº2: Îº1ê³¼ Îº2ëŠ” ì†ì‹¤ í•¨ìˆ˜ì˜ ì—¬ëŸ¬ ë¶€ë¶„ ê°„ì˜ íŠ¸ë ˆì´ë“œì˜¤í”„ ë˜ëŠ” ê· í˜•ì„ ê²°ì •í•˜ëŠ” ê³„ìˆ˜ì…ë‹ˆë‹¤. ì´ ê³„ìˆ˜ëŠ” í•™ìŠµ ê³¼ì •ì—ì„œ ì†ì‹¤ í•¨ìˆ˜ì˜ ì—¬ëŸ¬ êµ¬ì„± ìš”ì†Œì˜ ì¤‘ìš”ë„ ë˜ëŠ” ì˜í–¥ì„ ì œì–´í•©ë‹ˆë‹¤. Îº1ê³¼ Îº2ë¥¼ ì¡°ì •í•¨ìœ¼ë¡œì¨ ëª¨ë¸ì€ ì›í•˜ëŠ” í•™ìŠµ ëª©í‘œì— ë”°ë¼ ì†ì‹¤ í•¨ìˆ˜ì˜ íŠ¹ì • ì¸¡ë©´ì„ ë‹¤ë¥¸ ì¸¡ë©´ë³´ë‹¤ ìš°ì„ ìˆœìœ„ë¥¼ ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì†ì‹¤ì˜ ë‹¤ë¥¸ ë¶€ë¶„ ê°„ì˜ íŠ¸ë ˆì´ë“œ ì˜¤í”„: ì´ ë¬¸ì¥ì€ Îº1ê³¼ Îº2ê°€ í›ˆë ¨ ì¤‘ì— ì†ì‹¤ í•¨ìˆ˜ì˜ ê°€ì¤‘ì¹˜ì™€ ê· í˜•ì„ ê²°ì •í•˜ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•œë‹¤ëŠ” ì ì„ ê°•ì¡°í•©ë‹ˆë‹¤. Îº1ê³¼ Îº2ë¥¼ ì ì ˆí•˜ê²Œ ì„¤ì •í•˜ë©´ ëª¨ë¸ì€ ì „ì²´ ì†ì‹¤ì— ëŒ€í•œ ë‹¤ì–‘í•œ êµ¬ì„± ìš”ì†Œ(ì˜ˆ: PN ë° MN ìŒì˜ ê±°ë¦¬ ë§ˆì§„)ì˜ ê¸°ì—¬ë„ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ê´€ë¦¬í•˜ì—¬ í•™ìŠµ ê³¼ì •ê³¼ ê°ì • ë²”ì£¼ ê°„ì˜ ê´€ê³„ë¥¼ í¬ì°©í•˜ëŠ” ëª¨ë¸ì˜ ëŠ¥ë ¥ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. EXPERIMENTAL RESULTS AND DISCUSSIONSA. Experimental setup In this section, we evaluate the proposed framework for cross-corpus SER. Three most common emotion corporacontaining English speech, including IEMOCAP (I) [21], RAVDESS (R) [22], and MSP-IMPROV (M) [23], are employed in our experiments. Two of the above corpora are randomly selected as the source and target corpora respectively, and six groups of cross-corpus SER tasks (source corpus-target corpus) I-R, I-M, R-M, R-I, M-I, M-R, are conducted. We select four common emotional categories, i.e., anger, neutral, happy, and sad, in our experiments. ì „ì´ í•™ìŠµ ê³¼ì •ì—ì„œëŠ” ì‹¤í—˜ ê²°ê³¼ì˜ ì‹ ë¢°ì„±ì„ í™•ì¸í•˜ê¸°ìœ„í•´ ê° íƒ€ê²Ÿ ì½”í¼ìŠ¤ ë§ˆë‹¤ ë‹¤ë¥¸ ì„¤ì •ìœ¼ë¡œ êµì°¨ê²€ì¦(CV)ì„ ì‹¤ì‹œí–ˆìŠµë‹ˆë‹¤. IEMOCAPì˜ ê²½ìš°, í•œ ì„¸ì…˜ì„ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë°ì´í„°ì…‹ì„ ë¶„í• í–ˆìŠµë‹ˆë‹¤. RAVDESSì˜ ê²½ìš° í™”ìì˜ idì— ë”°ë¼ ë°ì´í„°ì…‹ì„ 6ê°œì˜ ë™ì¼í•œ í•˜ìœ„ì§‘í•©ìœ¼ë¡œ ë¶„í• í•œ í›„ í•˜ë‚˜ì˜ ì§‘í•©ì„ í…ŒìŠ¤íŠ¸ ì§‘í•©ìœ¼ë¡œ, ë‚˜ë¨¸ì§€ë¥¼ í•™ìŠµë°ì´í„°ë¡œ ì„ íƒí–ˆìŠµë‹ˆë‹¤. MSPì˜ ê²½ìš° 6 fold leave-one-speaker-out CVë¥¼ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤(ê° í´ë“œ ë‚´ì—ì„œ â€˜í•œ ëª…ì˜ í™”ìë§Œ ë‚¨ê²¨ë‘ê¸°â€™ ì „ëµì´ ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤. ì¦‰, ê° í´ë“œë§ˆë‹¤ í•œ í™”ìì˜ ë°ì´í„°ëŠ” í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ë‚¨ê²¨ë‘ê³  ë‚˜ë¨¸ì§€ í™”ìì˜ ë°ì´í„°ëŠ” ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.) For few-shot learning based transfer learning, 5 random fixed samples of each emotion in the training set of the target corpus are selected for fine-tuning, and all of the samples in the testing set are used for testing. The training set and the testing set are obtained from the cross-validation settings. We compared the proposed EMOFSL with basic transfer learning(BTL) and few shot learning based transfer learning(FSTL) in terms of unweighted accuracy assessing the SER performance. B. Results and discussions Table II shows the cross-corpus SER performance of the compared methods, in which 5 samples from each of the 4 emotions are selected to form the support set in the fine-tuning process. Few-shot learning method guided by emotion distance (EMOFSL) is compared with basic transfer learning (BTL) and few-shot learning based transfer learning (FSTL) in terms of unweighted accuracy rate (UAR). It can be observed that our proposed EMOFSL performs better than the other two baseline methods, which proves the effectiveness of emotion category distance prior knowledge for cross-corpus SER task. It can be also observed that in most cross-corpus tasks, the few-shot learning-based transfer learning performs better than the basic transfer learning, which verifies that metric loss obtained from the positive and negative sample pairs helps the learning of speech emotion on the target corpus. Figure 3 presents the confusion matrices of the proposed EMOFSL method for six cross-corpus tasks. One can observe that the recognition results of anger and sad in all tasks are generally better than other emotions, indicating that these two emotions in speech are easier to be recognized, which is partly related to the prior knowledge of the emotional distance we added to the metric loss. Figure 4 shows the confusion matrix comparison between the proposed method and the baseline methods. We can see that with metric loss representing the distance relationship of different emotion categories, FSTL and EMOFSL achieve better recognition on anger and sad. Figure 3 and 4 also indicate that happy is often misidentified as anger and neutral is often misidentified as sad, which is consistent with our previous understanding in the continuous emotion space that arousal is easier to distinguish than valence in speech emotion expression. Figure 5 shows the performance comparison of BTL, FSTL, and EMOFSL when the size of support set k is 5, 10,and 15, respectively. The source corpus of the cross-corpus task is RAVDESS, and the target corpus is IEMOCAP. One can observe that: 1) As the number of training samples in the support set increases, the SER performance of the three methods improves accordingly. 2) In the case of different support set sizes, the performance of EMOFSL is better than the other comparison methods. 3) The performance of FSTL is better than that of BTL in most cases. CONCLUSION Cross-corpus speech emotion recognition faces the problem of sparse target set data. To solve this problem, this study introduces the prior knowledge of emotion distance to guide the few-shot learning process of cross-corpus emotion recognition, thereby alleviating the problem of insufficient emotional information to a certain extent. Experimental results show that the proposed method performs better than traditional few-shot learning verifying the efficiency of the emotion distance prior knowledge. In addition, this paper introduces a self-supervised pre-training model with stronger speech representation ability than traditional features as a feature extractor, which also improves the performance of cross-corpus speech emotion recognition.","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Few Shot Learning","slug":"Paper/Few-Shot-Learning","permalink":"https://jmj3047.github.io/categories/Paper/Few-Shot-Learning/"}],"tags":[{"name":"Speech Emotion Recognition","slug":"Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/tags/Speech-Emotion-Recognition/"},{"name":"Few Shot Learning","slug":"Few-Shot-Learning","permalink":"https://jmj3047.github.io/tags/Few-Shot-Learning/"},{"name":"Metric Learning","slug":"Metric-Learning","permalink":"https://jmj3047.github.io/tags/Metric-Learning/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Few Shot Learning","slug":"Paper/Few-Shot-Learning","permalink":"https://jmj3047.github.io/categories/Paper/Few-Shot-Learning/"}]},{"title":"Transductive learning VS Inductive Learning","slug":"Transductive_Inductive","date":"2024-02-19T15:00:00.000Z","updated":"2024-02-20T10:46:59.840Z","comments":true,"path":"2024/02/20/Transductive_Inductive/","link":"","permalink":"https://jmj3047.github.io/2024/02/20/Transductive_Inductive/","excerpt":"","text":"Inductive Learning í•™ìŠµ ë°ì´í„°ë¥¼ ì´ìš©í•˜ì—¬ í•™ìŠµí•œ íŒ¨í„´ì´ë‚˜ ì§€ì‹ì„ ìŠµë“í•˜ì—¬ ë³´ì§€ ëª»í•œ ë°ì´í„°(í‰ê°€ ë°ì´í„°)ë“¤ì— ëŒ€í•´ì„œì˜ˆì¸¡í•˜ê¸° ìœ„í•œ ë°©ë²• ëª¨ë¸ì„ ì¼ë°˜í™”í•˜ì—¬ ì˜ˆì¸¡ì„ ì˜ í•  ìˆ˜ ìˆê²Œ í•˜ëŠ” ë°©ë²• supervised learning í•™ìŠµëœ ëª¨ë¸ì´ ì´ì „ì— ë³´ì§€ ëª»í•œ ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•´ ì¼ë°˜í™” í•  ìˆ˜ ìˆë„ë¡ í•™ìŠµí•˜ëŠ” ê²ƒì„ ì˜ë¯¸ ì£¼ì–´ì§„ í›ˆë ¨ ë°ì´í„°ì—ì„œ ì¼ë°˜ì ì¸ ê·œì¹™ì´ë‚˜ íŒ¨í„´ì„ ì¶”ì¶œí•˜ì—¬ ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì„ ëª©í‘œ ë¶„ë¥˜ ë¬¸ì œì—ì„œ ê³ ì–‘ì´ì™€ ê°œì˜ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµí•œ í›„ ìƒˆë¡œìš´ ì´ë¯¸ì§€ê°€ ê³ ì–‘ì´ì¸ì§€ ê°œì¸ì§€ ë¶„ë¥˜ í•˜ëŠ” ê²ƒ induction is reasoning from observed training cases to general rules, which are then applied to the test cases. ëª¨ë¸ì„ ì„¤ê³„í•œ ë’¤ ì˜¤ì§ training dataë§Œì„ ì‚¬ìš©í•˜ì—¬ ë¶„ë¥˜ í˜¹ì€ íšŒê·€ë¥¼ ìœ„í•œ ê·œì¹™(rule)ì„ ìŠ¤ìŠ¤ë¡œ ì¶”ë¡  ê´€ì¸¡ëœ ë°ì´í„°ëŠ” training dataset ë¿ì´ë©° í•™ìŠµì„ í†µí•´ training datasetë¶„í¬ì— ìµœì í™”ëœ ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ ê³„ì‚°, ì´í›„ í•™ìŠµì´ ëë‚œë’¤ unlabeled testing datasetì˜ labelì„ ì¶”ë¡ í•˜ëŠ” ê²ƒ ì´ë¯¸ ë ˆì´ë¸”ë§ëœ í›ˆë ¨ ë°ì´í„° ì…‹ì„ í™œìš©í•˜ì—¬ í•™ìŠµì„ ì§„í–‰, ì´ë ‡ê²Œ êµ¬ì¶•ëœ ëª¨ë¸ì€ í›ˆë ¨ ë°ì´í„° ì…‹ì—ì„œ ë³¸ ì ì´ ì—†ëŠ” í…ŒìŠ¤íŠ¸ ë°ì´í„° ì…‹ì˜ ë ˆì´ë¸”ì„ ì˜ˆì¸¡í•˜ëŠ”ë° ì‚¬ìš© Transductive Learning ì£¼ì–´ì§„ ë°ì´í„°ì— ëŒ€í•´ ì˜ˆì¸¡í•˜ë„ë¡ ëª¨ë¸ì„ êµ¬ì¶• ê¸°ì¡´ì— í•™ìŠµëœ ëª¨ë¸ì„ ê°€ì§€ê³  ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ìƒˆë¡œìš´ ë°ì´í„°ê°€ ë“¤ì–´ì˜¤ë©´ ê·¸ê²ƒì„ í•¨ê»˜ ë°˜ì˜í•´ ë‹¤ì‹œ ëª¨ë¸ì„ êµ¬ì°©í•  ìˆ˜ ìˆìŒ semi supervised learning í›ˆë ¨ ë°ì´í„°ì™€ ë™ì‹œì— í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ê³ ë ¤í•˜ì—¬ í•™ìŠµí•˜ëŠ” ë°©ë²• í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í•¨ê»˜ ê³ ë ¤í•˜ì—¬ ëª¨ë¸ì„ êµ¬ì¶•í•˜ê³  íŠ¹ì • í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ìˆ˜í–‰ ë ˆì´ë¸”ì´ ì—†ëŠ” ë°ì´í„°ì— ëŒ€í•´ ë ˆì´ë¸”ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒ In logic, statistical inference, and supervised learning, Transduction is reasoning from observed specific (training) cases to specific (test) cases. ì‚¬ì „ì— ë¯¸ë¦¬ training dataset ë¿ë§Œ ì•„ë‹ˆë¼ testing datasetë„ ì•Œê³  ìˆëŠ” ìƒíƒœì´ë©°, testing datsetì˜ labelì„ ì•Œì§€ëŠ” ëª»í•˜ì§€ë§Œ í•™ìŠµì´ ì§„í–‰ë˜ëŠ” ë™ì•ˆ labeled data (training data)ì˜ íŠ¹ì§• ê³µìœ  í˜¹ì€ ì „íŒŒ, ë°ì´í„° ê°„ì˜ ì—°ê´€ì„±, íŠ¹ì§• íŒ¨í„´ ë“± ì¶”ê°€ì ì¸ ì •ë³´ë¥¼ í™œìš©í•¨ìœ¼ë¡œì¨ testing datasetì˜ labelì„ ì¶”ë¡  í›ˆë ¨ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë ˆì´ë¸”ì˜ ìœ ë¬´ë¡œ ë‚˜ëˆ„ì§€ ì•Šê³  train test splitìœ¼ë¡œ ë ˆì´ë¸”ì´ ìˆë˜ ì—†ë˜ ëª¨ë‘ ì‚¬ìš©í•˜ê³  ëœë¤ìœ¼ë¡œ í›ˆë ¨&#x2F;í…ŒìŠ¤íŠ¸ ì…‹ì„ ë‚˜ëˆ”, ê·¸ë¦¬ê³  ë ˆì´ë¸”ì´ ì—†ëŠ” ë°ì´í„°ì—ì„œ ë¹„ì§€ë„í•™ìŠµê³¼ ê°™ì´ ìˆ¨ì–´ ìˆëŠ” íŒ¨í„´ì„ ì¶”ì¶œí•˜ë„ë¡ í•™ìŠµ Reference https://www.inflearn.com/questions/896304/transductive-learning-amp-inductive-learning Yones, C., Georgina Stegmayer, and Diego H. Milone. â€œGenome-wide pre-miRNA discovery from few labeled examples.â€ Bioinformatics 34.4 (2018): 541-549 https://dodonam.tistory.com/476 https:&#x2F;&#x2F;velog.io&#x2F;@kimdyun&#x2F;Inductive-Transductive-Learning-ì°¨ì´ì  https://blog.naver.com/song_gina&#x2F;222149366893 https://newsight.tistory.com/313 https://dos-tacos.github.io/translation/transductive-learning/","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Few Shot Learning","slug":"Paper/Few-Shot-Learning","permalink":"https://jmj3047.github.io/categories/Paper/Few-Shot-Learning/"}],"tags":[{"name":"Transductive Learning","slug":"Transductive-Learning","permalink":"https://jmj3047.github.io/tags/Transductive-Learning/"},{"name":"Inductive Learning","slug":"Inductive-Learning","permalink":"https://jmj3047.github.io/tags/Inductive-Learning/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Few Shot Learning","slug":"Paper/Few-Shot-Learning","permalink":"https://jmj3047.github.io/categories/Paper/Few-Shot-Learning/"}]},{"title":"Meta Transfer Learning for Few Shot Learning","slug":"MTL_for_FSL","date":"2024-01-28T15:00:00.000Z","updated":"2024-02-20T10:46:01.565Z","comments":true,"path":"2024/01/29/MTL_for_FSL/","link":"","permalink":"https://jmj3047.github.io/2024/01/29/MTL_for_FSL/","excerpt":"","text":"Meta Learning â€œLearn to Learnâ€: ìƒˆë¡œìš´ í…ŒìŠ¤í¬ë¥¼ ë” ë¹¨ë¦¬ í•™ìŠµ í•˜ê¸° ìœ„í•´ ì´ì „ì˜ í•™ìŠµ ê²½í—˜ì„ ì ê·¹ì ìœ¼ë¡œ í™œìš©í•œë‹¤. í•µì‹¬ì•„ì´ë””ì–´: í•™ìŠµ ì—ì´ì „íŠ¸ê°€ ë‹¨ìˆœíˆ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ìì‹ ì˜ í•™ìŠµ ëŠ¥ë ¥ì„ ìŠ¤ìŠ¤ë¡œ í–¥ìƒ ì‹œí‚¨ë‹¤ â†’ ì´ í•™ìŠµ ë°©ë²•ì€ ì¼ì¢…ì˜ inductive biasë¼ê³  ë³¼ ìˆ˜ ìˆìŒ. í•™ìŠµí•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•œë‹¤ëŠ” ê²ƒ ë”¥ëŸ¬ë‹ì—ì„œëŠ” íŒŒë¼ë¯¸í„° ì´ˆê¸°í™” ì‹œ, ë¬´ì‘ìœ„ë¡œ ì´ˆê¸°í™”í•¨ â†’ ì´ ìƒˆë¡œìš´ í…ŒìŠ¤í¬ë¥¼ ë¹ ë¥´ê²Œ í•™ìŠµí•˜ëŠ” â€œì¢‹ì€ ì´ˆê¸°í™”â€ë¥¼ í•™ìŠµí•  ìˆ˜ ì—†ë‚˜? ex) MAML(Model-Agnostic Meta Learning) Algorithm â‡’ ì¢‹ì€ ì´ˆê¸°í™”ë¼ëŠ” í•™ìŠµ ë°©ë²•ì„ í•™ìŠµ $\\therefore$ ìƒˆë¡œìš´ íƒœìŠ¤í¬ì— ëŒ€í•œ ë¹ ë¥¸ í•™ìŠµ ê°€ëŠ¥ ie, MAMLë¡œ í•™ìŠµí•œ ë©”íƒ€ ëŸ¬ë‹ ëª¨ë¸ì€ ìƒˆë¡œìš´ íƒœìŠ¤í¬ê°€ ì£¼ì–´ì¡Œì„ ë•Œ ì´ë¥¼ ë¹ ë¥´ê²Œ í•™ìŠµí• ìˆ˜ ìˆëŠ” â€˜ì¢‹ì€ ì´ˆê¸°í™”â€™ì—ì„œ ì‹œì‘í•˜ì—¬ ì†ŒëŸ‰ì˜ ë°ì´í„°ì™€ ì‘ì€ ê²½ì‚¬í•˜ê°•ë²•ë§Œìœ¼ë¡œ ë¹ ë¥¸ í•™ìŠµì´ ê°€ëŠ¥ Multi Task Learningê³¼ ì°¨ì´ì  Multi task learning: í•˜ë‚˜ì˜ ëª¨ë¸ì´ ì—¬ëŸ¬ê°€ì§€ task í•™ìŠµ â‡’ í•™ìŠµí•œ task ë“¤ê³¼ ê°™ì€ ì—¬ëŸ¬ task ìˆ˜í–‰ Meta learning: ì—¬ëŸ¬ task í•™ìŠµ ê·¸ëŸ¬ë‚˜ í•™ìŠµí•˜ëŠ” ê±¸ í•™ìŠµ â‡’ ìƒˆë¡œìš´ taskê°€ ì£¼ì–´ì¡Œì„ ë•Œ ë¹ ë¥´ê²Œ í•™ìŠµ Few shot Learning Few shot learningì„ ì˜ í•˜ê¸° ìœ„í•´ ë‚˜ì˜¬ ë°©ë²•ë¡ ì´ Meta Learning nê°œì˜ í›ˆë ¨ dataë¥¼ ë³´ì—¬ì£¼ëŠ” ê²ƒì„ few shot, 1ê°œì˜ í›ˆë ¨ dataë¥¼ ë³´ì—¬ì£¼ëŠ” ê²ƒì´ 1 shot n way k shot: n(í´ë˜ìŠ¤ì˜ ê°œìˆ˜), k(í´ë˜ìŠ¤ë³„ data ê°œìˆ˜) 2 class 3 shot â†’ 2 way 3 shot ë…¼ë¬¸ì—ì„œëŠ” 5 way 1 shot, 20 way 5shotì´ ì„±ëŠ¥ í‰ê°€ ë°©ë²•ìœ¼ë¡œ ë§ì´ ì‚¬ìš©ë¨ Meta learning, Few shot learningì˜ ëŒ€í‘œì ì¸ ì ‘ê·¼ ë°©ë²• Metric Based Learning(ê±°ë¦¬ í•™ìŠµ ê¸°ë°˜): íš¨ìœ¨ì ì¸ ê±°ë¦¬ ì¸¡ì • í•™ìŠµì´ í•µì‹¬ support set, query set ê°„ì˜ ìœ ì‚¬ë„ ì¸¡ì • ë°©ë²•(ìœ í´ë¦¬ë””ì•ˆìœ¼ë¡œ ê±°ë¦¬ ì¸¡ì •) ëŒ€í‘œì ì¸ ì•Œê³ ë¦¬ì¦˜: Siamese Network, Prototypical Network, Relation Network ëª¨ë¸ì€ ì£¼ì–´ì§„ ì„œí¬íŠ¸ ë°ì´í„°ë¥¼ feature spaceì— ë‚˜íƒ€ë‚´ì–´ ì¸¡ì •ì„ ë½‘ì•„ëƒ„ ê°™ì€ í´ë˜ìŠ¤ë©´ ê°€ê¹ê²Œ, ë©€ë©´ ë©€ê²Œ ë¶„ë¦¬ Model Based Learning(ëª¨ë¸ í•™ìŠµ ê¸°ë°˜): ë©”ëª¨ë¦¬ë¥¼ ì´ìš©í•œ ìˆœí™˜ ì‹ ê²½ë§ì´ í•µì‹¬ ì ì€ ìˆ˜ì˜ í•™ìŠµë‹¨ê³„ë¡œë„ íŒŒë¼ë¯¸í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆëŠ”ì§€ ëª¨ë¸ì— ë³„ë„ memoryë¥¼ ë‘ì–´ í•™ìŠµì†ë„ë¥¼ ì¡°ì ˆ MANN(Memory Augmented NN) â†’ ì™¸ë¶€ ë©”ëª¨ë¦¬ ê³µìœ  Optimizer Learning(ìµœì í™” í•™ìŠµ ë°©ì‹): ëª¨ë¸ íŒŒë¼ë¯¸í„° ìµœì í™”ê°€ í•µì‹¬ MAML: ê¸°ìˆ ì—­ì „íŒŒ â†’ í° scaleì˜ dataë¥¼ ìœ„í•œ ì„¤ê³„ ì‹¤ì„  ê° Taskì˜ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ í•©ì‚°í•˜ì—¬ ëª¨ë¸ì„ ì—…ë°ì´íŠ¸ 1,2,3ì˜ ë°ì´í„°ì—ì„œ í•™ìŠµëœ ê·¸ë˜ë””ì–¸íŠ¸ ì •ë³´ë¡œ ì „ë°˜ì ì¸ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•¨ ì ì„  ê³µí†µ íŒŒë¼ë¯¸í„°ë¡œë¶€í„° ë‹¤ì‹œ ëª¨ë¸ì´ í•™ìŠµí•˜ë©´ì„œ ì„¸ë¶€ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ â‡’ ì´ ê³¼ì •ì„ ìµœì ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ì„ë•Œê¹Œì§€ ë°˜ë³µí•˜ë©´ ëª¨ë¸ ìµœì ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ì„ ìˆ˜ ìˆìŒ â‡’ ìµœì ì˜ ì´ˆê¸° íŒŒë¼ë¯¸í„° ê°’ì„ ì„¤ì •í•˜ëŠ” ê±¸ ë°°ìš°ëŠ” ë°©ë²• Reptile, Meta-SGD ğŸ’¡ Few shot Learningì˜ ë°©ë²•ìœ¼ë¡œ ê³ ì•ˆëœ ê²ƒì´ Meta learning, Transfer learning(fine tuning) Self Supervised Learningê³¼ì˜ ì°¨ì´ì  SSL Transfer learningì˜ í•œ ì¢…ë¥˜ pre trained ëª¨ë¸ì„ ì¤‘ì‹¬ìœ¼ë¡œ í•™ìŠµ, ì†ŒëŸ‰ì˜ ë°ì´í„°ë¡œ ì¬í•™ìŠµ transfer learningì—ì„œ multi task learningì„ í• ë•Œ pretrained ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¨ ë’¤ì— ê°ê°ì˜ taskì— ë§ê²Œ fine tuning í•¨. transfer learning Meta Learning ì—¬ëŸ¬ê°œì˜ taskë¥¼ ë™ì‹œì— í•™ìŠµ &amp; ê° task ê°„ì˜ ì°¨ì´ë¡œ í•™ìŠµ(Meta parameter) ì „ì²´ í•™ìŠµ ì´í›„ ì†ŒëŸ‰ì˜ ë°ì´í„°(few-shot)ìœ¼ë¡œë„ ì¶”ë¡ í•  ìˆ˜ ìˆëŠ” ë²”ìš©ì ì¸ ëª¨ë¸ ìƒì„± meta learning Episode Training ëª¨ë“  classë¥¼ í™œìš©í•˜ì§€ ì•ŠìŒ ê¸°ì¡´ ë°©ì‹ A B C D E Train 80 80 80 80 80 Test 20 20 20 20 20 Episode ë°©ì‹ Task 1: A,B í´ë˜ìŠ¤ ë¶„ë¥˜ê¸° Task 2: C, D í´ë˜ìŠ¤ ë¶„ë¥˜ê¸° Task 3: D, E í´ë˜ìŠ¤ ë¶„ë¥˜ê¸° â‡’ ì´ëŸ¬ê³  ë‚˜ì„œ ì™„ì „íˆ ìƒˆë¡œìš´ ë°ì´í„°ë¡œ ë¶„ë¥˜ ì„±ëŠ¥ í™•ì¸ í•™ìŠµ ë°©ë²• train - test data split: ì „ì²´ í•™ìŠµ ë°ì´í„°ë¥¼ meta-train, meta-test(meta-trainì— ë“±ì¥í•˜ì§€ ì•Šì€ ì™„ì „íˆ ìƒˆë¡œìš´ class) ë‚˜ëˆ” task sampling: meta tarin ë°ì´í„° ì…‹ì„ ê° task dataë¡œ ìª¼ê°¬. ì „ì²´ class ì¤‘ ì¼ë¶€ class ë°ì´í„°ê°€ task1(episode)ì— sampling support-query dataset split: ê° task ë³„ ë°ì´í„°ë“¤ì„ ë‹¤ì‹œ suppor set(training set), query set(test set)ìœ¼ë¡œ ìƒ˜í”Œë§ (ê¸°ì¡´ì˜ DL ë°©ì‹ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ ì´ ë‘˜ì˜ dataëŠ” ê²¹ì¹˜ì§€ ì•ŠìŒ) Task Training: ê°ê°ì˜ taskë¡œ í•™ìŠµì„ ì§„í–‰í•˜ë©° ëª¨ë¸ì„ ìƒì„±í•¨ meta test evaluation: ìƒì„± ëª¨ë¸ì— meta testì˜ support setìœ¼ë¡œ ìƒˆë¡œìš´ ì´ë¯¸ì§€ classë¥¼ í•™ìŠµ ì‹œí‚¤ê³  ìµœì¢…ì ìœ¼ë¡œ meta test query ì…‹ì„ ë¶„ë¥˜í•´ë‚´ëŠ” ê²ƒì´ ëª©ì  Goal: í•™ìŠµì— í™œìš©ë˜ì§€ ì•ŠëŠ” í´ë˜ìŠ¤ì˜ ë°ì´í„°(meta-test)ì—ì„œë„ ì¼ë¶€ meta test support dataë¡œ í›ˆë ¨í•œ ë’¤ meat test query ë°ì´í„°ë¥¼ êµ¬ë¶„í• ìˆ˜ ìˆëŠ”ê°€ â‡’ ì—¬ê¸°ì„œ ë‹¤ì–‘í•œ í•™ìŠµ ë°©ë²•ì´ ê³ ì•ˆë¨. í¬ê²Œ 3ê°€ì§€ í•™ìŠµ ê¸°ë²•ìœ¼ë£¨ ë¶„ë¥˜ë¨","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Few Shot Learning","slug":"Paper/Few-Shot-Learning","permalink":"https://jmj3047.github.io/categories/Paper/Few-Shot-Learning/"}],"tags":[{"name":"Meta Transfer Learning","slug":"Meta-Transfer-Learning","permalink":"https://jmj3047.github.io/tags/Meta-Transfer-Learning/"},{"name":"Few Shot Learning","slug":"Few-Shot-Learning","permalink":"https://jmj3047.github.io/tags/Few-Shot-Learning/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Few Shot Learning","slug":"Paper/Few-Shot-Learning","permalink":"https://jmj3047.github.io/categories/Paper/Few-Shot-Learning/"}]},{"title":"Speaker to Emotion, Domain Adaptation for Speech Emotion Recognition with Residual Adapters","slug":"Speaker_to_Emotion","date":"2023-11-06T15:00:00.000Z","updated":"2023-11-12T14:07:28.067Z","comments":true,"path":"2023/11/07/Speaker_to_Emotion/","link":"","permalink":"https://jmj3047.github.io/2023/11/07/Speaker_to_Emotion/","excerpt":"","text":"Journal&#x2F;Conference : Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)Year(published year): 2019Author: Yuxuan Xi, Pengcheng Li, Yan Song, Yiheng Jiang, Lirong DaiSubject: Domain Adaptation, Speech Emotion Recognition Speaker to Emotion: Domain Adaptation for Speech Emotion Recognition with Residual Adapters Summary The paper proposes a new method for domain adaptation in speech emotion recognition using residual adapters. The proposed method transfers information from a speaker corpus to an emotion corpus, resulting in significant improvements in SER performance. The paper demonstrates the effectiveness of the proposed method through experiments and shows that domain-agnostic parameters learned by VoxCeleb2 are necessary for effective domain adaptation in SER. IntroductionRecently, deep learning based systems have achieved significant progress for SER, but to be successful, sufficient labeled data is needed, particularly due to the complexity of emotional information. However, existing corpora, such as IEMOCAP [15], CHEAVD [14], FAU-AIBO [29], and EMODB [30], are generally size-limited, in part due to annotation cost, and also suffer label ambiguity. One possible solution is to utilize emotion information from multiple corpora. Based on this approach, several transfer learning and multi-task learning (MTL) based methods have been proposed [1], [2], [3], [4]. Transfer learning focuses on adapting knowledge from available auxiliary resources to the target domain. However, due to the limited size of emotion corpora, SER performance is far from satisfactory and it is still difficult to apply successful deep learning architectures like ResNet and DenseNet to further improve performance. Based on this view, we propose a domain adaptive model which can utilize a common representation between emotion and speaker identity to further improve SER accuracy, using ResNet as a backbone architecture. Specifically, the proposed method aims to tackle the lack of labeled corpus by employing a residual adapter model [12] to transfer the information from VoxCeleb to a specific SER target dataset. The residual adapter resembles ResNet [10], with the major difference that all convolutional layers are replaced by adapter modules. In this paper, the residual adapter model is trained using VoxCeleb2 data with speaker labels, then emotion corpora are used to train the domain-specific parameters, and different fully-connected layers are used to predict the classification score, as shown in Fig.1. The main difference lies in that the proposed residual adapter utilizes supervised learning to exploit the relationship between speaker and emotion data. To prove the effectiveness of our method, we first use ResNet that is trained with emotion data only as a baseline system, and then conduct a series of experiments as shown in Fig.1, including: (1) A ResNet trained by VoxCeleb2 data as the feature extractor, then the classifier trained for SER. (2) The same ResNet fine-tuned with emotion data â€“ the common practice in transfer learning. (3) The proposed residual adapter method, furthermore testing the adapter module alone, aiming to demonstrate that features learned from the speaker classification task can be beneficial to SER. Overview of Speaker-To-Emotion Domain Adaptation FrameworkSER encompasses some existing problems. (1) Deep learning based methods have become prevalent in recent years [20], [21], [22], owing to the powerful representation learning ability of neural networks. In general, increasing network depth benefits performance, but the limited scale of emotion corpora greatly restricts the network complexity in practice. (2) Existing methods mainly focus on cross-corpus learning among emotion corpora, but due to the difficulty and cost of labeling emotion data, cross-corpus methods still have limitations. Speaker-labeled corpora are potential choices, as described in Section I, where speaker characteristics such as age and gender can influence SER results. This fact indicates that there is some shared representation between speaker characteristics and emotion. On the other hand, the scale of speaker corpora are much larger than those for emotion, a fact that aids in training a deep neural network. Based on those motivations, the VoxCeleb2 [13] corpus is selected in this paper for initial model training. In order to utilize speaker labels, a complex network is first trained by speaker corpora, and then adapted to the target emotion corpora. The first method is a feature extractor, which constructs an emotion classifier by retraining the topmost FC layer. The second method is finetuning, which takes the same structure as the feature extractor, then all network parameters as well as the FC classifier, are fine-tuned using the emotion corpora. Although these twomethods can exploit the information from both speaker and emotion data, they have some obvious problems. Firstly, the network parameters are pre-trained for speaker verification, which may be quite different from SER, therefore directly utilizing the model may not be a appropriate choice. Secondly, due to the limited scale of the target emotion corpora, training the whole network may be difficult and may cause an overfittingproblem. On the other hand, the fine-tuning stage may result in forgetting the source corpora, reducing the benefit of the auxiliary information. ì²« ë²ˆì§¸ ë°©ë²•ì€ íŠ¹ì§• ì¶”ì¶œê¸°ì…ë‹ˆë‹¤. ì´ ë°©ë²•ì€ í™”ì ì½”í¼ìŠ¤ ëŒ€í•´ í›ˆë ¨ëœ ì‚¬ì „ í›ˆë ¨ëœ ë³µì¡í•œ ë„¤íŠ¸ì›Œí¬ë¥¼ ê°€ì ¸ì™€ ìµœìƒìœ„ ì™„ì „ ì—°ê²°(FC) ê³„ì¸µë§Œ ì¬í›ˆë ¨í•˜ì—¬ ê°ì • ë¶„ë¥˜ê¸°ë¥¼ êµ¬ì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. FC ë ˆì´ì–´ëŠ” ì¶”ì¶œëœ íŠ¹ì§•ì„ ì¶œë ¥ í´ë˜ìŠ¤ì— ë§¤í•‘í•˜ëŠ” ë„¤íŠ¸ì›Œí¬ì˜ ë§ˆì§€ë§‰ ë ˆì´ì–´ì…ë‹ˆë‹¤. ì´ ë ˆì´ì–´ë§Œ ì¬í•™ìŠµí•˜ë©´ í™”ì ì½”í¼ìŠ¤ì˜ ì •ë³´ë¥¼ ê·¸ëŒ€ë¡œ í™œìš©í•˜ë©´ì„œ ë„¤íŠ¸ì›Œí¬ë¥¼ ëª©í‘œ ê°ì • ì½”í¼ìŠ¤ì— ë§ê²Œ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ë°©ë²•ì€ íŠ¹ì§• ì¶”ì¶œê¸°ì™€ ë™ì¼í•œ ë³µì¡í•œ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°ë¥¼ ì·¨í•˜ê³  ê°ì • ì½”í¼ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ FC ë¶„ë¥˜ê¸°ë¥¼ í¬í•¨í•œ ëª¨ë“  ë„¤íŠ¸ì›Œí¬ íŒŒë¼ë¯¸í„°ë¥¼ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ë¯¸ì„¸ ì¡°ì •ì„ í†µí•´ ë„¤íŠ¸ì›Œí¬ê°€ íŠ¹ì§• ì¶”ì¶œê¸° ë°©ì‹ë³´ë‹¤ ë” ê´‘ë²”ìœ„í•˜ê²Œ ëŒ€ìƒ ê°ì • ì½”í¼ìŠ¤ì— ì ì‘í•  ìˆ˜ ìˆì§€ë§Œ, ëŒ€ìƒ ê°ì • ì½”í¼ìŠ¤ì˜ ì œí•œëœ ê·œëª¨ì— ê³¼ì í•©í•˜ê³  í™”ì ì½”í¼ìŠ¤ì˜ ì •ë³´ë¥¼ ìŠì–´ë²„ë¦´ ìœ„í—˜ì´ ìˆìŠµë‹ˆë‹¤. To address these issues, this paper attempts to establish the third method, a new domain adaptation. In this method, the deep learning model is first trained using VoxCeleb2 data with speaker labels as usual. But during the adaptation stage, some extra emotion-specific parameters are added to the original model, then the emotion corpora are utilized to only fine-tune the additional parameters which coexist alongside the previously trained parameters. Through the proposed framework, the information forgetting problem is avoided, and because the emotion corpora is only utilized to fine-tune a part of the network, the over-fitting problem may be mitigated. ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì„¸ ë²ˆì§¸ ë°©ë²•ì¸ ìƒˆë¡œìš´ ë„ë©”ì¸ ì ì‘ ë°©ì‹ì„ ì‹œë„ í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì—ì„œëŠ” ë¨¼ì € í‰ì†Œì™€ ê°™ì´ í™”ì ë ˆì´ë¸”ì´ ìˆëŠ” VoxCeleb2 ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ í›ˆë ¨í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì ì‘ë‹¨ê³„ ì—ì„œëŠ” ì›ë˜ ëª¨ë¸ì— ëª‡ ê°€ì§€ ì¶”ê°€ ê°ì • ê´€ë ¨ íŒŒë¼ë¯¸í„°ë¥¼ ë”í•œë‹¤ìŒ, ê°ì • ì½”í¼ìŠ¤ë¥¼ í™œìš©í•˜ì—¬ ì´ì „ì— í•™ìŠµëœ íŒŒë¼ë¯¸í„°ë¥¼ í•¨ê»˜ ê³µì¡´í•˜ëŠ” ì¶”ê°€ íŒŒë¼ë¯¸í„°ë§Œ ë¯¸ì„¸ì¡°ì • í•©ë‹ˆë‹¤. ì œì•ˆëœ í”„ë ˆì„ì›Œí¬ë¥¼ í†µí•´ ì •ë³´ ë§ê° ë¬¸ì œë¥¼ í”¼í•  ìˆ˜ ìˆìœ¼ë©° ê°ì • ì½”í¼ìŠ¤ ë„¤íŠ¸ì›Œí¬ì˜ ì¼ë¶€ë¶„ë§Œ ë¯¸ì„¸ì¡°ì • í•˜ëŠ”ë° í™œìš©í•˜ê¸° ë•Œë¬¸ì— ê³¼ì í•© ë¬¸ì œë¥¼ ì™„í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Residual Adapter ModelModel DesignThe basic idea of constructing an adapter module is to linearly parameterize the convolutional filter group, which is the same as introducing an intermediate convolutional layer. ê¸°ë³¸ ì•„ì´ë””ì–´ëŠ” ì»¨ë³¼ë£¨ì…˜ í•„í„° ê·¸ë£¹ì„ ì„ í˜•ì ìœ¼ë¡œ íŒŒë¼ë¯¸í„°í™”í•˜ëŠ” ê²ƒìœ¼ë¡œ ì´ëŠ” ì¤‘ê°„ ì»¨ë³¼ë£¨ì…˜ ê³„ì¸µì„ ë„ì…í•˜ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤. For the training progress, firstly the model is trained on the initial task, using a large corpus. Next, the parameters are fixed and other domain adapters are trained using the target domain corpus. ì´ ë¬¸ì¥ì€ ì”ì—¬ ì–´ëŒ‘í„°ë¥¼ ì‚¬ìš©í•œ ë„ë©”ì¸ ì ì‘ì„ ìœ„í•œ í›ˆë ¨ ê³¼ì •ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì²« ë²ˆì§¸ ë‹¨ê³„ëŠ” ì´ˆê¸° ì‘ì—…(ì´ ê²½ìš° í™”ì ê²€ì¦)ì„ ìœ„í•´ ëŒ€ê·œëª¨ ì½”í¼ìŠ¤ì—ì„œ ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ í›ˆë ¨í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ëª¨ë¸ì´ í•™ìŠµë˜ë©´ íŒŒë¼ë¯¸í„°ê°€ ê³ ì •ë˜ê³  ëª©í‘œ ë„ë©”ì¸ ë§ë­‰ì¹˜(ì´ ê²½ìš° ê°ì • ì½”í¼ìŠ¤)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¶”ê°€ ë„ë©”ì¸ ì–´ëŒ‘í„°ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤. ë„ë©”ì¸ ì–´ëŒ‘í„°ë¥¼ ì›ë˜ ëª¨ë¸ì— ì¶”ê°€í•˜ê³  ëŒ€ìƒ ë„ë©”ì¸ ì½”í¼ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¯¸ì„¸ ì¡°ì •í•˜ì—¬ ëª¨ë¸ì„ ìƒˆ ë„ë©”ì¸ì— ë§ê²Œ ì¡°ì •í•©ë‹ˆë‹¤. ì œì•ˆí•œ ë°©ë²•ì€ ì›ë˜ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ê³ ì •í•˜ê³  ì¶”ê°€ ë„ë©”ì¸ ì–´ëŒ‘í„°ë§Œ ë¯¸ì„¸ ì¡°ì •í•¨ìœ¼ë¡œì¨ ì „ì²´ ë„¤íŠ¸ì›Œí¬ë¥¼ ë¯¸ì„¸ ì¡°ì •í•  ë•Œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ê³¼ì í•© ë° ì •ë³´ ë§ê° ë¬¸ì œë¥¼ ë°©ì§€í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. Adapter module and network structure The residual adapter model is constructed on the ResNet20 model with a network structure outlined in Table I. EXPERIMENTS AND ANALYSISData description and pre-processingIn this study, both speaker data and emotion data are utilized. For speaker-labeled data, we choose the VoxCeleb2 corpus [13]. VoxCeleb2 is a large-scale speaker-labeled database, prevalent for SV tasks, that was collected from more than 6000 celebrities on YouTube. VoxCeleb2 consists of 2442 hours, with more than a million speech utterances, covering different ages, genders, accents and scenes. For the emotion part, we select Interactive Emotional Dyadic Motion Capture (IEMOCAP) [15] and Chinese Natural Audio-Visual Emotion Database (CHEAVD) [14] 2.0 databases. IEMOCAP has scripted and improvised parts, depending on the recording scenarios. We choose the improvised data part in order to exclude undesired contextual information. Labels of neutral, angry, happy and sad are used. CHEAVD 2.0 is a Chinese emotion corpus, the official data of the Multimodal Emotion Recognition Challenge (MEC) 2017. CHEAVD contains data selected from Chinese movies, soap operas and TV shows. It contains 8 emotion labels (angry, happy, sad, worried, anxious, surprise, disgust, neutral). The corpus is divided into training, validation and testing sets. We use the training&#x2F;validation split for performance evaluation, the hyper-parameter tuning is based on validation set, keeping the evaluation that same as in [25]. Magnitude spectrograms are utilized as input features, with the spectrograms extracted over 40 ms Hamming windows with a 10 ms window shift and 1600 FFT points. Then 0-4000 Hz spectrogram are utilized since human vocal expression is mainly located in this frequency range. The speech utterances are cut into 2 s portions with 1 s overlap, and zero-padding applied for utterances shorter than 2 s. Thus the input spectrograms have a size of 400 200. For each spectrogram, we then apply a $\\mu$-law expansion, as used and described in our previous paper [18]. ìœˆë„ìš° ì‹œí”„íŠ¸ê°€ 10ë°€ë¦¬ì´ˆì¸ 40ë°€ë¦¬ì´ˆ í•´ë° ìœˆë„ìš°ì™€ 1600ê°œì˜ FFT í¬ì¸íŠ¸ë¡œ ì¶”ì¶œí•œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ì„ ì…ë ¥ í”¼ì³ë¡œ í™œìš©í•©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ì‚¬ëŒì˜ ìŒì„± í‘œí˜„ì´ ì£¼ë¡œ ì´ ì£¼íŒŒìˆ˜ ë²”ìœ„ì— ìœ„ì¹˜í•˜ê¸° ë•Œë¬¸ì— 0-4000Hz ìŠ¤í™íŠ¸ë¡œê·¸ë¨ì„ í™œìš©í•©ë‹ˆë‹¤. ìŒì„± ë°œí™”ëŠ” 1ì´ˆê°€ ê²¹ì¹˜ëŠ” 2ì´ˆ ë¶€ë¶„ìœ¼ë¡œ ì˜ë¦¬ê³ , 2ì´ˆë³´ë‹¤ ì§§ì€ ë°œí™”ì— ëŒ€í•´ì„œëŠ” ì œë¡œ íŒ¨ë”©ì„ ì ìš©í•˜ì—¬ ì…ë ¥ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ì˜ í¬ê¸°ëŠ” 400Ã—200ì…ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ê° ìŠ¤í™íŠ¸ë¡œê·¸ë¨ì— ëŒ€í•´ ì´ì „ ë…¼ë¬¸ [18]ì—ì„œ ì‚¬ìš© ë° ì„¤ëª…í•œ ëŒ€ë¡œ Î¼ ë²•ì¹™ í™•ì¥ì„ ì ìš©í•©ë‹ˆë‹¤. ì§„í­ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ì€ ì‹œê°„ì— ë”°ë¼ ë³€í™”í•˜ëŠ” ì‹ í˜¸ ì£¼íŒŒìˆ˜ì˜ ì§„í­ì„ ì‹œê°ì ìœ¼ë¡œ í‘œí˜„í•œ ê²ƒì…ë‹ˆë‹¤. ì´ëŠ” ì‹œê°„ì— ë”°ë¥¸ ì‹ í˜¸ì˜ ì£¼íŒŒìˆ˜ ë‚´ìš©ì„ ë¶„ì„í•˜ëŠ” ë°©ë²•ì¸ ì‹ í˜¸ì˜ ë‹¨ì‹œê°„ í‘¸ë¦¬ì— ë³€í™˜(STFT)ì˜ í¬ê¸°ë¥¼ êµ¬í•˜ì—¬ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. STFTëŠ” ì‹ í˜¸ë¥¼ ì§§ê²Œ ê²¹ì¹˜ëŠ” ìœˆë„ìš°ë¡œ ë‚˜ëˆ„ê³  ê° ìœˆë„ìš°ì— ìœˆë„ìš° í•¨ìˆ˜ë¥¼ ì ìš©í•œ ë‹¤ìŒ ê° ìœˆë„ìš°ì˜ í‘¸ë¦¬ì— ë³€í™˜ì„ ì·¨í•˜ì—¬ ê³„ì‚°í•©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ê²°ê³¼ ë³µì†Œìˆ˜ ê°’ì˜ STFT ê³„ìˆ˜ì˜ í¬ê¸°ë¥¼ ì·¨í•˜ì—¬ í¬ê¸° ìŠ¤í™íŠ¸ë¡œê·¸ë¨ì„ ì–»ìŠµë‹ˆë‹¤. í¬ê¸° ìŠ¤í™íŠ¸ë¡œê·¸ë¨ì€ ì‹œê°„ ê²½ê³¼ì— ë”°ë¥¸ ì‹ í˜¸ì˜ ì£¼íŒŒìˆ˜ ë‚´ìš©ì„ ê°„ê²°í•˜ê³  ìœ ìµí•˜ê²Œ í‘œí˜„í•˜ê¸° ë•Œë¬¸ì— ìŒì„± ì²˜ë¦¬ ë° ê¸°íƒ€ ì‹ í˜¸ ì²˜ë¦¬ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” íŠ¹ì§• í‘œí˜„ì…ë‹ˆë‹¤. For VoxCeleb2 data, we randomly choose 50 speakers to train the ResNet20 with adapters. For IEMOCAP improvised data, we conduct a 5-fold cross-validation, where 4 sections are used to train the network and the remaining 2 speakers are used as validation and test data. Experiment setup Baseline: We use IEMOCAP and CHEAVD to train a plain ResNet20 with results in the top row of Table II. Obviously, emotion data is insufficient to train a ResNet, so UA, WA are unsatisfactory for IEMOCAP and CHEAVD, in line with our expectations. Fine-tuning: We use VoxCeleb2 data to pre-train a plain ResNet20 then, after training, the FC layer of the network is replaced and the whole network fine-tuned by the target emotion corpus. The result is not significantly better than baseline, likely to be because the number of parameters is too large for the smaller extent of emotion data to train. On the other hand, forgetting the learned speaker information may be another problem which would reduce the accuracy. VoxCeleb2 ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¼ë°˜ ResNet20ì„ ì‚¬ì „ í›ˆë ¨í•œ ë‹¤ìŒ, í›ˆë ¨ í›„ ë„¤íŠ¸ì›Œí¬ì˜ FC ê³„ì¸µì„ êµì²´í•˜ê³  ì „ì²´ ë„¤íŠ¸ì›Œí¬ë¥¼ ëª©í‘œ ê°ì • ì½”í¼ìŠ¤ë¡œ ë¯¸ì„¸ ì¡°ì •í•©ë‹ˆë‹¤. ê²°ê³¼ëŠ” ê¸°ì¤€ì„ ë³´ë‹¤ í¬ê²Œ ë‚˜ì•„ì§€ì§€ ì•ŠëŠ”ë°, ì´ëŠ” í›ˆë ¨í•  ê°ì • ë°ì´í„°ì˜ ë²”ìœ„ê°€ ì‘ì•„ ë§¤ê°œë³€ìˆ˜ ìˆ˜ê°€ ë„ˆë¬´ ë§ê¸° ë•Œë¬¸ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤. ë°˜ë©´ì— í•™ìŠµëœ í™”ì ì •ë³´ë¥¼ ìŠì–´ë²„ë¦¬ëŠ” ê²ƒë„ ì •í™•ë„ë¥¼ ë–¨ì–´ëœ¨ë¦¬ëŠ” ë˜ ë‹¤ë¥¸ ë¬¸ì œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Feature extractor: When fixing the parameters learned by the primary domain, the network becomes a feature extractor. In this experiment we fix all ResNet20 parameters and train the FC layer with emotion corpora. The performance is worse than the fine-tuning method, this indicates utilizing only speaker information is not appropriate for SER. ê¸°ë³¸ ë„ë©”ì¸ì—ì„œ í•™ìŠµí•œ íŒŒë¼ë¯¸í„°ë¥¼ ìˆ˜ì •í•˜ë©´ ë„¤íŠ¸ì›Œí¬ëŠ” íŠ¹ì§• ì¶”ì¶œê¸°ê°€ ë©ë‹ˆë‹¤. ì´ ì‹¤í—˜ì—ì„œëŠ” ëª¨ë“  ResNet20 íŒŒë¼ë¯¸í„°ë¥¼ ìˆ˜ì •í•˜ê³  ê°ì • ë§ë­‰ì¹˜ë¡œ FC ë ˆì´ì–´ë¥¼ í›ˆë ¨í•©ë‹ˆë‹¤. ë¯¸ì„¸ ì¡°ì • ë°©ì‹ë³´ë‹¤ ì„±ëŠ¥ì´ ë–¨ì–´ì§€ëŠ”ë°, ì´ëŠ” í™”ì ì •ë³´ë§Œ í™œìš©í•˜ëŠ” ê²ƒì´ SERì— ì í•©í•˜ì§€ ì•Šë‹¤ëŠ” ê²ƒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. Residual Adapter: We next evaluate the residual adapter model. We use VoxCeleb2 data to train the same ResNet20 with adapter modules. During the adapting process, all of the parameters of the 3 3 filters are fixed, then the adapters are trained using emotion data. The result significantly outperforms the baseline system, especially for IEMOCAP, where the UA and WA achieve 67.58% and 72.73%. On CHEAVD they achieve 34.08% and 43.96%. We attempted to increase the number of speakers during residual adapter training, but the performance did not benefit from this, perhaps because a more complex model is needed. ë‹¤ìŒìœ¼ë¡œ ì”ì—¬ ì–´ëŒ‘í„° ëª¨ë¸ì„ í‰ê°€í•©ë‹ˆë‹¤. VoxCeleb2 ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì–´ëŒ‘í„° ëª¨ë“ˆë¡œ ë™ì¼í•œ ResNet20ì„ í›ˆë ¨í•©ë‹ˆë‹¤. ì ì‘ ê³¼ì •ì—ì„œ 3ê°œì˜ í•„í„°ì˜ ëª¨ë“  íŒŒë¼ë¯¸í„°ê°€ ê³ ì •ëœ ë‹¤ìŒ ê°ì • ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì–´ëŒ‘í„°ë¥¼ í›ˆë ¨í•©ë‹ˆë‹¤. ê·¸ ê²°ê³¼ ê¸°ì¤€ ì‹œìŠ¤í…œë³´ë‹¤ í›¨ì”¬ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë°œíœ˜í•˜ë©°, íŠ¹íˆ IEMOCAPì˜ ê²½ìš° UAì™€ WAê°€ 67.58%ì™€ 72.73%ë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤. CHEAVDì—ì„œëŠ” 34.08%ì™€ 43.96%ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ì”ë¥˜ ì–´ëŒ‘í„° í›ˆë ¨ ì¤‘ì— í™”ì ìˆ˜ë¥¼ ëŠ˜ë¦¬ë ¤ê³  ì‹œë„í–ˆì§€ë§Œ ë” ë³µì¡í•œ ëª¨ë¸ì´ í•„ìš”í•˜ê¸° ë•Œë¬¸ì— ì„±ëŠ¥ì— ë„ì›€ì´ ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Evaluation of adapters: Finally, we want to clarify if the improvement in SER performance has benefited from domain-agnostic parameters learned by VoxCeleb2, or simply because adapters have fewer parameters so the model can be trained by emotion corpora. To answer the question, we keep the same experiment configuration with the residual adapters, retain the model but set all 3X3 convolutional filter weights to 0, so the domain-agnostic parameters will offer no information. As a result, the accuracy significantly drops, which proves the necessity of domain-agnostic parameters. ë§ˆì§€ë§‰ìœ¼ë¡œ SER ì„±ëŠ¥ì˜ ê°œì„ ì´ VoxCeleb2ê°€ í•™ìŠµí•œ ë„ë©”ì¸ì— êµ¬ì• ë°›ì§€ ì•ŠëŠ” íŒŒë¼ë¯¸í„°ì˜ ë•ë¶„ì¸ì§€, ì•„ë‹ˆë©´ ë‹¨ìˆœíˆ ê°ì • ì½”í¼ìŠ¤ë¡œ ëª¨ë¸ì„ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ì–´ëŒ‘í„°ì˜ íŒŒë¼ë¯¸í„° ìˆ˜ê°€ ì ê¸° ë•Œë¬¸ì¸ì§€ ëª…í™•íˆ í•˜ê³ ì í•©ë‹ˆë‹¤. ì´ ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•´ ì”ì—¬ ì–´ëŒ‘í„°ì™€ ë™ì¼í•œ ì‹¤í—˜ êµ¬ì„±ì„ ìœ ì§€í•˜ê³  ëª¨ë¸ì„ ìœ ì§€í•˜ë˜ 3ê°œì˜ ì»¨ë³¼ë£¨ì…˜ í•„í„° ê°€ì¤‘ì¹˜ë¥¼ ëª¨ë‘ 0ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ë„ë©”ì¸ì— êµ¬ì• ë°›ì§€ ì•ŠëŠ” íŒŒë¼ë¯¸í„°ê°€ ì•„ë¬´ëŸ° ì •ë³´ë„ ì œê³µí•˜ì§€ ì•Šë„ë¡ í–ˆìŠµë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ ì •í™•ë„ê°€ í¬ê²Œ ë–¨ì–´ì§€ë©° ë„ë©”ì¸ì— êµ¬ì• ë°›ì§€ ì•ŠëŠ” íŒŒë¼ë¯¸í„°ì˜ í•„ìš”ì„±ì´ ì…ì¦ë©ë‹ˆë‹¤. Comparison to state-of-the-art systems Compared to [25], our method does not exceed their WA but has better UA, indicating that the performance of data-limited small classes is improved. In fact these results show that the proposed residual adapter model can effectively utilize speaker characteristic information from the VoxCeleb2 training data, yet also provide discrimination ability for the SER task. In future we believe there is potential to exploit a deeper network for SER to further improve performance.","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speech Emotion Recognition","slug":"Paper/Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/categories/Paper/Speech-Emotion-Recognition/"}],"tags":[{"name":"Speech Emotion Recognition","slug":"Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/tags/Speech-Emotion-Recognition/"},{"name":"Domain Adaptation","slug":"Domain-Adaptation","permalink":"https://jmj3047.github.io/tags/Domain-Adaptation/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speech Emotion Recognition","slug":"Paper/Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/categories/Paper/Speech-Emotion-Recognition/"}]},{"title":"Sentence Bert","slug":"Sentence_Bert","date":"2023-09-10T15:00:00.000Z","updated":"2023-09-11T13:19:14.471Z","comments":true,"path":"2023/09/11/Sentence_Bert/","link":"","permalink":"https://jmj3047.github.io/2023/09/11/Sentence_Bert/","excerpt":"","text":"ë“¤ì–´ê°€ë©°ì´ ê¸€ì€ Sentence-BERT: Sentence Embeddings using Siamese BERT-Networksë¥¼ì†Œê°œí•˜ê³  ë…¼ë¬¸ì˜ í•µì‹¬ êµ¬ì¡°ì¸ Sbertë¥¼ ì½”ë“œë¡œ êµ¬í˜„í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ì„¤ëª…í•©ë‹ˆë‹¤. Sentence Bertê°€ í•„ìš”í•œ ì´ìœ Sentence BertëŠ” Bertì„ ë¬¸ì¥ ì„ë² ë”©(Sentence Embedding)ì„ ìƒì„±í•˜ëŠ” ëª¨ë¸ë¡œ í™œìš©í•  ìˆ˜ ìˆë„ë¡ Fine-tuningí•˜ëŠ” ë°©ë²•(ë˜ëŠ” ëª¨ë¸ëª…) ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ë•Œ Sentence embeddingë¼ í•¨ì€ ë¬¸ì¥ ì •ë³´ë¥¼ ë²¡í„° ê³µê°„ì˜ ìœ„ì¹˜ë¡œ í‘œí˜„í•œ ê°’ì„ ë§í•˜ë©°, ë¬¸ì¥ì„ ë²¡í„° ê³µê°„ì— ë°°ì¹˜í•¨ìœ¼ë¡œì„œ ë¬¸ì¥ ê°„ ë¹„êµ, í´ëŸ¬ìŠ¤í„°ë§, ì‹œê°í™” ë“± ë‹¤ì–‘í•œ ë¶„ì„ ê¸°ë²•ì„ ì´ìš©í•  ìˆ˜ ìˆëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤. ì‚¬ì‹¤ Sbert ì´ì „ì—ë„ Bert ëª¨ë¸ì„ í™œìš©í•´ Sentence Embeddingì„ ìƒì„±í•˜ëŠ” ë°©ë²•ì´ ì¡´ì¬í–ˆì§€ë§Œ, ì´ëŸ¬í•œ ë°©ë²•ì€ ê³¼ê±° ëª¨ë¸(Glove,Infer-Sent)ì˜ ì„±ëŠ¥ì— ë¯¸ì¹˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì´ìœ  ë•Œë¬¸ì— Transformer ê¸°ë°˜ ëª¨ë¸ì„ í™œìš©í•´ ë¬¸ì¥ ê°„ ìœ ì‚¬ë„ë¥¼ ë¹„êµí•˜ëŠ” Taskì—ì„œëŠ” sentence embedding ë°©ë²•ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  ì£¼ë¡œ ë‘ ê°œì˜ ë¬¸ì¥ì„ ëª¨ë¸ì— ë„£ì–´ Cross-Attentionì„ í™œìš©í•´ ë¹„êµí•˜ëŠ” ë°©ì‹ì„ í™œìš©í–ˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ ì¼ëŒ€ì¼ë¡œ ë°©ì‹ì´ë¼ í•˜ë©´ ë‘ ê°œì˜ ë¬¸ì¥ì„ í•˜ë‚˜ë¡œ ë¬¶ì€ Input Dataë¥¼ Bert ëª¨ë¸ì— ë„£ì€ ë’¤ ëª¨ë¸ ë‚´ë¶€ì—ì„œ ë‘ ë¬¸ì¥ ê°„ ê´€ê³„ë¥¼ íŒŒì•…í•˜ê³  ëª¨ë¸ì˜ Output ì¤‘ [CLS] í† í°ì„ í™œìš©í•´ ë‘ ë¬¸ì¥ì˜ ìœ ì‚¬ë„ë¥¼ íŒŒì•…í•˜ëŠ” ë°©ë²•ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. Sentence Bert ë…¼ë¬¸ì—ì„œëŠ” ë¬¸ì¥ê³¼ ë¬¸ì¥ì„ ë¹„êµí•˜ëŠ” Taskì¸ Named Entity Recognition(NER), Semantic Textual Similarity(STS)ë¥¼ ìˆ˜í–‰í•˜ëŠ”ë° Senetnece Embeddingì„ í™œìš©í•˜ê³  ìˆì§€ë§Œ, Senetence Embeddingì€ ì´ëŸ¬í•œ Task ë¿ë§Œì•„ë‹ˆë¼ ë¬¸ì¥ê³¼ ë‹¨ì–´ ê°„ ì—°ê´€ì„± ë¹„êµë¥¼ í†µí•œ í‚¤ì›Œë“œ ì¶”ì¶œ, íŠ¹ì • ë¬¸ì„œì˜ ì¹´í…Œê³ ë¦¬ ì„ ì • ë“± ë‹¤ì–‘í•œ Taskì—ì„œ ì‘ìš©ì´ ê°€ëŠ¥í•˜ë¯€ë¡œ ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œí•œ ë…¼ë¬¸ì´ë‚˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì¡´ì¬í•©ë‹ˆë‹¤. ë‹¤ìŒì˜ ë§í¬ë“¤ì€ Setnece Bertë¥¼ í™œìš©í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ë…¼ë¬¸ë“¤ì…ë‹ˆë‹¤. Sbert ê³µì‹ í˜ì´ì§€ ì‘ìš© ì˜ˆì‹œ Bertopic : í† í”½ ì¶”ì¶œ ë¼ì´ë¸ŒëŸ¬ë¦¬ keyBert : ë¬¸ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ ë¼ì´ë¸ŒëŸ¬ë¦¬ Cross-Encoderì™€ Bi-Encoderí•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” Bert ëª¨ë¸ ë‚´ë¶€ì˜ Cross-Ateentionì„ í™œìš©í•´ ë¬¸ì¥ ê°„ ê´€ê³„ë¥¼ ë¹„êµí–ˆë˜ ê¸°ì¡´ ë°©ì‹ì„ Cross-Encoderë¼ëŠ” ìš©ì–´ë¡œ ì‚¬ìš©í•˜ê³  ìˆìœ¼ë©°, ë…¼ë¬¸ì—ì„œ ìƒˆë¡­ê²Œ ì†Œê°œí•˜ëŠ” êµ¬ì¡°ë¥¼ Bi-Encoderë¼ëŠ” ìš©ì–´ë¡œ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤. Cross-Encoderì™€ Bi-Encoderì˜ êµ¬ì¡° ì°¨ì´ëŠ” ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ìŠµë‹ˆë‹¤ ìœ„ ê·¸ë¦¼ì— ëŒ€í•´ ì„¤ëª…í•˜ë©´, Bi-EncoderëŠ” ë‘ ë¬¸ì¥ì„ ë¹„êµí•˜ê¸° ìœ„í•´ ê°œë³„ ë¬¸ì¥ì˜ Embedding ìƒì„±í•˜ëŠ” ë‹¨ê³„ -&gt; ëª¨ë¸ Outputì„ Poolingí•˜ì—¬ Sentence Embedding ìƒì„±í•˜ëŠ” ë‹¨ê³„ -&gt; CosineSimilarityë¥¼ í†µí•´ ë¬¸ì¥ê³¼ ë¬¸ì¥ ê°„ ê´€ê³„ ë¹„êµë¥¼ ë¹„êµí•˜ëŠ” ë‹¨ê³„ ì´ë ‡ê²Œ 3ë²ˆì˜ ë‹¨ê³„ë¥¼ ê±°ì¹©ë‹ˆë‹¤. ê¸°ì¡´ ë°©ì‹ì¸ Cross-EncoderëŠ” ë‘ ê°œì˜ ë¬¸ì¥ì„ Language Modelì— ë„£ì–´ ë‚´ë¶€ì—ì„œ ë¬¸ì¥ ê°„ ë¬¸ì¥ì˜ ê´€ê³„ë¥¼ ë¹„êµí•©ë‹ˆë‹¤. ì ˆì°¨ì  ì¸¡ë©´ì—ì„œ ë³´ë©´ Cross-Encoderê°€ ë” ê°„ë‹¨í•œ ë°©ë²•ì¸ ê²ƒ ê°™ì•„ ë³´ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ 100ê°œ ë¬¸ì¥ì„ ë¹„êµí•œë‹¤ê³  ê°€ì •í•  ë•Œ Cross-EncoderëŠ” 100ê°œì˜ ë¬¸ì¥ì„ 1:1ë¡œ ë¹„êµí•´ì•¼ í•˜ë¯€ë¡œ 100C2íšŒë¥¼ ìˆ˜í–‰í•´ì•¼ í•˜ëŠ” ë°˜ë©´ Bi-EncoderëŠ” ì¼ë‹¨ ë¬¸ì¥ì„ embeddingí•˜ë©´ ë¹„êµí•˜ëŠ” ê³¼ì • ìì²´ëŠ” ë‹¨ìˆœí•˜ë¯€ë¡œ ë¬¸ì¥ì„ embeddingí™” í•˜ê¸° ìœ„í•´ 100íšŒë§Œ ìˆ˜í–‰í•˜ë©´ ë©ë‹ˆë‹¤. êµ¬ì¡° ìì²´ëŠ” Cross-Encoderê°€ ë‹¨ìˆœí•´ë³´ì´ì§€ë§Œ ì‹¤ì œë¡œëŠ” Bi-Encoder ë°©ì‹ì´ íš¨ìœ¨ì„± ë©´ì—ì„œ í›¨ì”¬ ë” íš¨ê³¼ì ì„ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Cross-Encoderì™€ Bi-Encoderì— ëŒ€í•´ ê°œë³„ì ìœ¼ë¡œ ì•Œì•„ë³´ê¸° ì „ Cross-Encoderì™€ Bi-Encoderì˜ íŠ¹ì§•ì— ëŒ€í•´ ê°„ë‹¨íˆ ì•Œì•„ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. ë¨¼ì € Cross-EncoderëŠ” ë¬¸ì¥ ê°„ ê´€ê³„ë¥¼ íŒŒì•…í•˜ëŠ” ì„±ëŠ¥ì´ ìš°ìˆ˜í•œ ì¥ì ì´ ìˆì§€ë§Œ ì•ì„œ ì„¤ëª…í–ˆë“¯ ë¹„êµí•´ì•¼í•˜ëŠ” ë¬¸ì¥ìˆ˜ê°€ ë§ì•„ì§ˆìˆ˜ë¡ ì—°ì‚°ì´ ê¸‰ì¦í•œë‹¤ëŠ” ì¹˜ëª…ì ì¸ ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤. ë°˜ë©´ Bi-EncoderëŠ” Embedding ê³¼ì •ì—ì„œ ì •ë³´ì†ì‹¤ì´ ë°œìƒí•˜ë¯€ë¡œ ì„±ëŠ¥ì— ìˆì–´ì„œ Cross-Encoderì— ë¯¸ì¹˜ì§€ ëª»í•˜ì§€ë§Œ, ì‹¤ì‹œê°„ ë¬¸ì œ í•´ê²°ì— í™œìš©ë  ìˆ˜ ìˆì„ë§Œí•œ ë¹ ë¥¸ ì—°ì‚° ì†ë„ë¥¼ ë³´ì¥í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ íŠ¹ì§•ì—ì„œ ë³´ë“¯ ì´ ë‘˜ì€ ìƒí˜¸ ë³´ì™„ì ì¸ ê´€ê³„ì— ìˆìŠµë‹ˆë‹¤. Bi-EncoderëŠ” Cross-Encoderì˜ ëŠë¦° ì—°ì‚°ì†ë„ë¥¼ ë³´ì™„í•  ìˆ˜ ìˆê³ , Cross-EncoderëŠ” Bi-Encoderì˜ ë¶€ì¡±í•œ ë¬¸ì¥ ë¹„êµ ì„±ëŠ¥ì„ ë³´ì™„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‹¤ì œë¡œë„ ì´ëŸ¬í•œê°œë³„ íŠ¹ì§•ì„ í™œìš©í•´ ê²€ìƒ‰ ê¸°ëŠ¥ì„ êµ¬í˜„í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ ê·¸ë¦¼ì€ Bi-Encoderì™€ Cross-Encoderì˜ ê°œë³„ ì¥ì ì„ ì‚´ë ¤ íš¨ê³¼ì ì¸ ê²€ìƒ‰ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” êµ¬ì¡°ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ êµ¬ì¡°ëŠ” Bi-Encoderì˜ ë¹ ë¥¸ ì—°ì‚°ì†ë„ë¥¼ í™œìš©í•´ queryì™€ ìœ ì‚¬í•œ ë¬¸ì¥ì„ ì¶”ë ¤ë‚¸ ë‹¤ìŒ, Cross-Encoderë¥¼ í™œìš©í•´ ì¶”ë ¤ë‚¸ ë¬¸ì¥ê³¼ Query ê°„ ì—°ê´€ì„±ì„ ë‹¤ì‹œ ê³„ì‚°í•´ ìˆœìœ„ë¥¼ ë©”ê¸°ëŠ” ë°©ì‹ìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤. ì œê°€ ìˆ˜í–‰í–ˆë˜ ë¯¸ë‹ˆí”„ë¡œì íŠ¸ì¸ Sentence Bertë¥¼ í™œìš©í•´ ì—°ê´€ì„± ë†’ì€ ë„ì„œ ì¶”ì²œí•˜ê¸°ë¥¼ ì½ì–´ë³´ë©´ ì´ëŸ¬í•œ êµ¬ì¡°ë¥¼ ì–´ë–»ê²Œ ì½”ë“œë¡œ êµ¬í˜„í•  ìˆ˜ ìˆëŠ”ì§€ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Cross-Encoderë¨¼ì € ê¸°ì¡´ ë°©ì‹ì¸ Cross-Encoderì— ëŒ€í•´ì„œ ì„¤ëª…í•œ ë’¤, ë…¼ë¬¸ì—ì„œ ì†Œê°œí•˜ëŠ” Bi-Encoderì— ëŒ€í•´ì„œ ì„¤ëª…í•˜ê² ìŠµë‹ˆë‹¤. â– Cross-Encoder êµ¬ì¡° ì´í•´í•˜ê¸°Cross-Encoder êµ¬ì¡°ëŠ” Language Modelì— classification layerë¥¼ ìŒ“ì€ êµ¬ì¡°ì…ë‹ˆë‹¤. ì•„ë˜ ê·¸ë¦¼ì—ì„œ íŒŒë€ìƒ‰ ë„¤ëª¨ ë°•ìŠ¤ë¥¼ Language Modelì´ë¼ í•˜ë©° ê·¸ ìœ„ì˜ ë…¸ë€ìƒ‰ í…Œë‘ë¦¬ë¥¼ Classification Layerë¼ í•©ë‹ˆë‹¤. Language Modelì€ Bert ë¿ë§Œì•„ë‹ˆë¼ Electra, Roberta ë“± Encoder ê¸°ë°˜ ëª¨ë¸ì´ë©´ ëª¨ë‘ í™œìš©í•  ìˆ˜ìˆìŠµë‹ˆë‹¤. Cross-Encoder ë‚´ë¶€ì˜ ë°ì´í„° íë¦„ì„ ë³´ë©´ Language Modelì˜ Outputì„ ì‚°ì¶œí•œ ë’¤ CLS Poolingì„ ê±°ì³ ë‹¤ì‹œ Classification Layerì˜ Input Dataë¡œ í™œìš©ë˜ê³  ìˆìŒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë•Œ CLS poolingì´ë¼ í•˜ë©´ ë¬¸ì¥ì˜ ì—¬ëŸ¬ token embedding ì¤‘ [CLS] token embeddingì„ ë¬¸ì¥ embeddingìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. CLS Poolingì„ ë‹¤ë¥´ê²Œ í‘œí˜„í•˜ìë©´ ë¬¸ì¥ê³¼ ë¬¸ì¥ì˜ ê´€ê³„ë¥¼ ë‚˜íƒ€ë‚´ê³  ìˆëŠ” ì •ë³´ë“¤ì€ [CLS] tokenì— ëª¨ë‘ ë…¹ì•„ë“¤ì–´ìˆìœ¼ë‹ˆ [CLS] tokenì™¸ ë‚˜ë¨¸ì§€ëŠ” ë¬¸ì¥ embeddingìœ¼ë¡œ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤ë¼ëŠ” ì˜ë¯¸ë¡œ ì´í•´í•˜ì‹œë©´ ë˜ê² ìŠµë‹ˆë‹¤. Cross-Encoderì˜ êµ¬ì¡°ëŠ” Language Modelê³¼ Classification Headë¡œ êµ¬ì„±ëœ ë§¤ìš° ê°„ë‹¨í•œ êµ¬ì¡°ì´ë©° ì•„ë˜ì˜ ì½”ë“œëŠ” ì´ëŸ¬í•œ êµ¬ì¡°ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ì•„ë˜ ì½”ë“œì—ì„œ ì£¼ëª©í•´ì•¼í•  ì ì€ argumentsë¡œ í™œìš©ë˜ëŠ” num_labelsì˜ ì¡´ì¬ì…ë‹ˆë‹¤. Cross-Encoder Classì—ì„œ num_labelsê°€ í™œìš©ë˜ëŠ” ëª©ì ì€ ëª¨ë¸ì˜ Loss Functionì„ ì ìš©í•˜ëŠ”ë° ìˆìŠµë‹ˆë‹¤. ì½”ë“œ ë§ˆì§€ë§‰ ë¶€ë¶„ì—ì„œ num_labelsê°€ í™œìš©ë˜ëŠ” ì½”ë“œë¥¼ ë³¼ ìˆ˜ ìˆëŠ”ë°, num_labelsì´ 1ì¸ ê²½ìš° MSEë¥¼ Loss functionì„ í™œìš©í•˜ê³  ê·¸ì™¸ì¸ ê²½ìš° Cross Entropyë¥¼ Loss functionìœ¼ë¡œ í™œìš©í•˜ê³  ìˆëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. num_labels ê°’ì— ë”°ë¼ Loss functionì´ ë‹¬ë¼ì§€ëŠ” ì´ìœ ëŠ” input Dataë¡œ ì‚¬ìš©ë˜ëŠ” íƒ€ì…ì´ Numerical Dataì¸ì§€ Categorical Dataì¸ì§€ ì—¬ë¶€ì— ë”°ë¼ ì‚¬ìš©í•´ì•¼í•˜ëŠ” Loss functionì´ ë‹¤ë¥´ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849from torch.nn import CrossEntropyLoss, MSELossclass CrossEncoder(nn.Module): def __init__(self, model, num_labels) -&gt; None: super().__init__() self.model = model self.model.config.num_labels = num_labels self.classifier = classificationHead(self.model.config) def forward( self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, labels=None, output_attentions=None, output_hidden_states=None, return_dict=None, ): model = self.model( input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, ) # Last-hidden-states ì¶”ì¶œ sequence_output = model[0] # classificationHeadì— Last-hidden-state ëŒ€ì… logits = self.classifier(sequence_output) loss = None if labels is not None: if self.model.config.num_labels == 1: # Regression Modelì€ MSE Loss í™œìš© loss_fct = MSELoss() else: # classification Modelì€ Cross entropy í™œìš© loss_fct = CrossEntropyLoss() loss = loss_fct(logits.view(-1, 3), labels.view(-1)) return &#123;&quot;loss&quot;: loss, &quot;logit&quot;: logits&#125; else: return &#123;&quot;logit&quot;: logits&#125; CLS í† í°ì´ë€? BERTëŠ” í•™ìŠµì„ ìœ„í•´ ê¸°ì¡´ transformerì˜ input êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ë©´ì„œë„ ì¶”ê°€ë¡œ ë³€í˜•í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤. Tokenizationì€ WorldPiece ë°©ë²•ì„ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤. ìœ„ ê·¸ë¦¼ì²˜ëŸ¼ ì„¸ ê°€ì§€ ì„ë² ë”©(Token, Segment, Position)ì„ ì‚¬ìš©í•´ì„œ ë¬¸ì¥ì„ í‘œí˜„í•©ë‹ˆë‹¤. ë¨¼ì € Token Embeddingì—ì„œëŠ” ë‘ ê°€ì§€ íŠ¹ìˆ˜ í† í°(CLS, SEP)ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ì„ êµ¬ë³„í•˜ê²Œ ë˜ëŠ”ë°ìš”. Special Classification token(CLS)ì€ ëª¨ë“  ë¬¸ì¥ì˜ ê°€ì¥ ì²« ë²ˆì§¸(ë¬¸ì¥ì˜ ì‹œì‘) í† í°ìœ¼ë¡œ ì‚½ì…ë©ë‹ˆë‹¤. ì´ í† í°ì€ Classification taskì—ì„œëŠ” ì‚¬ìš©ë˜ì§€ë§Œ, ê·¸ë ‡ì§€ ì•Šì„ ê²½ìš°ì—” ë¬´ì‹œë©ë‹ˆë‹¤. ë˜, Special Separator token(SEP)ì„ ì‚¬ìš©í•˜ì—¬ ì²« ë²ˆì§¸ ë¬¸ì¥ê³¼ ë‘ ë²ˆì§¸ ë¬¸ì¥ì„ êµ¬ë³„í•©ë‹ˆë‹¤. ì—¬ê¸°ì— segment Embeddingì„ ë”í•´ì„œ ì•ë’¤ ë¬¸ì¥ì„ ë”ìš± ì‰½ê²Œ êµ¬ë³„í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤ë‹ˆë‹¤. ì´ í† í°ì€ ê° ë¬¸ì¥ì˜ ëì— ì‚½ì…ë©ë‹ˆë‹¤. Position Embeddingì€ transformer êµ¬ì¡°ì—ì„œë„ ì‚¬ìš©ëœ ë°©ë²•ìœ¼ë¡œ ê·¸ë¦¼ê³¼ ê°™ì´ ê° í† í°ì˜ ìœ„ì¹˜ë¥¼ ì•Œë ¤ì£¼ëŠ” ì„ë² ë”©ì…ë‹ˆë‹¤. ìµœì¢…ì ìœ¼ë¡œ ì„¸ ê°€ì§€ ì„ë² ë”©ì„ ë”í•œ ì„ë² ë”©ì„ inputìœ¼ë¡œ ì‚¬ìš©í•˜ê²Œ ë©ë‹ˆë‹¤. â– Classification layer êµ¬ì¡° ì´í•´í•˜ê¸°Cross-Encoderì˜ ì „ì²´ êµ¬ì¡°ì™€ ì½”ë“œë¥¼ ì†Œê°œí–ˆìœ¼ë‹ˆ ì´ì œ Classification Layerì˜ ë‚´ë¶€ êµ¬ì¡°ì— ëŒ€í•´ì„œ ì„¤ëª…í•˜ê² ìŠµë‹ˆë‹¤. ì•„ë˜ ê·¸ë¦¼ì€ Classificationì˜ ë‚´ë¶€ êµ¬ì¡°ì™€ ê°œë³„ layerë¥¼ í†µí•´ ë‚˜ì˜¤ëŠ” Output Tensorì˜ í¬ê¸°ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. layerì˜ ìµœì¢… outputì˜ í¬ê¸°ëŠ” [1,N]ì´ë©°, ì—¬ê¸°ì„œ Nì€ num_labelsê³¼ ë™ì¼í•œ ê°’ì´ì ì‚°ì¶œí•´ì•¼í•˜ëŠ” ì¹´í…Œê³ ë¦¬ ê°œìˆ˜ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ë§Œì•½ Regression ìœ í˜•ì˜ outputì´ í•„ìš”í•œ ê²½ìš° N &#x3D; 1ë¡œ ì„¤ì •í•´ì•¼ í•˜ë©°, kê°œì˜ ì¹´í…Œê³ ë¦¬ë¥¼ êµ¬ë¶„í•´ì•¼í•˜ëŠ” Outputì´ í•„ìš”í•œ ê²½ìš° N &#x3D; kë¡œ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤. 1234567891011121314151617181920212223from torch import Tensor, nnclass classificationHead(nn.Module): def __init__(self, config): super().__init__() self.dense = nn.Linear(config.hidden_size, config.hidden_size) classifier_dropout = ( config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob ) self.gelu = nn.functional.gelu self.dropout = nn.Dropout(classifier_dropout) # [batch, embed_size] =&gt; [batch, num_labels] self.out_proj = nn.Linear(config.hidden_size, config.num_labels) def forward(self, features, **kwargs): x = features[:, 0, :] # [CLS] í† í° ì¶”ì¶œ x = self.dropout(x) x = self.dense(x) x = self.gelu(x) x = self.dropout(x) # label ê°œìˆ˜ë§Œí¼ ì°¨ì› ì¶•ì†Œ [batch, embed_size] =&gt; [batch, num_labels] x = self.out_proj(x) return x â– Cross-Encoder í•™ìŠµCross-Encoderë¥¼ ì‹¤ì œ í•™ìŠµí•˜ëŠ” ê³¼ì •ì€ Cross-Encoder í•™ìŠµ íŠœí† ë¦¬ì–¼(Jupyter Notebook)ì„ ì°¸ê³ í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤. í•´ë‹¹ íŠœí† ë¦¬ì–¼ì€ ğŸ¤— Transformersë¥¼ í™œìš©í•´ ì‘ì„±ë˜ì—ˆìœ¼ë¯€ë¡œ Huggingfaceì— ìµìˆ™í•˜ì§€ ì•Šìœ¼ì‹  ë¶„ë“¤ì€ ì¶”ê°€ì ìœ¼ë¡œ ë§í¬ë¥¼ ì°¸ê³ í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤. Bi-Encoderì´ì œ Sentence Bert ë…¼ë¬¸ì˜ í•µì‹¬ êµ¬ì¡°ì¸ Bi-Encoderì— ëŒ€í•´ ì„¤ëª…í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. Bi-EncoderëŠ” ë¬¸ì¥ ê°„ ë¹„êµê°€ í•„ìš”í•œ Taskì— ëŒ€í•´ í›¨ì‹  ë†’ì€ í¼í¬ë¨¼ìŠ¤ë¥¼ ë³´ì—¬ì£¼ëŠ” ì¥ì ì´ ìˆë‹¤ê³  ì„¤ëª…í•œ ë°” ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì†ë„ë¥¼ ë³´ì¥í•  ìˆ˜ ìˆëŠ” ì´ìœ ëŠ” Sentence Embeddingì„ í™œìš©í•´ ë¬¸ì¥ì„ ë²¡í„° ê³µê°„ì— ìœ„ì¹˜ì‹œì¼œ CosineSimilarityë¥¼ í™œìš©í•´ ê³„ì‚°í•˜ê¸° ë•Œë¬¸ì´ì—ˆìŠµë‹ˆë‹¤. ì•„ë˜ í‘œ ì£¼í™©ìƒ‰ìœ¼ë¡œ ì³ì ¸ìˆëŠ” ì‹¤ì„  ì¤‘ Avg. Bert EmbeddingsëŠ” ì´ì „ì— ì‹œë„í–ˆë˜ Sentence Embedding ë°©ì‹ì˜ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ë©°, ì´ëŸ¬í•œ ì„±ëŠ¥ì€ ê³¼ê±° ëª¨ë¸ì¸ Glove, InferSent ì„±ëŠ¥ì—ë„ ë¯¸ì¹˜ì§€ ëª»í•˜ê³  ìˆìŒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°˜ë©´ NLI ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµí•œ SentenceBert ëª¨ë¸ì˜ ì„±ëŠ¥ì€ Glove, InferSent ì„±ëŠ¥ì„ ì••ë„í•  ë¿ë§Œì•„ë‹ˆë¼ ê¸°ì¡´ ë°©ì‹ì˜ ì„±ëŠ¥ ëŒ€ë¹„ ì•½ 1.8ë°° ì´ìƒì˜ ì„±ëŠ¥ì„ ë³´ì—¬ì¤Œì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. â– Sentence Bert êµ¬ì¡° 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950from transformers import ElectraModel, ElectraTokenizerimport torch.nn as nnimport torchmodel = ElectraModel.from_pretrained(&quot;monologg/koelectra-base-v3-discriminator&quot;)tokenizer = ElectraTokenizer.from_pretrained(&quot;monologg/koelectra-base-v3-discriminator&quot;)class modelWithPooling(nn.Module): def __init__(self, model, pooling_type=&quot;mean&quot;) -&gt; None: super().__init__() self.model = model # base model ex)BertModel, ElectraModel ... self.pooling_type = pooling_type # pooling type ì„¤ì •(ê¸°ë³¸ mean) def forward(self, **kwargs): features = self.model(**kwargs) # [batch_size, src_token, embed_size] attention_mask = kwargs[&quot;attention_mask&quot;] last_hidden_state = features[&quot;last_hidden_state&quot;] if self.pooling_type == &quot;cls&quot;: &quot;&quot;&quot; [cls] ë¶€ë¶„ë§Œ ì¶”ì¶œ &quot;&quot;&quot; cls_token = last_hidden_state[:, 0] # [batch_size, embed_size] result = cls_token if self.pooling_type == &quot;max&quot;: &quot;&quot;&quot; ë¬¸ì¥ ë‚´ í† í° ì¤‘ ê°€ì¥ ê°’ì´ í° tokenë§Œ ì¶”ì¶œ &quot;&quot;&quot; input_mask_expanded = ( attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float() ) # Set padding tokens to large negative value last_hidden_state[input_mask_expanded == 0] = -1e9 max_over_time = torch.max(last_hidden_state, 1)[0] result = max_over_time if self.pooling_type == &quot;mean&quot;: &quot;&quot;&quot; ë¬¸ì¥ ë‚´ í† í°ì„ í•©í•œ ë’¤ í‰ê·  &quot;&quot;&quot; # padding ë¶€ë¶„ ì°¾ê¸° = [batch_size, src_token, embed_size] input_mask_expanded = ( attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float() ) # paddingì¸ ê²½ìš° 0 ì•„ë‹Œ ê²½ìš° 1ê³±í•œ ë’¤ ì´í•© = [batch_size, embed_size] sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1) # í‰ê·  ë‚´ê¸°ìœ„í•œ token ê°œìˆ˜ sum_mask = input_mask_expanded.sum(1) sum_mask = torch.clamp(sum_mask, min=1e-9) result = sum_embeddings / sum_mask # input.shape : [batch_size, src_token, embed_size] =&gt; output.shape : [batch_size, embed_size] return &#123;&quot;sentence_embedding&quot;: result&#125; â– Sbert í•™ìŠµ êµ¬ì¡° : Categorical Dataë¥¼ í•™ìŠµí•˜ëŠ” ê²½ìš°SbertëŠ” í•™ìŠµì— í™œìš©ë  ë°ì´í„°ì…‹ì— ë”°ë¼ í•™ìŠµ êµ¬ì¡°ê°€ ë‹¬ë¼ì§‘ë‹ˆë‹¤. ë”°ë¼ì„œ ìì‹ ì´ í™œìš©í•  ë°ì´í„°ì…‹ì´ numerical ë°ì´í„°ì…‹ì¸ì§€, categorical ë°ì´í„°ì…‹ì¸ì§€ êµ¬ë¶„ì„ í•´ì•¼í•©ë‹ˆë‹¤. ë¨¼ì € categorical ë°ì´í„° ìœ í˜•ì— ëŒ€í•´ì„œ ì„¤ëª…í•˜ê² ìŠµë‹ˆë‹¤. ì˜ˆì œì—ì„œ í™œìš©í•˜ëŠ” ë°ì´í„°ì…‹ì€ ìì—°ì–´ì¶”ë¡ (NLI) ë°ì´í„°ì…‹ì´ë©° êµ¬ì¡°ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. {&#39;sen1&#39;: &#39;ê·¸ë¦¬ê³  ê·¸ê°€ ë§í–ˆë‹¤, &quot;ì—„ë§ˆ, ì € ì™”ì–´ìš”.&quot;&#39;,&#39;sen2&#39;: &#39;ê·¸ëŠ” í•™êµ ë²„ìŠ¤ê°€ ê·¸ë¥¼ ë‚´ë ¤ì£¼ìë§ˆì ì—„ë§ˆì—ê²Œ ì „í™”ë¥¼ê±¸ì—ˆë‹¤.&#39;,&#39;gold_label&#39;: &#39;neutral&#39;} categorical ë°ì´í„°ë¡œ Sbertë¥¼ í•™ìŠµí•˜ëŠ” êµ¬ì¡°ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. 1ì°¨ë¡œ SBert ëª¨ë¸ì„ í†µí•´ ì‚°ì¶œí•œ embedding vectorë¥¼ ê°ê° U,Vë¼ í•  ë•Œ U,V,|U-V|ë¥¼ í•˜ë‚˜ì˜ Tensorë¡œ concatì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ê·¸ ë‹¤ìŒ softmax Classifierë¥¼ í†µí•´ entailment, neutral, contraditionì„ íŒë‹¨í•˜ê³  Lossë¥¼ êµ¬í•´ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤. â– categorical Data í•™ìŠµ êµ¬ì¡°1234567891011121314151617181920212223242526272829303132from torch import nnclass modelForClassificationTraining(nn.Module): def __init__(self, model, *inputs, **kwargs): super().__init__() # í•™ìŠµí•  ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° self.model = modelWithPooling(model) # ëª¨ë¸ embed_size sentence_embedding_dimension = self.model.model.config.hidden_size # concat í•´ì•¼í•˜ëŠ” vector ê°œìˆ˜(U,V, |U-V|) num_vectors_concatenated = 3 # embed_size * 3 =&gt; 3 ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œì‹œí‚¤ëŠ” classifier self.classifier = nn.Linear(num_vectors_concatenated * sentence_embedding_dimension, 3) def forward(self, features, answer): &quot;&quot;&quot; ìƒ´ ë„¤íŠ¸ì›Œí¬ëŠ” í•˜ë‚˜ì˜ ëª¨ë¸ë¡œ ë‘ ê°œì˜ outputì„ ì‚°ì¶œí•˜ëŠ” êµ¬ì¡°ì„. í•˜ë‚˜ì˜ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì§€ë§Œ ê°ê° ì¶œë ¥í•˜ë¯€ë¡œ Input ë°ì´í„° ìƒí˜¸ ê°„ ì˜í–¥ì„ ì¤„ ìˆ˜ ì—†ê²Œ ë¨. &quot;&quot;&quot; # ê°œë³„ ë°ì´í„° ìƒì„± embeddings = [self.model(**input_data)[&quot;sentence_embedding&quot;] for input_data in features] rep_a, rep_b = embeddings # U,V, |U-V| vector ë³‘í•© vectors_concat = [] vectors_concat.append(rep_a) vectors_concat.append(rep_b) vectors_concat.append(torch.abs(rep_a - rep_b)) features = torch.cat(vectors_concat, 1) # ë³‘í•©í•œ vector ì°¨ì› ì¶•ì†Œ outputs = self.classifier(features) # Loss ê³„ì‚° loss_fct = nn.CrossEntropyLoss() loss = loss_fct(outputs, answer.view(-1)) return &#123;&quot;loss&quot;: loss&#125; â– Sbert êµ¬ì¡° : Numerical Dataë¥¼ í•™ìŠµí•˜ëŠ” ê²½ìš°Numerical DataëŠ” ë¬¸ì¥ê³¼ ë¬¸ì¥ ê°„ ë¹„êµë¥¼ ìˆ˜ì¹˜ë£Œ í‘œí˜„í•œ ë°ì´í„°ë¥¼ ë§í•©ë‹ˆë‹¤. { &#39;sen1&#39;: &#39;ë¹„í–‰ê¸°ê°€ ì´ë¥™í•˜ê³  ìˆë‹¤.&#39;, &#39;sen2&#39;: &#39;ë¹„í–‰ê¸°ê°€ ì´ë¥™í•˜ê³  ìˆë‹¤.&#39;, &#39;score&#39;: &#39;5.000&#39;} Numerical í•™ìŠµ êµ¬ì¡°ëŠ” ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ í™œìš©í•´ Embedding Vectorë¥¼ ë¹„êµí•©ë‹ˆë‹¤. â– Numerical Data í•™ìŠµ êµ¬ì¡°123456789101112131415161718from torch import nnclass modelForRegressionTraining(nn.Module): def __init__(self, model, *inputs, **kwargs): super().__init__() # í•™ìŠµì„ ìˆ˜í–‰í•  ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° self.model = modelWithPooling(model) def forward(self, features, answer): # Sentence 1, Sentence 2ì— ëŒ€í•œ Embedding embeddings = [self.model(**input_data)[&quot;sentence_embedding&quot;] for input_data in features] # Sentence 1, Sentence 2ì— ëŒ€í•œ Cosine Similarity ê³„ì‚° cos_score_transformation = nn.Identity() outputs = cos_score_transformation(torch.cosine_similarity(embeddings[0], embeddings[1])) # label score Normalization answer = answer / 5 # 0 ~ 5 =&gt; 0 ~ 1 loss_fct = nn.MSELoss() loss = loss_fct(outputs, answer.view(-1)) return &#123;&quot;loss&quot;: loss&#125; Bi-Encoder í™œìš©í•™ìŠµì´ ì™„ë£Œë˜ë©´ í•™ìŠµì— í™œìš©ëœ êµ¬ì¡°ëŠ” ë²„ë¦¬ê³  Sentence Bertë§Œ ì¶”ì¶œí•˜ì—¬ í™œìš©í•©ë‹ˆë‹¤. ì´ì™€ ê´€ë ¨í•œ ì˜ˆì œëŠ” Sbert ê¹ƒí—ˆë¸Œ í˜ì´ì§€ì— ì½”ë“œë¡œ ìì„¸íˆ ì„¤ëª…í•˜ê³  ìˆìœ¼ë‹ˆ ì‘ìš© ë°©ë²•ì— ëŒ€í•´ ê¶ê¸ˆí•œ ê²½ìš° í•´ë‹¹ ë§í¬ë¥¼ ì°¸ê³  ë°”ëë‹ˆë‹¤. Reference https://yangoos57.github.io/blog/DeepLearning/paper/Sbert/Sbert/ https://hwiyong.tistory.com/392","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"NLP","slug":"Paper/NLP","permalink":"https://jmj3047.github.io/categories/Paper/NLP/"}],"tags":[{"name":"Sentence Bert","slug":"Sentence-Bert","permalink":"https://jmj3047.github.io/tags/Sentence-Bert/"},{"name":"Bi Encoder","slug":"Bi-Encoder","permalink":"https://jmj3047.github.io/tags/Bi-Encoder/"},{"name":"Cross Encoder","slug":"Cross-Encoder","permalink":"https://jmj3047.github.io/tags/Cross-Encoder/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"NLP","slug":"Paper/NLP","permalink":"https://jmj3047.github.io/categories/Paper/NLP/"}]},{"title":"Deploying Machine Learning Models in Production_Quiz","slug":"MLOps4_Quiz","date":"2023-08-16T15:00:00.000Z","updated":"2023-08-27T03:53:38.260Z","comments":true,"path":"2023/08/17/MLOps4_Quiz/","link":"","permalink":"https://jmj3047.github.io/2023/08/17/MLOps4_Quiz/","excerpt":"","text":"ê°œìš” Coursera ML Ops Course 4 Quiz 1. Introduction to Model Serving Link: https://www.coursera.org/learn/deploying-machine-learning-models-in-production/home/week/1 2. Introduction to Model Serving Infrastructure 3. TensorFlow Serving 4. Model serving architecture Link: https://www.coursera.org/learn/deploying-machine-learning-models-in-production/home/week/2 5. Scaling Infrastructure 6. Online Inference 7. Data Preprocessing 8. Batch inference scenarios 9. Batch Processing with ETL 10. ML Experiments Management and Workflow Automation Link: https://www.coursera.org/learn/deploying-machine-learning-models-in-production/home/week/3 11. MLOps Methodology 12. Model Management and Deployment Infrastructure 13. Model Monitoring and Logging Link: https://www.coursera.org/learn/deploying-machine-learning-models-in-production/home/week/4 14. Model Decay 15. GDPR and Privacy","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"Coursera","slug":"Coursera","permalink":"https://jmj3047.github.io/tags/Coursera/"},{"name":"Quiz","slug":"Quiz","permalink":"https://jmj3047.github.io/tags/Quiz/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}]},{"title":"Machine Learning Modeling Pipelines in Production_Quiz","slug":"MLOps3_Quiz","date":"2023-08-15T15:00:00.000Z","updated":"2023-08-17T14:37:02.694Z","comments":true,"path":"2023/08/16/MLOps3_Quiz/","link":"","permalink":"https://jmj3047.github.io/2023/08/16/MLOps3_Quiz/","excerpt":"","text":"ê°œìš”Coursera ML Ops Course 3 Quiz 1. Hyperparameter Tuning and Neural Architecture Search Link: https://www.coursera.org/learn/machine-learning-modeling-pipelines-in-production/home/week/1 2. AutoML 3. Dimensionality Reduction Link: https://www.coursera.org/learn/machine-learning-modeling-pipelines-in-production/home/week/2 4. Quantization and Pruning 5. High-Performance Modeling Link: https://www.coursera.org/learn/machine-learning-modeling-pipelines-in-production/home/week/3 6. Knowledge Distillation 7. Model Analysis Link: https://www.coursera.org/learn/machine-learning-modeling-pipelines-in-production/home/week/4 8. Model Analysis and Debugging 9. Continuous Evaluation and Monitoring 10. Explainable AI Link: https://www.coursera.org/learn/machine-learning-modeling-pipelines-in-production/home/week/5 11. Interpretability 12. Understanding Model Predictions","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"Coursera","slug":"Coursera","permalink":"https://jmj3047.github.io/tags/Coursera/"},{"name":"Quiz","slug":"Quiz","permalink":"https://jmj3047.github.io/tags/Quiz/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}]},{"title":"A fine-tuned wav2vec2.0/Hubert benchmark for SER, Speaker verification and spoken language understanding","slug":"wav2vec_hubert_for_SER_SV_SLU","date":"2023-08-03T15:00:00.000Z","updated":"2023-08-08T13:50:44.993Z","comments":true,"path":"2023/08/04/wav2vec_hubert_for_SER_SV_SLU/","link":"","permalink":"https://jmj3047.github.io/2023/08/04/wav2vec_hubert_for_SER_SV_SLU/","excerpt":"","text":"Journal&#x2F;Conference: arXiv preprint arXiv:2111.02735Year(published year): 2022Author: Yingzhi Wang, Abdelmoumene Boumadane, Abdelwahab HebaSubject: wav2vec 2.0, HuBERT, speech emotion recognition, speaker verification, spoken language understanding A fine-tuned wav2vec2.0&#x2F;Hubert benchmark for SER, Speaker verification and spoken language understandingIntroduction The wav2vec 2.0 model architecture contains mainly three modules. A convolutional neural network (CNN) feature encoder encodes the raw waveform inputs into latent speech representations. Mask operations are applied before they are fed to the Transformer based contextualized encoder. A quantization module is used to quantize the latent speech representations from the CNN encoder into a discretized embedding which is then used as the target. HuBERT shares the same architecture as wav2vec 2.0. Specifically, HuBERT consumes masked continuous speech features to predict predetermined cluster assignments. The predictive loss is applied over the masked regions, forcing the model to learn good high-level representations of unmasked inputs in order to infer the targets of masked ones correctly. In the field of Speech Emotion Recognition (SER), Speaker Verification (SV) and Spoken Language Understanding (SLU), it is still vague whether self-supervised models can produce better performance compared with traditional supervised models (spectral features + CNN-based feature extraction + RNN&#x2F;Transformer based time series modeling) [12, 13, 14, 15, 16]. For SER, [22] combined the features from frozen wav2vec2.0 with other hand-crafted prosodic features and then fed them into a 1d-CNN for a deeper extraction. [23] explored wav2vec fine-tuning strategies and 65.4% WA on IEMOCAP was achieved. Taking inspiration from [10] and [11], we added another fine-tuning method by splitting a pre-trained wav2vec 2.0&#x2F;HuBERT model into two parts: the CNN feature encoder and the Transformer contextualized encoder. We froze the CNN feature encoder and only fine-tuned the Transformer contextualized encoder. We then tested partially fine-tuned wav2vec2.0&#x2F;HuBERT pre-trained models together with the entirely fine-tuned ones with the following tasks below: Speech Emotion Recognition on IEMOCAP Speaker Verification on VoxCeleb1 Spoken Language Understanding on SLURP [26] The code and fine-tuned models for SER and SLU have been open-sourced on SpeechBrain [27]. METHOD In this section, we will first introduce the pre-training of wav2vec 2.0&#x2F;HuBERT model, then we will show our fine-tuning methods and downstream models for each task. Pretrained wav2vec 2.0wav2vec 2.0 ì‚¬ì „ í›ˆë ¨ì€ BERT[28]ì˜ ë§ˆìŠ¤í¬ ì–¸ì–´ ëª¨ë¸ë§ê³¼ ìœ ì‚¬í•˜ë©° ìì²´ ê°ë… ì„¤ì •ì—ì„œ ìˆ˜í–‰ë©ë‹ˆë‹¤. CNN ì¸ì½”ë” í‘œí˜„ì˜ ì—°ì†ì ì¸ ì‹œê°„ ë‹¨ê³„ëŠ” ë¬´ì‘ìœ„ë¡œ ë§ˆìŠ¤í‚¹ë˜ë©°, ëª¨ë¸ì€ ì»¨í…ìŠ¤íŠ¸í™”ëœ ì¸ì½”ë”ì˜ ì¶œë ¥ì—ì„œ ë§ˆìŠ¤í‚¹ëœ í”„ë ˆì„ì— ëŒ€í•´ ì–‘ìí™”ëœ ë¡œì»¬ ì¸ì½”ë” í‘œí˜„ì„ ì¬í˜„í•˜ë„ë¡ í›ˆë ¨ë©ë‹ˆë‹¤. Training Objective sim($c_t$, $q_t$): cosine similarity between the contextualized encoder outputs $c_t$ and the quantized CNN encoder representations $q_t$. t is the masked time step $Q_t$: the union of candidate representations $\\tilde{q}$ which includes $q_t$ and K &#x3D; 100 distractors $\\mathcal{K}$ is the temperature which is set to 0.1. The distractors are outputs of the local encoder sampled from masked frames belonging to the same utterance as $q_t$. The contrastive loss is then given by $L_m$ summed over all masked frames. At the end, an L2 regularization is added to the contrastive loss, as well as a diversity loss to increase the use of the quantized codebook representations. In this work, we compare four released wav2vec 2.0 pre-trained models the wav2vec 2.0 base model (12 transformer blocks and 768 embedding dimension) its ASR fine-tuned version the wav2vec 2.0 large model (24 transformer blocks and 1024 embedding dimension) its ASR fine-tuned version. Both base and large models are pre-trained on 960h LibriSpeech [31] data, which is also used for their ASR fine-tuning. ASR fine-tuned models for both wav2vec 2.0 and HuBERT are taken into consideration because we assume that some tasks may benefit from the ASR fine-tuning. Pretrained HuBERTwav2vec 2.0ê³¼ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ, CNNìœ¼ë¡œ ì¸ì½”ë”©ëœ ì˜¤ë””ì˜¤ í”¼ì²˜ëŠ” HuBERTì—ì„œ ë¬´ì‘ìœ„ë¡œ ë§ˆìŠ¤í‚¹ë©ë‹ˆë‹¤. HuBERT ì‚¬ì „ í›ˆë ¨ì˜ first iterationì„ ìœ„í•œ ë ˆì´ë¸”ì„ ìƒì„±í•˜ê¸° ìœ„í•´ 39ì°¨ì› MFCC íŠ¹ì§•ì— K-í‰ê·  í´ëŸ¬ìŠ¤í„°ë§ì´ ì ìš©ë©ë‹ˆë‹¤. ì´í›„ ë°˜ë³µì„ ìœ„í•œ ë” ë‚˜ì€ íƒ€ê¹ƒì„ ìƒì„±í•˜ê¸° ìœ„í•´ k-í‰ê·  í´ëŸ¬ìŠ¤í„°ë§ì€ ì´ì „ ë°˜ë³µì—ì„œ ì‚¬ì „ í•™ìŠµëœ HuBERT ëª¨ë¸ì—ì„œ ì¶”ì¶œí•œ latent featuresì— ëŒ€í•´ ì‘ë™í•©ë‹ˆë‹¤. í´ëŸ¬ìŠ¤í„° ë ˆì´ë¸”ì„ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ íŠ¸ëœìŠ¤í¬ë¨¸ ë¸”ë¡ ìœ„ì— projection layerê°€ ì¶”ê°€ë©ë‹ˆë‹¤. Cross-entropy loss is computed over masked timestamps, which can be defined as: $M \\subset [T]$ denotes the set of indices to be masked for a length- $T$ sequence $X$ $\\tilde{X} &#x3D; r(X;M)$ denotes a corrupted version of $X$ where $x_t$ is replaced with a mask embedding $\\tilde{x}$ if $t \\in M$. A masked prediction model $f$ takes as input $\\tilde{X}$and predicts a distribution over the target indicies at each timestep $p_f(\\cdot | \\tilde{X} ; t)$. To improve target quality, cluster ensembles are utillized in case that an individual clustering model performs badly, $Z^(k)$ then denotes the target sequences generated by the $k$-th clustering model. HuBERT pre-training uses the same optimizer and learning rate scheduler as wav2vec 2.0. For ASR fine-tuning, the projection layer is removed and replaced by a randomly initialized softmax layer, then the CTC loss is optimized. For more details of the pre-training of HuBERT, please refer to [11]. Like wav2vec 2.0, we compare three released HuBERT pretrained models the HuBERT base model (12 transformer blocks and 768 embedding dimension, of which no ASR fine tuned version is released) the HuBERT large model (24 transformer blocks and 1024 embedding dimension) its ASR fine-tuned version. The HuBERT base model is pre-trained on 960h LibriSpeech data, while the large model is pre-trained on 60k hours Libri-Light [32] data. The ASR fine-tuning is also based on 960h LibriSpeech data. Fine-tuning Partial fine-tuning: the CNN based feature encoder and the transformer-based contextualized encoder. We froze the CNN-based feature encoder, fixing all the parameters of these CNN blocks, and only fine-tuned the parameters of the transformer blocks. Partial fine-tuning can be understood as a domain adaptation training for the top level, which aims to prevent interference and damage to the bottom CNN layers that already have an expressive ability. Entire fine-tuning: the CNN and Transformer modules are both fine-tuned during the downstream training process. By training general features at the bottom level, entire fine-tuning allows higher-level expressions to be more complete and more targeted. Then we directly added simple downstream adaptors (classifier&#x2F;decoder) to wav2vec 2.0&#x2F;Hu-BERT without adding another heavy and redundant encoder. The downstream adaptors for each task are presented as below. For SER, an average time pooling and one linear layer are added as a simple downstream classifier (Fig.2). The average time pooling compresses variant time lengths into one, then the linear layer effectuates an utterance-level classification minimizing the cross-entropy loss. For SV, a Speaker Identification (SID) task is first implemented using the same downstream framework as SER. Pairwise cosine-similarity scores are then produced for SV on the pre-trainedSID embeddings before the linear classification layer. ExperimentsDatasetsThe three most widely used and most representative datasets were chosen in our experiments, which are IEMOCAP for SER, VoxCeleb1 for SV and SLURP for SLU. IEMOCAP: The Interactive Emotional Dyadic Motion Capture (IEMOCAP) dataset has approximately 12 hours of data and consists of scripted and improvised dialogues by 10 speakers. In order to form a contrast in this work, we used 4 emotional classes as in SUPERB: anger, happiness, sadness and neutral, following the work of [34]. The evaluation metric is weighted accuracy (WA) and the experiments were carried out on two different split settings: Speaker-Dependent (SD) setting and Speaker-Independent (SI) setting. For SD, the results were averaged on 5 different random seeds for train-validation-test split. For SI, a 10-fold cross-validation was performed with a leave-two- speaker-out strategy (one for validation and one for test). ì•½ 12ì‹œê°„ ë¶„ëŸ‰ì˜ ë°ì´í„°ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, 10ëª…ì˜ í™”ìê°€ ëŒ€ë³¸ì— ë”°ë¼ ì¦‰í¥ì ìœ¼ë¡œ ì—°ê¸°í•œ ëŒ€í™”ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ ì‘ì—…ì—ì„œ ëŒ€ë¹„ë¥¼ í˜•ì„±í•˜ê¸° ìœ„í•´ [34]ì˜ ì—°êµ¬ì— ë”°ë¼ ë¶„ë…¸, í–‰ë³µ, ìŠ¬í””, ì¤‘ë¦½ì˜ 4ê°€ì§€ ê°ì • í´ë˜ìŠ¤ë¥¼ SUPERBì—ì„œì™€ ê°™ì´ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. í‰ê°€ ì§€í‘œëŠ” ê°€ì¤‘ ì •í™•ë„(WA)ì´ë©° ì‹¤í—˜ì€ ë‘ ê°€ì§€ ë‹¤ë¥¸ ë¶„í•  ì„¤ì •ì—ì„œ ìˆ˜í–‰ë˜ì—ˆìŠµë‹ˆë‹¤: í™”ì ì˜ì¡´ì (SD) ì„¤ì •ê³¼ í™”ì ë…ë¦½ì (SI) ì„¤ì •ì…ë‹ˆë‹¤. SDì˜ ê²½ìš°, í›ˆë ¨-ê²€ì¦-í…ŒìŠ¤íŠ¸ ë¶„í• ì„ ìœ„í•´ 5ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ë¬´ì‘ìœ„ ì‹œë“œì— ëŒ€í•œ ê²°ê³¼ë¥¼ í‰ê· í™”í–ˆìŠµë‹ˆë‹¤. SIì˜ ê²½ìš°, 2ëª…ì˜ ìŠ¤í”¼ì»¤ë¥¼ ì œì™¸í•˜ëŠ” ì „ëµ(í•˜ë‚˜ëŠ” ê²€ì¦ìš©, í•˜ë‚˜ëŠ” í…ŒìŠ¤íŠ¸ìš©)ì„ ì‚¬ìš©í•˜ì—¬ 10ë°° êµì°¨ ê²€ì¦ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤. VoxCeleb1: 1,251ëª…ì˜ í™”ìë¡œë¶€í„° ë‚˜ì˜¨ 10ë§Œ ê°œ ì´ìƒì˜ ë°œí™”, ì´ 351ì‹œê°„ ë¶„ëŸ‰ì˜ ì˜¤ë””ì˜¤ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë¨¼ì € speaker identification ì‘ì—…ì„ êµ¬í˜„í•˜ì—¬ ëª¨ë¸ì´ 1211ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ë³´ì´ìŠ¤ í”„ë¦°íŠ¸ë¥¼ êµ¬ë³„í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•˜ë„ë¡ í–ˆìŠµë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ì‚¬ì „ í•™ìŠµëœ speaker identification ëª¨ë¸ì˜ ì„ë² ë”©ì—ì„œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬ 40ëª…ì˜ í™”ìë¡œ êµ¬ì„±ëœ vox1-o í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì— ëŒ€í•œ ê²€ì¦ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤. ì‹¤í—˜ì—ì„œëŠ” VoxCeleb2ì™€ ë…¸ì´ì¦ˆ ì¦ê°•ì„ ì‚¬ìš©í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í‰ê°€ ì§€í‘œë¡œ ë™ì¼ ì˜¤ë¥˜ìœ¨(EER)ì„ ì‚¬ìš©í–ˆìœ¼ë©°, í›ˆë ¨-ê²€ì¦ ë¶„í• ì„ ìœ„í•´ 5ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ì‹œë“œì—ì„œ ê²°ê³¼ë¥¼ í‰ê· í–ˆìŠµë‹ˆë‹¤. Fine-tuning settingsWe rename the models we compare with a method as below. EF&#x2F;PF&#x2F;Frozen: Entirely Fine-tuned&#x2F;Partially Fine-tuned&#x2F;Not fine-tuned w2v&#x2F;hbt: wav2vec 2.0&#x2F;HuBERT based model base&#x2F;large: base&#x2F;large pre-trained model -&#x2F;960h: with&#x2F;without ASR fine-tuning using 960h LibriSpeech data EF-w2v-base : an entirely fine-tuned wav2vec 2.0 base model PF-hbt-large-960h : a partially fine-tuned HuBERT large model with an ASR fine-tuning. For more detailed parameters of released pre-trained wav2vec 2.0&#x2F;Hu-BERT models, please refer to [10] and [11]. During the fine-tuning process, we applied two different schedulers to respectively adjust the fine-tuning learning rate of the wav2vec 2.0&#x2F;HuBERT encoder and the learning rate of the downstream model. Both the schedulers use an Adam Optimizer and linearly anneal the learning rates according to the performance of validation stage. For SER and SV, the initialized fine-tuning learning rate and the downstream learning rate are set to $10^{-5}$ and $10^{-4}$. Results and discussionSpeech Emotion Recognition &amp; Speaker Verification [17]: SUPERBâ€™s results as a non-fine-tuned baseline state-of-the-art baselines Head-Fusion ACNN [35] for SER-SD (Speaker-Dependent setting) Attention Pooling based representation [36] for SER-SI (Speaker-Independent setting) and Siamese Capsule network [37] for SV SER: ì „ì²´ ë¯¸ì„¸ ì¡°ì •ë³´ë‹¤ ë¶€ë¶„ ë¯¸ì„¸ ì¡°ì •ì´ ë” ë‚˜ì€ ë¯¸ì„¸ ì¡°ì • ë°©ë²•ì¸ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. IEMOCAPì€ ë°ì´í„°ê°€ 12ì‹œê°„ë°–ì— ë˜ì§€ ì•ŠëŠ” ì‘ì€ ë°ì´í„° ì„¸íŠ¸ì´ë¯€ë¡œ ë„ˆë¬´ ë§ì€ íŒŒë¼ë¯¸í„°ë¥¼ í•™ìŠµì‹œí‚¤ë©´ ê³¼ì í•©ì´ ì‰½ê²Œ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ ASR ë¯¸ì„¸ ì¡°ì •ì´ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ SER ì‘ì—…ì— ë„ì›€ì´ ë˜ì§€ ì•ŠëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ëŠ”ë°, ì´ëŠ” ASR ë¯¸ì„¸ ì¡°ì • ì¤‘ì— prosodic informationê°€ ì†ì‹¤ë˜ì—ˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤. CONCLUSIONSIn this work we explored different fine-tuning methods on two of the most powerful self-supervised models (wav2vec 2.0 and HuBERT), then benchmarked their performance on Speech Emotion Recognition, Speaker Verification and Spoken Language Understanding tasks. State-of-the-art results were achieved for all the three tasks, proving the excellent generalizability of wav2vec 2.0&#x2F;HuBERT on learning prosodic, voice-print and semantic representations. We hope to show the broad prospects of self-supervised learning and also provide some useful insights for its industrial applications.","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speech Emotion Recognition","slug":"Paper/Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/categories/Paper/Speech-Emotion-Recognition/"}],"tags":[{"name":"Wav2vec 2.0","slug":"Wav2vec-2-0","permalink":"https://jmj3047.github.io/tags/Wav2vec-2-0/"},{"name":"HuBERT","slug":"HuBERT","permalink":"https://jmj3047.github.io/tags/HuBERT/"},{"name":"Speaker Verification,","slug":"Speaker-Verification","permalink":"https://jmj3047.github.io/tags/Speaker-Verification/"},{"name":"Spoken Language Understanding","slug":"Spoken-Language-Understanding","permalink":"https://jmj3047.github.io/tags/Spoken-Language-Understanding/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speech Emotion Recognition","slug":"Paper/Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/categories/Paper/Speech-Emotion-Recognition/"}]},{"title":"SUPERB, Speech processing Universal PERformance Benchmark","slug":"SUPERB","date":"2023-08-01T15:00:00.000Z","updated":"2023-08-08T10:47:10.707Z","comments":true,"path":"2023/08/02/SUPERB/","link":"","permalink":"https://jmj3047.github.io/2023/08/02/SUPERB/","excerpt":"","text":"Journal&#x2F;Conference: arXiv preprint arXiv:2105.01051Year(published year): 2021Author: Shu-wen Yang, Po-Han Chi, Yung-Sung Chuang, Cheng-I Jeff Lai, Kushal Lakhotia,Yist Y. Lin, Andy T. Liu, Jiatong Shi, Xuankai Chang6, Guan-Ting Lin,Tzu-Hsien Huang, Wei-Cheng Tseng, Ko-tik Lee, Da-Rong Liu, Zili Huang, Shuyan Dong,Shang-Wen Li, Shinji Watanabe6, Abdelrahman Mohamed, Hung-yi LeeSubject: Speech, Self-Supervised Learning, Representation Learning, Model Generalization, Benchmark, Evaluation SUPERB: Speech processing Universal PERformance Benchmark Summary The paper introduces SUPERB, a standardized benchmark for evaluating the generalizability of pretrained models on various speech processing tasks. The framework uses a universal representation encoder that is pretrained on self-supervised learning (SSL) tasks and then fine-tuned on downstream tasks with lightweight prediction heads. The results show that the SUPERB framework yields competitive performance compared to traditional supervised pipelines and outperforms log mel filterbank (FBANK) by a large margin, demonstrating the potential of developing powerful, generalizable, and reusable pretrained models for speech processing. Introduction SSL has been explored in speech, including pretraining with generative loss [7, 8, 9, 10], discriminative loss [11, 12, 13, 14], or multi-task [15, 16]. While these works showed promising results of SSL on various speech processing tasks, unlike CV or NLP areas, they were investigated with different datasets and experimental setups. Absence of a shared benchmark makes it hard to compare and draw insights across the techniques. Furthermore, existing works explored a limited number of tasks or require heavyweight downstream training [9, 12, 14], blurring the generalizability and re-usability of SSL models across tasks. Both factors limit the impact of SSL on speech processing in research and industry. SUPERB aims to 360-degree examine modelsâ€™ capability and collects various tasks with limited labeled data from speech communities to align with common research interests. Compared to existing efforts, SUPERB targets at the direct usability of pretrained models on various popular tasks through any usage3. As finetuning pretrained models typically requires huge resources and hinders the re-usability, in this paper, we focus on investigating a simple framework solving all SUPERB tasks with a frozen, shared pretrained model, and lightweight prediction heads finetuned for each task. Speech processing Universal PERformance Benchmark Tasks are designed with the following principles: (1) conventional evaluation protocols from speech communities, (2) publicly available datasets for everyone to participate, (3) limited labeled data to effectively benchmark the generalizability of models. Content Four tasks are collected from ASR and Spoken Term Detection communities. The former aims to transcribe speech into text content; the latter is to detect the spoken content with minimal effort even without transcribing. Phoneme Recognition(PR)PR transcribes an utterance into the smallest content units. We include alignment modeling in the PR task to avoid the potential inaccurate forced alignment. LibriSpeech [23] train-clean-100&#x2F;dev-clean&#x2F;test-clean subsets are adopted in SUPERB for training&#x2F;validation&#x2F;testing. Phoneme transcriptions are obtained from the LibriSpeech official g2p-model-5 and the conversion script in Kaldi librispeech s5 recipe. The evaluation metric is phone error rate (PER). Automatic Speech Recognition(ASR)ASR transcribes utterances into words. While PR analyzes the improvement in modeling phonetics, ASR reflects the significance of the improvement in a real-world scenario. LibriSpeech train-clean-100&#x2F;devclean&#x2F; test-clean subsets are used for training&#x2F;validation&#x2F;testing. The evaluation metric is word error rate (WER). Keyword Spotting(KS)KS detects preregistered keywords by classifying utterances into a predefined set of words. The task is usually performed on-device for the fast response time. Thus, accuracy, model size, and inference time are all crucial. We choose the widely used Speech Commands dataset v1.0 [24] for the task. The dataset consists of ten classes of keywords, a class for silence, and an unknown class to include the false positive. The evaluation metric is accuracy (ACC). Query by Example Spoken Term Detection(QbE)QbE detects a spoken term (query) in an audio database (documents) by binary discriminating a given pair of query and document into a match or not. The English subset in QUESST 2014 [25] challenge is adopted since we focus on investigating English as the first step. The evaluation metric is maximum term weighted value (MTWV) which balances misses and false alarms. SpeakerSpeaker Identification(SI)SID classifies each utterance for its speaker identity as a multi-class classification, where speakers are in the same predefined set for both training and testing. The widely used VoxCeleb1 [26] is adopted, and the evaluation metric is accuracy (ACC). Automatic Speaker Verification(ASV)ASV verifies whether the speakers of a pair of utterances match as a binary classification, and speakers in the testing set may not appear in the training set. Thus, ASV is more challenging than SID. Vox- Celeb1 [26] is used without VoxCeleb2 training data and noise augmentation. The evaluation metric is equal error rate (EER). Speaker Diarization(SD)SD predicts who is speaking when for each timestamp, and multiple speakers can speak simultaneously.The model has to encode rich speaker characteristics for each frame and should be able to represent mixtures of signals. LibriMix [27] is adopted where LibriSpeech train-clean-100&#x2F;dev-clean&#x2F;test-clean are used to generate mixtures for training&#x2F;validation&#x2F;testing. We focus on the two-speaker scenario as the first step. The time-coded speaker labels were generated using alignments from Kaldi LibriSpeech ASR model. The evaluation metric is diarization error rate (DER). SemanticsTwo tasks are collected from Spoken Language Understanding (SLU) community. While most works for these tasks are done in two stages: transcribing speech into text and predicting semantics on transcribed text, we focus on inferring high-level semantics directly from raw audio in an end-to-end fashion. Intent Classification(IC)IC classifies utterances into predefined classes to determine the intent of speakers. We use the Fluent Speech Commands [28] dataset, where each utterance is tagged with three intent labels: action, object, and location. The evaluation metric is accuracy (ACC). Slot Filling(SF)SF predicts a sequence of semantic slot-types from an utterance, like a slot-type FromLocation for a spokenword Taipei, which is known as a slot-value. Both slot-types and slot-values are essential for an SLU system to function [18]. The evaluation metrics thus include slot-type F1 score and slotvalue CER [29]. Audio SNIPS [18] is adopted, which synthesized multi-speaker utterances for SNIPS [30]. Following the standard split in SNIPS,US-accent speakers are further selected for training, and others are for validation&#x2F;testing. ParalinguisticsEmotion Recognition(ER)ER predicts an emotion class for each utterance. The most widely used ER dataset IEMOCAP [31] is adopted, and we follow the conventional evaluation protocol: we drop the unbalance emotion classes to leave the final four classes (neutral, happy, sad, angry) with a similar amount of data points and cross-validates on five folds of the standard splits. The evaluation metric is accuracy (ACC). Framework: Universal Representation Our framework aims to explore how simple and general the solution can be. Thus, we freeze the parameters of pretrained models across tasks and extract fixed representations to be fed into each task-specialized prediction head (small downstream model). Compared to previous setups in speech representation learning [9, 12, 13], the framework puts an explicit constraint on downstream models to be as lightweight as possible for all tasks, as their parameter size and required training resources are also crucial for the framework to be simple and re-usable in various use cases. With the above principles, the pretrained model solving all SUPERB tasks in this framework would be a universal representation encoder. Self-supervised pretrained modelsSSL models explored in this paper are summarized in Table 1 and categorized into three learning approaches: generative modeling, discriminative modeling, and multi-task learning. ìŒì„± ì²˜ë¦¬ì˜ ë§¥ë½ì—ì„œ íŒë³„ ëª¨ë¸ë§ì€ ìë™ ìŒì„± ì¸ì‹(ASR), í™”ì ì‹ë³„, êµ¬ì–´ ê°ì§€ ë“± ë‹¤ì–‘í•œ ì¢…ë¥˜ì˜ ìŒì„±ì„ êµ¬ë³„í•˜ëŠ” ê²ƒì´ ëª©í‘œì¸ ì‘ì—…ì— ìì£¼ ì‚¬ìš©ë©ë‹ˆë‹¤. ë°˜ë©´ ìƒì„± ëª¨ë¸ë§ì€ ì›ë³¸ ìŒì„±ê³¼ ìœ ì‚¬í•œ ìƒˆë¡œìš´ ìŒì„± ìƒ˜í”Œì„ ìƒì„±í•˜ëŠ” ê²ƒì´ ëª©í‘œì¸ TTS(í…ìŠ¤íŠ¸ ìŒì„± ë³€í™˜), ìŒì„± ë³€í™˜, ì†ŒìŠ¤ ë¶„ë¦¬ì™€ ê°™ì€ ì‘ì—…ì— ìì£¼ ì‚¬ìš©ë©ë‹ˆë‹¤. Generative modeling ìƒì„± ëª¨ë¸ë§ì€ ì…ë ¥ ë°ì´í„°ì˜ í™•ë¥  ë¶„í¬ë¥¼ ëª¨ë¸ë§í•˜ê³  ì´ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì›ë³¸ ë°ì´í„°ì™€ ìœ ì‚¬í•œ ìƒˆë¡œìš´ ë°ì´í„° ìƒ˜í”Œì„ ìƒì„±í•˜ëŠ” ë¨¸ì‹  ëŸ¬ë‹ì˜ í•œ ìœ í˜•ì…ë‹ˆë‹¤. ìƒì„± ëª¨ë¸ì€ ì´ë¯¸ì§€ í•©ì„±, í…ìŠ¤íŠ¸ ìƒì„±, ìŒì„± í•©ì„±ê³¼ ê°™ì€ ì‘ì—…ì— ìì£¼ ì‚¬ìš©ë©ë‹ˆë‹¤. APC adopts the language model-like pretraining scheme on a sequence of acoustic features (FBANK) with unidirectional RNN and generates future frames conditioning on past frames. VQ-APC further applies vector-quantization (VQ) layers onto APCâ€™s representation to make it compact and low bit-rate. Mockingjay adopts the BERT-like pretraining on Transformer encoders by masking the input acoustic features in time axis and re-generating the masked parts. TERA extends Mockingjay to further mask the frequency bins. NPC improves the inference speed upon APC by replacing RNN with CNN and changing the future generation to masked reconstruction as Mockingjay. De-CoAR 2.0 improves upon Mockingjay by inserting a VQ layer right before the final prediction like VQ-APC, and is trained by larger input mask, larger batch size, and more unlabeled data. Discriminative modeling íŒë³„ ëª¨ë¸ë§ì€ ì…ë ¥ ë°ì´í„°ê°€ ì£¼ì–´ì¡Œì„ ë•Œ ì¶œë ¥ì˜ ì¡°ê±´ë¶€ í™•ë¥ ì„ ëª¨ë¸ë§í•˜ëŠ” ë¨¸ì‹  ëŸ¬ë‹ì˜ í•œ ìœ í˜•ì…ë‹ˆë‹¤. íŒë³„ ëª¨ë¸ì€ ë¶„ë¥˜, íšŒê·€, ì‹œí€€ìŠ¤ ë¼ë²¨ë§ê³¼ ê°™ì€ ì‘ì—…ì— ìì£¼ ì‚¬ìš©ë©ë‹ˆë‹¤. CPC discriminates the correlated positive samples from negative samples with contrastive InfoNCE loss, which maximizes the mutual information between raw data and representations. Modified CPC [34] and wav2vec [12] proposed several architecture changes to improve CPC. vq-wav2vec introduces a VQ module to wav2vec. The module discretizes speech into a sequence of tokens after InfoNCE pretraining. Tokens are used as pseudo-text to train a BERT as did in NLP for contextualized representations. wav2vec 2.0 merges the pipeline of vq-wav2vec into one end-to-end training scheme by applying time masking in the latent space and replacing BERTâ€™s token prediction with InfoNCEâ€™s negative sampling to handle the intractable normalization on continuous speech. Motivated by DeepCluster [36], Hu-BERT [35] enables BERTâ€™s token prediction via off-line clustering on representations. The clustered labels at the masked locations are then predicted. Multi-task learningMLT is applied in PASE+ [16], where lots of pretraining objectives are adopted: waveform generation, prosody features regression, contrastive InfoMax objectives, and more. Multiple contaminations are also applied to input speech like reverberation and additive noise. Downstream models and policiesWe design our framework to keep the downstream models and their finetuning simple, while ensuring the performance across pretrained models is comparable and the best model in each task is competitive. Since the last-layer representation is not always the best, the framework collects multiple hidden states from the pretrained model and weighted-sum them as the final representation. For a fair comparison, we also limit the space for downstream hyper-parameters tuning5. Downstream models and algorithms are summarized in the following and will be released in detail as a part of the challenge policy. PR,KS, SID, IC, ER are simple tasks that are solvable with linear downstream models. Hence, we use a frame-wise linear transformation for PR with CTC loss; mean-pooling followed by a linear transformation with cross-entropy loss for utterance-level tasks (KS, SID, IC, and ER). These five tasks also serve as the direct indication of representationsâ€™ quality following the conventional linear evaluation protocol. For ASR, a vanilla 2-layer 1024-unit BLSTM is adopted and optimized by CTC loss on characters. The trained model is decoded with LibriSpeech official 4-gram LM powered by KenLM [37] and flashlight [38] toolkit. We mostly follow the system proposed by GTTS-EHU for QUESST at MediaEval 2014 [39] for QbE but replace the conventional supervised phoneme posteriorgram (PPG) with SSL representations. We run Dynamic Time Warping[40] on all hidden states separately with standard distance functions and obtain a score for each query-document pair. The best distance function &#x2F; hidden state pair is reported. Regarding SF, slot-type labels are represented as special tokens to wrap the slot-values in transcriptions. SF is then re-formulated as an ASR problem. The finetuning scheme is the same as in our ASR task, except for the pre-processing to encode slot-types into transcriptions and post-processing to decode slot-types and slot-values from hypotheses. As for ASV, we adopt the well-known x-vector [41] as the downstream model and change Softmax loss to AMSoftmax loss with the same hyper-parameters as [26]. The simple cosine-similarity backend is used to produce pairwise matching scores. We employ the end-to-end training scheme with permutation-invariant training (PIT) loss [42] to SD, instead of using clustering-based methods. We leverage a single-layer 512-unit LSTM for the downstream model. Experiment For the tasks using linear models, FBANK cannot work on any task, while SSL representations all perform well to some degree with different specializations. It is a surprise that wav2vec 2.0 and HuBERT conquers PR and IC with just linear models and outperforms others by a large margin. Their results on SID and ER are also highly competitive. FBANK achieves competitive performance when allowing non-linear downstream models in ASR, SF, ASV, and SD, and yields better performance than some SSL representations. ConclusionWe present SUPERB, a challenge to generally benchmark the capability of SSL pretrained models on speech processing. We demonstrate a simple framework to solve all SUPERB tasks which leverages a frozen, shared pretrained model and achieves competitive performance with minimal architecture changes and downstream finetuning. We have open-sourced the evaluation toolkit2 and will release the detailed challenge policy on the leaderboard website1. We welcome the community to participate and drive the research frontier.","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speech Representations","slug":"Paper/Speech-Representations","permalink":"https://jmj3047.github.io/categories/Paper/Speech-Representations/"}],"tags":[{"name":"Self-Supervised Learning","slug":"Self-Supervised-Learning","permalink":"https://jmj3047.github.io/tags/Self-Supervised-Learning/"},{"name":"Representation Learning","slug":"Representation-Learning","permalink":"https://jmj3047.github.io/tags/Representation-Learning/"},{"name":"Model Generalization","slug":"Model-Generalization","permalink":"https://jmj3047.github.io/tags/Model-Generalization/"},{"name":"Benchmark","slug":"Benchmark","permalink":"https://jmj3047.github.io/tags/Benchmark/"},{"name":"Evaluation","slug":"Evaluation","permalink":"https://jmj3047.github.io/tags/Evaluation/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speech Representations","slug":"Paper/Speech-Representations","permalink":"https://jmj3047.github.io/categories/Paper/Speech-Representations/"}]},{"title":"Speaker Normalization for Self-Supervised Speech Emotion Recognition","slug":"Speaker_Normalization_for_SER","date":"2023-07-16T15:00:00.000Z","updated":"2023-07-18T16:52:28.193Z","comments":true,"path":"2023/07/17/Speaker_Normalization_for_SER/","link":"","permalink":"https://jmj3047.github.io/2023/07/17/Speaker_Normalization_for_SER/","excerpt":"","text":"Journal&#x2F;Conference : ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)Year(published year): 2022Author: Itai Gat, Hagai Aronowitz, Weizhong Zhu, Edmilson Morais, Ron HoorySubject: Speech emotion recognition, speaker normalization, self-supervised learning Speaker Normalization for Self-Supervised Speech Emotion Recognition Summary The paper proposes a method for speech emotion recognition that normalizes speaker characteristics to improve generalization capabilities of the model. The proposed method uses a pre-trained deep neural network for speech representation learning, called HuBERT, as the upstream model. The authors proposed two training strategies for their method: speaker normalization projector and train all parameters. They showed that the latter approach outperforms the former and achieves state-of-the-art results in speech emotion recognition. The authors evaluated their proposed method on both speaker-independent and speaker-dependent setups using various training set sizes and showed that their method outperforms the current state-of-the-art results for both setups. The proposed method has potential applications in various fields, such as human-robot interaction, virtual assistants, and mental health monitoring. IntroductionThe classic self-supervised learning process relies on a representation trained on a large unlabeled dataset, and a downstream task trained on a relatively small labeled dataset. Generally, our method enhances a downstream task performance by using a third dataset with labels different from the downstream task labels. For example, in this work, for speaker emotion recognition, our method normalizes undesired characteristics from the self-supervised representation to improve performance on the speech emotion recognition task. We carry this out by learning a feature representation that excels at speech emotion recognition while being robust enough for speaker characteristics (see Fig. 1). Our proposed method outperforms the current state-of-the-art results for both speaker-dependent and speaker-independent settings. In summary, we propose a general framework for speaker characteristics normalization from a self-supervised representation. We address the small dataset settings issue and propose a framework for it on the IEMOCAP benchmark. Through extensive experiments, we show that our method outperforms the current speech emotion recognition state-of-the-art results on several setups. ê¸°ì¡´ì˜ self-supervised learning processëŠ” ë ˆì´ë¸”ì´ ì§€ì •ë˜ì§€ ì•Šì€ ëŒ€ê·œëª¨ ë°ì´í„° ì„¸íŠ¸ì—ì„œ í•™ìŠµëœ í‘œí˜„ê³¼ ìƒëŒ€ì ìœ¼ë¡œ ì‘ì€ ë ˆì´ë¸”ì´ ì§€ì •ëœ ë°ì´í„° ì„¸íŠ¸ì—ì„œ í•™ìŠµëœ downstream taskì— ì˜ì¡´í•©ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ì´ ë°©ë²•ì€ downstream task labelê³¼ ë‹¤ë¥¸ ë ˆì´ë¸”ì„ ê°€ì§„ ì„¸ ë²ˆì§¸ ë°ì´í„° ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì´ ì—°êµ¬ì—ì„œëŠ” í™”ì ê°ì • ì¸ì‹ì˜ ê²½ìš°, self supervised representationì—ì„œ ì›í•˜ì§€ ì•ŠëŠ” íŠ¹ì„±ì„ ì •ê·œí™”í•˜ì—¬ ìŒì„± ê°ì • ì¸ì‹ ì‘ì—…ì˜ ì„±ëŠ¥ì„ ê°œì„ í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ í™”ì íŠ¹ì„±ì— ëŒ€í•´ ì¶©ë¶„íˆ robustí•˜ë©´ì„œë„ ìŒì„± ê°ì • ì¸ì‹ì— íƒì›”í•œ feature representationì„ í•™ìŠµí•˜ì—¬ ì´ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤(ê·¸ë¦¼ 1 ì°¸ì¡°). ìš°ë¦¬ê°€ ì œì•ˆí•œ ë°©ë²•ì€ speaker-dependent and speaker-independent settingsì—ì„œ í˜„ì¬ì˜ ìµœì‹  ê²°ê³¼ë³´ë‹¤ ì„±ëŠ¥ì´ ë›°ì–´ë‚©ë‹ˆë‹¤. ìš”ì•½í•˜ë©´, ìš°ë¦¬ëŠ” self supervised representationì—ì„œ speaker characterisitcs normalizationë¥¼ ìœ„í•œ ì¼ë°˜ì ì¸ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì‘ì€ ë°ì´í„° ì„¸íŠ¸ ì„¤ì • ë¬¸ì œë¥¼ í•´ê²°í•˜ê³  ì´ë¥¼ ìœ„í•œ í”„ë ˆì„ì›Œí¬ë¥¼ IEMOCAP benchmarkì—ì„œ ì œì•ˆí•©ë‹ˆë‹¤. extensive experimentì„ í†µí•´ ì´ ë°©ë²•ì´ ì—¬ëŸ¬ ì„¤ì •ì—ì„œ í˜„ì¬ ìŒì„± ê°ì • ì¸ì‹ì˜ ìµœì‹  ê²°ê³¼ë³´ë‹¤ ì„±ëŠ¥ì´ ìš°ìˆ˜í•˜ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. Related WorkSelf-supervised trained modelsìŒì„± ì²˜ë¦¬ì—ì„œ ì‚¬ìš©ë˜ëŠ” ëŒ€ë¶€ë¶„ì˜ self-supervised techniquesì€ ì„¸ ê°€ì§€ ë²”ì£¼ë¡œ ë‚˜ë‰©ë‹ˆë‹¤. ì²« ë²ˆì§¸ ë²”ì£¼ì—ì„œëŠ” constuctive InfoNCE lossê³¼ ê²°í•©ëœ ë‹¤ì–‘í•œ ì•„í‚¤í…ì²˜ê°€ ì‚¬ìš©ë©ë‹ˆë‹¤. ë‘ ë²ˆì§¸ ë²”ì£¼ëŠ” ë§ˆìŠ¤í‚¹ ëœ í† í° ë¶„ë¥˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤. ì„¸ ë²ˆì§¸ ë²”ì£¼ëŠ” future frame ìƒì„± ë° ì…ë ¥ì˜ ë§ˆìŠ¤í¬ ëœ ë¶€ë¶„ì„ ì¬êµ¬ì„±í•˜ëŠ” ì¸ì½”ë”-ë””ì½”ë” ì ‘ê·¼ ë°©ì‹ê³¼ ê°™ì€ ë‹¤ì–‘í•œ ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ ì¬êµ¬ì„± ì†ì‹¤ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. Feature normalizationNagrani et al. [17] suggest using a â€confusion loss,â€ which is a cross-entropy loss computed by comparing the prediction to a uniform distribution. Ganin et al. [18] use extra knowledge regarding the data-domain to tackle a domain adaptation problem. They propose to normalize domain features by negating gradients of a loss that predict the domain label. In contrast to those methods, we normalize cues based on a task rather than a domain. Additionally, our method focuses on the self-supervised representation framework. Nagrani ë“±[17]ì€ ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°ëœ êµì°¨ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ì¸ â€œí˜¼ë™ ì†ì‹¤â€ì„ ì‚¬ìš©í•  ê²ƒì„ ì œì•ˆí•©ë‹ˆë‹¤.ì˜ˆì¸¡ì„ ê· ì¼ ë¶„í¬ì™€ ë¹„êµí•˜ì—¬ ê³„ì‚°ë˜ëŠ” êµì°¨ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ì…ë‹ˆë‹¤. Ganin ë“±[18]ì€ ë°ì´í„° ë„ë©”ì¸ì— ê´€í•œ ì¶”ê°€ ì§€ì‹ì„ ì‚¬ìš©í•˜ì—¬ ë„ë©”ì¸ ì ì‘ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤. ì´ë“¤ì€ ë„ë©”ì¸ ë ˆì´ë¸”ì„ ì˜ˆì¸¡í•˜ëŠ” ì†ì‹¤ì˜ ê¸°ìš¸ê¸°ë¥¼ ìŒìˆ˜í™” í•˜ì—¬ ë„ë©”ì¸ íŠ¹ì§•ì„ ì •ê·œí™” í•  ê²ƒì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë°©ë²•ê³¼ ë‹¬ë¦¬, ìš°ë¦¬ëŠ” ë„ë©”ì¸ì´ ì•„ë‹Œ ì‘ì—…ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¨ì„œë¥¼ ì •ê·œí™” í•©ë‹ˆë‹¤. ë˜í•œ, ìš°ë¦¬ì˜ ë°©ë²•ì€ ìê¸° ì§€ë„ í‘œí˜„ í”„ë ˆì„ ì›Œí¬ì— ì¤‘ì ì„ ë‘¡ë‹ˆë‹¤. Emotion RecognitionìŒì„± ê°ì • ì¸ì‹ì€ ë°œí™”ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°ì •ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤. ìŒì„± ê°ì • ì¸ì‹ì—ì„œ ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ë²¤ì¹˜ë§ˆí¬ëŠ” ëŒ€í™”í˜• ê°ì • ë‹¤ì´ë‚˜ë¯¹ ëª¨ì…˜ ìº¡ì³ ë°ì´í„°ë² ì´ìŠ¤(IEMOCAP). ìŒì„± ê°ì • ì¸ì‹ì„ ìœ„í•´ ì´ˆê¸° E2E ë°©ì‹ì€ CNNê³¼ LSTMì„ ê²°í•©í•©ë‹ˆë‹¤. ì´í›„ attention based modelì€ ë‹¤ìŒê³¼ ê°™ì€ ì´ìœ ë¡œ CNNê³¼ LSTM ì¡°í•©ë³´ë‹¤ ì„±ëŠ¥ì´ ë›°ì–´ë‚¬ìŠµë‹ˆë‹¤. ìµœê·¼ ëª‡ ë…„ ë™ì•ˆ self-supervised learning modelì€ ë ˆì´ë¸”ì´ ì§€ì •ë˜ì§€ ì•Šì€ ë°ì´í„°ë¡œë¶€í„° high quality representationsì„ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ìœ¼ë¡œ ì¸í•´ ìŒì„± ì²˜ë¦¬ ì—°êµ¬ì—ì„œ í° ê´€ì‹¬ì„ ë¶ˆëŸ¬ì¼ìœ¼í‚¤ê³  ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ ë°˜ì˜í•˜ì—¬ Yang ë“±[25]ì€ ë²¤ì¹˜ë§ˆí¬ì—ì„œ self supervised modelì´ ê°ì • ì¸ì‹ì—ì„œ ìµœì²¨ë‹¨ ê²°ê³¼ë¥¼ ìƒì„±í•œë‹¤ëŠ” ê²ƒì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” self supervised modelê³¼ normalization of speaker characteristicsë¥¼ ê²°í•©í•˜ì—¬ ìŒì„± ê°ì • ì¸ì‹ì„ í–¥ìƒì‹œí‚¤ëŠ” ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤. MethodIn the following, we propose an approach for learning a task while normalizing cues from adifferent task (possibly from another dataset) in an upstream downstream architecture. Speaker NormalizationUpstream - Downstream ì ‘ê·¼ ë°©ì‹ì„ ì‚¬ìš©í•˜ë©´ ë‹¨ì¼ Upstream modelë³´ë‹¤ í•˜ë‚˜ì˜ task ì´ìƒì„ í• ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ í™”ì ì‹ë³„ê³¼ ê°ì • ì¸ì‹ ê³¼ì œë¥¼ ëª¨ë‘ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ë°©ë²•ì—ëŠ” ì„¸ê°€ì§€ discriminative learnersë¥¼ ê³ ë ¤í•©ë‹ˆë‹¤. ì²«ë²ˆì§¸ëŠ” upstream model $h_w$, ë‘ë²ˆì§¸ëŠ” emotion recognition learner $g_{w_{er}}$, ê·¸ë¦¬ê³  ì„¸ë²ˆì§¸ëŠ” speaker identification classifier $g_{w_{id}}$. ê°ì • Downstream ì‘ì—…ì—ì„œ ì›ì¹˜ ì•ŠëŠ” í™”ì íŠ¹ì„±ì„ í™œìš©í•˜ì§€ ëª»í•˜ë„ë¡ Upstream í‘œí˜„ì—ì„œ ì´ë¥¼ ì •ê·œí™”í•˜ì—¬ ê°ì • ë¶„ë¥˜ê¸°ê°€ ì´ëŸ¬í•œ ë‹¨ì„œë¥¼ í™œìš©í•  ìˆ˜ ì—†ë„ë¡ í•  ê²ƒì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ speaker identification task ê³¼ ê´€ë ¨í•˜ì—¬ Upstream ëª¨ë¸ì˜ gradientsë¥¼ ìŒìˆ˜í™”(negating)í•  ê²ƒì„ ì œì•ˆí•©ë‹ˆë‹¤. ê°„ë‹¨í•˜ê²Œ ì„¤ëª…í•˜ê¸° ìœ„í•´ stochastic gradient descent(SGD)ì„ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•˜ì§€ë§Œ, ëª¨ë“  ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì— ì‰½ê²Œ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì ‘ê·¼ ë°©ì‹ì€ ë‘ ë‹¨ê³„ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. ì²«ë²ˆì§¸ ë‹¨ê³„ëŠ” standard gradient-based optimization. SGD ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ëŠ” standard gradient-based learningì—ì„œ Upstream ë° Downstream ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ë‹¨ê³„ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ $\\eta$ëŠ” í•™ìŠµë¥ ì´ê³  $l_{er}$ì€ ê°ì •ì¸ì‹ ì†ì‹¤(ì˜ˆ: êµì°¨ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤)ì…ë‹ˆë‹¤. ë‘ë²ˆì§¸ ë‹¨ê³„ ì—ì„œëŠ” Upstream ëª¨ë¸ì˜ í™”ì ID íŠ¹ì§•ì„ ë‹¤ìŒê³¼ ê°™ì´ ì •ê·œí™” í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œ $\\lambda$ëŠ” í™”ì ID ì†ì‹¤ì— ëŒ€í•´ Upstream ëª¨ë¸ì— ëŒ€í•´ì„œ gradient ascent ë‹¨ê³„ë¥¼ ìˆ˜í–‰í•˜ì—¬ Upstream ëª¨ë¸ì˜ í™”ì íŠ¹ì§•ì„ ì–´ëŠì •ë„ ì •ê·œí™”í• ì§€ ì„¤ì •í•˜ëŠ” íŒŒë¼ë¯¸í„° ì…ë‹ˆë‹¤. ê·¸ë¦¼1ì— ë°©ë²•ì„ ì„¤ëª…í•´ ë‘ì—ˆìŠµë‹ˆë‹¤. ì´ ë‹¨ê³„ëŠ” independent í•˜ê¸° ë•Œë¬¸ì— ê°ì •ì¸ì‹ì— ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ì— í™”ì ì‹ë³„ ë ˆì´ë¸”ì„ ì§€ì •í•  í•„ìš”ê°€ ì—†ìœ¼ë©° ê·¸ ë°˜ëŒ€ì˜ ê²½ìš°ë„ ë§ˆì°¬ê°€ì§€ì…ë‹ˆë‹¤. Training StrategiesSelf-supervised upstream modelì€ ë§ì€ íŒŒë¼ë¯¸í„°ê°€ ìˆìŠµë‹ˆë‹¤. HuBERT Large ëª¨ë¸ì—ëŠ” 3ì–µ 1700ë§Œê°œì˜ íŒŒë¼ë¯¸í„°ê°€ ìˆìœ¼ë©° HuBERT X-Largeì—ëŠ” ê±°ì˜ 10ì–µê°œì˜ íŒŒë¼ë¯¸í„°ê°€ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì´ëŸ¬í•œ ë„¤íŠ¸ì›Œí¬ë¥¼ fine tuningí•˜ëŠ” ê²ƒì€ ì–´ë ¤ìš¸ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ ë‘ ê°€ì§€ training procedureë¥¼ ì œì•ˆí•©ë‹ˆë‹¤: Speaker Normalization projector: ë§¤ê°œë³€ìˆ˜ $\\hat{w}$ê°€ ìˆëŠ” ìƒˆë¡œìš´ ë¹„ì„ í˜• ë ˆì´ì–´ë¥¼ ë„ì…í•©ë‹ˆë‹¤. $\\hat{w}$ì€ Upstreamê³¼ Down Stream ëª¨ë¸ ì‚¬ì´ì˜ ê²Œì´íŠ¸ ì…ë‹ˆë‹¤. ê°ì •ì¸ì‹ ë‹¨ê³„ì—ì„œ upstream modelì„ ìµœì í™” í•˜ê¸° ìœ„í•´ $\\hat{w}$ì„ ì¶”ê°€í•©ë‹ˆë‹¤. ë°˜ë©´ speaker identification ì‘ì—…ì—ëŠ” ìˆ˜ì‹(3)ì„ ìˆ˜ì •í•˜ì—¬ $\\hat{w}$ì„ ë‹¨ë…ìœ¼ë¡œ ìµœì í™” í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ upstreamì˜ ìµœì í™” ë‹¨ê³„ë¥¼ ê±´ë„ˆë›¸ ìˆ˜ ìˆìœ¼ë¯€ë¡œ speaker ID ë‹¨ê³„ì—ì„œ gradient computation overheadë¥¼ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ í›ˆë ¨í•©ë‹ˆë‹¤: ì´ ì ‘ê·¼ë°©ì‹ì—ì„œëŠ” ìœ„ì—ì„œ ì„¤ëª…í•œ ë‚´ìš©ì— ë”°ë¼ up stream, down stream íŒŒë¼ë¯¸í„° ëª¨ë‘ë¥¼ í›ˆë ¨í•©ë‹ˆë‹¤. ë‹¤ìŒ ì„¹ì…˜ì—ì…”ëŠ” ë‹¤ì–‘í•œ í›ˆë ¨ ì„¸íŠ¸ í¬ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ í™”ì independent ì„¤ì •ê³¼ dependent ì„¤ì •ì— ëŒ€í•œ ë‘ê°€ì§€ í›ˆë ¨ ì „ëµì— ëŒ€í•´ì„œ ì„¤ëª…í•©ë‹ˆë‹¤. Experimentsì´ ì„¹ì…˜ì—ì„œ ìš°ë¦¬ëŠ” ìš°ë¦¬ì˜ ì ‘ê·¼ ë°©ì‹ì„ ì†Œê°œí•˜ê³  ì´ì „ ì—°êµ¬ì™€ ë¹„êµí•©ë‹ˆë‹¤. Experimental setupIEMOCAP ë°ì´í„°ì…‹ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ê°ì • í´ë˜ìŠ¤ëŠ” ì¤‘ë¦½, í–‰ë³µ, ìŠ¬í””, ë¶„ë…¸ ë„¤ ê°€ì§€ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. Upstream ëª¨ë¸ì—ëŠ” HuBERT ê¸°ë³¸ ëª¨ë¸ê³¼ large ëª¨ë¸ì„ ëª¨ë‘ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. Downstream ëª¨ë¸ì˜ ê²½ìš° HuBERTì˜ temporal dimensionì—ì„œ ë¹„ì„ ì˜ projectionì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. $\\lambda$ì˜ í•˜ì´í¼ íŒŒë¼ë¯¸í„° ë²”ìœ„ëŠ” [0.01, 0.0001]ë¡œ ì¡ì•˜ì§€ë§Œ ê²°êµ­ ê°€ì¥ ì¢‹ì€ ê²°ê³¼ë¥¼ ë‚´ëŠ” ê²ƒì€ $\\lambda$ &#x3D; 0.001ì¼ë•Œ ì˜€ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ìš°ë¦¬ëŠ” ëª¨ë“  ì‹¤í—˜ì— ì´ $\\lambda$ ê°’ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. Emotion Recognitionspeaker-independent, ë‘ í™”ìì˜ ë°œí™”ë¥¼ í…ŒìŠ¤íŠ¸ì— ì‚¬ìš©í•˜ê³  ë‹¤ë¥¸ 8ëª…ì˜ ë°œí™”ë¥¼ ê° í™”ìì˜ í›ˆë ¨ ë° ê²€ì¦ì— ì‚¬ìš©í•˜ëŠ” 5 fold cross validationã…‡ã„¹ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤. stopping criteriaëŠ” speaker out of distribution evaluationì—ì„œë„ ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤. validation ì„¸íŠ¸ì—ì„œ ê°€ì¥ ì¢‹ì€ epochë¥¼ ê¸°ì¤€ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì˜ ì •í™•ë„ë¥¼ ë³´ê³ í•©ë‹ˆë‹¤. ì´ ì‘ì—…ì—ì„œëŠ” unknown speakerì— ëŒ€í•œ generalization capabilitiesë¥¼ ì¡°ì‚¬í•©ë‹ˆë‹¤. ë”°ë¼ì„œ speaker independent setupì— ì´ˆì ì„ ë§ì¶¥ë‹ˆë‹¤. ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³  ìš°ë¦¬ì˜ ë°©ë²•ì€ speaker dependent setupë„ ê°œì„ í•©ë‹ˆë‹¤. ê·¸ ì„¤ì •ì€ train test splitì´ ëœë¤ì´ê³  trainê³¼ test ì„¸íŠ¸ì— ëª¨ë“  í™”ìê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. í‘œ1ì—ëŠ” speaker dependent(SD) ì„¤ì •ê³¼ independent(five-fold) ì„¤ì •ì˜ ê²°ê³¼ê°€ ë‚˜ì™€ìˆìŠµë‹ˆë‹¤. speaker independent ê²½ìš° ë‘ë²ˆì§¸ í›ˆë ¨ ì ˆì°¨ê°€ SOTA ê²°ê³¼ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê°œì„ ì€ HuBERT Large and Base ëª¨ë‘ì—ì„œ ì¼ê´€ë˜ê²Œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. ë˜í•œ ì´ ë°©ë²•ì€ speaker dependent ì„¤ì •ì—ì„œ SOTAê²°ê³¼ì—ì„œ 0.5% ê°œì„ í–ˆìŠµë‹ˆë‹¤. Upstream modelì—ì„œ speaker informationì„ ì •ê·œí™”í•˜ëŠ” ë°©ë²•ì˜ abilityë¥¼ í‰ê°€í–ˆìŠµë‹ˆë‹¤. speaker identificationì„ ìœ„í•´ ê³ ì •ëœ ì—…ìŠ¤íŠ¸ë¦¼(ì¦‰, ì—…ìŠ¤íŠ¸ë¦¼ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ì§€ ì•Šê³ ) HuBERT Largeì— ëŒ€í•´ ë¶„ë¥˜ê¸°ë¥¼ ë‘ ë²ˆ í›ˆë ¨ì‹œì¼°ìŠµë‹ˆë‹¤. ë¨¼ì € speaker normalization method ì´ì „ì— HuBERTì— ëŒ€í•´ down stream ëª¨ë¸ì„ í›ˆë ¨ì‹œì¼°ìŠµë‹ˆë‹¤. 60.7%ì˜ ì •í™•ë„ë¥¼ ì–»ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ì œì•ˆí•œ ë°©ë²•ìœ¼ë¡œ í›ˆë ¨ëœ ê³ ì •ëœ HuBERTì— ëŒ€í•´ additional speaker ID down stream modelì„ í›ˆë ¨í–ˆìŠµë‹ˆë‹¤. ê·¸ ê²°ê³¼ 45.9%ì˜ ì •í™•ë„ë¥¼ ì–»ì—ˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì›í•˜ëŠ” ëŒ€ë¡œ ìš°ë¦¬ì˜ ë°©ë²•ì€ Upstream ëª¨ë¸ì˜ í™”ì ID íŠ¹ì§•ì— í•´ë¥¼ ë¼ì³¤ìŠµë‹ˆë‹¤. í‘œ 1: 5 fold cross validationì„ ìœ„í•´ ì˜¤ë””ì˜¤ ê¸°ëŠ¥ë§Œ ì‚¬ìš©í•œ speaker independent ì„¤ì •ê³¼ ëœë¤ train test splitì„ ì‚¬ìš©í•œ speaker dependent ì„¤ì •ì— ëŒ€í•œ IEMOCAPì˜ ìµœì‹  ê²°ê³¼. 5-fold ë° SD ì„¤ì •ì˜ ê²½ìš° weighted accuracy(WA) metricì„ ë³´ê³ í•©ë‹ˆë‹¤. speaker noramalization projector(SNP)ì™€ Train All Parameters(TAP)ëŠ” Training strategies ì— ì„¤ëª…ëœ í›ˆë ¨ ì „ëµ ì…ë‹ˆë‹¤. ìš°ë¦¬ì˜ ë°©ë²•ì„ ì‚¬ìš©í•˜ë©´ í˜„ì¬ speaker independent SOTA ë³´ë‹¤ 2.3% ê°œì„ ë˜ì—ˆê³  speaker dependent SOTA ë³´ë‹¤ëŠ” 0.5%ê°€ ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤. ì¶”ê°€ì ìœ¼ë¡œ HuBERT Base and Large Modelsì—ì„œ ë‘˜ë‹¤ ê°œì„ ëœ ìš°ë¦¬ì˜ ì ‘ê·¼ ë°©ë²•ì¸ low resource ì„¤ì •ì˜ AUCë¥¼ ì œì‹œí•©ë‹ˆë‹¤. Small data settingsì ì€ ë¦¬ì†ŒìŠ¤ë¡œ ìŒì„± ê°ì • ì¸ì‹ì„ í…ŒìŠ¤íŠ¸ í•˜ê¸° ìœ„í•´ í›ˆë ¨ ì„¸íŠ¸ì˜ í´ë˜ìŠ¤ë‹¹ ìƒ˜í’€ ìˆ˜ë¥¼ ëŠ˜ë¦´ê²ƒì„ ì œì•ˆí•©ë‹ˆë‹¤. ê²°ê³¼ë¥¼ stabilize í•˜ê¸° ìœ„í•´ ìš°ë¦¬ëŠ” ê° ë‹¨ê³„ë¥¼ ì„œë¡œ ë‹¤ë¥¸ random splitìœ¼ë¡œ ë‹¤ì„¯ë²ˆ ì‹¤í–‰í•˜ê³  ê° ë‹¨ê³„ì˜ meanì„ ê³„ì‚°í•©ë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ ì£¼ì–´ì§„ ë°©ë²•ì˜ ì „ë°˜ì  ì„±ëŠ¥ì„ ì •ëŸ‰í™” í•˜ê¸° ìœ„í•´ Fig2ì˜ AUCë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. ì§ê´€ì ìœ¼ë¡œ AUC ì ìˆ˜ëŠ” í‰ê°€í•˜ëŠ” ê° ì„¤ì •ì— ëŒ€í•œ ì ìˆ˜ì˜ í‰ê· ì„ ë°˜ì˜í•©ë‹ˆë‹¤. ê·¸ë¦¼ 2ëŠ” ìš°ë¦¬ê°€ ì œì•ˆí•œ low-source ì„¤ì •ì— ëŒ€í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ê° ë‹¨ê³„ì—ì„œ ìš°ë¦¬ëŠ” HuBERT Large modelì„ ìš°ë¦¬ì˜ ë°©ë²•ì„ ì‚¬ìš©í•œê²ƒê³¼ ì‚¬ìš©í•˜ì§€ ì•Šì€ê²ƒìœ¼ë¡œ í›ˆë ¨í–ˆìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ê° methodì— ëŒ€í•œ AUCë¥¼ í‘œ1ì— ë³´ê³ í–ˆìŠµë‹ˆë‹¤. ìš°ë¦¬ì˜ ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ ê¸°ë³¸ê³¼ ëŒ€í˜• HuBERT ëª¨ë¸ì„ ëª¨ë‘ ê°œì„ í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ê·¸ë¦¼ 2ì—ì„œ ìš°ë¦¬ì˜ ë°©ë²•ì€ ëª¨ë“¤ ì„¤ì •ì—ì„œ HuBERT ì •í™•ë„ë¥¼ í–¥ìƒ ì‹œí‚¨ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Conclusionì´ ë…¼ë¬¸ì—ì„œëŠ” self-supervised feature representationì—ì„œ speaker characteristics normalizationë¥¼ ìœ„í•œ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì‹œí–ˆìŠµë‹ˆë‹¤. ìš°ë¦¬ì˜ ì ‘ê·¼ ë°©ì‹ì€ í•œ ê³¼ì œì— ëŒ€í•œ íŒë³„ í•™ìŠµê³¼ ë‹¤ë¥¸ ê³¼ì œì— ëŒ€í•œ ì ëŒ€ì  í•™ìŠµì„ ê²°í•©í•©ë‹ˆë‹¤. ë˜í•œ, ì´ ë°©ë²•ì€ ê° ê³¼ì œë§ˆë‹¤ ë‹¤ë¥¸ ë°ì´í„° ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ì–‘í•œ ëª¨ë¸ì„ ëŒ€ìƒìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•œ ê²°ê³¼ ìŒì„± ê°ì • ì¸ì‹ì—ì„œ ê°•ë ¥í•œ ìµœì²¨ë‹¨ ê²°ê³¼ë¥¼ ì–»ì—ˆìŠµë‹ˆë‹¤. ë˜í•œ ìˆ˜ì •ëœ ë²„ì „ì˜ IEMOCAPì„ ì‚¬ìš©í•˜ì—¬ ë¦¬ì†ŒìŠ¤ê°€ ì ì€ í™˜ê²½ì—ì„œ ì—°êµ¬í•  ê²ƒì„ ì œì•ˆí•˜ê³  ê·¸ ê²°ê³¼ ìš°ë¦¬ ë°©ë²•ì´ ì„±ê³µì ì„ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤.","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speech Emotion Recognition","slug":"Paper/Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/categories/Paper/Speech-Emotion-Recognition/"}],"tags":[{"name":"Speech Emotion Recognition","slug":"Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/tags/Speech-Emotion-Recognition/"},{"name":"Self-Supervised Learning","slug":"Self-Supervised-Learning","permalink":"https://jmj3047.github.io/tags/Self-Supervised-Learning/"},{"name":"Speaker Normalization","slug":"Speaker-Normalization","permalink":"https://jmj3047.github.io/tags/Speaker-Normalization/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speech Emotion Recognition","slug":"Paper/Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/categories/Paper/Speech-Emotion-Recognition/"}]},{"title":"Speech Emotion Recognition Using Self-Supervised Features","slug":"SER_for_self_supervised","date":"2023-07-14T15:00:00.000Z","updated":"2023-07-18T00:21:15.630Z","comments":true,"path":"2023/07/15/SER_for_self_supervised/","link":"","permalink":"https://jmj3047.github.io/2023/07/15/SER_for_self_supervised/","excerpt":"","text":"Journal&#x2F;Conference : ICASSP IEEEYear(published year): 2022Author: Edmilson Morais, Ron Hoory, Weizhong Zhu, Itai Gat, Matheus Damasceno and Hagai AronowitzSubject: Speech Emotion Recognition, self-supervised features, end-to-end systems Speech Emotion Recognition Using Self-Supervised Features Summary The paper proposes a modular End-to-End (E2E) Speech Emotion Recognition (SER) system based on an Upstream + Downstream architecture model paradigm. The proposed system uses self-supervised features extracted from speech signals and script transcriptions of the speech signals. The authors compare the performance of different Upstream models for speech-based feature extraction, including Wav2vec 2.0 and huBERT. The authors fine-tune the features extracted from these models and combine them using an aggregator to create multimodal feature vectors. The authors achieve state-of-the-art performance on the IEMOCAP dataset, demonstrating the effectiveness of their multimodal approach to SER. Introductionì¸ê°„ì˜ ê°ì •ì€ ë³¸ì§ˆì ìœ¼ë¡œ ë³µì¡í•˜ê³  ì—¬ì „íˆ ì–´ë ¤ìš´ ì—°êµ¬ ë¬¸ì œ ì…ë‹ˆë‹¤. ì¸ê°„ì€ ì¢…ì¢… ìŒì„± íŠ¹ì„±, ì–¸ì–´ì  ë‚´ìš©, í‘œì •, ì‹ ì²´ ë™ì‘ê³¼ ê°™ì€ ì—¬ëŸ¬ê°€ì§€ ë‹¨ì„œë¥¼ ë™ì‹œì— ì‚¬ìš©í•˜ì—¬ ê°ì •ì„ í‘œí˜„í•˜ê¸° ë•Œë¬¸ì— SERì€ ë³¸ì§ˆì ìœ¼ë¡œ ë³µì¡í•œ multi modal ì‘ì—…ì…ë‹ˆë‹¤. ë˜í•œ ë°ì´í„° ìˆ˜ì§‘ì˜ ì–´ë ¤ì›€ìœ¼ë¡œ ì¸í•´ ê³µê°œì ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„° ì„¸íŠ¸ì—ëŠ” ê°ì • í‘œí˜„ì˜ ê°œì¸ì  ì°¨ì´ë¥¼ ì œëŒ€ë¡œ ì»¤ë²„í•  ìˆ˜ ìˆëŠ” í™”ìê°€ ì¶©ë¶„í•˜ì§€ ì•Šì€ ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. ê·¸ ê²°ê³¼ SERì— í†µí•©ëœ ê°€ì¥ ì¼ë°˜ì ì¸ ë”¥ëŸ¬ë‹ ê¸°ìˆ  ì¤‘ ì¼ë¶€ëŠ” transfer learning, multitask learning , multimodal system ë¶„ì•¼ì™€ ê´€ë ¨ ìˆìŠµë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì˜ ì£¼ìš” ëª©í‘œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: (1)ë‹¤ì–‘í•œ self supervised featureì„ ì‰½ê²Œ ì‚¬ìš©&#x2F;í†µí•©í•  ìˆ˜ ìˆëŠ” upstream + downstream ì•„í‚¤í…ì²˜ ëª¨ë¸ íŒ¨ëŸ¬ë‹¤ì„ì— ê¸°ë°˜í•œ ëª¨ë“ˆí˜• ì—”ë“œíˆ¬ì—”ë“œ(E2E) SER ì‹œìŠ¤í…œì„ ì†Œê°œí•˜ê³ , (2)ë‹¤ì–‘í•œ êµ¬ì„±ì—ì„œ ì œì•ˆëœ E2E ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ ë¹„êµ&#x2F;ë¶„ì„í•˜ëŠ” ì¼ë ¨ì˜ ì‹¤í—˜ì„ ì œì‹œí•˜ë©°, (3)ìŒì„± ëª¨ë‹¬ë¦¬í‹°ë§Œ ì‚¬ìš©í•¨ì—ë„ ë¶ˆêµ¬í•˜ê³  ì œì•ˆëœ E2E ì‹œìŠ¤í…œì´ ìŒì„± ë° í…ìŠ¤íŠ¸ ëª¨ë‹¬ë¦¬í‹°ë¥¼ ëª¨ë‘ ì‚¬ìš©í•˜ëŠ” ë©€í‹°ëª¨ë‹¬ ì‹œìŠ¤í…œì´ ë‹¬ì„±í•˜ëŠ” SOTA ê²°ê³¼ì™€ ë¹„êµí•˜ì—¬ SOTA ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŒì„ ë³´ì—¬ì£¼ëŠ” ê²ƒì…ë‹ˆë‹¤. Proposed Modelì´ ë…¼ë¬¸ì—ì„œ SERì˜ ë¬¸ì œì ì„ ì—°ì†ì ì¸ ìŒì„±ì„ ë¶ˆì—°ì†ì ì¸ ê°ì • ë ˆì´ë¸”ë¡œ ë²”ì£¼í™”í•˜ëŠ” ê²ƒì´ë¼ê³  ì—¬ê¸°ê³  ì´ë¥¼ ê³µì‹í™” í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œ ì‚¬ìš©ë˜ëŠ” ëª¨ë¸ì€ Upstream + Downstream ëª¨ë¸ì…ë‹ˆë‹¤. Upstream model: task-independent model, pretrained in self-supervised fashion and it works as Encoder or Front-End Model responsible for feature extraction. ì¼ë°˜ì ìœ¼ë¡œ Front End ëª¨ë¸ì€ ì…ë ¥ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê³  ì¶”ê°€ ì²˜ë¦¬ ë˜ëŠ” ë¶„ë¥˜ë¥¼ ìœ„í•´ Downstream ëª¨ë¸ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê´€ë ¨ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ëŠ” ì—­í• ì„ ë‹´ë‹¹í•˜ëŠ” ëª¨ë¸ ìœ í˜•ì…ë‹ˆë‹¤. ìŒì„± ì²˜ë¦¬ì˜ ë§¥ë½ì—ì„œ Front End ëª¨ë¸ì€ ì¼ë°˜ì ìœ¼ë¡œ ì›ì‹œ ìŒì„± ì‹ í˜¸ì—ì„œ MFCC ë˜ëŠ” í•„í„°ë±…í¬ ì—ë„ˆì§€ì™€ ê°™ì€ ìŒí–¥ì  íŠ¹ì§•ì„ ì¶”ì¶œí•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ì´ëŸ¬í•œ íŠ¹ì§•ì€ ë¶„ë¥˜ê¸°ë‚˜ ìŒì„± ì¸ì‹ ì‹œìŠ¤í…œê³¼ ê°™ì€ Downstream ëª¨ë¸ì— ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤. ë°˜ë©´ì— ì¸ì½”ë”ëŠ” ì…ë ¥ ë°ì´í„°ë¥¼ ì…ë ¥ì˜ ê°€ì¥ ì¤‘ìš”í•œ íŠ¹ì§•ì„ í¬ì°©í•˜ëŠ” ì €ì°¨ì› í‘œí˜„ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ì¼ì¢…ì˜ ì‹ ê²½ë§ ê³„ì¸µì…ë‹ˆë‹¤. ì¸ì½”ë”ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì…ë ¥ ë°ì´í„°ê°€ ê³ ì°¨ì›ì ì´ê³  ë³µì¡í•œ ì´ë¯¸ì§€ ë˜ëŠ” ìŒì„± ì¸ì‹ê³¼ ê°™ì€ ì‘ì—…ì„ ìœ„í•œ ë”¥ ëŸ¬ë‹ì— ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ ë…¼ë¬¸ì—ì„œ ì„¤ëª…í•˜ëŠ” SER ëª¨ë¸ì˜ ë§¥ë½ì—ì„œ ì—…ìŠ¤íŠ¸ë¦¼ ëª¨ë¸ì€ í”„ë¡ íŠ¸ì—”ë“œ ëª¨ë¸ì´ì ì¸ì½”ë”ì…ë‹ˆë‹¤. ì…ë ¥ ìŒì„± ì‹ í˜¸ë¥¼ ì²˜ë¦¬í•˜ê³  ê´€ë ¨ íŠ¹ì§•ì„ ì¶”ì¶œí•œ ë‹¤ìŒ ë¶„ë¥˜ë¥¼ ìœ„í•´ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ëª¨ë¸ì— ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. ë˜í•œ ì—…ìŠ¤íŠ¸ë¦¼ ëª¨ë¸ì€ ìŒì„±ì˜ ì¼ë°˜ì ì¸ íŠ¹ì§•ì„ í•™ìŠµí•˜ê¸° ìœ„í•´ ìê°€ ì§€ë„ ë°©ì‹ìœ¼ë¡œ ì‚¬ì „ í•™ìŠµë˜ë¯€ë¡œ SER ì‘ì—…ì— ê°•ë ¥í•œ íŠ¹ì§• ì¶”ì¶œê¸°ê°€ ë©ë‹ˆë‹¤. Downstream model: task-dependent model, responsible for final task of classifying the features generated by the Upstream model into categorical labels of emotion. DatasetIEMOCAPì€ 12ì‹œê°„ì˜ multimodal ë°ì´í„°ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ 5ê°œì˜ ì„¸ì…˜ê³¼ 10ëª…ì˜ í™”ìë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, í•œ ì„¸ì…˜ì€ ë‘ ëª…ì˜ ë…ì  í™”ìì˜ ëŒ€í™”ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. ì´ì „ ì—°êµ¬ì™€ ë¹„êµí•  ìˆ˜ ìˆë„ë¡ â€˜í™”ë‚œâ€™, â€˜í–‰ë³µâ€™, â€˜í¥ë¶„â€™, â€˜ìŠ¬í””â€™, â€˜ì¤‘ë¦½â€™ ì— ì†í•˜ëŠ” ë ˆì´ë¸”ë§Œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ê° ê°ì • í´ë˜ìŠ¤ì˜ í¬ê¸° ê· í˜•ì„ ë§ì¶”ê¸° ìœ„í•´ â€˜í–‰ë³µâ€™ê³¼ â€˜í¥ë¶„â€™ í´ë˜ìŠ¤ë¥¼ ë³‘í•©í•˜ì—¬ ì´ 5,531ê°œì˜ ë°œí™”(í–‰ë³µ 1636, í™”ë‚¨ 1103, ìŠ¬í”” 1084, ì¤‘ë¦½ 1708)ë¥¼ ì‚°ì¶œí–ˆìŠµë‹ˆë‹¤. leave-one-session-out 5-fold cross validation (CV) is used and the average result reported. At each fold of the 5-fold CV setup, 2 speakers are used for testing and the samples from the 8 speakers remaining are randomly split into 80% for training and 20% for validation. The splitting done here is exactly the same as the one done by SUPERB [18], which splits each of the 5 IEMOCAP folds into three subsets: a training-set, a validation-set and a test-set. The fine-tuning of our Upstream model is performed by training it jointly with a simple Mean-Average Pooling Network followed by a Linear Classifier, as described in Figure 2. Fine-tuning of the upstream modelsSER ì‹œìŠ¤í…œì„ í–¥ìƒ ì‹œí‚¤ê¸° ìœ„í•´ ì„¹ì…˜ì—ì„œ ì„¤ëª…ëœ IEMOCAP ë°ì´í„° ì„¸íŠ¸ì˜ ë²”ì£¼í˜• ê°ì •ë ˆì´ë¸”ì„ ì‚¬ìš©í•˜ì—¬ Upstream modelì„ ë¯¸ì„¸ ì¡°ì • í•˜ì˜€ìŠµë‹ˆë‹¤. ì´ ë¯¸ì„¸ì¡°ì •ì€ 5 foldì˜ IEMOCAP dataset ê°ê°ì— ëŒ€í•´ ìˆ˜í–‰ë©ë‹ˆë‹¤. ë”°ë¼ì„œ ë¯¸ì„¸ì¡°ì • í”„ë¡œì„¸ìŠ¤ê°€ ëë‚˜ë©´ 5ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ fune tuned ëœ upstream modelì´ ì„¸ì…˜ë³„ë¡œ ìƒì„±ë©ë‹ˆë‹¤. 2.1 Upstream modelì˜ ë¯¸ì„¸ì¡°ì •ì€ ê·¸ë¦¼2ì— ì„¤ëª…ëœ ëŒ€ë¡œ ê°„ë‹¨í•œ average pooling networkì™€ Linear Classifierë¥¼ í•¨ê»˜ í›ˆë ¨í•˜ì—¬ ìˆ˜í–‰í•©ë‹ˆë‹¤. Average of checkpoints ë”¥ ëŸ¬ë‹ì˜ ë§¥ë½ì—ì„œ ì²´í¬í¬ì¸íŠ¸ëŠ” í•™ìŠµ ì¤‘ íŠ¹ì • ì‹œì ì˜ ëª¨ë¸ ë§¤ê°œë³€ìˆ˜(wieght and bias)ì— ëŒ€í•œ ìŠ¤ëƒ…ìƒ·ì…ë‹ˆë‹¤. ì²´í¬í¬ì¸íŠ¸ëŠ” ì¼ë°˜ì ìœ¼ë¡œ í›ˆë ¨ ì¤‘ì— ì£¼ê¸°ì ìœ¼ë¡œ ì €ì¥ë˜ë©°, íŠ¹ì • ì‹œì ë¶€í„° í›ˆë ¨ì„ ì¬ê°œí•˜ê±°ë‚˜ ìœ íš¨ì„± ê²€ì‚¬ ì„¸íŠ¸ì—ì„œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ë° ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë…¼ë¬¸ì— ì„¤ëª…ëœ SER ì‹œìŠ¤í…œì—ì„œ ì €ìëŠ” ì²´í¬í¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ ì¤‘ì— ë¯¸ì„¸ ì¡°ì •ëœ ì—…ìŠ¤íŠ¸ë¦¼ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì¶”ì í•©ë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œëŠ” ë¯¸ì„¸ ì¡°ì •ëœ ì—…ìŠ¤íŠ¸ë¦¼ ëª¨ë¸ì˜ ì •í™•ë„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ IEMOCAP ë°ì´í„° ì„¸íŠ¸ì˜ 5ë°°ìˆ˜ ê°ê°ì— ëŒ€í•´ ìµœê³ ì˜ ì²´í¬í¬ì¸íŠ¸ 5ê°œë¥¼ ìœ íš¨ì„± ê²€ì‚¬ ë“± ê°ì¢… ì„¸íŠ¸ì— ì €ì¥í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì²´í¬í¬ì¸íŠ¸ëŠ” ëª¨ë¸ íŒŒë¼ë¯¸í„°ì˜ í‰ê· ì„ ê³„ì‚°í•˜ëŠ” ë° ì‚¬ìš©ë˜ë©°, ì´ëŠ” ì—…ìŠ¤íŠ¸ë¦¼ ëª¨ë¸ì˜ ì¶œë ¥ ë¶„ì‚°ì„ ì¤„ì´ê³  SER ì‹œìŠ¤í…œì˜ ì „ë°˜ì ì¸ ì„±ëŠ¥ì„ ê°œì„ í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤. why should output variance be minimized? SER ì‹œìŠ¤í…œì˜ ë§¥ë½ì—ì„œ ì—…ìŠ¤íŠ¸ë¦¼ ëª¨ë¸ì˜ ì¶œë ¥ ë¶„ì‚°ì„ ìµœì†Œí™”í•˜ëŠ” ê²ƒì€ ì‹œìŠ¤í…œì˜ ì „ë°˜ì ì¸ ì„±ëŠ¥ì„ ê°œì„ í•˜ëŠ” ë° ë„ì›€ì´ ë˜ê¸° ë•Œë¬¸ì— ì¤‘ìš”í•©ë‹ˆë‹¤. ì¶œë ¥ ë¶„ì‚°ì€ ë™ì¼í•œ ì…ë ¥ ìŒì„± ì‹ í˜¸ê°€ ì£¼ì–´ì¡Œì„ ë•Œ ì—…ìŠ¤íŠ¸ë¦¼ ëª¨ë¸ì—ì„œ ìƒì„±ë˜ëŠ” ì¶œë ¥ íŠ¹ì§•ì˜ ë³€ë™ì„±ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì¶œë ¥ ë¶„ì‚°ì´ ë†’ìœ¼ë©´ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ëª¨ë¸ì´ ìˆ˜ì‹ í•˜ëŠ” íŠ¹ì§•ì´ ì¼ê´€ë˜ì§€ ì•Šê±°ë‚˜ ë…¸ì´ì¦ˆê°€ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì…ë ¥ ìŒì„± ì‹ í˜¸ë¥¼ ì •í™•í•˜ê²Œ ë¶„ë¥˜í•˜ê¸°ê°€ ë” ì–´ë ¤ì›Œì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¯¸ì„¸ ì¡°ì •ëœ ì—…ìŠ¤íŠ¸ë¦¼ ëª¨ë¸ì˜ ì²´í¬í¬ì¸íŠ¸ë¥¼ í‰ê· í™”í•¨ìœ¼ë¡œì¨ ì‘ì„±ìëŠ” ëª¨ë¸ì˜ ì¶œë ¥ ë¶„ì‚°ì„ ì¤„ì´ê³  ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ëª¨ë¸ì—ì„œ ì‘ì—…í•  ìˆ˜ ìˆëŠ” ë³´ë‹¤ ì¼ê´€ë˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” íŠ¹ì§•ì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ SER ì‹œìŠ¤í…œì˜ ì „ë°˜ì ì¸ ì •í™•ë„ì™€ ê²¬ê³ ì„±ì„ ê°œì„ í•˜ê³  ë‹¤ì–‘í•œ ìŒì„± ì‹ í˜¸ì™€ ë‹¤ì–‘í•œ ë§¥ë½ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë°œíœ˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. why did they use W2V2 and huBert both in Upstream model? ì´ ë…¼ë¬¸ì˜ ì €ìë“¤ì€ SER ì‹œìŠ¤í…œì˜ ì—…ìŠ¤íŠ¸ë¦¼ ëª¨ë¸ì— Wav2vec 2.0(W2V2)ê³¼ huBERTë¥¼ ëª¨ë‘ ì‚¬ìš©í–ˆëŠ”ë°, ì´ëŠ” ì‚¬ì „ í•™ìŠµëœ ë‹¤ì–‘í•œ ëª¨ë¸ì„ ê²°í•©í•˜ì—¬ ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ëª¨ìƒ‰í•˜ê¸° ìœ„í•´ì„œì˜€ìŠµë‹ˆë‹¤. W2V2ì™€ huBERTëŠ” ëª¨ë‘ ìŒì„± ì²˜ë¦¬ë¥¼ ìœ„í•´ ì‚¬ì „ í•™ìŠµëœ ìµœì²¨ë‹¨ ëª¨ë¸ì´ë©° ë‹¤ì–‘í•œ ìŒì„± ì‘ì—…ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë°œíœ˜í•˜ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. ì—…ìŠ¤íŠ¸ë¦¼ ëª¨ë¸ì— ë‘ ëª¨ë¸ì„ ëª¨ë‘ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ ì €ìë“¤ì€ ê° ëª¨ë¸ì˜ ê°•ì ì„ í™œìš©í•˜ê³  ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ëª¨ë¸ì—ì„œ ì‘ì—…í•  ìˆ˜ ìˆëŠ” ë”ìš± ê°•ë ¥í•˜ê³  ì •í™•í•œ ê¸°ëŠ¥ì„ ìƒì„±í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. íŠ¹íˆ W2V2ëŠ” ìŒì„± ì‹ í˜¸ì˜ ë¬¸ë§¥í™”ëœ í‘œí˜„ì„ ì¶”ì¶œí•˜ë„ë¡ ì„¤ê³„ëœ ë°˜ë©´, huBERTëŠ” í™”ìë³„ í‘œí˜„ì„ ì¶”ì¶œí•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ë‘ ëª¨ë¸ì„ ê²°í•©í•¨ìœ¼ë¡œì¨ ì €ìë“¤ì€ ì…ë ¥ ìŒì„± ì‹ í˜¸ì˜ ë¬¸ë§¥ ì •ë³´ì™€ í™”ìë³„ ì •ë³´ë¥¼ ëª¨ë‘ í¬ì°©í•˜ëŠ” íŠ¹ì§•ì„ ìƒì„±í•  ìˆ˜ ìˆì—ˆê³ , ì´ëŠ” SER ì‹œìŠ¤í…œì˜ ì „ë°˜ì ì¸ ì„±ëŠ¥ì„ ê°œì„ í•˜ëŠ” ë° ë„ì›€ì´ ë˜ì—ˆìŠµë‹ˆë‹¤. Experimental SetupExperiment for evaluation SER ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ì—¬ëŸ¬ ì‹¤í—˜ ì¤‘ì—ì„œ ê·¸ë¦¼3ê³¼ í‘œ 1ì˜ (1.A), (1.B) ì„¸íŠ¸ì— ì„¤ëª…ëœ ì‹¤í—˜ì„ ì„ íƒí–ˆìŠµë‹ˆë‹¤. ì´ ì‹¤í—˜ì˜ ëª©í‘œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: (1) the importance of fine-tuning the Upstream model; (2) the importance of averaging the Upstream and Downstream Model Checkpoints; (3) how Wav2vec 2.0 and huBERT can be combined to boost SER performance and (4) the performance of the two aggregators used: Mean Pooling and ECAPA-TDNN. what is aggregator? ì´ ë…¼ë¬¸ì—ì„œ ì„¤ëª…í•˜ëŠ” SER ì‹œìŠ¤í…œì˜ ë§¥ë½ì—ì„œ aggregatorëŠ” ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ëª¨ë¸ì˜ êµ¬ì„± ìš”ì†Œë¡œ, ì—…ìŠ¤íŠ¸ë¦¼ ëª¨ë¸ì˜ ì¶œë ¥ íŠ¹ì§•ì„ ê°ì • ì¸ì‹ì„ ìœ„í•œ ìµœì¢… ë¶„ë¥˜ê¸°ì— ì…ë ¥í•  ìˆ˜ ìˆëŠ” ë‹¨ì¼ íŠ¹ì§• ë²¡í„°ë¡œ ê²°í•©í•˜ëŠ” ì—­í• ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤. ì—…ìŠ¤íŠ¸ë¦¼ ëª¨ë¸ì€ ì…ë ¥ ìŒì„± ì‹ í˜¸ì—ì„œ featureë¥¼ ì¶”ì¶œí•˜ëŠ” ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì…ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ì´ëŸ¬í•œ featureëŠ” aggregatorë¡œ ì „ë‹¬ë˜ì–´ ê°ì • ì¸ì‹ ì‘ì—…ì„ ìœ„í•´ ì…ë ¥ ìŒì„± ì‹ í˜¸ì— ëŒ€í•œ ê°€ì¥ ì¤‘ìš”í•œ ì •ë³´ë¥¼ ìº¡ì²˜í•˜ëŠ” single feature vectorë¡œ ê²°í•©ë©ë‹ˆë‹¤. SER ì‹œìŠ¤í…œì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” aggregatorì—ëŠ” í‰ê·  í’€ë§ê³¼ ECAPA-TDNN ë“± ë‹¤ì–‘í•œ ìœ í˜•ì´ ìˆìŠµë‹ˆë‹¤. ì„œë¡œ ë‹¤ë¥¸ ìœ í˜•ì˜ ì…ë ¥ íŠ¹ì§• ë˜ëŠ” ì„œë¡œ ë‹¤ë¥¸ ìœ í˜•ì˜ ìŒì„± ì‹ í˜¸ì— ë” ì í•©í•œ aggregatorê°€ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ aggregator ì„ íƒì´ ì‹œìŠ¤í…œ ì„±ëŠ¥ì— ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì „ë°˜ì ìœ¼ë¡œ aggregatorëŠ” ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ëª¨ë¸ì´ ì‘ì—…í•  ìˆ˜ ìˆëŠ” ê°•ë ¥í•˜ê³  ì •í™•í•œ íŠ¹ì§• ë²¡í„°ë¥¼ ìƒì„±í•˜ëŠ” ë° ë„ì›€ì„ ì¤Œìœ¼ë¡œì¨ SER ì‹œìŠ¤í…œì—ì„œ ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤. Fig3ì— ì˜í•˜ë©´, ì‹¤í—˜(1-4)ì—ì„œ ì‚¬ìš©ëœ Upstream model used is either Wav2vec 2.0 or huBERT and the Downstream model is composed by a Mean Average Aggregator followed by a linear classifier. (1-2)ì˜ ì‹¤í—˜ì—ì„œëŠ” ì—…ìŠ¤íŠ¸ë¦¼ ëª¨ë¸ê³¼ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ëª¨ë¸ ëª¨ë‘ í‰ê· í™” í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (3-4)ì˜ ì‹¤í—˜ì—ì„œëŠ” ì—…ìŠ¤íŠ¸ë¦¼ ëª¨ë¸ê³¼ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ëª¨ë¸ ëª¨ë‘ í‰ê· ì„ ëƒˆìŠµë‹ˆë‹¤. ì‹¤í—˜(5-6)ê³¼ (3-4)ëŠ” ìœ ì‚¬í•˜ë©° ìœ ì¼í•œ ì°¨ì´ì ì€ ì‚¬ìš©ëœ aggregatorì¸ ECAPA-TDNNì´ ì‚¬ìš©ëœ ê²ƒì…ë‹ˆë‹¤. Experiment 7 and 8 in the paper describe different types of feature fusion that were used to combine the output features of the Wav2vec 2.0 and huBERT models in the Upstream component of the SER system. ì‹¤í—˜(7)ì—ì„œëŠ” ECAPA-TDNN aggregatorë¥¼ í†µê³¼ í•˜ê¸° ì§ì „ì— W2V2ê³¼ huBERT ê¸°ëŠ¥ ê°„ì˜ early fusionì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. In Experiment 7, the authors used early fusion to combine the output features of the Wav2vec 2.0 and huBERT models. Early fusion involves combining the input features from two different modalities (in this case, speech and text) before they are processed by the Upstream models. Specifically, the authors concatenated the output features of the two models before passing them through the ECAPA-TDNN aggregator. ì‹¤í—˜(8)ì—ì„œëŠ” ë‘ê°œì˜ ECAPA-TDNNì´ ìƒì„±í•œ utterance embeddingì„ ë‚˜ì¤‘ì— ìœµí•©í–ˆëŠ”ë°, ì²«ë²ˆì§¸ëŠ” W2V2 ê°€ëŠ¥ì—ì„œ ì‘ë™í•˜ê³  ë‘ë²ˆì§¸ëŠ” huBERTê¸°ëŠ¥ì—ì„œ ì‘ë™í•©ë‹ˆë‹¤. In Experiment 8, the authors used later fusion to combine the output features of the Wav2vec 2.0 and huBERT models. Later fusion involves combining the output features of the two models after they have been processed by the Upstream models. Specifically, the authors used two separate ECAPA-TDNN aggregators to process the output features of the two models, and then concatenated the resulting feature vectors before passing them through the final classifier. Fusion refers to the process of combining information from multiple sources to produce a single output. In the context of the SER system described in the paper, fusion is used to combine the output features of the Wav2vec 2.0 and huBERT models in order to produce a more robust and accurate feature vector for the Downstream model to work with. By combining the strengths of the two models, the authors were able to improve the overall performance of the SER system. Experiments to be used as baselines Fig 3 ì‹¤í—˜ê³¼ ë¹„êµí•˜ê¸° ìœ„í•œ base lineìœ¼ë¡œ ìŒì„± í”¼ì²˜ëŠ” Fbank, í…ìŠ¤íŠ¸ëŠ” BERTë¥¼ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. ì‹¤í—˜(9)ì—ì„œëŠ” í‘œì¤€ í•„í„° ë±…í¬ê°€ ì—…ìŠ¤íŠ¸ë¦¼ëª¨ë¸ë¡œ ì‚¬ìš©ë˜ê³  ì‹¤í—˜(10)ì—ì„œëŠ” BERT ëª¨ë¸ì´ ì—…ìŠ¤íŠ¸ë¦¼ ëª¨ë¸ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤. ì‹¤í—˜ (11)ì—ì„œëŠ” ìŒì„±ì—ì„œëŠ” Fbank, í…ìŠ¤íŠ¸ featureëŠ” BERTë¥¼ ì‚¬ìš©í•˜ì—¬ later fusion fashion ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤. It is important to emphasize that the Fbank used here does not have explicit pitch information attached to it and that the fine-tuning optimization process of the BERT model may not follow the most advanced SOTA techniques available nowadays. However, despite not being as carefully prepared as it could be, these baseline models can help us to obtain insight on how powerful these fine-tuned and averaged Wav2vec 2.0 and huBERT features are. RESULTS í‘œ ì„¤ëª…: In column 2 of Table 1, under the term (#), we indicate the number of the 11 experiments evaluated. In column 3 we indicate the input modality used in each experiment. In column 4 under the term Upstream model we can find the indication of the Input feature; if the Upstream model has been fine-tuned (FT); and if the Upstream model has been Averaged (AVG). The symbol â€œ+â€ in experiment 7 (huBERT + W2V2) indicates early fusion of the features and the symbol â€œ&amp;â€ in the experiments 8 and 11 indicates later fusion of the features. In column 5 under the term Downstream model we can find the indication of the Aggregator Model used (AGG); the Classification Model (Classifier) used; and if the full Downstream Model has been averaged (AVG). Since the test sets of IEMOCAP are slightly imbalanced between different emotion categories, in column 6 of Table 1 under the term Accuracy we report both Weighted Accuracy (WACC) and Unweighted Accuracy (UACC). Finally, in column 1 of Table 1 under the term SET we have: in (1.A) the subset of experiments from Figure 3 that use Mean Pooling as Aggregator; in (1.B) the subset of experiments from Figure 3 that use ECAPA-TDNN as Aggregator and in (2) the baseline experiments described in Figure 4.","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speech Emotion Recognition","slug":"Paper/Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/categories/Paper/Speech-Emotion-Recognition/"}],"tags":[{"name":"Speech Emotion Recognition","slug":"Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/tags/Speech-Emotion-Recognition/"},{"name":"Self-Supervised Features","slug":"Self-Supervised-Features","permalink":"https://jmj3047.github.io/tags/Self-Supervised-Features/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speech Emotion Recognition","slug":"Paper/Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/categories/Paper/Speech-Emotion-Recognition/"}]},{"title":"Machine Learning Data Lifecycle in Production_Quiz","slug":"MLOps2_Quiz","date":"2023-07-10T15:00:00.000Z","updated":"2023-08-27T03:51:52.199Z","comments":true,"path":"2023/07/11/MLOps2_Quiz/","link":"","permalink":"https://jmj3047.github.io/2023/07/11/MLOps2_Quiz/","excerpt":"","text":"ê°œìš”Coursera ML Ops Course 2 Quiz 1. Intro to MLEP Link: https://www.coursera.org/learn/machine-learning-data-lifecycle-in-production/home/week/1 2. Data Collection 3. Data Labeling 4.Issues in Training Data 5.Feature Engineering and Preprocessing Link: https://www.coursera.org/learn/machine-learning-data-lifecycle-in-production/home/week/2 6. Feature Transformation 7. Feature Selection 8. Data Journey Link: https://www.coursera.org/learn/machine-learning-data-lifecycle-in-production/home/week/3 9.Schema Environments 10. Enterprise Data Storage 11. Advanced Labelling Link: https://www.coursera.org/learn/machine-learning-data-lifecycle-in-production/home/week/4 12. Data Augmentation 13. Different Data Types","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"Coursera","slug":"Coursera","permalink":"https://jmj3047.github.io/tags/Coursera/"},{"name":"Quiz","slug":"Quiz","permalink":"https://jmj3047.github.io/tags/Quiz/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}]},{"title":"Machine Learning Data Lifecycle in Production","slug":"MLOps_Part2","date":"2023-07-10T15:00:00.000Z","updated":"2023-07-17T10:30:48.185Z","comments":true,"path":"2023/07/11/MLOps_Part2/","link":"","permalink":"https://jmj3047.github.io/2023/07/11/MLOps_Part2/","excerpt":"","text":"Course Link Lecture 2 in MLOps Data Label Collecting Data You need to make sure that your data covers the same region of your feature space as the prediction request that youâ€™ll get your training data and you want to make sure that youâ€™ve really maximize the predictive signal in that data. And you need to worry about the data quality not just at the beginning but throughout the life of the application. So part of that is making sure that your sourcing data responsibly and youâ€™re thinking about things like bias and fairness. One of the key aspects of collecting data is to make sure that youâ€™re collecting it responsibly and paying attention to things like security, privacy and fairness. key points, first of all, always account for fair Raters and fair representation in your data set to avoid potential biases. And take into account who those labelers are and what their incentives are, because if you design the incentives incorrectly, you could get a lot of garbage in your data. The cost is certainly always going to be an important consideration. So if you can find a way to do it with a high level of quality but at less cost, thatâ€™s great. But you need enough data. You need to find a way to do that. Itâ€™s one of the challenges of production applications and finally data freshness too. Youâ€™re going to be working with data and depending on how the world changes around the application and the data that you have, youâ€™re going to need to refresh that data on some regular basis and detect when you need to do that. So those are all issues you need to think about to really manage collection of data and to do it in a responsible way. Labeling Data The key points of what weâ€™re talking about here, model performance decays over time. It may decay slowly over time, in things like cats and dogs, that doesnâ€™t change very quickly, or it may change very fast, things like markets. Model retraining will help you improve or maintain your performance. Certainly as your model performance decays, itâ€™ll help you do that. Data labeling, assuming youâ€™re doing supervised learning, which is pretty common, data labeling is a key part of that. You really need to think about how youâ€™re going to approach that in your particular problem, in your particular domain and with the systems that you have available to you. Validating Data Just to wrap up, this week, you saw differences between ML modeling in academic or research environments and production ML systems. We discussed responsible data collection and how to really approach building a fair production ML system. We learned about process feedback and direct labeling and also human labeling. We looked at some of the issues that you can have with data and how to identify and detect those issues. Feature Preprocessing The art of feature engineering tries to improve your modelâ€™s ability to learn while reducing if possible, the compute resources it requires, it does this by transforming and projecting, eliminating and or combining the features in your raw data to form a new version of your data set. So typically across the ML pipeline, you incorporate the original features often transformed or projected to a new space and or combinations of your features. Objective function must be properly tuned to make sure your model is heading in the right direction and that is consistent with your feature engineering. You can also update your model by adding new features from the set of data that is available to you unlike many things in ML, this tends to be an iterative process that gradually improves your results as you iterate or you hope it does. You have to monitor that and if itâ€™s not improving, maybe back up and take another approach. To review some key points, as the quote from Andrew Ng demonstrates, feature engineering can be very difficult and time consuming but it is also very important to success. You want to squeeze the most out of your data and you do that using feature engineering, by doing that, you enable your models to learn better. You also want to make sure that you concentrate predictive information, your data into as few features as possible to make the best and least expensive use of your compute resources. And you need to make sure that you apply the same feature engineering during serving as you applied during training. Hereâ€™s some of the main Preprocessing Operation. One of the most important Preprocessing Operations is Data cleansing, which in broad terms consists in eliminating or correcting erroneous data. Youâ€™ll often need to perform transformations on your data, so scaling or normalizing your numeric values, for example. Since models, especially neural networks, are sensitive to the amplitude or range of numerical features, data preprocessing helps Machine Learning build better predictive Models. Dimensionality reduction involves reducing the number of features by creating lower dimension and more robust data represents. Feature construction can be used to create new features by using several different techniques, which weâ€™ll talk about some of them. Key points. Data preprocessing is a technique thatâ€™s used to transform raw data into useful data for training the Model. Feature Engineering consists in mapping raw input data and creating a feature vector from it using different techniques with different kinds of data. It can also include things like mapping data from one space into a different space, which depending on the characteristics of a Model, say a linear Model versus a neural network can have a big difference in how well the Model can learn from it. feature engineering Key points for this particular section, feature engineering. Itâ€™s going to prepare and tune and transform and extract and construct features where weâ€™re going to work with features and change them starting with our raw data through to the data that weâ€™re going to give to our model. Feature engineering is very important for model refinement. Really, it can make the difference between successfully modeling something and not. Feature engineering really helps with ML Analysis and really developing that intuitive understanding of our data. feature crosses What are Feature crosses? Well, they combine multiple features together into a new feature. Thatâ€™s fundamentally what a feature across. It encodes non-linearity in the feature space, or encodes the same information and fewer features. We can create many different kinds of feature crosses and it really depends on our data. Feature Crossing as a way to create synthetic features, often encoding non-linearity in the features space. Weâ€™re going to transform both categorical and numerical. We could do that in, into either continuous variables or the other way around. Feature selection feature space: a feature space is defined by the n dimensional space that your features defined. So if you have two features, a feature space is two dimensional. If you have three features, its three dimensional and so forth, it does not include the target label. filter methods for filter methods, weâ€™re primarily using correlation to look for the features that contain the information that weâ€™re going to use to predict our target. we are going to start with all of the features and weâ€™re going to select the best subset and weâ€™re going to give those to our model and thatâ€™s going to give us our performance for the model with this subset of our features. wrapper method It stores supervised method, but weâ€™re going to use this with a model and thereâ€™s different ways to do it. But basically youâ€™re iterating through, itâ€™s a search method against the features that you have using a model as the measure of their effectiveness. We can do it through forward elimination and weâ€™ll talk about this in a second. Forward elimination, backward elimination or recurrent feature elimination. We start with all of our feature, regenerate a subset of those features, and weâ€™ll talk about how that gets generated, that gets given to our model. The results that is generated from that model is then used to generate the next subset. That becomes this feedback loop to select the best subset of our features using our model as a measure. That gives us the performance of the final best subset that is selected. Embedded method(feature importance) L1 or L2 regularization is essentially an embedded method for doing feature selection. Feature importance is another method. Both of these are highly connected to the model that youâ€™re using. So these both L1 regularization and feature importance really sort of an intrinsic characteristic of the model that youâ€™re working with.\\ Data Storage MLMD(ML meta data) ML metadata stores a wide range of information about the results of the components and execution runs of a pipeline. You learned a lot about the architecture and nomenclature of ML metadata or MLMD and the artifacts and entities which it contains. This should give you some idea of how you can leverage MLMD to track metadata and the results flowing through your pipeline to better understand your training process, both now and in previous training runs of your pipeline. Data Warehouse Data warehouses are meant for analyzing data, whereas databases are often used for transaction purposes. Inside a data warehouse, there may be a delay between storing the data and the data getting reflected in the system. But in a database, data is usually available immediately after itâ€™s stored. Data warehouses store data as a function of time, and therefore, historical data is also available. Data warehouses are typically capable of storing a larger amount of data compared to databases. Queries in data warehouses are complex in nature and tend to run for a long time. Whereas queries in database are simple and tend to run in real time. Normalization is not necessary for data warehouses, but it should be used with databases. Data Lake The primary difference between them is that in a data warehouse, data is stored in a consistent format which follows a schema, whereas in data lakes, the data is usually in its raw format. In data lakes, the reason for storing the data is often not determined ahead of time. This is usually not the case for a data warehouse, where itâ€™s usually stored for a particular purpose. Data warehouses are often used by business professionals as well, whereas data lakes are typically used only by data professionals such as data scientists. Since the data in data warehouses is stored in a consistent format, changes to the data can be complex and costly. Data lakes however are more flexible, and make it easier to make changes to the data.","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}],"tags":[{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Coursera","slug":"Coursera","permalink":"https://jmj3047.github.io/tags/Coursera/"},{"name":"AI Studies","slug":"AI-Studies","permalink":"https://jmj3047.github.io/tags/AI-Studies/"},{"name":"ML Operations","slug":"ML-Operations","permalink":"https://jmj3047.github.io/tags/ML-Operations/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}]},{"title":"Domain Invariant Feature Learning for Speaker-Independent Speech Emotion Recognition","slug":"DIFL_SI_SER","date":"2023-07-02T15:00:00.000Z","updated":"2023-07-05T07:14:22.920Z","comments":true,"path":"2023/07/03/DIFL_SI_SER/","link":"","permalink":"https://jmj3047.github.io/2023/07/03/DIFL_SI_SER/","excerpt":"","text":"Journal&#x2F;Conference : IEEE&#x2F;ACM Transactions on Audio, Speech, and Language ProcessingYear(published year): 2022Author: Cheng Lu , Yuan Zong , Member, IEEE, Wenming Zheng , Senior Member, IEEE, Yang Li , Member, IEEE, Chuangao Tang , and BjÃ¶rn W. Schuller , Fellow, IEEESubject: Domain Invariant Feature Learning(DIFL), Speech Emotion Recognition(SER) Domain Invariant Feature Learning for Speaker-Independent Speech Emotion Recognition Summary ë©€í‹°ì†ŒìŠ¤ UDAë¥¼ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ ê°„ë‹¨í•˜ë©´ì„œë„ íš¨ê³¼ì ì¸ ë„ë©”ì¸ ë¶ˆë³€ í•™ìŠµ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ìš°ë¦¬ê°€ ì•„ëŠ” í•œ, ë©€í‹° ì†ŒìŠ¤ UDAì˜ ê´€ì ì—ì„œ í™”ì ë…ë¦½ì  SERì„ ë‹¤ë£¬ ì—°êµ¬ëŠ” ì´ë²ˆì´ ì²˜ìŒì…ë‹ˆë‹¤. We propose a simple, yet effective domain-invariant learning framework to carry out multi-source UDA. To the best of our knowledge, this is the first study dealing with speaker-independent SER from the perspective of multi-source UDA. í™”ìì˜ ì •ì²´ì„±ê³¼ ê°ì •ì˜ í˜¼ë™ì„ ì œê±°í•˜ê¸° ìœ„í•´ íŠ¹ì§• ì¶”ì¶œê¸° ë¸”ë¡ì— ê³„ì¸µì  ë¶„í¬ ì •ë ¬ ë°©ë²•ì„ ì œì•ˆí•˜ê³ , ê³„ì¸µì  ì •ë ¬ ê³„ì¸µì— í¬í•¨ëœ ê°•-ì•½ ì •ë ¬ ì „ëµì„ í™œìš©í•˜ì—¬ ë¡œì»¬ ë° ê¸€ë¡œë²Œ íŠ¹ì§•ì— ëŒ€í•´ ê°ê° ê°•-ì•½ ì •ë ¬ì„ êµ¬í˜„í•©ë‹ˆë‹¤. We propose a hierarchical distribution alignment method in the feature extractor block to remove the confusion of speakersâ€™ identity and emotion, in which a strong-weak alignment strategy embedded in hierarchical alignment layers is utilized to realize the strong and weak alignment for local and global features, respectively. í™”ì ë…ë¦½ì  SERì— ëŒ€í•œ ë„ë©”ì¸ ë¶ˆë³€ íŠ¹ì§•ì„ ì¶”ê°€ë¡œ í•™ìŠµí•˜ê¸° ìœ„í•´ ë‹¤ì¤‘ íŒë³„ìë¥¼ ë„ì…í•˜ì—¬ ë„ë©”ì¸ íŒë³„ìì™€ í™”ì íŒë³„ìë¥¼ ì‚¬ìš©í•˜ì—¬ í™”ì ë¶ˆë³€ íŠ¹ì§•ì„ ì–»ê³  ë ˆì´ë¸” íŒë³„ìë¥¼ ì‚¬ìš©í•˜ì—¬ ê°ì •ê³¼ ê´€ë ¨ëœ íŒë³„ íŠ¹ì§•ì„ ì–»ìŠµë‹ˆë‹¤. We introduce multiple discriminators to further learn domain invariant features for the speaker-independent SER, in which the domain discriminator and speaker discriminator are used to obtain speaker-invariant features while the label discriminator is used to obtain discriminative features that are emotion-related. I. IntroductionDIFLì˜ ì •ì˜ ë„ë©”ì¸ ë¶ˆë³€ íŠ¹ì§• í•™ìŠµ(DIFL)ì€ í™”ì ë…ë¦½ì ì¸ ìŒì„± ê°ì • ì¸ì‹ì„ ë‹¤ë£¨ê¸° ìœ„í•´ â€œí™”ì ë…ë¦½ì ì¸ ìŒì„± ê°ì • ì¸ì‹ì„ ìœ„í•œ ë„ë©”ì¸ ë¶ˆë³€ íŠ¹ì§• í•™ìŠµâ€ ë…¼ë¬¸ì—ì„œ ì œì•ˆëœ ë°©ë²•ì…ë‹ˆë‹¤. DIFLì˜ ê¸°ë³¸ ì•„ì´ë””ì–´ëŠ” ë‹¤ì¤‘ ì†ŒìŠ¤ ë¹„ì§€ë„ ë„ë©”ì¸ ì ì‘(multi-source unsupervised domain adaptation, UDA)ì˜ ê´€ì ì—ì„œ í™”ìê°€ ë‹¬ë¼ì„œ ë°œìƒí•˜ëŠ” í›ˆë ¨ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„° ê°„ì˜ ë„ë©”ì¸ ì´ë™ì„ ì œê±°í•˜ì—¬ í™”ì ë¶ˆë³€ ê°ì • íŠ¹ì§•ì„ í•™ìŠµí•˜ëŠ” ê²ƒ(learn speaker invariant emotion feature)ì…ë‹ˆë‹¤. multi-source UDAì˜ ì •ì˜ ë‹¤ì¤‘ ì†ŒìŠ¤ ë¹„ì§€ë„ ë„ë©”ì¸ ì ì‘(UDA)ì€ í•˜ë‚˜ ì´ìƒì˜ ì†ŒìŠ¤ ë„ë©”ì¸ì—ì„œ í•™ìŠµëœ ëª¨ë¸ì„ ë ˆì´ë¸”ì´ ì§€ì •ëœ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ëŒ€ìƒ ë„ë©”ì¸ì— ì ì‘ì‹œí‚¤ê¸° ìœ„í•´ ë¨¸ì‹  ëŸ¬ë‹ì— ì‚¬ìš©ë˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤. ë‹¤ì¤‘ ì†ŒìŠ¤ UDAì—ì„œëŠ” ëª¨ë¸ì´ ì—¬ëŸ¬ ì†ŒìŠ¤ ë„ë©”ì¸ì—ì„œ ë™ì‹œì— í•™ìŠµë˜ë©°, ëª©í‘œëŠ” ëŒ€ìƒ ë„ë©”ì¸ì— ì˜ ì¼ë°˜í™”í•  ìˆ˜ ìˆëŠ” ë„ë©”ì¸ ë¶ˆë³€í˜• í‘œí˜„ì„ í•™ìŠµí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ ê¸°ë²•ì€ ì—¬ëŸ¬ ì†ŒìŠ¤ì˜ ë°ì´í„°ê°€ ì„œë¡œ ë‹¤ë¥¸ ë¶„í¬ì™€ íŠ¹ì„±ì„ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ë³µì¡í•œ ì‹¤ì œ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ë‹¤ë£° ë•Œ íŠ¹íˆ ìœ ìš©í•©ë‹ˆë‹¤. ë‹¤ì¤‘ ì†ŒìŠ¤ ë¹„ì§€ë„ ë„ë©”ì¸ ì ì‘ì€ ì „ì´ í•™ìŠµì˜ í•œ ìœ í˜•ì…ë‹ˆë‹¤. ì „ì´ í•™ìŠµì€ í•˜ë‚˜ ì´ìƒì˜ ì†ŒìŠ¤ ë„ë©”ì¸ì—ì„œ í•™ìŠµí•œ ì§€ì‹ì„ ëª©í‘œ ë„ë©”ì¸ìœ¼ë¡œ ì˜®ê¸°ëŠ” ê³¼ì •ì„ ë§í•©ë‹ˆë‹¤. ë‹¤ì¤‘ ì†ŒìŠ¤ UDAì—ì„œëŠ” ì—¬ëŸ¬ ì†ŒìŠ¤ ë„ë©”ì¸ì—ì„œ í•™ìŠµí•œ ì§€ì‹ì„ ëª©í‘œ ë„ë©”ì¸ìœ¼ë¡œ ì „ì†¡í•˜ì—¬ ëª©í‘œ ì‘ì—…ì—ì„œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê°œì„ í•©ë‹ˆë‹¤. ê´€ë ¨ ë„ë©”ì¸ì˜ ì§€ì‹ì„ í™œìš©í•¨ìœ¼ë¡œì¨ ëª¨ë¸ì´ ë³´ì´ì§€ ì•ŠëŠ” ìƒˆë¡œìš´ ë°ì´í„°ì— ë” ì˜ ì ì‘í•  ìˆ˜ ìˆëŠ” ë³´ë‹¤ ì¼ë°˜í™” ê°€ëŠ¥í•œ í‘œí˜„ì„ í•™ìŠµí•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ ì´ ì•„ì´ë””ì–´ì˜ í•µì‹¬ì…ë‹ˆë‹¤.â†’ ê·¸ëŸ¬ë‹ˆê¹Œ ì´ ë…¼ë¬¸ì—ì„œ ì†ŒìŠ¤ ë„ë©”ì¸ì—ì„œ íƒ€ê²Ÿ ë„ë©”ì¸ìœ¼ë¡œ ë„˜ì–´ê°€ëŠ” ê³¼ì •ì˜ í™”ìì™€ ê´€ë ¨ëœ íŠ¹ì„±ì„ ì œê±°ì‹œì¼œì„œ ê°ì • ì¸ì‹ì´ ë” ì˜ë˜ê²Œ í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•˜ì˜€ìŒ. UDAì˜ ì •ì˜ ë‹¤ì¤‘ ì†ŒìŠ¤ UDAëŠ” ë¹„ì§€ë„ ë„ë©”ì¸ ì ì‘ ëª¨ë¸ì˜ í•œ ìœ í˜•ì…ë‹ˆë‹¤. ë¹„ì§€ë„ ë„ë©”ì¸ ì ì‘ì€ ëŒ€ìƒ ë„ë©”ì¸ì˜ ë ˆì´ë¸”ì´ ì§€ì •ëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  ì†ŒìŠ¤ ë„ë©”ì¸ì—ì„œ í•™ìŠµëœ ëª¨ë¸ì„ ë ˆì´ë¸”ì´ ì§€ì •ëœ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ëŒ€ìƒ ë„ë©”ì¸ì— ì ì‘ì‹œí‚¤ëŠ” í”„ë¡œì„¸ìŠ¤ë¥¼ ë§í•©ë‹ˆë‹¤. ëª©í‘œëŠ” ê³µìœ ëœ íŠ¹ì§• ê³µê°„ì—ì„œ ì†ŒìŠ¤ ë„ë©”ì¸ê³¼ ëŒ€ìƒ ë„ë©”ì¸ì˜ ë¶„í¬ë¥¼ ì •ë ¬í•˜ì—¬ ëŒ€ìƒ ë„ë©”ì¸ì— ì˜ ì¼ë°˜í™”í•  ìˆ˜ ìˆëŠ” ë„ë©”ì¸ ë¶ˆë³€ í‘œí˜„ì„ í•™ìŠµí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ê³µìœ  íŠ¹ì§• ê³µê°„ì—ì„œ ì†ŒìŠ¤ ë„ë©”ì¸ê³¼ ëŒ€ìƒ ë„ë©”ì¸ ê°„ì˜ ê±°ë¦¬ ë˜ëŠ” ë¶ˆì¼ì¹˜ë¥¼ ìµœì†Œí™”í•˜ë©´ì„œ ë‹¹ë©´í•œ ì‘ì—…ì— ëŒ€í•œ íŒë³„ ì •ë³´ë¥¼ ë³´ì¡´í•¨ìœ¼ë¡œì¨ ë‹¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ì¤‘ ì†ŒìŠ¤ UDAëŠ” ì´ ì•„ì´ë””ì–´ë¥¼ ì—¬ëŸ¬ ì†ŒìŠ¤ ë„ë©”ì¸ìœ¼ë¡œ í™•ì¥í•˜ì—¬ ëª¨ë¸ì´ ëŒ€ìƒ ë„ë©”ì¸ì— ì˜ ì¼ë°˜í™”í•˜ë©´ì„œ ë™ì‹œì— ì—¬ëŸ¬ ë‹¤ë¥¸ ì†ŒìŠ¤ ë„ë©”ì¸ì— ì ì‘í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•©ë‹ˆë‹¤. ë¹„ì§€ë„ ë„ë©”ì¸ ì ì‘ì€ ë¨¸ì‹  ëŸ¬ë‹ì—ì„œ í•œ ë„ë©”ì¸ì—ì„œ í•™ìŠµëœ ëª¨ë¸ì„ ë ˆì´ë¸”ì´ ì§€ì •ëœ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ë‹¤ë¥¸ ë„ë©”ì¸ì— ì ì‘ì‹œí‚¤ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤. ëª©í‘œëŠ” ê³µìœ ëœ íŠ¹ì§• ê³µê°„ì—ì„œ ì†ŒìŠ¤ ë„ë©”ì¸ê³¼ ëŒ€ìƒ ë„ë©”ì¸ì˜ ë¶„í¬ë¥¼ ì •ë ¬í•˜ì—¬ ëŒ€ìƒ ë„ë©”ì¸ì— ì˜ ì¼ë°˜í™”í•  ìˆ˜ ìˆëŠ” í‘œí˜„ì„ í•™ìŠµí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ë‹¤ì¤‘ ì†ŒìŠ¤ UDAëŠ” ì´ ì•„ì´ë””ì–´ë¥¼ ì—¬ëŸ¬ ì†ŒìŠ¤ ë„ë©”ì¸ìœ¼ë¡œ í™•ì¥í•˜ì—¬ ëª¨ë¸ì´ ëŒ€ìƒ ë„ë©”ì¸ì— ì˜ ì¼ë°˜í™”í•˜ë©´ì„œ ë™ì‹œì— ì—¬ëŸ¬ ë‹¤ë¥¸ ì†ŒìŠ¤ ë„ë©”ì¸ì— ì ì‘í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•©ë‹ˆë‹¤. â†’ UDAëŠ” source ë„ë©”ì¸ì—ì„œ í•™ìŠµì´ ì™„ë£Œëœ ëª¨ë¸ì„ target ë„ë©”ì¸(ë°ì´í„°ê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ”)ì— ì ì‘ì‹œí‚¤ëŠ”ë° ì‚¬ìš©ë˜ëŠ” ë°©ë²•. ëª©í‘œëŠ” source ë„ë©”ì¸ ëª¨ë¸ì„ target ë„ë©”ì¸ì´ ì˜ ì¼ë°˜í™” ì‹œì¼œì„œ ì ì‘ ì‹œí‚¤ëŠ” ê²ƒì„. multi-source UDAëŠ” ì´ëŸ¬í•œ ì ì„ ë³´ì™„í•˜ê¸° ìœ„í•´ì„œ source domainì˜ ê°œìˆ˜ë¥¼ ë‹¤ì¤‘ìœ¼ë¡œ ë§Œë“¤ì–´ ì¼ë°˜í™”ê°€ ë” ì˜ë˜ê²Œ ì ì‘ ì‹œí‚´. II. Related Work Overview of the Domain Invariant Feature Learning (DIFL) framework for speaker-independent SER, including the feature extractor block with a hierarchical alignment layer (HAL), and the discriminator block with a domain adversarial layer (DAL) consisting of a label discriminator $G_l$, source-target domain discriminator $G_{st}$, and speaker discriminator $G_{sp}$, where the GRL represents the gradient reversal layer. A. Speaker-Independent SER ê¸°ì¡´ì˜ ì—°êµ¬ ë°©í–¥: (1) feature fusion; (2) classifier enhancement; and (3) speaker normalization. feature fusion: The feature fusion category aims to fuse multiple hand-crafted features to obtain a discriminative feature set, which is mostly used in early works. classifier enhancement: to enhance the robustness of the classifier to deal with speaker-independent SER, which are usually combined with suited feature fusion based methods. speaker normalization: The speaker normalization category mainly utilizes speaker normalization (e. g., mean normalization, cumulative distribution mapping, factor analysis) to reduce the specific variability of speakers during the feature extraction. B. Multi source UDA It mainly focuses on the situation that the source data are collected from multiple different domains [36]. Therefore, multi-source UDA not only considers the domain shift between the source and target domains, but also handles the discrepancy across multiple domains in the source data [36], [37]. latent space transformation The latent space transformation based methods attempt to convert different domain features to a specific latent space, and then use the divergence-based or adversarial-based loss to align the domain shift. It is worth noting that the divergence-based methods are non-parametric, only depending on the selection of divergence functions and embedding position in feature layers, while the adversarial-based methods need to learn new parameters of the discriminator. In this paper, we combine these two methods in the DIFL framework to find the optimal balance among their advantages and disadvantages. intermediate domain generation intermediate domain generation strategy tries to generate the new adapted domain for each domain in the source data, where these new domains are indistinguishable from the target domain. These methods are all based on GANs [44] or auto-encoders [45], e. g., Coupled GAN [46], CycleGAN in MADAN [47], or variational auto-encoder [45], as the generator to generate the intermediate domain from the latent space. III. Proposed MethodA. Hierarchial Representation for Emotional Speechê°ì •ì  ìŒì„±ì„ ìœ„í•œ ê³„ì¸µì  í‘œí˜„ì€ ìŒì„± ë°ì´í„°ë¥¼ ê³„ì¸µì  ë˜ëŠ” ê³„ì¸µì  êµ¬ì¡°ë¡œ í‘œí˜„í•˜ëŠ” ë°©ì‹ì„ ë§í•˜ë©°, ê° ê³„ì¸µì€ ìŒì„± ë°ì´í„°ì˜ ì ì  ë” ì¶”ìƒì ì´ê±°ë‚˜ ë³µì¡í•œ íŠ¹ì§•ì„ í¬ì°©í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ê³„ì¸µì  í‘œí˜„ì€ ì—¬ëŸ¬ ê°œì˜ ì»¨ë³¼ë£¨ì…˜ ë¸”ë¡ê³¼ ì™„ì „ ì—°ê²°(FC) ë¸”ë¡ìœ¼ë¡œ êµ¬ì„±ëœ ì‹¬ì¸µ ì»¨ë³¼ë£¨ì…˜ ì‹ ê²½ë§(DCNN)ì„ ì‚¬ìš©í•˜ì—¬ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. DCNNì—ì„œ ê° ì»¨ë³¼ë£¨ì…˜ ë¸”ë¡ì—ëŠ” ì»¨ë³¼ë£¨ì…˜, ì¼ê´„ ì •ê·œí™”, ReLU ë° MaxPooling ì—°ì‚°ì´ í¬í•¨ë©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì—°ì‚°ì€ ìŒì„± ë°ì´í„°ì—ì„œ ë¡œì»¬ í”¼ì²˜ë¥¼ ì¶”ì¶œí•˜ëŠ” ë° ì‚¬ìš©ë˜ë©°, ì´ í”¼ì²˜ëŠ” fc ë¸”ë¡ì„ í†µê³¼í•˜ì—¬ íŠ¹ì • ì‘ì—…ê³¼ ê´€ë ¨ì„±ì´ ë†’ê³  ë³€ë³„ë ¥ì´ ê°•í•œ ê¸€ë¡œë²Œ í”¼ì²˜ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤. ì†ŒìŠ¤ ë°ì´í„°ì™€ íƒ€ê¹ƒ ë°ì´í„°ì˜ íŠ¹ì§• ë§µì€ ê°ê° ì†ŒìŠ¤ ë°ì´í„°ì˜ ê²½ìš° Cm(Xs)ì™€ Ln(Xs), íƒ€ê¹ƒ ë°ì´í„°ì˜ ê²½ìš° Cm(Xt)ì™€ Ln(Xt)ë¡œ í‘œí˜„ë©ë‹ˆë‹¤. ì—¬ê¸°ì„œ Xsì™€ XtëŠ” ê°ê° ì†ŒìŠ¤ ë„ë©”ì¸ê³¼ íƒ€ê¹ƒ ë„ë©”ì¸ì˜ ì›ì‹œ ìŒì„± ë°ì´í„°ë¥¼ ë‚˜íƒ€ë‚´ê³ , Cmê³¼ Lnì€ ê°ê° ì»¨ë³¼ë£¨ì…˜ê³¼ fc ë¸”ë¡ì—ì„œ ì–»ì€ íŠ¹ì§• ë§µì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ê³„ì¸µì  í‘œí˜„ì„ ì‚¬ìš©í•˜ë©´ DCNNì€ ìŒì„± ë°ì´í„°ì—ì„œ ë³´ë‹¤ ìœ ìµí•˜ê³  ì°¨ë³„ì ì¸ íŠ¹ì§•ì„ ì¶”ì¶œí•  ìˆ˜ ìˆìœ¼ë©°, ì´ë¥¼ í†µí•´ ìŒì„± ê°ì • ì¸ì‹(SER) ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê³„ì¸µì  í‘œí˜„ì„ í†µí•´ ëª¨ë¸ì€ ê¸°ì¡´ì˜ ìˆ˜ì‘ì—… í”¼ì²˜ë¡œëŠ” í¬ì°©í•˜ê¸° ì–´ë ¤ìš´ ë³µì¡í•˜ê³  ë¯¸ë¬˜í•œ ìŒì„± ê°ì •ì˜ ë³€í™”ë¥¼ í¬ì°©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. local feature and global feature ë”¥ëŸ¬ë‹ì˜ ë§¥ë½ì—ì„œ ë¡œì»¬ í”¼ì²˜ëŠ” ì²˜ìŒ ëª‡ ê°œì˜ ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ì™€ ê°™ì€ ì‹ ê²½ë§ì˜ ì–•ì€ ë ˆì´ì–´ì—ì„œ ì…ë ¥ ë°ì´í„°ì—ì„œ ì¶”ì¶œë˜ëŠ” ë‚®ì€ ìˆ˜ì¤€ì˜ í”¼ì²˜ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë¡œì»¬ í”¼ì²˜ëŠ” ê°€ì¥ìë¦¬, ëª¨ì„œë¦¬, í…ìŠ¤ì²˜ì™€ ê°™ì€ ì…ë ¥ ë°ì´í„°ì˜ ë¡œì»¬ íŒ¨í„´ê³¼ êµ¬ì¡°ë¥¼ í¬ì°©í•˜ë©° ë‹¤ì–‘í•œ ì‘ì—…ê³¼ ë„ë©”ì¸ì— ê±¸ì³ ë¹„êµì  ê°•ë ¥í•œ ì¼ë°˜í™” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. ë°˜ë©´ì— ê¸€ë¡œë²Œ í”¼ì²˜ëŠ” ì™„ì „ ì—°ê²°(FC) ë ˆì´ì–´ì™€ ê°™ì€ ì‹ ê²½ë§ì˜ ì‹¬ì¸µ ë ˆì´ì–´ì—ì„œ ì…ë ¥ ë°ì´í„°ì—ì„œ ì¶”ì¶œë˜ëŠ” ë†’ì€ ìˆ˜ì¤€ì˜ í”¼ì²˜ë¥¼ ë§í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ê¸€ë¡œë²Œ íŠ¹ì§•ì€ í™”ìì˜ ì‹ ì›ì´ë‚˜ ìŒì„±ì— í‘œí˜„ëœ ê°ì • ë“± ì…ë ¥ ë°ì´í„°ì˜ ê³ ì°¨ì›ì ì¸ ì˜ë¯¸ ì •ë³´ë¥¼ í¬ì°©í•˜ë©°, íŠ¹ì • ì‘ì—…ê³¼ ì—°ê´€ì„±ì´ ë†’ê³  ë³€ë³„ë ¥ì´ ê°•í•©ë‹ˆë‹¤. ìŒì„± ê°ì • ì¸ì‹(SER)ì˜ ë§¥ë½ì—ì„œ ê°ì •ì  ìŒì„±ì˜ ë¡œì»¬ íŠ¹ì§•ì€ ì¼ë°˜ì ìœ¼ë¡œ í¬ë¨¼íŠ¸ì˜ ëª¨ì–‘ê³¼ ê¸°ë³¸ ì£¼íŒŒìˆ˜ì˜ ìœ„ì¹˜ì™€ ê°™ì€ ìŒì„±ì˜ ì €ìˆ˜ì¤€ ìŒí–¥ íŠ¹ì„±ì„ í¬ì°©í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë¡œì»¬ í”¼ì²˜ëŠ” ìŒì„± ë°ì´í„°ì˜ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ì—ì„œ ì¶”ì¶œë˜ë©° ìŒì„± ì‹ í˜¸ì˜ ë¡œì»¬ íŒ¨í„´ê³¼ êµ¬ì¡°ë¥¼ ìº¡ì²˜í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ë°˜ë©´ì— ê°ì •ì  ìŒì„±ì˜ ê¸€ë¡œë²Œ íŠ¹ì§•ì—ëŠ” í™”ìì˜ ì‹ ì›ì´ë‚˜ ìŒì„±ì— í‘œí˜„ëœ ê°ì •ê³¼ ê°™ì€ ì‘ì—…ë³„ ì˜ë¯¸ ì •ë³´ê°€ í¬í•¨ë©ë‹ˆë‹¤. ì´ëŸ¬í•œ ê¸€ë¡œë²Œ íŠ¹ì§•ì€ DCNNì˜ FC ë ˆì´ì–´ì—ì„œ ì¶”ì¶œë˜ë©° ìŒì„± ì‹ í˜¸ 2ì˜ ë†’ì€ ìˆ˜ì¤€ì˜ ì˜ë¯¸ ì •ë³´ë¥¼ ìº¡ì²˜í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ì €ìˆ˜ì¤€ íŠ¹ì§•ê³¼ ê³ ìˆ˜ì¤€ íŠ¹ì§•ì˜ ì •ë ¬ì„ ê²°í•©í•¨ìœ¼ë¡œì¨ DCNNì€ í™”ì ë…ë¦½ì  SERì—ì„œ ì„œë¡œ ë‹¤ë¥¸ í™”ìì™€ ê°ì • í‘œí˜„ì˜ í˜¼ë™ ë¬¸ì œë¥¼ ì²˜ë¦¬í•˜ê³  SER ëª¨ë¸ ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ë…¼ë¬¸ì˜ ì €ìëŠ” ìŒì„±ì˜ ê°ì •ì  í‘œí˜„ì„ í¬ì°©í•˜ê¸° ìœ„í•´ ë¡œì»¬ ë° ê¸€ë¡œë²Œ íŠ¹ì§•ì„ ëª¨ë‘ ì‚¬ìš©í•©ë‹ˆë‹¤. ë¡œì»¬ í”¼ì²˜ëŠ” ìŒì„± ë°ì´í„°ì˜ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ì—ì„œ ì¶”ì¶œë˜ë©° í¬ë¨¼íŠ¸ì˜ ëª¨ì–‘ê³¼ ê¸°ë³¸ ì£¼íŒŒìˆ˜ì˜ ìœ„ì¹˜ ë“± ìŒì„±ì˜ ì €ìˆ˜ì¤€ ìŒí–¥ íŠ¹ì„±ì„ í¬ì°©í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë¡œì»¬ íŠ¹ì§•ì€ ì¼ë°˜ì ì´ë©° ë‹¤ì–‘í•œ ì‘ì—…ê³¼ ë„ë©”ì¸ì— ê±¸ì³ ë¹„êµì  ê°•ë ¥í•œ ì¼ë°˜í™” íŠ¹ì„±ì„ ê°–ìŠµë‹ˆë‹¤. ë°˜ë©´ì— ê¸€ë¡œë²Œ íŠ¹ì§•ì—ëŠ” í™”ìì˜ ì‹ ì›ì´ë‚˜ ìŒì„±ì— í‘œí˜„ëœ ê°ì •ê³¼ ê°™ì€ ì‘ì—…ë³„ ì˜ë¯¸ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê¸€ë¡œë²Œ íŠ¹ì§•ì€ DCNNì˜ FC ê³„ì¸µì—ì„œ ì¶”ì¶œë˜ë©° ìŒì„± ì‹ í˜¸ì˜ ë†’ì€ ìˆ˜ì¤€ì˜ ì˜ë¯¸ ì •ë³´ë¥¼ ìº¡ì²˜í•©ë‹ˆë‹¤. ê¸€ë¡œë²Œ íŠ¹ì§•ì€ íŠ¹ì • ì‘ì—…ê³¼ ê´€ë ¨ì´ ë†’ê³  ë³€ë³„ë ¥ì´ ê°•í•©ë‹ˆë‹¤. ì €ìˆ˜ì¤€ íŠ¹ì§•ê³¼ ê³ ìˆ˜ì¤€ íŠ¹ì§•ì˜ ì •ë ¬ì„ ê²°í•©í•¨ìœ¼ë¡œì¨ DCNNì€ í™”ì ë…ë¦½ì  SERì—ì„œ ë‹¤ì–‘í•œ í™”ìì™€ ê°ì • í‘œí˜„ì˜ í˜¼ë™ ë¬¸ì œë¥¼ ì²˜ë¦¬í•˜ê³  SER ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¡œì»¬ ë° ê¸€ë¡œë²Œ í”¼ì²˜ë¥¼ ëª¨ë‘ ì‚¬ìš©í•˜ì—¬ ì–»ì€ ê°ì •ì  ìŒì„±ì˜ ê³„ì¸µì  í‘œí˜„ì„ í†µí•´ ëª¨ë¸ì€ ê¸°ì¡´ì˜ ìˆ˜ì‘ì—… í”¼ì²˜ë¡œëŠ” í¬ì°©í•˜ê¸° ì–´ë ¤ìš´ ë³µì¡í•˜ê³  ë¯¸ë¬˜í•œ ìŒì„± ê°ì •ì˜ ë³€í™”ë¥¼ í¬ì°©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. B. Hierarchical Alignment Layer (HAL) hierarchical representationì„ ì–»ì„ë•Œ ë¨¼ì € í›ˆë ¨ ë°ì´í„° ì…‹ê³¼ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì…‹ ê°„ì— ì„œë¡œ ë‹¤ë¥¸ í™”ìë¡œ ì¸í•´ ë°œìƒí•˜ëŠ” ë„ë©”ì¸ ì´ë™ì„ ì œê±°í•˜ëŠ”ê±¸ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ hierarchical alignment layer(HAL)ì„ ì œì•ˆí•©ë‹ˆë‹¤. HALì€ ì†ŒìŠ¤ ë°ì´í„°ì™€ ëŒ€ìƒ ë°ì´í„°ì˜ íŠ¹ì§• ë§µì„ ê³„ì¸µì  ë°©ì‹ìœ¼ë¡œ ì •ë ¬í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤. íŠ¹ì§• ë§µì€ ê°ê° ìŒì„± ë°ì´í„°ì—ì„œ ë¡œì»¬ ë° ê¸€ë¡œë²Œ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ëŠ” DCNNì˜ ì»¨ë³¼ë£¨ì…˜ ë° ì™„ì „ ì—°ê²°(FC) ë¸”ë¡ì—ì„œ ì–»ìŠµë‹ˆë‹¤. í”¼ì²˜ ë§µì˜ ê³„ì¸µì  ì •ë ¬ì„ í†µí•´ ëª¨ë¸ì€ ê¸°ì¡´ì˜ ìˆ˜ì‘ì—… í”¼ì²˜ë¡œëŠ” í¬ì°©í•˜ê¸° ì–´ë ¤ìš´ ë³µì¡í•˜ê³  ë¯¸ë¬˜í•œ ìŒì„± ê°ì •ì˜ ë³€í™”ë¥¼ í¬ì°©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. HALì€ í™”ì ë…ë¦½ì  SERì„ ìœ„í•œ ë„ë©”ì¸ ë¶ˆë³€ íŠ¹ì§• í•™ìŠµì„ ìœ„í•´ ì œì•ˆëœ ë°©ë²•ì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. ê³„ì¸µì  í‘œí˜„ê³¼ ë„ë©”ì¸ ì ëŒ€ì  í•™ìŠµì„ ì‚¬ìš©í•˜ì—¬ ì œì•ˆëœ ë°©ë²•ì€ ì†ŒìŠ¤ ë„ë©”ì¸ê³¼ íƒ€ê²Ÿ ë„ë©”ì¸ ê°„ì˜ ë„ë©”ì¸ ê²©ì°¨ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì¤„ì´ê³  SER ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. HALì€ SWAS(ê°•ì•½ ì •ë ¬ ì „ëµ)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œì»¬ ë° ê¸€ë¡œë²Œ íŠ¹ì§•ì˜ ì •ë ¬ì„ ê°•í™”í•˜ê³  ìŒì„± íŠ¹ì§•ì˜ ê°ì • ë³€ë³„ë ¥ì„ ë³´ì¥í•©ë‹ˆë‹¤. SWASëŠ” ì„œë¡œ ë‹¤ë¥¸ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œì»¬ í”¼ì²˜ëŠ” ê°•í•˜ê²Œ ì •ë ¬í•˜ê³  ê¸€ë¡œë²Œ í”¼ì²˜ëŠ” ì•½í•˜ê²Œ ì •ë ¬í•©ë‹ˆë‹¤. íŠ¹íˆ ê°•ë ¥í•œ íŠ¹ì§• ë¶„í¬ ì •ë ¬ì—ì„œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ì€ ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ë¥¼ í†µí•´ ë¡œì»¬ íŠ¹ì§•ì„ ì–»ëŠ”ë°, ì´ëŠ” ëŒ€ë¶€ë¶„ í¬ë¨¼íŠ¸ì™€ ì—ë„ˆì§€ ë¶„í¬(ì¦‰, ì´ë¯¸ì§€ì˜ ê°€ì¥ìë¦¬, ëª¨ì–‘, ìƒ‰ìƒ)ì˜ ì—ì§€ ë˜ëŠ” ëª¨ì–‘ ì„¤ëª…ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ë¡œì»¬ íŠ¹ì§•ì€ í›ˆë ¨ ë°ì´í„° ì„¸íŠ¸ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„¸íŠ¸ ê°„ì— ì„œë¡œ ë‹¤ë¥¸ í™”ìë¡œ ì¸í•´ ë°œìƒí•˜ëŠ” ë„ë©”ì¸ ì´ë™ì„ ì¤„ì´ê¸° ìœ„í•´ ê°•ë ¥í•˜ê²Œ ì •ë ¬ë©ë‹ˆë‹¤. ì•½í•œ íŠ¹ì§• ë¶„í¬ ì •ë ¬ì—ì„œ DCNNì˜ FC ë ˆì´ì–´ëŠ” íŠ¹ì • ì‘ì—…ê³¼ ê´€ë ¨ì´ ë†’ê³  ë³€ë³„ë ¥ì´ ê°•í•œ ì „ì—­ íŠ¹ì§•ì„ ì–»ìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê¸€ë¡œë²Œ íŠ¹ì§•ì€ ì‘ì—…ë³„ ì •ë³´ë¥¼ ë³´ì¡´í•˜ê³  ì‘ì—…ë³„ ì •ë³´ì˜ ì†ì‹¤ë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆëŠ” ê³¼ë„í•œ ì •ë ¬ì„ í”¼í•˜ê¸° ìœ„í•´ ì•½í•˜ê²Œ ì •ë ¬ë©ë‹ˆë‹¤. ë¡œì»¬ íŠ¹ì§•ê³¼ ê¸€ë¡œë²Œ íŠ¹ì§•ì˜ ê°•-ì•½ ì •ë ¬ì„ ê²°í•©í•¨ìœ¼ë¡œì¨ SWASëŠ” HALì˜ ì •ë ¬ì„ ê°•í™”í•˜ê³  SER ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ ì œì•ˆëœ ê°•-ì•½ ì •ë ¬ ì „ëµì„ ì‚¬ìš©í•˜ì—¬ í™”ì ë…ë¦½ì  SERì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. In the proposed Strong-Weak Alignment Strategy (SWAS), the strong alignment is applied to the local features obtained from the convolutional layers, while the weak alignment is applied to the global features obtained from the fully connected (fc) layers. The strong alignment is achieved by aligning the domain distributions of local features generated by convolutional layers using a larger weight Î»l. The specific calculation process of strong alignment in each convolutional layer is shown in Fig. 2(b) of the paper. The strong alignment process involves applying global max pooling (GMP) and global average pooling (GAP) to the local features and then using a weighted sum of the two pooling results to obtain the aligned features. On the other hand, the weak alignment is achieved by aligning the domain distributions of global features obtained by fc layers using a smaller weight Î»g. The specific calculation process of weak alignment in each fc layer is shown in Fig. 3(c) of the paper. The weak alignment process involves applying a domain adversarial loss to the global features to encourage the model to learn domain-invariant representations. By using different weights for strong and weak alignment, SWAS can effectively align the local and global features and improve the performance of the SER model. The proposed SWAS can also reduce the domain shift caused by different speakers between the training and testing datasets and improve the generalization ability of the SER model. C. Domain Adversarial Layer(DAL) What is divergence? ë°œì‚°ì€ ë‘ í™•ë¥  ë¶„í¬ ê°„ì˜ ì°¨ì´ë¥¼ ì •ëŸ‰í™”í•˜ëŠ” í†µê³„ì  ì²™ë„ì…ë‹ˆë‹¤. ë¹„ì§€ë„ ë„ë©”ì¸ ì ì‘(UDA)ì˜ ë§¥ë½ì—ì„œ ë°œì‚° ì¸¡ì •ê°’ì€ ì†ŒìŠ¤ ë„ë©”ì¸ê³¼ ëŒ€ìƒ ë„ë©”ì¸ ê°„ì˜ ë¶„í¬ ë¶ˆì¼ì¹˜ë¥¼ í‰ê°€í•˜ê³  ë‘ ë„ë©”ì¸ì˜ íŠ¹ì§• ê³µê°„ ì •ë ¬ì„ ì•ˆë‚´í•˜ëŠ” ë° ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. page 3ì—ì„œ ì–¸ê¸‰í•œ ê²ƒì²˜ëŸ¼ ìµœëŒ€ í‰ê·  ë¶ˆì¼ì¹˜(MMD), ë ˆë‹ˆ-ë°œì‚°, L2 ê±°ë¦¬, ëª¨ë©˜íŠ¸ ê±°ë¦¬ ë“± UDAì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ ìœ í˜•ì˜ ë°œì‚° ì¸¡ì •ê°’ì´ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë°œì‚° ì¸¡ì •ê°’ì€ ì†ŒìŠ¤ ë„ë©”ì¸ê³¼ ëŒ€ìƒ ë„ë©”ì¸ì˜ íŠ¹ì§• ë¶„í¬ ê°„ì˜ ë¶ˆì¼ì¹˜ë¥¼ ì¸¡ì •í•˜ê³  ë‘ ë„ë©”ì¸ì˜ íŠ¹ì§• ê³µê°„ ì •ë ¬ì„ ìœ ë„í•˜ëŠ” ë° ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ page 4ì—ì„œ ì–¸ê¸‰í–ˆë“¯ì´ ë°œì‚° í•¨ìˆ˜ì˜ ì„ íƒì€ ë¯¼ê°í•  ìˆ˜ ìˆìœ¼ë©° ë³µì¡í•˜ê³  ë¯¸ë¬˜í•œ ìŒì„± ê°ì •ì˜ ë³€í™”ë¥¼ ì²˜ë¦¬í•˜ê¸°ì— ì¶©ë¶„íˆ ê°•ë ¥í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ í™”ì ë…ë¦½ì  ìŒì„± ê°ì • ì¸ì‹(SER)ì—ì„œ UDAë¥¼ ìœ„í•œ ë³´ë‹¤ íš¨ê³¼ì ì´ê³  ê°•ë ¥í•œ ì†”ë£¨ì…˜ìœ¼ë¡œ ë„ë©”ì¸ ì ëŒ€ì  í•™ìŠµ(DAL)ê³¼ ê°™ì€ ì ëŒ€ì  ê¸°ë°˜ UDA ë°©ë²•ì´ ì œì•ˆë˜ê³  ìˆìŠµë‹ˆë‹¤. DALì€ ë„ë©”ì¸ ë¶„ë¥˜ê¸°ë¥¼ í›ˆë ¨ì‹œì¼œ ì†ŒìŠ¤ ë„ë©”ì¸ê³¼ ëŒ€ìƒ ë„ë©”ì¸ì„ êµ¬ë¶„í•˜ëŠ” ë™ì‹œì— íŠ¹ì§• ì¶”ì¶œê¸°ë¥¼ í›ˆë ¨ì‹œì¼œ ë„ë©”ì¸ ë¶„ë¥˜ê¸°ë¥¼ ì†ì´ëŠ” ë„ë©”ì¸ ë¶ˆë³€ í‘œí˜„ì„ ìƒì„±í•˜ëŠ” ê³¼ì •ì„ í¬í•¨í•©ë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œ ì œì•ˆí•œ ë°©ë²•ì€ DALì„ ì‚¬ìš©í•˜ì—¬ ì†ŒìŠ¤ ë„ë©”ì¸ê³¼ íƒ€ê²Ÿ ë„ë©”ì¸ ê°„ì˜ ë„ë©”ì¸ ê°­ì„ íš¨ê³¼ì ìœ¼ë¡œ ì¤„ì´ê³  SER ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. What is divergency based UDA? 2ì— ë”°ë¥´ë©´ ë°œì‚° ê¸°ë°˜ UDAëŠ” ë°œì‚° ì¸¡ì •ê°’ì„ í™œìš©í•˜ì—¬ ì†ŒìŠ¤ ë„ë©”ì¸ê³¼ íƒ€ê²Ÿ ë„ë©”ì¸ ê°„ì˜ ë¶ˆì¼ì¹˜ë¥¼ í‰ê°€í•˜ëŠ” ë¹„ì§€ë„ ë„ë©”ì¸ ì ì‘(UDA) ë°©ë²•ì˜ í•œ ìœ í˜•ì„ ë§í•©ë‹ˆë‹¤. ë°œì‚° ì¸¡ì •ê°’ì€ ë‘ í™•ë¥  ë¶„í¬ ê°„ì˜ ì°¨ì´ë¥¼ ì •ëŸ‰í™”í•˜ëŠ” í†µê³„ì  ì¸¡ì •ê°’ì…ë‹ˆë‹¤. UDAì˜ ë§¥ë½ì—ì„œ ë°œì‚° ì¸¡ì •ê°’ì€ ì†ŒìŠ¤ ë„ë©”ì¸ê³¼ ëŒ€ìƒ ë„ë©”ì¸ ê°„ì˜ ë¶„í¬ ë¶ˆì¼ì¹˜ë¥¼ ì¸¡ì •í•˜ê³  ë‘ ë„ë©”ì¸ì˜ íŠ¹ì§• ê³µê°„ì˜ ì •ë ¬ì„ ì•ˆë‚´í•˜ëŠ” ë° ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í™”ì ë…ë¦½ì  ìŒì„± ê°ì • ì¸ì‹(SER)ì˜ ê²½ìš°, ë°œì‚° ê¸°ë°˜ UDA ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ ì†ŒìŠ¤ ë„ë©”ì¸ê³¼ ëª©í‘œ ë„ë©”ì¸ì˜ íŠ¹ì§• ê³µê°„ì„ ì •ë ¬í•˜ê³  í›ˆë ¨ ë°ì´í„° ì„¸íŠ¸ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„¸íŠ¸ ê°„ì— ì„œë¡œ ë‹¤ë¥¸ í™”ìë¡œ ì¸í•´ ë°œìƒí•˜ëŠ” ë„ë©”ì¸ ì´ë™ì„ ì¤„ì˜€ìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ 5ì—ì„œ ì–¸ê¸‰í–ˆë“¯ì´ ë°œì‚° ê¸°ë°˜ UDA ë°©ë²•ì€ ë°œì‚° í•¨ìˆ˜ì˜ ì„ íƒì— ë¯¼ê°í•  ìˆ˜ ìˆìœ¼ë©° ë³µì¡í•˜ê³  ë¯¸ë¬˜í•œ ìŒì„± ê°ì •ì˜ ë³€í™”ë¥¼ ì²˜ë¦¬í•˜ê¸°ì— ì¶©ë¶„íˆ ê°•ë ¥í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì´ ë…¼ë¬¸ì—ì„œ ì œì•ˆí•œ ë°©ë²•ì—ì„œëŠ” ë„ë©”ì¸ ì ëŒ€ì  í•™ìŠµ(DAL)ê³¼ ê°™ì€ ì ëŒ€ì  ê¸°ë°˜ UDA ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ í™”ì ë…ë¦½ì  SERì„ ìœ„í•œ ë³´ë‹¤ ê°•ë ¥í•˜ê³  íš¨ê³¼ì ì¸ ì†”ë£¨ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤. DALì€ ë„ë©”ì¸ ë¶„ë¥˜ê¸°ë¥¼ í›ˆë ¨ì‹œì¼œ ì†ŒìŠ¤ ë„ë©”ì¸ê³¼ ëŒ€ìƒ ë„ë©”ì¸ì„ êµ¬ë¶„í•˜ëŠ” ë™ì‹œì— íŠ¹ì§• ì¶”ì¶œê¸°ë¥¼ í›ˆë ¨ì‹œì¼œ ë„ë©”ì¸ ë¶„ë¥˜ê¸°ë¥¼ ì†ì´ëŠ” ë„ë©”ì¸ ë¶ˆë³€ í‘œí˜„ì„ ìƒì„±í•˜ëŠ” ì ëŒ€ì  í›ˆë ¨ì˜ ì¼ì¢…ì…ë‹ˆë‹¤. ì œì•ˆí•œ ë°©ë²•ì€ DALì„ ì‚¬ìš©í•˜ì—¬ ì†ŒìŠ¤ ë„ë©”ì¸ê³¼ íƒ€ê²Ÿ ë„ë©”ì¸ ê°„ì˜ ë„ë©”ì¸ ê°­ì„ íš¨ê³¼ì ìœ¼ë¡œ ì¤„ì´ê³  SER ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Why HAL is problem using in divergency based UDA? 5ì— ë”°ë¥´ë©´, HALì€ ë°œì‚° ê¸°ë°˜ UDAë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ë©° ë‹¤ì–‘í•œ ë©”íŠ¸ë¦­ í•™ìŠµ ì „ëµì„ ì‚¬ìš©í•˜ì—¬ ì†ŒìŠ¤ ë„ë©”ì¸ê³¼ ëŒ€ìƒ ë„ë©”ì¸ ê°„ì˜ íŠ¹ì§• ë¶„í¬ ë¶ˆì¼ì¹˜ë¥¼ ì¸¡ì •í•˜ë ¤ê³  ì‹œë„í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì„œë¡œ ë‹¤ë¥¸ ë„ë©”ì¸ì—ì„œ ê³µí†µ ì •ë ¬ ê³µê°„ì„ ì„¤ì •í•˜ëŠ” ê²ƒì€ ì„ íƒëœ ë°œì‚° í•¨ìˆ˜ì— ë”°ë¼ ë‹¬ë¼ì§€ê¸° ë•Œë¬¸ì— ì´ íŠ¹ì§• ê³µê°„ì€ ë§ì€ ì œì•½ì´ ìˆê³  ê²¬ê³ ì„±ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. ì¦‰, ë°œì‚° ê¸°ë°˜ UDAì—ì„œ HALì„ ì‚¬ìš©í•  ë•Œì˜ ë¬¸ì œì ì€ ì†ŒìŠ¤ ë„ë©”ì¸ê³¼ ëŒ€ìƒ ë„ë©”ì¸ ê°„ì˜ íŠ¹ì§• ë¶„í¬ ë¶ˆì¼ì¹˜ë¥¼ ì¸¡ì •í•˜ê¸° ìœ„í•´ í†µê³„ì  ê°€ì„¤ê³¼ ë°œì‚° í•¨ìˆ˜ì— ì˜ì¡´í•œë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ ì ‘ê·¼ ë°©ì‹ì€ ë°œì‚° í•¨ìˆ˜ì˜ ì„ íƒì— ë¯¼ê°í•  ìˆ˜ ìˆìœ¼ë©° ë³µì¡í•˜ê³  ë¯¸ë¬˜í•œ ìŒì„± ê°ì •ì˜ ë³€í™”ë¥¼ ì²˜ë¦¬í•˜ê¸°ì— ì¶©ë¶„íˆ ê°•ë ¥í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì´ ë…¼ë¬¸ì—ì„œ ì œì•ˆí•œ ë°©ë²•ì€ ë„ë©”ì¸ ì ëŒ€ì  í•™ìŠµ(DAL)ê³¼ ê°™ì€ ì ëŒ€ì  ê¸°ë°˜ UDA ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ ìœ ë§í•œ ì†”ë£¨ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤. DALì€ ë„ë©”ì¸ ë¶„ë¥˜ê¸°ë¥¼ í›ˆë ¨ì‹œì¼œ ì†ŒìŠ¤ ë„ë©”ì¸ê³¼ ëŒ€ìƒ ë„ë©”ì¸ì„ êµ¬ë¶„í•˜ëŠ” ë™ì‹œì— íŠ¹ì§• ì¶”ì¶œê¸°ë¥¼ í›ˆë ¨ì‹œì¼œ ë„ë©”ì¸ ë¶„ë¥˜ê¸°ë¥¼ ì†ì´ëŠ” ë„ë©”ì¸ ë¶ˆë³€ í‘œí˜„ì„ ìƒì„±í•˜ëŠ” ì ëŒ€ì  í›ˆë ¨ì˜ ì¼ì¢…ì…ë‹ˆë‹¤. ì œì•ˆí•œ ë°©ë²•ì€ DALì„ ì‚¬ìš©í•˜ì—¬ ì†ŒìŠ¤ ë„ë©”ì¸ê³¼ íƒ€ê²Ÿ ë„ë©”ì¸ ê°„ì˜ ë„ë©”ì¸ ê°­ì„ íš¨ê³¼ì ìœ¼ë¡œ ì¤„ì´ê³  SER ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. DALê³¼ DANNì˜ ì°¨ì´ both DANN and DAL are adversarial-based UDA methods that aim to reduce the domain gap between the source and target domains in SER. However, DANN involves training a feature extractor and domain classifier simultaneously, while DAL involves adding a domain adversarial layer to the feature extractor. Exactly how did they use DAL to reduce the gap between source and target domain? ë³¸ ë…¼ë¬¸ì—ì„œ ì œì•ˆí•œ ë°©ë²•ì—ì„œëŠ” í™”ì ë…ë¦½ì  ìŒì„± ê°ì • ì¸ì‹(SER)ì—ì„œ ì†ŒìŠ¤ ë„ë©”ì¸ê³¼ íƒ€ê¹ƒ ë„ë©”ì¸ ê°„ì˜ ë„ë©”ì¸ ê°„ê·¹ì„ ì¤„ì´ê¸° ìœ„í•´ ë„ë©”ì¸ ì ëŒ€ì  ê³„ì¸µ(DAL)ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ ì‹¬ì¸µ íŠ¹ì§• ì¶”ì¶œê¸° ì´í›„ íŒë³„ì ë¸”ë¡ì— DALì„ ì¶”ê°€í•˜ì—¬ ì†ŒìŠ¤ ë°ì´í„°ì™€ íƒ€ê¹ƒ ë°ì´í„°ì˜ ìŒì„± ê°ì • íŠ¹ì§•ì„ íšë“í•¨ìœ¼ë¡œì¨ ë„ë©”ì¸ ì´ë™ì„ ë”ìš± ì¤„ì´ê³  ìŒì„±ì˜ ë†’ì€ ìˆ˜ì¤€ì˜ ì˜ë¯¸ë¡ ì  ê°ì • íŠ¹ì§•ì„ íšë“í–ˆìŠµë‹ˆë‹¤. í•™ìŠµ ê³¼ì •ì—ì„œ íŠ¹ì§• ì¶”ì¶œê¸°ëŠ” ì‘ì—…ë³„ ì†ì‹¤ì„ ìµœì†Œí™”í•˜ë„ë¡ í•™ìŠµí•˜ê³ , ë„ë©”ì¸ ì ëŒ€ ê³„ì¸µì€ ë„ë©”ì¸ ë¶„ë¥˜ ì†ì‹¤ì„ ìµœì†Œí™”í•˜ì—¬ ë„ë©”ì¸ ë¶ˆë³€ í‘œí˜„ì„ ìƒì„±í•˜ë„ë¡ í•™ìŠµí–ˆìŠµë‹ˆë‹¤. ë„ë©”ì¸ ë¶„ë¥˜ ì†ì‹¤ì€ ì†ŒìŠ¤ ë„ë©”ì¸ê³¼ íƒ€ê¹ƒ ë„ë©”ì¸ì„ êµ¬ë¶„í•˜ë„ë¡ í•™ìŠµëœ ë„ë©”ì¸ ë¶„ë¥˜ê¸°ì˜ ì¶œë ¥ì„ ê¸°ë°˜ìœ¼ë¡œ ê³„ì‚°ë˜ì—ˆìŠµë‹ˆë‹¤. ë„ë©”ì¸ ë¶„ë¥˜ ì†ì‹¤ì„ ìµœì†Œí™”í•¨ìœ¼ë¡œì¨ ë„ë©”ì¸ ì ëŒ€ì  ê³„ì¸µì€ ë„ë©”ì¸ ë¶„ë¥˜ê¸°ë¡œ êµ¬ë¶„í•  ìˆ˜ ì—†ëŠ” ë„ë©”ì¸ ë¶ˆë³€í˜• í‘œí˜„ì„ ìƒì„±í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí–ˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ DALì€ ì†ŒìŠ¤ ë„ë©”ì¸ê³¼ íƒ€ê¹ƒ ë„ë©”ì¸ ê°„ì˜ ë„ë©”ì¸ ê°­ì„ íš¨ê³¼ì ìœ¼ë¡œ ì¤„ì´ê³  SER ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê°œì„ í–ˆìŠµë‹ˆë‹¤. ì œì•ˆëœ ë°©ë²•ì€ ë‘ ê°œì˜ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ì„¸íŠ¸ì—ì„œ í‰ê°€ë˜ì—ˆìœ¼ë©°, ì‹¤í—˜ ê²°ê³¼ í™”ì ë…ë¦½ì  SERì—ì„œ ì—¬ëŸ¬ ìµœì‹  ë°©ë²•ë³´ë‹¤ ì„±ëŠ¥ì´ ë›°ì–´ë‚œ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. Explain how does DAL work in this paper with generator and discriminator ë³¸ ë…¼ë¬¸ì—ì„œ ì œì•ˆí•œ ë°©ë²•ì—ì„œëŠ” í™”ì ë…ë¦½ì  ìŒì„± ê°ì • ì¸ì‹(SER)ì—ì„œ ì†ŒìŠ¤ ë„ë©”ì¸ê³¼ íƒ€ê¹ƒ ë„ë©”ì¸ ê°„ì˜ ë„ë©”ì¸ ê°„ê·¹ì„ ì¤„ì´ê¸° ìœ„í•´ ë„ë©”ì¸ ì ëŒ€ì  ê³„ì¸µ(DAL)ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. DALì€ ìƒì„±ê¸°-íŒë³„ê¸° í”„ë ˆì„ì›Œí¬ë¡œ êµ¬í˜„ë˜ì—ˆìœ¼ë©°, ìƒì„±ê¸°ëŠ” íŠ¹ì§• ì¶”ì¶œê¸°, íŒë³„ê¸°ëŠ” ë„ë©”ì¸ ì ëŒ€ì  ê³„ì¸µ 3ìœ¼ë¡œ êµ¬ì„±ë˜ì—ˆìŠµë‹ˆë‹¤. í›ˆë ¨ ê³¼ì •ì—ì„œ ì œë„ˆë ˆì´í„°(íŠ¹ì§• ì¶”ì¶œê¸°)ëŠ” ì˜ˆì¸¡ëœ ê°ì • ë ˆì´ë¸”ê³¼ ì§€ìƒ ì§„ì‹¤ ë ˆì´ë¸” ê°„ì˜ êµì°¨ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ì¸ ì‘ì—…ë³„ ì†ì‹¤ì„ ìµœì†Œí™”í•˜ë„ë¡ í›ˆë ¨ë˜ì—ˆìŠµë‹ˆë‹¤. ë™ì‹œì— íŒë³„ì(ë„ë©”ì¸ ì ëŒ€ ë ˆì´ì–´)ëŠ” ì˜ˆì¸¡ëœ ë„ë©”ì¸ ë ˆì´ë¸”ê³¼ ê¸°ì¤€ê°’ ë„ë©”ì¸ ë ˆì´ë¸” ê°„ì˜ ì´ì§„ êµì°¨ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ì¸ ë„ë©”ì¸ ë¶„ë¥˜ ì†ì‹¤ì„ ìµœì†Œí™”í•˜ì—¬ ë„ë©”ì¸ ë¶ˆë³€ í‘œí˜„ì„ ìƒì„±í•˜ë„ë¡ í›ˆë ¨ë˜ì—ˆìŠµë‹ˆë‹¤. ë„ë©”ì¸ ì ëŒ€ì  ê³„ì¸µì€ ë¼ë²¨ íŒë³„ì, ì†ŒìŠ¤-ëŒ€ìƒ ë„ë©”ì¸ íŒë³„ì, í™”ì íŒë³„ìë¼ëŠ” ì„¸ ê°€ì§€ í•˜ìœ„ íŒë³„ìë¡œ êµ¬ì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ë ˆì´ë¸” íŒë³„ê¸°ëŠ” ì…ë ¥ ìŒì„± ìƒ˜í”Œì˜ ê°ì • ë ˆì´ë¸”ì„ ì˜ˆì¸¡í•˜ë„ë¡ í›ˆë ¨ë˜ì—ˆê³ , ì†ŒìŠ¤-íƒ€ê²Ÿ ë„ë©”ì¸ íŒë³„ê¸°ëŠ” ì†ŒìŠ¤ ë„ë©”ì¸ê³¼ íƒ€ê²Ÿ ë„ë©”ì¸ì„ êµ¬ë¶„í•˜ë„ë¡ í›ˆë ¨ë˜ì—ˆìŠµë‹ˆë‹¤. í™”ì íŒë³„ê¸°ëŠ” ì†ŒìŠ¤ ë„ë©”ì¸ì—ì„œ ì„œë¡œ ë‹¤ë¥¸ í™”ìë¥¼ êµ¬ë³„í•˜ë„ë¡ í›ˆë ¨ë˜ì—ˆìŠµë‹ˆë‹¤. ìƒì„±ê¸°ì™€ íŒë³„ê¸°ê°€ ì ëŒ€ì ì¸ ë°©ì‹ìœ¼ë¡œ í›ˆë ¨ë˜ë„ë¡ í•˜ê¸° ìœ„í•´ ìƒì„±ê¸°ì™€ íŒë³„ê¸° ì‚¬ì´ì— ê·¸ë¼ë””ì–¸íŠ¸ ë°˜ì „ ë ˆì´ì–´(GRL)ë¥¼ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤. GRLì€ ì—­ì „íŒŒ ì¤‘ì— ê¸°ìš¸ê¸°ë¥¼ ë°˜ì „ì‹œì¼œ ì œë„ˆë ˆì´í„°ê°€ íŒë³„ê¸°ê°€ êµ¬ë³„í•  ìˆ˜ ì—†ëŠ” ë„ë©”ì¸ ë¶ˆë³€ í‘œí˜„ì„ ìƒì„±í•  ìˆ˜ ìˆë„ë¡ í–ˆìŠµë‹ˆë‹¤. ì œë„ˆë ˆì´í„°ì™€ íŒë³„ê¸°ë¥¼ ì ëŒ€ì  ë°©ì‹ìœ¼ë¡œ í›ˆë ¨í•¨ìœ¼ë¡œì¨ DALì€ ì†ŒìŠ¤ ë„ë©”ì¸ê³¼ íƒ€ê¹ƒ ë„ë©”ì¸ ê°„ì˜ ë„ë©”ì¸ ê°­ì„ íš¨ê³¼ì ìœ¼ë¡œ ì¤„ì´ê³  SER ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê°œì„ í–ˆìŠµë‹ˆë‹¤. ì œì•ˆëœ ë°©ë²•ì€ ë‘ ê°œì˜ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ì„¸íŠ¸ì—ì„œ í‰ê°€ë˜ì—ˆìœ¼ë©°, ì‹¤í—˜ ê²°ê³¼ í™”ì ë…ë¦½ì  SERì—ì„œ ì—¬ëŸ¬ ìµœì‹  ë°©ë²•ë³´ë‹¤ ì„±ëŠ¥ì´ ë›°ì–´ë‚œ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. D. DIFL for Speaker Independent SERThe DIFL framework effectively combines the hierarchical representation, hierarchical alignment, and domain adversarial approach into the deep network, which can be extended to any DCNNs. ì´ ë…¼ë¬¸ì—ì„œ ì œì•ˆí•œ DIFL(ë„ë©”ì¸ ë¶ˆë³€ íŠ¹ì§• í•™ìŠµ) ë°©ë²•ì€ ê³„ì¸µì  í‘œí˜„, ê³„ì¸µì  ì •ë ¬ ë° ë„ë©”ì¸ ì ëŒ€ì  ì ‘ê·¼ ë°©ì‹ì„ ì‹¬ì¸µ ë„¤íŠ¸ì›Œí¬ì— ê²°í•©í•˜ì—¬ ë‹¤ì¤‘ ì†ŒìŠ¤ UDAì˜ ê´€ì ì—ì„œ í™”ì ë…ë¦½ì  SERì„ ì²˜ë¦¬í•©ë‹ˆë‹¤. DIFL ë°©ì‹ì€ ê³„ì¸µì  ì •ë ¬ ê³„ì¸µì´ ìˆëŠ” íŠ¹ì§• ì¶”ì¶œê¸° ë¸”ë¡ê³¼ ê°ì •, í™”ì, ë„ë©”ì¸ì˜ ì—¬ëŸ¬ íŒë³„ìê°€ ìˆëŠ” íŒë³„ì ë¸”ë¡ì˜ ë‘ ê°€ì§€ ì£¼ìš” ë¸”ë¡ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. íŠ¹ì§• ì¶”ì¶œê¸° ë¸”ë¡ì€ ì…ë ¥ ìŒì„± ìƒ˜í”Œì—ì„œ ë†’ì€ ìˆ˜ì¤€ì˜ ì˜ë¯¸ì  íŠ¹ì§•ì„ ì¶”ì¶œí•˜ëŠ” ì—­í• ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤. ì—¬ëŸ¬ ê³„ì¸µìœ¼ë¡œ êµ¬ì„±ëœ ì‹¬ì¸µ ì‹ ê²½ë§ìœ¼ë¡œ êµ¬ì„±ë˜ë©°, ê° ê³„ì¸µì€ ì…ë ¥ íŠ¹ì§•ì˜ ê³„ì¸µì  í‘œí˜„ì„ í•™ìŠµí•©ë‹ˆë‹¤. ê³„ì¸µì  ì •ë ¬ ë ˆì´ì–´ê°€ íŠ¹ì§• ì¶”ì¶œê¸° ë¸”ë¡ì— ì¶”ê°€ë˜ì–´ ì†ŒìŠ¤ ë„ë©”ì¸ê³¼ ëŒ€ìƒ ë„ë©”ì¸ì„ ì„œë¡œ ë‹¤ë¥¸ ì¶”ìƒí™” ìˆ˜ì¤€ì—ì„œ ì •ë ¬í•©ë‹ˆë‹¤. ê³„ì¸µì  ì •ë ¬ ë ˆì´ì–´ëŠ” ì—¬ëŸ¬ í•˜ìœ„ ë ˆì´ì–´ë¡œ êµ¬ì„±ë˜ë©°, ê° í•˜ìœ„ ë ˆì´ì–´ëŠ” ì†ŒìŠ¤ ë„ë©”ì¸ê³¼ ëŒ€ìƒ ë„ë©”ì¸ì„ íŠ¹ì • ì¶”ìƒí™” ìˆ˜ì¤€ì—ì„œ ì •ë ¬í•©ë‹ˆë‹¤. íŒë³„ì ë¸”ë¡ì€ ë„ë©”ì¸ ë¶ˆë³€ íŠ¹ì§• í•™ìŠµì„ ì´‰ì§„í•˜ëŠ” ì—­í• ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤. ê°ì • íŒë³„ì, í™”ì íŒë³„ì, ë„ë©”ì¸ íŒë³„ì ë“± ì—¬ëŸ¬ íŒë³„ìë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. ê°ì • íŒë³„ìëŠ” ì…ë ¥ ìŒì„± ìƒ˜í”Œì˜ ê°ì • ë ˆì´ë¸”ì„ ì˜ˆì¸¡í•˜ë„ë¡ í›ˆë ¨ë˜ë©°, í™”ì íŒë³„ìëŠ” ì†ŒìŠ¤ ë„ë©”ì¸ì—ì„œ ì„œë¡œ ë‹¤ë¥¸ í™”ìë¥¼ êµ¬ë³„í•˜ë„ë¡ í›ˆë ¨ë©ë‹ˆë‹¤. ë„ë©”ì¸ íŒë³„ê¸°ëŠ” ì†ŒìŠ¤ ë„ë©”ì¸ê³¼ ëŒ€ìƒ ë„ë©”ì¸ì„ êµ¬ë¶„í•˜ë„ë¡ í›ˆë ¨ë©ë‹ˆë‹¤. ë„ë©”ì¸ ë¶ˆë³€ íŠ¹ì§• í•™ìŠµì„ ì´‰ì§„í•˜ê¸° ìœ„í•´ DIFL ë°©ë²•ì€ ìµœëŒ€ í‰ê·  ë¶ˆì¼ì¹˜(MMD) ë©”íŠ¸ë¦­ì„ ì‚¬ìš©í•˜ì—¬ ì†ŒìŠ¤ ë„ë©”ì¸ê³¼ ëŒ€ìƒ ë„ë©”ì¸ ê°„ì˜ ê±°ë¦¬ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤. MMD ë©”íŠ¸ë¦­ì€ ì†ŒìŠ¤ ë„ë©”ì¸ê³¼ ëŒ€ìƒ ë„ë©”ì¸ ê°„ì˜ ë„ë©”ì¸ ì´ë™ì„ ìµœì†Œí™”í•˜ê³  ì ëŒ€ì  ë„ë©”ì¸ ì ì‘ì„ ì´‰ì§„í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ë„ë©”ì¸ ë° í™”ì íŒë³„ìëŠ” ì ì§„ì ìœ¼ë¡œ ë„ë©”ì¸ ë…ë¦½ì ì¸ íŠ¹ì§•ì„ ì–»ê¸° ìœ„í•´ í™œìš©ë©ë‹ˆë‹¤. ë˜í•œ DIFLì˜ ë ˆì´ë¸” íŒë³„ìëŠ” íŠ¹ì§•ì˜ íŒë³„ ê°€ëŠ¥ì„±ë„ ë³´ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê³„ì¸µì  í‘œí˜„, ê³„ì¸µì  ì •ë ¬ ë° ë„ë©”ì¸ ì ëŒ€ì  ì ‘ê·¼ ë°©ì‹ì„ ê²°í•©í•¨ìœ¼ë¡œì¨ DIFL ë°©ë²•ì€ ì†ŒìŠ¤ ë„ë©”ì¸ê³¼ ëŒ€ìƒ ë„ë©”ì¸ ê°„ì˜ ë„ë©”ì¸ ê²©ì°¨ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì¤„ì´ê³  SER ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì œì•ˆëœ ë°©ë²•ì€ ë‘ ê°œì˜ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ì„¸íŠ¸ì—ì„œ í‰ê°€ë˜ì—ˆìœ¼ë©°, ì‹¤í—˜ ê²°ê³¼ í™”ì ë…ë¦½ì  SERì—ì„œ ì—¬ëŸ¬ ìµœì‹  ë°©ë²•ë³´ë‹¤ ì„±ëŠ¥ì´ ìš°ìˆ˜í•œ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. MK-MMD MK-MMDëŠ” ë‹¤ì¤‘ ì»¤ë„ ìµœëŒ€ í‰ê·  ë¶ˆì¼ì¹˜ì˜ ì•½ìì…ë‹ˆë‹¤. ë‘ í™•ë¥  ë¶„í¬ ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ì¸¡ì •í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ë©”íŠ¸ë¦­ì…ë‹ˆë‹¤. ì´ ë…¼ë¬¸ì—ì„œëŠ” í™”ì ë…ë¦½ì  ìŒì„± ê°ì • ì¸ì‹(SER) 5ì—ì„œ ì†ŒìŠ¤ ë„ë©”ì¸ê³¼ ëŒ€ìƒ ë„ë©”ì¸ì˜ íŠ¹ì§• ë¶„í¬ ê°„ì˜ ë¶ˆì¼ì¹˜ë¥¼ ì¸¡ì •í•˜ëŠ” ë° MK-MMDë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. MMD(ìµœëŒ€ í‰ê·  ë¶ˆì¼ì¹˜)ëŠ” ì»¤ë„ ê¸°ë°˜ ë©”íŠ¸ë¦­ìœ¼ë¡œ, ê³ ì°¨ì› ì»¤ë„ íë²„íŠ¸ ê³µê°„(RKHS)ì—ì„œ ë‘ í™•ë¥  ë¶„í¬ ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤. MK-MMDëŠ” ë„ë©”ì¸ ê°„ì˜ ë¶ˆì¼ì¹˜ë¥¼ ì¸¡ì •í•˜ê¸° ìœ„í•´ ì–‘ì˜ ë°˜ì •í™•(PSD) ì»¤ë„ì˜ ì¼ë ¨ì˜ ì„ í˜• ì¡°í•©ì„ ì‚¬ìš©í•˜ì—¬ MMDë¥¼ í™•ì¥í•©ë‹ˆë‹¤. ì´ ë…¼ë¬¸ì—ì„œëŠ” ê°€ìš°ì‹œì•ˆ ì»¤ë„(ì¦‰, RBF ì»¤ë„)ì„ MK-MMDì˜ ì»¤ë„ í•¨ìˆ˜ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. MK-MMDëŠ” ë„ë©”ì¸ ì´ë™ì„ ì¤„ì´ê³  ì ëŒ€ì  ë„ë©”ì¸ ì ì‘ì„ ì´‰ì§„í•˜ê¸° ìœ„í•´ ì œì•ˆëœ DIFL(ë„ë©”ì¸ ë¶ˆë³€ íŠ¹ì§• í•™ìŠµ) ë°©ë²•ì˜ íŠ¹ì§• í•™ìŠµ ë‹¨ê³„ì— ì‚¬ìš©ë©ë‹ˆë‹¤. íŠ¹íˆ MK-MMDëŠ” ë‹¤ì–‘í•œ í™”ìì— ì˜í•´ ë°œìƒí•˜ëŠ” íŠ¹ì§• ë¶„í¬ì˜ ê±°ë¦¬ë¥¼ ì¸¡ì •í•˜ëŠ” ë° ì‚¬ìš©ë˜ë©°, ì´ë¥¼ í†µí•´ ì†ŒìŠ¤-íƒ€ê²Ÿ ë„ë©”ì¸ ê°„ì˜ ë¶ˆì¼ì¹˜ë¥¼ ì¤„ì´ê³  íŠ¹ì§• ì •ë ¬ ì¤‘ ê°ì • íŒë³„ ì„±ëŠ¥ ì €í•˜ë¥¼ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ MK-MMDëŠ” ì†ŒìŠ¤ ë„ë©”ì¸ê³¼ íƒ€ê²Ÿ ë„ë©”ì¸ ê°„ì˜ ë„ë©”ì¸ ê°­ì„ íš¨ê³¼ì ìœ¼ë¡œ ì¤„ì´ê³  SER ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. GRL, UDA, DIFLì˜ ê´€ê³„ ë…¼ë¬¸ì—ì„œ ì œì•ˆí•œ DIFL(ë„ë©”ì¸ ë¶ˆë³€ íŠ¹ì§• í•™ìŠµ) ë°©ë²•ì—ì„œëŠ” ì‹¬ì¸µ ì‹ ê²½ë§ì—ì„œ ë„ë©”ì¸ ì ëŒ€ì  ì ‘ê·¼ ë°©ì‹ì„ êµ¬í˜„í•˜ê¸° ìœ„í•´ ê·¸ë¼ë””ì–¸íŠ¸ ë°˜ì „ ë ˆì´ì–´(GRL)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. GRLì€ í™”ì ë…ë¦½ì  ìŒì„± ê°ì • ì¸ì‹(SER) ì—ì„œ ì†ŒìŠ¤ ë„ë©”ì¸ê³¼ íƒ€ê²Ÿ ë„ë©”ì¸ ê°„ì˜ ë„ë©”ì¸ ê²©ì°¨ë¥¼ ì¤„ì´ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ë¹„ì§€ë„ ë„ë©”ì¸ ì ì‘(UDA) í”„ë ˆì„ì›Œí¬ì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œì…ë‹ˆë‹¤. GRLì€ DIFL ë°©ì‹ì—ì„œ íŠ¹ì§• ì¶”ì¶œê¸° ë¸”ë¡ê³¼ íŒë³„ê¸° ë¸”ë¡ ì‚¬ì´ì— ì¶”ê°€ë©ë‹ˆë‹¤. ìˆœë°©í–¥ ì „íŒŒ ì¤‘ì— GRLì€ ì…ë ¥ì„ ë³€ê²½í•˜ì§€ ì•Šê³  ìœ ì§€í•˜ë©°, ì—­ì „íŒŒ ì¤‘ì—ëŠ” ê¸°ìš¸ê¸°ë¥¼ ë°˜ì „ì‹œì¼œ ë„ë©”ì¸ ë¶ˆë³€ íŠ¹ì§• í•™ìŠµì„ ì´‰ì§„í•©ë‹ˆë‹¤. GRLì„ ì‚¬ìš©í•˜ë©´ íŠ¹ì§• ì¶”ì¶œê¸° ë¸”ë¡ì´ íŒë³„ì ë¸”ë¡ìœ¼ë¡œ êµ¬ë³„í•  ìˆ˜ ì—†ëŠ” ë„ë©”ì¸ ë¶ˆë³€í˜• í‘œí˜„ì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. DIFL ë°©ë²•ì˜ UDA í”„ë ˆì„ì›Œí¬ëŠ” ìµœëŒ€ í‰ê·  ë¶ˆì¼ì¹˜(MMD) ë©”íŠ¸ë¦­ì„ ì‚¬ìš©í•˜ì—¬ êµ¬í˜„ë©ë‹ˆë‹¤. MMD ë©”íŠ¸ë¦­ì€ ì†ŒìŠ¤ ë„ë©”ì¸ê³¼ ëŒ€ìƒ ë„ë©”ì¸ì˜ íŠ¹ì§• ë¶„í¬ ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ì¸¡ì •í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. MMD ë©”íŠ¸ë¦­ì„ ìµœì†Œí™”í•¨ìœ¼ë¡œì¨ DIFL ë°©ë²•ì€ ì†ŒìŠ¤ ë„ë©”ì¸ê³¼ íƒ€ê¹ƒ ë„ë©”ì¸ ê°„ì˜ ë„ë©”ì¸ ê°­ì„ ì¤„ì´ê³  ì ëŒ€ì  ë„ë©”ì¸ ì ì‘ì„ ì´‰ì§„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”¥ ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬ëŠ” DIFL ë°©ë²•ì—ì„œ GRLê³¼ UDAë¥¼ ê²°í•©í•˜ì—¬ ì†ŒìŠ¤ ë„ë©”ì¸ê³¼ íƒ€ê¹ƒ ë„ë©”ì¸ ê°„ì˜ ë„ë©”ì¸ ì´ë™ì— ê°•í•œ ë„ë©”ì¸ ë¶ˆë³€ íŠ¹ì§•ì„ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ DIFL ë°©ë²•ì€ ì†ŒìŠ¤ ë„ë©”ì¸ê³¼ íƒ€ê²Ÿ ë„ë©”ì¸ ê°„ì˜ ë„ë©”ì¸ ê°­ì„ íš¨ê³¼ì ìœ¼ë¡œ ì¤„ì´ê³  SER ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì œì•ˆëœ ë°©ë²•ì€ ë‘ ê°œì˜ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ì„¸íŠ¸ì—ì„œ í‰ê°€ë˜ì—ˆìœ¼ë©°, ì‹¤í—˜ ê²°ê³¼ í™”ì ë…ë¦½ì  SERì—ì„œ ì—¬ëŸ¬ ìµœì‹  ë°©ë²•ë³´ë‹¤ ì„±ëŠ¥ì´ ë›°ì–´ë‚œ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. IV. EXPERIMENTSA. Speech Emotion Databases Emo-DB: The Berlin Database of Emotional Speech (Emo-DB) is a German-language database containing speech samples from 10 actors (5 male, 5 female) expressing 7 different emotions (anger, boredom, disgust, fear, happiness, sadness, and neutral). The database consists of 535 utterances with a total duration of approximately 1 hour. eNTERFACE: The eNTERFACE database is an English-language audio-visual emotion database containing speech samples from 44 subjects from different nationalities expressing 6 basic emotions (anger, disgust, fear, happiness&#x2F;joy, sadness, and surprise). The database consists of 1,290 English sentences extracted from videos with a sampling rate of 48 kHz. CASIA: The Chinese Academy of Sciences Institute of Automation (CASIA) emotional speech database is an acted Chinese-language database containing speech samples from four actors (2 male and 2 female) expressing six different emotional states (anger, fear, happiness, neutral, sadness and surprise). The database consists of 1200 public sentences with a sampling rate of 16 kHz. B. Experimental Setting1) ë°ì´í„° ì „ì²˜ë¦¬ì‹¤í—˜ì— ì‚¬ìš©ëœ ë©œ-ìŠ¤í™íŠ¸ë¡œê·¸ë¨ì€ ìŒì„± ì‹ í˜¸ì— ëŒ€í•´ 512ê°œì˜ ìƒ˜í”Œ í¬ì¸íŠ¸ì™€ ì ˆë°˜ì´ ê²¹ì¹˜ëŠ” ì°½ ê¸¸ì´ë¡œ STFTë¥¼ ìˆ˜í–‰í•œ í›„ ì‚¬ëŒì˜ ì²­ê° ì§€ê°ì— ë” ê·¼ì ‘í•œ ë©œ-í•„í„° ë±…í¬ë¥¼ í†µê³¼í•˜ì—¬ ì–»ì€ ê²ƒìœ¼ë¡œ, ë©œ-í•„í„° ë²ˆí˜¸ëŠ” 80ì…ë‹ˆë‹¤. ì…ë ¥ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ì˜ ì¼ê´€ì„±ì„ ë³´ì¥í•˜ê¸° ìœ„í•´ ì‚¬ìš©ëœ ë°ì´í„°ë² ì´ìŠ¤ì˜ ìƒ˜í”Œë§ ì†ë„ë¥¼ 16kHzë¡œ ì •ê·œí™”í•˜ê³  ë‹¨ì¼ ì±„ë„ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. ë˜í•œ ê° ë°ì´í„°ë² ì´ìŠ¤ì˜ ìŒì„± ê¸¸ì´ë„ ì •ê·œí™”í•˜ì—¬ ì „ì²´ í›ˆë ¨ ì„¸íŠ¸ ìƒ˜í”Œì˜ í‰ê· ê°’ì— í‘œì¤€ í¸ì°¨ë¥¼ ë”í•œ ê°’ì„ ê· ì¼í•œ ê¸¸ì´ë¡œ ì„ íƒí•˜ê³ , ê¸¸ì´ê°€ ë¶€ì¡±í•œ ê° ë¬¸ì¥ì€ 0ìœ¼ë¡œ ì±„ìš°ê³  ê¸´ ë¬¸ì¥ì€ ì„¸ê·¸ë¨¼íŠ¸ ë‹¨ìœ„ë¡œ ì˜ë¼ëƒ…ë‹ˆë‹¤. 2) ì‹¤í—˜ í”„ë¡œí† ì½œì œì•ˆí•œ ë°©ë²•ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•´ [9], [11], [35]ì—ì„œ ì œì•ˆí•œ ë°”ì™€ ê°™ì´ í™”ì ë…ë¦½ì  SER ì‹¤í—˜ì—ì„œ LOSO(Leave-One-Speaker-Out) êµì°¨ ê²€ì¦ ì „ëµì„ ì±„íƒí•©ë‹ˆë‹¤. íŠ¹íˆ, eNTERFACE ë°ì´í„°ë² ì´ìŠ¤ì—ëŠ” 44ëª…ì˜ í™”ìê°€ í¬í•¨ë˜ì–´ ìˆì§€ë§Œ, 6ë²ˆì§¸ ìƒ˜í”Œì€ ì»·ì´ ì—†ëŠ” ë™ì˜ìƒì…ë‹ˆë‹¤. ë”°ë¼ì„œ ë³¸ ì‹¤í—˜ì—ì„œëŠ” 6ë²ˆì§¸ ìƒ˜í”Œì„ ì œì™¸í•œ 43ê°œì˜ í”¼ì‚¬ì²´ì™€ 1,287ê°œì˜ ë¹„ë””ì˜¤ í´ë¦½ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ë˜í•œ ì‹¤í—˜ê³¼ í‰ê°€ë¥¼ ë³´ë‹¤ íš¨ìœ¨ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ í™”ì ë…ë¦½ì  SER [11], [56]ì˜ ë‘ ë²¤ì¹˜ë§ˆí¬ ì—°êµ¬ì— ë”°ë¼ LOSGO(Leave-One-Speaker-Group-Out) ì „ëµì„ ì±„íƒí–ˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ eNTERFACEì˜ í™”ìëŠ” 5ê°œì˜ í™”ì ê·¸ë£¹ìœ¼ë¡œ ë‚˜ë‰˜ê³ , Emo-DBì™€ CASIAëŠ” ê°ê° 10ê°œì˜ í™”ìì™€ 4ê°œì˜ í™”ì ê·¸ë£¹ìœ¼ë¡œ ë‚˜ë‰˜ë©°, ë˜í•œ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ë‘ ê°€ì§€ í‰ê°€ ê¸°ì¤€[11], ì¦‰ ê°€ì¤‘ í‰ê·  íšŒìƒ(WAR)ê³¼ ê°€ì¤‘ í‰ê·  íšŒìƒ(UAR)ì„ ì±„íƒí•˜ì—¬ SERì˜ ì¸ì‹ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ”ë°, WARì€ â€˜ì •ìƒâ€™ ì¸ì‹ ì •í™•ë„ë¡œ ì‚¬ìš©ë˜ëŠ” ë°˜ë©´ UARì€ í´ë˜ìŠ¤ë³„ ì •í™•ë„(ì¦‰, í´ë˜ìŠ¤ë‹¹ íšŒìƒ)ë¥¼ í´ë˜ìŠ¤ ìˆ˜ë¡œ ë‚˜ëˆˆ ê°’ì„ ë°˜ì˜í•©ë‹ˆë‹¤. í™”ìì™€ ë…ë¦½ì ì¸ SERì—ì„œëŠ” ë°ì´í„°ë² ì´ìŠ¤ì˜ ê°ì • ì¹´í…Œê³ ë¦¬ê°€ ë¶ˆê· í˜•í•˜ê¸° ë•Œë¬¸ì— UARì€ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë³´ë‹¤ ê°€ì‹œì ìœ¼ë¡œ ì¸¡ì •í•©ë‹ˆë‹¤. LOSO &amp; LOSGO LOSOëŠ” ë°ì´í„° ì§‘í•©ì„ ì—¬ëŸ¬ ê°œì˜ í•˜ìœ„ ì§‘í•©ìœ¼ë¡œ ë‚˜ëˆ„ê³  êµì°¨ ê²€ì¦ì„ ë°˜ë³µí•  ë•Œë§ˆë‹¤ í•˜ë‚˜ì˜ í•˜ìœ„ ì§‘í•©ì„ í…ŒìŠ¤íŠ¸ ì§‘í•©ìœ¼ë¡œ ì‚¬ìš©í•˜ê³  ë‚˜ë¨¸ì§€ í•˜ìœ„ ì§‘í•©ì„ í›ˆë ¨ ì§‘í•©ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” Leave-One-Speaker-Outì˜ ì•½ìì…ë‹ˆë‹¤. ì´ ì „ëµì€ ë³´ì´ì§€ ì•ŠëŠ” ìŠ¤í”¼ì»¤ì—ì„œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. LOSGOëŠ” Leave-One-Speaker-Group-Outì˜ ì•½ìë¡œ, ë°ì´í„° ì§‘í•©ì„ ì—¬ëŸ¬ í™”ì ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ„ê³  êµì°¨ ê²€ì¦ì˜ ê° ë°˜ë³µì—ì„œ í•˜ë‚˜ì˜ í™”ì ê·¸ë£¹ì„ í…ŒìŠ¤íŠ¸ ì§‘í•©ìœ¼ë¡œ ì‚¬ìš©í•˜ê³  ë‚˜ë¨¸ì§€ í™”ì ê·¸ë£¹ì„ í›ˆë ¨ ì§‘í•©ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ ì „ëµì€ ë³´ì´ì§€ ì•ŠëŠ” í™”ì ê·¸ë£¹ì— ëŒ€í•œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê³  í™”ì ê°€ë³€ì„±ìœ¼ë¡œ ì¸í•œ í¸í–¥ì„ ì¤„ì´ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì œì•ˆëœ DIFL(ë„ë©”ì¸ ë¶ˆë³€ íŠ¹ì§• í•™ìŠµ) ë°©ë²•ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•´ LOSOì™€ LOSGO êµì°¨ ê²€ì¦ ì „ëµì„ ëª¨ë‘ ì‚¬ìš©í•˜ì—¬ eNTERFACE, Emo-DB ë° CASIA ë°ì´í„°ë² ì´ìŠ¤ì— ëŒ€í•œ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤. ê·¸ ê²°ê³¼ DIFL ë°©ë²•ì´ í™”ì ë…ë¦½ì  SERì—ì„œ ì—¬ëŸ¬ ìµœì‹  ë°©ë²•ë³´ë‹¤ ì„±ëŠ¥ì´ ìš°ìˆ˜í•˜ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. WAR, UAR Page 7ì— ì„¤ëª…ëœ ëŒ€ë¡œ WARê³¼ UARì€ ìŒì„± ê°ì • ì¸ì‹(SER) ì‹¤í—˜ì— ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ë‘ ê°€ì§€ í‰ê°€ ê¸°ì¤€ì…ë‹ˆë‹¤. WARì€ ê°€ì¤‘ í‰ê·  íšŒìƒë¥ ì„ ì˜ë¯¸í•˜ë©°, ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ ë¬¸ì œì— ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ì§€í‘œì…ë‹ˆë‹¤. SERì—ì„œ WARì€ â€˜ì •ìƒâ€™ ì¸ì‹ ì •í™•ë„ë¡œ ì‚¬ìš©ë˜ë©°, ì´ëŠ” ê° í´ë˜ìŠ¤ì˜ ìƒ˜í”Œ ìˆ˜ì— ë”°ë¼ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•œ ëª¨ë“  ê°ì • í´ë˜ìŠ¤ì˜ í‰ê·  íšŒìƒë¥ ì…ë‹ˆë‹¤. WARì€ ë°ì´í„° ì„¸íŠ¸ì˜ ëª¨ë“  ê°ì •ì„ ì¸ì‹í•˜ëŠ” ëª¨ë¸ì˜ ì „ë°˜ì ì¸ ì„±ëŠ¥ì„ ì¸¡ì •í•©ë‹ˆë‹¤. UARì€ ê°€ì¤‘ì¹˜ ì—†ëŠ” í‰ê·  íšŒìƒë¥ ì„ ì˜ë¯¸í•˜ë©°, SER ì‹¤í—˜ì— ì‚¬ìš©ë˜ëŠ” ë˜ ë‹¤ë¥¸ ì§€í‘œì…ë‹ˆë‹¤. UARì€ ì¶”ê°€ëœ í´ë˜ìŠ¤ë³„ ì •í™•ë„(ì¦‰, í´ë˜ìŠ¤ë‹¹ íšŒìƒë¥ )ë¥¼ í´ë˜ìŠ¤ ìˆ˜ë¡œ ë‚˜ëˆˆ ê°’ì„ ë°˜ì˜í•©ë‹ˆë‹¤. í™”ìì— ë…ë¦½ì ì¸ SERì—ì„œëŠ” ë°ì´í„°ë² ì´ìŠ¤ì˜ ê°ì • ì¹´í…Œê³ ë¦¬ê°€ ë¶ˆê· í˜•í•˜ê¸° ë•Œë¬¸ì— UARì€ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë³´ë‹¤ í†µì°°ë ¥ ìˆê²Œ ì¸¡ì •í•©ë‹ˆë‹¤. UARì€ ê° í´ë˜ìŠ¤ì˜ ìƒ˜í”Œ ìˆ˜ë¥¼ ê³ ë ¤í•˜ì§€ ì•Šê³  ëª¨ë“  ê°ì • í´ë˜ìŠ¤ì˜ í‰ê·  ë¦¬ì½œì„ ì¸¡ì •í•©ë‹ˆë‹¤. UARì€ ê³¼ì†Œ ëŒ€í‘œë˜ëŠ” ê°ì • í´ë˜ìŠ¤ì— ëŒ€í•œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ë° ìœ ìš©í•œ ì§€í‘œì…ë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì œì•ˆëœ DIFL(ë„ë©”ì¸ ë¶ˆë³€ íŠ¹ì§• í•™ìŠµ) ë°©ë²•ì˜ ì¸ì‹ ì„±ëŠ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•´ WARê³¼ UARì„ ëª¨ë‘ ì‚¬ìš©í•˜ì—¬ eNTERFACE, Emo-DB, CASIA ë°ì´í„°ë² ì´ìŠ¤ì— ëŒ€í•œ ì¸ì‹ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤. ê·¸ ê²°ê³¼, DIFL ë°©ì‹ì´ ì—¬ëŸ¬ ìµœì‹  ë°©ì‹ê³¼ ë¹„êµí•˜ì—¬ WARê³¼ UAR ëª¨ë‘ì—ì„œ ìƒë‹¹í•œ ê°œì„ ì„ ë‹¬ì„±í•˜ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ WARê³¼ UARì´ ë†’ì„ìˆ˜ë¡ ë°ì´í„° ì„¸íŠ¸ì—ì„œ ê°ì •ì„ ì¸ì‹í•˜ëŠ” ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ë” ìš°ìˆ˜í•˜ë‹¤ëŠ” ê²ƒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ WAR ë° UARì˜ í•´ì„ì€ íŠ¹ì • ë°ì´í„° ì„¸íŠ¸ì™€ ë‹¹ë©´í•œ ì‘ì—…ì— ë”°ë¼ ë‹¬ë¼ì§„ë‹¤ëŠ” ì ì— ìœ ì˜í•´ì•¼ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì¼ë¶€ ê°ì • í´ë˜ìŠ¤ê°€ ë‹¤ë¥¸ í´ë˜ìŠ¤ë³´ë‹¤ ìƒ˜í”Œ ìˆ˜ê°€ í˜„ì €íˆ ì ì€ ë¶ˆê· í˜• ë°ì´í„° ì„¸íŠ¸ì˜ ê²½ìš°, ëª¨ë¸ì´ ë‹¤ìˆ˜ í´ë˜ìŠ¤ì— í¸í–¥ëœ ê²½ìš° ë†’ì€ WARì´ ë°˜ë“œì‹œ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚˜íƒ€ë‚´ëŠ” ê²ƒì€ ì•„ë‹ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê²½ìš° UARì€ ê° í´ë˜ìŠ¤ì˜ ìƒ˜í”Œ ìˆ˜ë¥¼ ê³ ë ¤í•˜ì§€ ì•Šê³  ëª¨ë“  ê°ì • í´ë˜ìŠ¤ì˜ í‰ê·  ë¦¬ì½œì„ ì¸¡ì •í•˜ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë³´ë‹¤ í†µì°°ë ¥ ìˆê²Œ í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ í‰ê°€ ë©”íŠ¸ë¦­ì˜ ì„ íƒì€ íŠ¹ì • ì• í”Œë¦¬ì¼€ì´ì…˜ê³¼ ì‘ì—…ì˜ ìš”êµ¬ ì‚¬í•­ì— ë”°ë¼ ë‹¬ë¼ì§‘ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì¼ë¶€ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œëŠ” ì „ì²´ WAR ë˜ëŠ” UARì„ ìµœì í™”í•˜ëŠ” ê²ƒë³´ë‹¤ íŠ¹ì • ê°ì • í´ë˜ìŠ¤ì— ëŒ€í•´ ë†’ì€ ì •ë°€ë„ ë˜ëŠ” ë†’ì€ ë¦¬ì½œì„ ë‹¬ì„±í•˜ëŠ” ê²ƒì´ ë” ì¤‘ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ WARê³¼ UARì€ SER ì‹¤í—˜ì—ì„œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ë° ìœ ìš©í•œ ì§€í‘œì´ì§€ë§Œ ë°ì´í„° ì„¸íŠ¸ íŠ¹ì„±, ì‘ì—… ìš”êµ¬ ì‚¬í•­ ë° ëª¨ë¸ì˜ í•œê³„ì™€ ê°™ì€ ë‹¤ë¥¸ ìš”ì†Œë„ ê³ ë ¤í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. 3) Network Parametersì´ ë…¼ë¬¸ì—ì„œëŠ” ìŒì„± ê°ì • ì¸ì‹(SER) ì‹¤í—˜ì„ ìœ„í•´ ì œì•ˆëœ DIFL(ë„ë©”ì¸ ë¶ˆë³€ íŠ¹ì§• í•™ìŠµ) í”„ë ˆì„ì›Œí¬ì˜ ë„¤íŠ¸ì›Œí¬ íŒŒë¼ë¯¸í„°ì— ëŒ€í•´ ì„¤ëª…í•©ë‹ˆë‹¤. DIFL í”„ë ˆì„ì›Œí¬ëŠ” ê³„ì¸µì  í‘œí˜„ì„ ìœ„í•œ baseline ë„¤íŠ¸ì›Œí¬ë¡œ ê°€ë²¼ìš´ VGG ë„¤íŠ¸ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ë©°, ë…¼ë¬¸ 7ì—ì„œ DIFL_VGG6ì´ë¼ê³  í•©ë‹ˆë‹¤. DIFL_VGG6 ë„¤íŠ¸ì›Œí¬ëŠ” 4ê°œì˜ ì»¨ë³¼ë£¨ì…˜ ë¸”ë¡ê³¼ 2ê°œì˜ ì™„ì „ ì—°ê²°(fc) ë ˆì´ì–´ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. ê° ì»¨ë³¼ë£¨ì…˜ ë¸”ë¡ì—ëŠ” ì»¨ë³¼ë£¨ì…˜(ì»¨ë³¼ë£¨ì…˜ ì»¤ë„ì€ 3Ã—3), ë°°ì¹˜ ì •ê·œí™”, ReLU ë° ìµœëŒ€ í’€ë§ ì—°ì‚°ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. 4ê°œ ë ˆì´ì–´ì˜ ì»¨ë³¼ë£¨ì…˜ ì»¤ë„ ìˆ˜ëŠ” ê°ê° 64, 128, 256, 512ì´ë©°, fcì˜ ì°¨ì›ì€ 4,096ì…ë‹ˆë‹¤. ì´ ë…¼ë¬¸ì—ì„œëŠ” DIFL_VGG6 ì™¸ì—ë„ DIFL í”„ë ˆì„ì›Œí¬ë¥¼ ë‹¤ë¥¸ ë‘ ê°œì˜ baseline ë„¤íŠ¸ì›Œí¬, ì¦‰ VGG19ì™€ VGGishë¡œ í™•ì¥í•˜ì—¬ ê°ê° DIFL_VGG19ì™€ DIFL_VGGishë¼ê³  ë¶€ë¦…ë‹ˆë‹¤. C. Experimental Results and Analysisì´ ë…¼ë¬¸ì—ì„œëŠ” ì œì•ˆëœ ìŒì„± ê°ì • ì¸ì‹(SER)ì„ ìœ„í•œ DIFL í”„ë ˆì„ì›Œí¬ì˜ ì‹¤í—˜ ê²°ê³¼ë¥¼ ë³´ê³ í•©ë‹ˆë‹¤. ì´ ë…¼ë¬¸ì—ì„œëŠ” ê³µê°œì ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ì„¸ ê°€ì§€ ë°ì´í„°ë² ì´ìŠ¤, ì¦‰ eNTERFACE, Emo-DB, CASIAì—ì„œ ì œì•ˆí•œ ë°©ë²•ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê³  ì—¬ëŸ¬ ìµœì‹  ë°©ë²•ê³¼ ë¹„êµí•©ë‹ˆë‹¤. ì´ ë…¼ë¬¸ì—ì„œëŠ” ì œì•ˆëœ ë°©ë²•ì˜ ì¸ì‹ ì„±ëŠ¥ì„ ê°€ì¤‘ ì •í™•ë„(WAR)ì™€ ë¬´ê°€ì¤‘ ì •í™•ë„(UAR)ë¼ëŠ” ë‘ ê°€ì§€ í‰ê°€ ì§€í‘œë¥¼ í†µí•´ ë³´ê³ í•©ë‹ˆë‹¤. WARì€ ëª¨ë“  ê°ì • í´ë˜ìŠ¤ì—ì„œ ëª¨ë¸ì˜ í‰ê·  ì •í™•ë„ë¥¼ ì¸¡ì •í•˜ë©°, ê° í´ë˜ìŠ¤ì˜ ìƒ˜í”Œ ìˆ˜ì— ë”°ë¼ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤. UARì€ ê° í´ë˜ìŠ¤ì˜ ìƒ˜í”Œ ìˆ˜ë¥¼ ê³ ë ¤í•˜ì§€ ì•Šê³  ëª¨ë“  ê°ì • í´ë˜ìŠ¤ì—ì„œ ëª¨ë¸ì˜ í‰ê·  ì •í™•ë„ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤. ê·¸ ê²°ê³¼, ì œì•ˆëœ DIFL ë°©ì‹ì´ ê¸°ì¤€ ë°©ì‹ì— ë¹„í•´ WARê³¼ UAR ëª¨ë‘ì—ì„œ ìƒë‹¹í•œ ê°œì„ ì„ ë‹¬ì„±í•œ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. íŠ¹íˆ, eNTERFACE ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì œì•ˆëœ ë°©ë²•ì€ 80.5%ì˜ WARê³¼ 80.3%ì˜ UARì„ ë‹¬ì„±í•˜ì—¬ ìµœì‹  ë°©ë²•ë³´ë‹¤ í° í­ìœ¼ë¡œ ì„±ëŠ¥ì´ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤. Emo-DB ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì œì•ˆëœ ë°©ë²•ì€ 70.6%ì˜ WARê³¼ 70.4%ì˜ UARì„ ë‹¬ì„±í•˜ì—¬ ì—­ì‹œ ìµœì‹  ë°©ë²•ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. CASIA ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì œì•ˆëœ ë°©ë²•ì€ 70.2%ì˜ WARê³¼ 70.0%ì˜ UARì„ ë‹¬ì„±í•˜ì—¬ ìµœì‹  ë°©ë²•ê³¼ ë¹„ìŠ·í•œ ìˆ˜ì¤€ì…ë‹ˆë‹¤. ì´ ë…¼ë¬¸ì—ì„œëŠ” ì‹¤í—˜ ê²°ê³¼ì— ëŒ€í•œ ìì„¸í•œ ë¶„ì„ë„ ì œê³µí•©ë‹ˆë‹¤. ë¶„ì„ ê²°ê³¼, ì œì•ˆëœ DIFL ë°©ë²•ì€ ë„ë©”ì¸ ì´ë™ì— ê°•í•˜ê³  ë³´ì´ì§€ ì•ŠëŠ” ë„ë©”ì¸ì—ë„ ì˜ ì¼ë°˜í™”í•  ìˆ˜ ìˆëŠ” ë„ë©”ì¸ ë¶ˆë³€ íŠ¹ì§•ì„ í•™ìŠµí•˜ëŠ” ë° íš¨ê³¼ì ì„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ë˜í•œ ë¶„ì„ ê²°ê³¼ ì œì•ˆëœ ë°©ë²•ì€ SER ì‘ì—…ì—ì„œ í”íˆ ë°œìƒí•˜ëŠ” í™”ì ê°€ë³€ì„±ìœ¼ë¡œ ì¸í•œ ë„ë©”ì¸ ì´ë™ì„ ì²˜ë¦¬í•˜ëŠ” ë° íŠ¹íˆ íš¨ê³¼ì ì´ë¼ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ë˜í•œ, ë¶„ì„ ê²°ê³¼ ì œì•ˆí•œ ë°©ë²•ì´ ë‹¤ì–‘í•œ ê°ì • í´ë˜ìŠ¤ì˜ íŒë³„ ì •ë³´ë¥¼ í¬ì°©í•˜ê³  ë†’ì€ ì¸ì‹ ì •í™•ë„ë¥¼ ë‹¬ì„±í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì „ë°˜ì ìœ¼ë¡œ ì‹¤í—˜ ê²°ê³¼ì™€ ë¶„ì„ì€ ì œì•ˆí•œ DIFL í”„ë ˆì„ì›Œí¬ê°€ SER ì‘ì—…ì— íš¨ê³¼ì ì´ë©° ìµœì‹  ë°©ë²•ë³´ë‹¤ ìš°ìˆ˜í•˜ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. V. Conclusionì´ ë…¼ë¬¸ì—ì„œëŠ” ë‹¤ì¤‘ ì†ŒìŠ¤ ë¹„ì§€ë„ ë„ë©”ì¸ ì ì‘(UDA)ì˜ ê´€ì ì—ì„œ ìŒì„± ê°ì • ì¸ì‹(SER)ì„ ìœ„í•œ ë„ë©”ì¸ ë¶ˆë³€ íŠ¹ì§• í•™ìŠµ(DIFL) í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì œì•ˆëœ DIFL í”„ë ˆì„ì›Œí¬ëŠ” ë„ë©”ì¸ ì´ë™ì— ê°•í•˜ê³  ë³´ì´ì§€ ì•ŠëŠ” ë„ë©”ì¸ì—ë„ ì˜ ì¼ë°˜í™”í•  ìˆ˜ ìˆëŠ” ë„ë©”ì¸ ë¶ˆë³€ íŠ¹ì§•ì„ í•™ìŠµí•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. ì´ ë…¼ë¬¸ì—ì„œëŠ” ê³µê°œì ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ì„¸ ê°€ì§€ ë°ì´í„°ë² ì´ìŠ¤, ì¦‰ eNTERFACE, Emo-DB, CASIAì—ì„œ ì œì•ˆëœ ë°©ë²•ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê³  ì—¬ëŸ¬ ìµœì‹  ë°©ë²•ê³¼ ë¹„êµí•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ì œì•ˆí•œ DIFL ë°©ì‹ì´ ê¸°ì¤€ ë°©ì‹ì— ë¹„í•´ ê°€ì¤‘ ì •í™•ë„(WAR)ì™€ ë¬´ê°€ì¤‘ ì •í™•ë„(UAR) ëª¨ë‘ì—ì„œ ìœ ì˜ë¯¸í•œ ê°œì„ ì„ ë‹¬ì„±í•œ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼ì˜ ë¶„ì„ì€ ì œì•ˆëœ DIFL ë°©ë²•ì´ ë„ë©”ì¸ ì´ë™ì— ê°•í•˜ê³  ë³´ì´ì§€ ì•ŠëŠ” ë„ë©”ì¸ì—ë„ ì˜ ì¼ë°˜í™”í•  ìˆ˜ ìˆëŠ” ë„ë©”ì¸ ë¶ˆë³€ íŠ¹ì§•ì„ í•™ìŠµí•˜ëŠ” ë° íš¨ê³¼ì ì´ë¼ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ë˜í•œ ì œì•ˆëœ ë°©ë²•ì€ SER ì‘ì—…ì—ì„œ í”íˆ ë°œìƒí•˜ëŠ” í™”ì ê°€ë³€ì„±ìœ¼ë¡œ ì¸í•œ ë„ë©”ì¸ ì´ë™ì„ ì²˜ë¦¬í•˜ëŠ” ë° íŠ¹íˆ íš¨ê³¼ì ì´ë¼ëŠ” ê²ƒì„ ë¶„ì„ ê²°ê³¼ë¥¼ í†µí•´ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ë…¼ë¬¸ì€ ì‹¤í—˜ ê²°ê³¼ì™€ ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ ì œì•ˆëœ DIFL í”„ë ˆì„ì›Œí¬ê°€ SER ì‘ì—…ì˜ ë„ë©”ì¸ ì´ë™ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë° íš¨ê³¼ì ì´ë©° ê³µê°œì ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í•  ìˆ˜ ìˆë‹¤ê³  ê²°ë¡ ì§€ì—ˆìŠµë‹ˆë‹¤. ë˜í•œ ì´ ë…¼ë¬¸ì€ SER ì‘ì—…ì—ì„œ ë„ë©”ì¸ ë¶ˆë³€ íŠ¹ì§• í•™ìŠµì˜ ì¤‘ìš”ì„±ì„ ê°•ì¡°í•˜ê³  ì œì•ˆëœ DIFL í”„ë ˆì„ì›Œí¬ê°€ ìŒì„± ì¸ì‹, í™”ì í™•ì¸, ì–¸ì–´ ì‹ë³„ê³¼ ê°™ì€ ë‹¤ë¥¸ ê´€ë ¨ ì‘ì—…ìœ¼ë¡œ í™•ì¥ë  ìˆ˜ ìˆìŒì„ ì œì•ˆí•©ë‹ˆë‹¤.","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speech Emotion Recognition","slug":"Paper/Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/categories/Paper/Speech-Emotion-Recognition/"}],"tags":[{"name":"Speech Emotion Recognition","slug":"Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/tags/Speech-Emotion-Recognition/"},{"name":"Domain Invariant Feature Learning","slug":"Domain-Invariant-Feature-Learning","permalink":"https://jmj3047.github.io/tags/Domain-Invariant-Feature-Learning/"},{"name":"Domain Adversarial Layer","slug":"Domain-Adversarial-Layer","permalink":"https://jmj3047.github.io/tags/Domain-Adversarial-Layer/"},{"name":"Speaker Independent","slug":"Speaker-Independent","permalink":"https://jmj3047.github.io/tags/Speaker-Independent/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speech Emotion Recognition","slug":"Paper/Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/categories/Paper/Speech-Emotion-Recognition/"}]},{"title":"Introduction to Machine Learning in Production_Quiz","slug":"MLOps1_Quiz","date":"2023-06-28T15:00:00.000Z","updated":"2023-07-13T07:20:20.456Z","comments":true,"path":"2023/06/29/MLOps1_Quiz/","link":"","permalink":"https://jmj3047.github.io/2023/06/29/MLOps1_Quiz/","excerpt":"","text":"ê°œìš”Coursera ML Ops Course 1 Quiz 1. The Machine Learning Project Lifecycle Link: https://www.coursera.org/learn/introduction-to-machine-learning-in-production/home/week/1 2. Deployment 3. Selecting and Training a Model Link: https://www.coursera.org/learn/introduction-to-machine-learning-in-production/home/week/2 4. Modeling Challenges 5. Data Stage of the ML Production Lifecycle Link: https://www.coursera.org/learn/introduction-to-machine-learning-in-production/home/week/3 6. Scoping(optional)","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"Coursera","slug":"Coursera","permalink":"https://jmj3047.github.io/tags/Coursera/"},{"name":"Quiz","slug":"Quiz","permalink":"https://jmj3047.github.io/tags/Quiz/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}]},{"title":"Introduction to Machine Learning in Production","slug":"MLOps_Part1","date":"2023-06-25T15:00:00.000Z","updated":"2023-07-13T11:09:01.604Z","comments":true,"path":"2023/06/26/MLOps_Part1/","link":"","permalink":"https://jmj3047.github.io/2023/06/26/MLOps_Part1/","excerpt":"","text":"Course Link Lecture 1 in MLOps Overview the key steps involved in a typical machine learning project. It starts with scoping, where the project goals and variables (X and Y) are defined. Data collection follows, including establishing a baseline, labeling, and organizing the data. The next phase is model training, which involves selecting and training the model, as well as conducting error analysis. Iteration is emphasized, with the possibility of updating the model or collecting more data based on error analysis. Before deployment, a final check or audit is recommended to ensure system performance and reliability. Deployment involves writing the necessary software, monitoring the system, and maintaining it. Maintenance may involve further error analysis, model retraining, and incorporating live data feedback to improve the system. The script emphasizes that deployment is not the end but rather the start of ongoing learning and improvement for the system. Example: Speech recognition The challenges and considerations involved in deploying a machine learning model It highlights two major categories of challenges: machine learning&#x2F;statistical issues and software engineering issues. It addresses the concept of concept drift and data drift, which refer to changes in the data distribution and the desired mapping between inputs and outputs. The script also touches upon various software engineering decisions, such as real-time vs batch predictions, cloud vs edge deployment, resource allocation, latency and throughput requirements, logging, and security&#x2F;privacy considerations. It emphasizes the importance of monitoring system performance and adapting to changes in data. Finally, it mentions that the deployment process is ongoing and requires continuous maintenance and updates. Deploy patterns One of the most useful frameworks I have found for thinking about how to deploy a system is to think about deployment not as a 0, 1 is either deploy or not deploy, but instead to design a system thinking about what is the appropriate degree of automation. For example, in visual inspection of smartphones, one extreme would be if thereâ€™s no automation, so the human only system. Slightly mode automated would be if your system is running a shadow mode. So your learning algorithms are putting predictions, but itâ€™s not actually used in the factory. So that would be shadow mode. A slightly greater degree of automation would be AI assistance in which given a picture like this of a smartphone, you may have a human inspector make the decision. But maybe an AI system can affect the user interface to highlight the regions where thereâ€™s a scratch to help draw the personâ€™s attention to where it may be most useful for them to look. The user interface or UI design is critical for human assistance. But this could be a way to get a slightly greater degree of automation while still keeping the human in the loop. And even greater degree of automation maybe partial automation, where given a smartphone, if the learning algorithm is sure itâ€™s fine, then thatâ€™s its decision. It is sure itâ€™s defective, then we just go to algorithmâ€™s decision. But if the learning algorithm is not sure, in other words, if the learning algorithm prediction is not too confident, 0 or 1, maybe only then do we send this to a human. So this would be partial automation. Where if the learning algorithm is confident of its prediction, we go the learning algorithm. But for the hopefully small subset of images where the algorithm is not sure we send that to a human to get their judgment. And the human judgment can also be very valuable data to feedback to further train and improve the algorithm. I find that this partial automation is sometimes a very good design point for applications where the learning algorithms performance isnâ€™t good enough for full automation. And then of course beyond partial automation, there is full automation where we might have the learning algorithm make every single decision. So there is a spectrum of using only human decisions on the left, all the way to using only the AI systemâ€™s decisions on the right. And many deployment applications will start from the left and gradually move to the right. And you do not have to get all the way to full automation. You could choose to stop using AI assistance or partial automation or you could choose to go to full automation depending on the performance of your system and the needs of the application. Train focused on modeling Establish a baseline unstructured data and structured data: unstructured data has good Human Level Performance(HLP), while structured data doesnâ€™t and relies on Dictionary data set Prioritizing what to work on To summarize, when prioritizing what to work on, you might decide on the most important categories to work on based on, how much room for improvement there is, such as, compared to human-level performance or according to some baseline comparison. How frequently does that category appear? You can also take into account how easy it is to improve accuracy in that category. For example, if you have some ideas for how to improve the accuracy of speech with car noise, maybe your data augmentation, that might cause you to prioritize that category more highly than some other category where you just donâ€™t have as many ideas for how to improve the system. Then finally, how important it is to improve performance on that category. For example, you may decide that improving performance with car noise is especially important because when youâ€™re driving, you have a stronger desire to do search, especially search on maps and find addresses without needing to use your hands if your hands are supposed to be holding the steering wheel. skewed dataset The learning algorithm with some precision, even the high value of precision is not that useful usually if this recall is so low. hereâ€™s a common way of combining precision and recall using this formula, which is called the F_1 score. One intuition behind the F_1 score is that you want an algorithm to do well on both precision and recall, and if it does worse on either precision or recall, thatâ€™s pretty bad. F_1 is a way of combining precision and recall that emphasizes whichever of P or R precision or recall is worse. In mathematics, this is technically called a harmonic mean between precision and recall, which is like taking the average but placing more emphasis on whichever is the lower number. If you compute the F_1 score of these two models, it turns out to be 83.4 percent using the formula below here. Model 2 has a very bad recall, so its F_1 score is actually quite low as well and this lets us tell, maybe more clearly that Model 1 appears to be a superior model than Model 2. Performance auditing What is good data?","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}],"tags":[{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Coursera","slug":"Coursera","permalink":"https://jmj3047.github.io/tags/Coursera/"},{"name":"AI Studies","slug":"AI-Studies","permalink":"https://jmj3047.github.io/tags/AI-Studies/"},{"name":"ML Operations","slug":"ML-Operations","permalink":"https://jmj3047.github.io/tags/ML-Operations/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}]},{"title":"Sequence Model_Quiz","slug":"DL5_Quiz","date":"2023-06-18T15:00:00.000Z","updated":"2023-06-19T12:43:37.995Z","comments":true,"path":"2023/06/19/DL5_Quiz/","link":"","permalink":"https://jmj3047.github.io/2023/06/19/DL5_Quiz/","excerpt":"","text":"ê°œìš”Coursera Deep Learning Course 5 Quiz 1. Recurrent Neural Networks Link: https://www.coursera.org/learn/nlp-sequence-models/home/module/1 2. Natural Language Processing &amp; Word Embeddings Link: https://www.coursera.org/learn/nlp-sequence-models/home/module/2 3.Sequence Models &amp; Attention Mechanism Link: https://www.coursera.org/learn/nlp-sequence-models/home/module/3 4. Transformers Link: https://www.coursera.org/learn/nlp-sequence-models/home/module/4","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"Coursera","slug":"Coursera","permalink":"https://jmj3047.github.io/tags/Coursera/"},{"name":"Quiz","slug":"Quiz","permalink":"https://jmj3047.github.io/tags/Quiz/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}]},{"title":"Domain Invariant Feature Learning for Cross Corpus Speech Emotion Recognition","slug":"DIFL_Cross_Corpus_SER","date":"2023-06-12T15:00:00.000Z","updated":"2023-06-14T05:51:27.335Z","comments":true,"path":"2023/06/13/DIFL_Cross_Corpus_SER/","link":"","permalink":"https://jmj3047.github.io/2023/06/13/DIFL_Cross_Corpus_SER/","excerpt":"","text":"Journal&#x2F;Conference : ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)Year(published year): 2022Author: Yuan Gao, Shogo Okada, Longbiao Wang, Jiaxing Liu, Jianwu DangSubject: Speech Emotion Recognition, Domain Adaptation, Center Loss Domain Invariant Feature Learning for Cross Corpus Speech Emotion Recognition Summary I. Introductionì—°êµ¬ì˜ í•„ìš”ì„±ê¸°ì¡´ SERì˜ ì ‘ê·¼ ë°©ì‹ì€ ë™ì¼í•œ ë°ì´í„° ì…‹ì—ì„œ í›ˆë ¨ ë° í…ŒìŠ¤íŠ¸ ë¨. ìì—°í™˜ê²½ì—ì„œ ëŒ€ê·œëª¨ ì£¼ì„ì´ ë‹¬ë¦° ê°ì • ë°œí™”ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ê²ƒì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ê¸° ë•Œë¬¸ì— ê¸°ì¡´ ë°ì´í„° ì„¸íŠ¸ì—ëŠ” ì ì€ ìˆ˜ì˜ ìŒì„± ìƒ˜í”Œì´ í¬í•¨ë˜ì–´ ìˆì–´ ê°•ë ¥í•œ ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ í›ˆë ¨í•˜ê¸°ì—ëŠ” ì¶©ë¶„í•˜ì§€ ì•ŠìŒ. ë˜í•œ ì‹¤ì œ í™˜ê²½ì—ì„œ ìŒì„±ì˜ ê°ì • ì •ë³´ëŠ” ë„ë©”ì¸ ì •ë³´ì˜ ë³€í™”ë¡œ ì¸í•´ í•™ìŠµí•˜ê¸° ì–´ë ¤ì›€. ë”°ë¼ì„œ SER ì‹œìŠ¤í…œì„ ë¯¸ì§€ì˜ ë°ì´í„° ì„¸íŠ¸ì— ì ìš©í•  ê²½ìš°ì—ëŠ” ì¸ì‹ ì„±ëŠ¥ì´ í¬ê²Œ ì €í•˜ë˜ëŠ” ê²½ìš°ê°€ ë§ìŒ. ì‹¤ì œ ì–´í”Œë¦¬ì¼€ì´ì…˜ì„ ë‹¤ë£¨ê¸° ìœ„í•´ ë‹¤ì–‘í•œ ë°ì´í„° ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í‰ê°€í•´ì•¼ í•¨. ìµœê·¼ ì—°êµ¬ì—ì„œëŠ” ì—°êµ¬ìë“¤ì´ cross corpus SERì—ì„œ CNN, RNN, ë° attentionì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê¸°ë„ í•¨[11,12] contributionSERì˜ ì¼ë°˜í™” ëŠ¥ë ¥ì„ ë”ìš± í–¥ìƒ ì‹œí‚¤ê¸° ìœ„í•´ adversarial domain adaptation ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„° ê°„ì˜ ë„ë©”ì¸ ì°¨ì´ë¥¼ ì¤„ì˜€ìŒ. êµ¬ì²´ì ìœ¼ë¡œ adversarial trainingì„ í†µí•´ latent representationì˜ í™”ì, ì½”í¼ìŠ¤ ë° ê¸°íƒ€ ë„ë©”ì¸ ì •ë³´ë¥¼ ì œê±°í•¨. domain adaptationì€ feature extractorì™€ domain classifier ì‚¬ì´ì˜ gradientë¥¼ ì—­ì „ì‹œí‚´ìœ¼ë¡œì¨ ì´ë£¨ì–´ì§€ë©° ì´ë¥¼ í†µí•´ ëª¨ë¸ì€ ë¹„ê°ì •ì  ì •ë³´ì˜ í•™ìŠµ ì†ì‹¤ì„ ìµœëŒ€í™” í•  ìˆ˜ ìˆìŒ. ë˜í•œ ê¸°ì¡´ ì—°êµ¬ì—ì„œ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ê°ì • ë¶„ë¥˜ê¸°ëŠ” softmax ì†ì‹¤í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ decision boundaryë¥¼ ì°¾ê³  ê°ì •ì„ êµ¬ë¶„í•¨. feature representationì— ì°¨ë³„ì„±ì„ ë‘ê¸° ìœ„í•´ center lossë¥¼ í†µí•©í•¨, ê·¸ë¦¬ê³  ê·¸ê²ƒì€ feature extractorë¥¼ ìœ„í•œjoint supervisionì²˜ëŸ¼ feature representationê³¼ í•´ë‹¹ í´ë˜ìŠ¤ centerì˜ ê±°ë¦¬ë¥¼ ìµœì†Œí™” í•˜ê¸° ìœ„í•´ í›ˆë ¨ë˜ì—ˆìŒ. II. Adversarial Domain Adaptation For Feature ExtractionFig 1ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´, ìš°ë¦¬ëŠ” íŠ¹ì§• ì¶”ì¶œì„ ìœ„í•´ deep CNNê³¼ BLSTM layerì„ ì‚¬ìš©í•˜ë©°, ì´ layerì˜ parameterëŠ” [13]ì—ì„œ ì“°ì¸ê²ƒê³¼ ìœ ì‚¬í•¨. ìš°ë¦¬ëŠ” Domain Adversarial Neural Network(DANN)ìœ¼ë¡œ feature extractorë¥¼ ì¡°ì •í•˜ì˜€ìŒ. ë˜í•œ feature representationì˜ intra-class ë³€í™”ë¥¼ ì¤„ì´ê¸° ìœ„í•´ center lossë¥¼ ì‚¬ìš©í–ˆìŒ. DANNê³¼ center loss ë‘˜ë‹¤ domain divergenceë¥¼ í•´ê²°í• ìˆ˜ ìˆìŒ. Domain Adversarial Trainingì´ ì—°êµ¬ì—ì„œëŠ” ë¹„ê°ì •ì ì¸ ì •ë³´ë¥¼ ì œê±°í•˜ê¸° ìœ„í•´ íŠ¹ì§• ì¶”ì¶œê¸°ì— DANNì„ í†µí•©í–ˆìŒ. DANNì€ multi-task learning ëª¨ë¸ DANNì˜ recognition targetì€ emotion classifier(LE), domain classifier(LD)ì´ë‹¤. ë³¸ ì—°êµ¬ì—ì„œ LDì˜ domain recognition targetì€ ì½”í¼ìŠ¤, ì–¸ì–´, ì„±ë³„ì´ë‹¤. í•˜ë‚˜ì˜ trainingê³¼ì • ì•ˆì—ì„œ domain adaptationê³¼ feature representation í•™ìŠµì„ ë‹¬ì„±í•˜ê¸° ìœ„í•´ [14]ëŠ” domain classifierê³¼ feature extractor ì‚¬ì´ì— gradient reversal layer(GRL)ì„ ë‘ì—ˆìŒ. GRLì€ ì—­ì „íŒŒ ê³¼ì •ì—ì„œ íŠ¹ì • ìŒì˜ ìƒìˆ˜ë¥¼ ë„ë©”ì¸ ë¶„ë¥˜ ì‘ì—…ì˜ ê¸°ìš¸ê¸°ì— ê³±í• ìˆ˜ ìˆìŒ. ì†ŒìŠ¤ ë„ë©”ì¸ê³¼ íƒ€ê²Ÿ ë„ë©”ì¸ì—ì„œ í•™ìŠµí•œ feature distributionì´ ìš°ë¦¬ ëª¨ë¸ê³¼ êµ¬ë³„ë˜ì§€ ì•Šë„ë¡ DANN í•™ìŠµì„ ì‹œí‚´. ì´ë ‡ê²Œ GRLì„ í†µí•´ domain invariant representationì„ ì¶”ì¶œí•˜ì—¬ ì½”í¼ìŠ¤ ê°„ ê°ì •ì¸ì‹ì„ ìœ„í•œ ì¼ë°˜í™” ëŠ¥ë ¥ì„ í–¥ìƒí• ìˆ˜ ìˆìŒ. ì œì•ˆëœ feature extraction ëª¨ë¸ì˜ objective functionì€ ë‹¤ìŒê³¼ ê°™ìŒ. LE: center lossì™€ softmax lossë¥¼ ê²°í•©í•œ emotion classifierì˜ loss function(ìì„¸í•œ ë‚´ìš©ì€ Center Loss) ì´ íŠ¹ì • ì‘ì—…ì—ì„œëŠ” ì•ì„œ ì–¸ê¸‰í•œ ë¹„ê°ì •ì ì¸ ì •ë³´ë¥¼ feature extractor G(x, $\\theta$)ê°€ í•™ìŠµí•˜ì§€ ì•Šë„ë¡ $\\gamma$ë¥¼ 0.3ìœ¼ë¡œ ì„¤ì • í–ˆìŒ. DANN í•™ìŠµì„ í†µí•´ ëª¨ë¸ì€ feature distributionì˜ domain shiftë¥¼ ì œê±°í•  ìˆ˜ ìˆìŒ. domain classifierì˜ ì†ì‹¤í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„ë¨ Lg, Ll, LcëŠ” ì„±ë³„, ì–¸ì–´, ë§ë­‰ì¹˜ì˜ loss function LEë¥¼ ìµœì†Œí™” í•˜ê³  LDë¥¼ ìµœëŒ€í™” í•˜ëŠ” ì•ˆì •ì ì„ ì°¾ì•„ëƒ„ìœ¼ë¡œì¨ ìš°ë¦¬ê°€ ì œì•ˆí•œ feature extractorëŠ” emotion classifierì˜ inputì—ì„œ domain divergenceë¥¼ í¬ê²Œ ì¤„ì¼ ìˆ˜ ìˆìŒ. Center Lossì œì•ˆëœ feature extractorì™¸ì—ë„ softmax lossì™€ center loss[15]ë¥¼ joint supervisionìœ¼ë¡œ emotion classifier LEì—ì¶”ê°€ì ìœ¼ë¡œ ì ìš©í•¨. softmax loss functionì€ SERì—ì„œ ë‹¤ì–‘í•œ ê°ì •ì˜ decision boundaryë¥¼ ì°¾ê¸° ìœ„í•´ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë¨. M: ë¯¸ë‹ˆ ë°°ì¹˜ì˜ í¬ê¸°, N: emotion classì˜ ê°œìˆ˜. ë³¸ ì—°êµ¬ì—ì„œëŠ” training sampleê³¼ test sampleì— ëŒ€í•´ ë™ì¼í•œ emotion annotationì„ ì •ì˜í–ˆì§€ë§Œ, ì„œë¡œ ë‹¤ë¥¸ ë°ì´í„° ì„¸íŠ¸ì˜ feature distributionì´ ë¶„ë¦¬ ê°€ëŠ¥í•œ clusterë¡œ ë‚˜íƒ€ë‚˜ì§€ ì•Šì•˜ìŒ. ê·¸ë¦¬ê³  ê·¸ê²ƒì€ cross corpus SERì´ ì¼ë°˜ì ì¸ close-set identification ì‘ì—…ë³´ë‹¤ ë” ì–´ë µê²Œ ë§Œë“¦. ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ center lossë¥¼ ë„ì…í•˜ì—¬ ê° ê°ì • ì¹´í…Œê³ ë¦¬ì— ëŒ€í•œ class center cë¥¼ í•™ìŠµí•¨ìœ¼ë¡œì¨ feature distributionì˜ í´ë˜ìŠ¤ ë‚´(intra-class) ê±°ë¦¬ë¥¼ ì¤„ì˜€ìŒ. ì´ loss functionì€ input featureì™€ í•´ë‹¹ class center ì‚¬ì´ì˜ ìœ í´ë¦¬ë“œ ê±°ë¦¬ë¡œ ê³„ì‚°ë¨. Class center cë¥¼ ë³´ë‹¤ íš¨ìœ¨ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ í•˜ê¸° ìœ„í•´ loss functionì€ ê° ë¯¸ë‹ˆë°°ì¹˜ì— ëŒ€í•´ì„œ í•™ìŠµë˜ì—ˆìŒ. 5ë²ˆ ì‹ì—ì„œ mì€ ìƒˆ ë¯¸ë‹ˆë°°ì¹˜ì— ìˆëŠ” í´ë˜ìŠ¤ iì˜ ìƒ˜í”Œ ê°œìˆ˜. ê°ì • ë¶„ë¥˜ê¸° ì „ì²´ì˜ objective functionì€ ë‹¤ìŒì‹ìœ¼ë¡œ ì •ì˜ë¨ ê° loss termì˜ ê°€ì¤‘ì¹˜ë¥¼ controlí•˜ê¸° ìœ„í•´ $\\lambda$ë¥¼ 0.5ë¡œ ì„¤ì •. center lossì™€ softmax lossë¥¼ ê²°í•©í•˜ì—¬ ëª¨ë¸ì„ ë™í†µìœ¼ë¡œ ìµœì í™” í•¨ìœ¼ë¡œì¨ cross corpus SER ì‘ì—…ì„ ìœ„í•œ robustí•œ feature representationì„ ì¶”ì¶œí•  ìˆ˜ ìˆìŒ. III. Experimental SetupEmotional Speech Datasetë„¤ê°€ì§€ ê°ì • ì½”í¼ìŠ¤ë¥¼ ì‚¬ìš©í•´ ëª¨ë¸ì„ í‰ê°€í•©ë‹ˆë‹¤: IEMOCAP, MSP-IMPROV, SAVEE, Emo-DB IEMOCAP: ì˜¤ë””ì˜¤, ë¹„ë””ì˜¤, ì–¼êµ´ ëª¨ì…˜ ì •ë³´ë¥¼ í¬í•¨í•œ 12ì‹œê°„ ë¶„ëŸ‰ì˜ ì‹œì²­ê° ë°ì´í„°ì™€ 10ëª…ì˜ í™”ìì™€ í…ìŠ¤íŠ¸ í•„ì‚¬ë³¸ì´ í¬í•¨ë˜ì–´ ìˆìŒ. ì‹¤í—˜ì—ëŠ” ìŠ¤í¬ë¦½íŠ¸ ë°ì´í„°ì™€ ì¦‰í¥ ë°ì´í„° ëª¨ë‘ 5531 ë°œí™”ë¥¼ ì‚¬ìš©. í–‰ë³µ, ìŠ¬í””, ë¶„ë…¸, ì¤‘ë¦½ì˜ ê°ì •ì´ ê¸°ë¡ë¼ ìˆìŒ. MSP-IMPROV: ë‹¤ì´ë‚˜ë¯¹ ì„¸ì…˜ì—ì„œ ìƒí˜¸ì‘ìš©í•˜ëŠ” ë°°ìš°ë¡œë¶€í„° ê¸°ë¡ëœ ë‹¤ì¤‘ëª¨ë“œ ê°ì • ë°ì´í„° ë² ì´ìŠ¤. 12ëª…ì˜ ë°°ìš°ëŸ¬ë¶€í„° ë…¹ìŒëœ 8438ê°œì˜ ê°ì • ë¬¸ì¥ ë°œí™”ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŒ. í–‰ë³µ, ìŠ¬í””, ë¶„ë…¸, ì¤‘ë¦½ì˜ ê°ì • ì¹´í…Œê³ ë¦¬ SAVEE: ë‚¨ì„± í”¼í—˜ì 4ëª…ì˜ audio-visual ë…¹ìŒì„ í¬í•¨í•˜ê³  ìˆìŒ. 480ê°œì˜ ì›ì–´ë¯¼ ì˜ì–´ ë°œí™”ë¡œ êµ¬ì„±ë˜ì–´ ìˆê³  í–‰ë³µ, ìŠ¬í””, í˜ì˜¤, ë¶„ë…¸, ì§€ë£¨í•¨, ë‘ë ¤ì›€ì¸ 6ê°€ì§€ ê°ì •ì— ëŒ€í•´ì„œ 60ê°œ, ì¤‘ë¦½ì— ëŒ€í•´ 120ê°œì˜ ë°œí™”ê°€ í¬í•¨ë¨ Emo-DB: 10ëª…ì˜ ì „ë¬¸ ë°°ìš°ê°€ ë…¹ìŒí™˜ê²½ì—ì„œ ì—°ê¸°. ë°°ìš°ë“¤ì€ ê° ë¬¸ì¥ì„ 7ê°€ì§€ ê°ì • ìƒíƒœ(ì¤‘ë¦½, ì§€ë£¨í•¨, í˜ì˜¤, ìŠ¬í””, ë¶„ë…¸, í–‰ë³µ, ë‘ë ¤ì›€)ë¡œ í‘œí˜„í•¨. ì´ 535ê°œ ë°œí™” Experimental Settingsë‘ê°€ì§€ ê²€ì¦ ì²´ê³„ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í‰ê°€ cross-corpus evaluation: ëª¨ë¸ì€ IEMOCAPì— ëŒ€í•´ì„œë§Œ í›ˆë ¨í•˜ê³  ë‚˜ë¨¸ì§€ëŠ” ì„¸ê°œì˜ ë§ë­‰ì¹˜ì—ì„œ í…ŒìŠ¤íŠ¸ Multi-corpus evaluation: ë„¤ê°œì˜ ë°ì´í„° ì„¸íŠ¸ë¥¼ ëª¨ë‘ train set(80%)ì™€ test set(20%)ìœ¼ë¡œ ë‚˜ëˆ„ê³  ê° ì½”í¼ìŠ¤ì˜ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í‰ê°€. train setê³¼ test setì€ í™”ìê°€ ê²¹ì¹˜ì§€ ì•ŠìŒ. optimizerë¡œëŠ” adadeltaë¥¼ ì‚¬ìš©í–ˆìœ¼ë©° ë¯¸ë‹ˆë°°ì¹˜ì‚¬ì´ì¦ˆëŠ” 128. ì „ì²˜ë¦¬ ê³¼ì •ì—ì„œ ëª¨ë“  ë°ì´í„°ëŠ” 16kHzë¡œ ë‹¤ìš´ìƒ˜í”Œë§ í•˜ì˜€ìŒ. input featureë¡œëŠ” spectrogramì„ ì‚¬ìš©í•˜ì˜€ê³  input ë°œí™”ëŠ” 265msë¡œ ë¶„í• ë˜ë©° ê° ì„¸ê·¸ë¨¼íŠ¸ì— ëŒ€í•´ 25msì˜ í”„ë ˆì„ sizeë¡œ input spectrogramì´ ê³„ì‚°ë¨. input spectrogramì˜ time X frequencyëŠ” 32 X 129. IV. Results and AnalysisUnweighted Accuracy(UA)ë¥¼ í‰ê°€ê¸°ì¤€ìœ¼ë¡œ ì„ íƒ baseline: CNN + BLSTMì˜ ì¡°í•©. ì œì•ˆëœ ë‘ê°œì˜ DANN-based approaches ë“¤ì„ ë¹„êµ DANN_1: domain classifierì˜ ì¸ì‹ ëŒ€ìƒì´ í™”ìì™€ corpus DANN_2: speaker classificationì´ ì–¸ì–´ì™€ ì„±ë³„ ì¸ì‹ìœ¼ë¡œ ëŒ€ì²´ C: center loss, S: softmax loss multi corpus ì‹¤í—˜ ê²°ê³¼, domain recognition ëŒ€ìƒì€ DANN 1ì˜ ê²½ìš° í™”ìì™€ ì½”í¼ìŠ¤ì´ê³  DANN 2ì˜ ê²½ìš° ì„±ë³„, ì–¸ì–´, ë§ë­‰ì¹˜. Multi- corpus Evaluationí‘œ2ì—ì„œ ë‹¤ì¤‘ ì½”í¼ìŠ¤ í‰ê°€ ê²°ê³¼ë¥¼ ì œì‹œ. arousal ì¸ì‹ì˜ ê²½ìš° DANN 2ëŠ” ë¹„êµ ì‹¤í—˜ë³´ë‹¤ ì‘ì§€ë§Œ ê¾¸ì¤€í•œ ê°œì„ ìœ¼ë¡œ ë„¤ê°€ì§€ ë°ì´í„° ì„¸íŠ¸ ëª¨ë‘ì—ì„œ ê°€ì¥ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë‹¬ì„±. valenceì˜ ê²½ìš° ëŒ€ë¶€ë¶„ì˜ ë¹„êµ ì‹¤í—˜ì€ Emo DBì—ì„œ ë‚®ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìŒ. EmoDB í›ˆë ¨ì„¸íŠ¸ëŠ” ì£¼ë¡œ negative inputìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŒ. ë˜í•œ EmoDBì™€ ë‹¤ë¥¸ ë°ì´í„° ì„¸íŠ¸ì˜ ì–¸ì–´ ë¶ˆì¼ì¹˜ë¡œ ì¸í•´ ì´ ë°ì´í„°ì„¸íŠ¸ì˜ ì¸ì‹ ì„±ëŠ¥ì€ ìƒëŒ€ì ìœ¼ë¡œ ë‚®ìŒ. ì´ëŸ¬í•œ ìƒí™©ì„ì—ë„ ë¶ˆêµ¬í•˜ê³  ëª¨ë¸ì€ positive, negativeì— ëŒ€í•´ ë¹„êµì  ë™ë“±í•œ ì¸ì‹ ì •í™•ë„ë¥¼ ë³´ì˜€ê³  UAë¥¼ 10.45%ë‚˜ í–¥ìƒ ì‹œì¼°ìŒ. ë˜í•œ ì œì•ˆëœ center lossëŠ” ëª¨ë¸ì´ ë” ë§ì€ discriminative feature representationì„ ì¶”ì¶œí•˜ê³  í‰ê·  ì •í™•ë„ë¥¼ 3.28%ë‚˜ í–¥ìƒ ì‹œí‚´. ê²°ê³¼ëŠ” ì œì•ˆëœ ëª¨ë¸ì´ ë°ì´í„° ì„¸íŠ¸ ì „ë°˜ì—ì„œ ê°ì • ì •ë³´ë¥¼ ì¼ë°˜í™” í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤Œ Cross-corpus Evaluationêµì°¨ ì½”í¼ìŠ¤ í‰ê°€ê²°ê³¼ëŠ” ì œì•ˆëœ ëª¨ë¸ì˜ íš¨ê³¼ë¥¼ ì…ì¦í•¨. í‘œ3ê³¼ 4ì—ì„œ ë³¼ìˆ˜ ìˆë“¯ì´ DANN ê¸°ë°˜ ëª¨ë¸ì˜ í‰ê·  ì„±ëŠ¥ì€ arousalì—ì„œ baselineì— ë¹„í•´ í¬ê²Œ í–¥ìƒëœ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŒ. ë˜í•œ ë„¤ê°€ì§€ ë°ì´í„° ì„¸íŠ¸ì—ëŠ” í™”ì ìˆ˜ê°€ ë§ê¸° ë•Œë¬¸ì— í™”ì ì¸ì‹ì€ ì´ ì‘ì—…ì—ì„œ ë†’ì€ ì •í™•ë„ë¥¼ ë‹¬ì„±í•˜ê¸° ì–´ë ¤ì›€. ë”°ë¼ì„œ DANN_2ê°€ DANN_1ë³´ë‹¤ ë” ë‚˜ì€ í‰ê·  ì„±ëŠ¥ì„ ìƒì„±í•¨. ê·¸ëŸ¬ë‚˜ valence ì¸ì‹ ê°™ì€ ê²½ìš° DANNê³¼ baselineëª¨ë‘ ìƒëŒ€ì ìœ¼ë¡œ ë‚®ì€ ì„±ëŠ¥(60%ë¯¸ë§Œ)ì„ ë³´ì˜€ìŒ. Emo DBì˜ valenceì¸ì‹ì˜ ê²½ìš° ì–¸ì–´ ë¶ˆì¼ì¹˜ë¡œ ì¸í•´ ë„¤ê°€ì§€ ë¹„êµ ì‹¤í—˜ ì¸ì‹ ì„±ëŠ¥ì´ í™•ë¥  ìˆ˜ì¤€(chance level) ì´í•˜ì„. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” DANN í•™ìŠµì´ Valence ì¸ì‹ì˜ ê²½ìš°ì— ë” ë‹¬ì„±í•˜ê¸° ì–´ë µë‹¤ëŠ” ê²ƒì„ ë‚˜íƒ€ë‚´ë©° ì´ëŠ” [20]ì—ë„ ìì„¸í•˜ê²Œ ë‚˜ì™€ìˆìŒ. V. Conclusionì´ ë…¼ë¬¸ì—ì„œ cross corpus SER ì‹œìŠ¤í…œì˜ ì¼ë°˜í™” ëŠ¥ë ¥ì„ ë†’ì´ê¸° ìœ„í•œ adversarial domain adaptation ê³¼ center lossì— ëŒ€í•´ ì¡°ì‚¬í–ˆìŒ. SERì˜ domain invariant feature learningì˜ ë‹¨ê³„ë¡œ íŠ¹ì§• ì¶”ì¶œì„ DANNìœ¼ë¡œ ìˆ˜ì •í•˜ê³  ë‹¤ë¥¸ ë°ì´í„° ì„¸íŠ¸ì—ì„œ ë„ë©”ì¸ ì°¨ì´ë¥¼ ì¤„ì˜€ìŒ. ë˜í•œ ê°ì •ì¸ì‹ì„ ìœ„í•œ discriminative feature representationì„ í•™ìŠµí•˜ê¸° ìœ„í•´ center lossì™€ softmax loss functionì„ í†µí•©í•¨. ì‹¤í—˜ ê²°ê³¼ì— ë”°ë¥´ë©´ arousalì— ë¹„í•´ deep learning ëª¨ë¸ì€ valence informationì„ ì¼ë°˜í™” ì‹œí‚¤ëŠ”ê²Œ ë” ì–´ë ¤ì›€. ì œì•ˆëœ ëª¨ë¸ì€ ê¸°ì¡´ì˜ ë”¥ëŸ¬ë‹ ê¸°ë°˜ ëª¨ë¸ë³´ë‹¤ ë” ìœ ëª…í•œ í‰ê·  ê²°ê³¼ë¥¼ ë‹¬ì„±í•˜ì—¬ íš¨ê³¼ë¥¼ ì…ì¦í•¨.","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speech Emotion Recognition","slug":"Paper/Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/categories/Paper/Speech-Emotion-Recognition/"}],"tags":[{"name":"Speech Emotion Recognition","slug":"Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/tags/Speech-Emotion-Recognition/"},{"name":"Adversarial Domain Adaptation","slug":"Adversarial-Domain-Adaptation","permalink":"https://jmj3047.github.io/tags/Adversarial-Domain-Adaptation/"},{"name":"Center Loss","slug":"Center-Loss","permalink":"https://jmj3047.github.io/tags/Center-Loss/"},{"name":"Domain Invariant Feature Learning","slug":"Domain-Invariant-Feature-Learning","permalink":"https://jmj3047.github.io/tags/Domain-Invariant-Feature-Learning/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speech Emotion Recognition","slug":"Paper/Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/categories/Paper/Speech-Emotion-Recognition/"}]},{"title":"Sequence Model","slug":"DL_Part5","date":"2023-06-02T15:00:00.000Z","updated":"2023-06-19T12:48:18.552Z","comments":true,"path":"2023/06/03/DL_Part5/","link":"","permalink":"https://jmj3047.github.io/2023/06/03/DL_Part5/","excerpt":"","text":"Course Link Lecture 5 in Deep Learning RNN there is one-to-many. So, this was a music generation or sequenced generation as example. And then, thereâ€™s many-to-one, that would be an example of sentiment classification. Where you might want to read as input all the text with a movie review. And then, try to figure out that they liked the movie or not. There is many-to-many, so the name entity recognition, the example weâ€™ve been using, was this where Tx is equal to Ty. And then, finally, thereâ€™s this other version of many-to-many, where for applications like machine translation, Tx and Ty no longer have to be the same. What is language modeling? given any sentence, its job is to tell you what is the probability of that particular sentence, and by probability of sentence, I mean, if you were to pick up a random newspaper, open a random email, or pick a random webpage, what a language model does is it estimates the probability of that particular sequence of words. Gated Recurrent Unit very effective solution for addressing the vanishing gradient problem and will allow your neural network to capture much longer range dependencies This is another gate Gamma r. You can think of r as standing for relevance. This gate Gamma r tells you how relevant is C^t minus 1 to computing the next candidate for C^t. This gate Gamma r is computed pretty much as you expect with a new parameter matrix w _r, and then the same things as input x_t plus b_r. over many years, researchers have experimented with many different possible versions of how to design these units to try to have longer range connections. To try to have model long-range effects and also address vanishing gradient problems. The GRU is one of the most commonly used versions that researchers have converged to and then found as robust and useful for many different problems. LSTM one element of this is interesting is if you hook up a bunch of these in parallel so thatâ€™s one of them and you connect them, connect these temporarily. So thereâ€™s the input x 1, then x 2, x 3. So you can take these units and just hook them up as follows where the output a for a period of time, 70 input at the next time set. And similarly for C and Iâ€™ve simplified the diagrams a little bit at the bottom. And one cool thing about this, you notice is that this is a line at the top that shows how so long as you said the forget and the update gates, appropriately, it is relatively easy for the LSTM to have some value C0 and have that be passed all the way to the right to have, maybe C3 equals C0. And this is why the LSTM as well as the GRU is very good at memorizing certain values. Even for a long time for certain real values stored in the memory cells even for many, many times steps. peephole connection: one common variation you see of LSTMs So thatâ€™s it for the LSTM, as you can imagine, there are also a few variations on this that people use. Perhaps the most common one, is that instead of just having the gate values be dependent only on a t-1, xt. Sometimes people also sneak in there the value c t -1 as well. This is called a peephole connection. if you see peephole connection, what that means is that the gate values may depend not just on a t-1 but and on x t but also on the previous memory cell value. And the peephole connection can go into all three of these gates computations. GRU vs LSTM the advantage of the GRU is that itâ€™s a simpler model. And so itâ€™s actually easier to build a much bigger network only has two gates, so computation runs a bit faster so it scales the building, somewhat bigger models. But the LSTM is more powerful and more flexible since thereâ€™s three gates instead of two. If you want to pick one to use, I think LSTM has been the historically more proven choice. So if you had to pick one, I think most people today will still use the LSTM as the default first thing to try. Bidirectional RNN you can have a model that uses RNN, or GRU, LSTM, and is able to make predictions anywhere even in the middle of the sequence, but take into account information potentially from the entire sequence. The disadvantage of the bidirectional RNN is that, you do need the entire sequence of data before you can make predictions anywhere. So, for example, if youâ€™re building a speech recognition system then BRNN will let you take into account the entire speech other friends. But if you use this straightforward implementation, you need to wait for the person to stop talking to get the entire utterance before you can actually process it, and make a speech recognition prediction. So the real time speech recognition applications, there is somewhat more complex models as well rather than just using the standard by the rational RNN as youâ€™re seeing here. But for a lot of natural language processing applications where you can get the entire sentence all at the same time, the standard BRNN and algorithm is actually very effective. NLP word embeddings this is how you can carry out transfer learning using word embeddings. Step 1 is to learn word embeddings from a large text corpus, a very large text corpus or you can also download pre-trained word embeddings online. There are several word embeddings that you can find online under very permissive licenses. And you can then take these word embeddings and transfer the embedding to new task, where you have a much smaller labeled training sets. And use this, letâ€™s say, 300 dimensional embedding, to represent your words. One nice thing also about this is you can now use relatively lower dimensional feature vectors. So rather than using a 10,000 dimensional one-hot vector, you can now instead use maybe a 300 dimensional dense vector. Although the one-hot vector is fast and the 300 dimensional vector that you might learn for your embedding will be a dense vector. And then, finally, as you train your model on your new task, on your named entity recognition task with a smaller label data set, one thing you can optionally do is to continue to fine tune, continue to adjust the word embeddings with the new data. In practice, you would do this only if this task 2 has a pretty big data set. If your label data set for step 2 is quite small, then usually, I would not bother to continue to fine tune the word embeddings. Word2Vec you saw how the Skip-Gram model allows you to construct a supervised learning task. So we map from context to target and how that allows you to learn a useful word embedding. But the downside of that was the Softmax objective was slow to compute. Negative Sampling technique is called negative sampling because what youâ€™re doing is, you have a positive example, the orange and then juice. And then you will go and deliberately generate a bunch of negative examples, negative samplings, hence, the name negative sampling, with which to train four more of these binary classifiers. And on every iteration, you choose four different random negative words with which to train your algorithm on. To summarize, youâ€™ve seen how you can learn word vectors in a Softmax classier, but itâ€™s very computationally expensive. And in this video, you saw how by changing that to a bunch of binary classification problems, you can very efficiently learn words vectors. And if you run this algorithm, you will be able to learn pretty good word vectors. Now of course, as is the case in other areas of deep learning as well, there are open source implementations. And there are also pre-trained word vectors that others have trained and released online under permissive licenses. And so if you want to get going quickly on a NLP problem, itâ€™d be reasonable to download someone elseâ€™s word vectors and use that as a starting point. Attention greedy search greedy search is an algorithm from computer science which says to generate the first word just pick whatever is the most likely first word according to your conditional language model. Going to your machine translation model and then after having picked the first word, you then pick whatever is the second word that seems most likely, then pick the third word that seems most likely. it turns out that the greedy approach, where you just pick the best first word, and then, after having picked the best first word, try to pick the best second word, and then, after that, try to pick the best third word, that approach doesnâ€™t really work. Because â€˜goingâ€™ is much more common word than â€˜visitingâ€™ so if you use greedy search to translate, â€˜goingâ€™ has higher possibility to be chosen. However the best choice of translation is the first sentence. one major difference between this and the earlier language modeling problems is rather than wanting to generate a sentence at random, you may want to try to find the most likely English sentence, most likely English translation. But the set of all English sentences of a certain length is too large to exhaustively enumerate. So, we have to resort to a search algorithm. Beam Search with a beam of three being searched considers three possibilities at a time. Notice that if the beam width was said to be equal to one, say cause thereâ€™s only one, then this essentially becomes the greedy search algorithm which we had discussed in the last video but by considering multiple possibilities say three or ten or some other number at the same time beam search will usually find a much better output sentence than greedy search. how do you choose the beam width? Share the pros and cons of setting beam to be very large versus very small. If the beam width is very large, then you consider a lot of possibilities and so you tend to get a better result because youâ€™re consuming a lot of different options, but it will be slower. The memory requirements will also grow and also be computationally slower. Whereas if you use a very small beam width, then you get a worse result because you are just keeping less possibilities in mind as the algorithm is running, but you get a result faster and the memory requirements will also be lower. I would say try out a variety of values of beam as see what works for your application, but when beam is very large, there is often diminishing returns. For many applications, I would expect to see a huge gain as you go from beam of one, which is basically research to three to maybe 10, but the gains as you go from the thousands of thousand beam width might not be as big. even though y* is a better translation, the RNN ascribed y* in lower probability than the inferior translation. So in this case, I will say the RNN model is at fault. So the error analysis process looks as follows. You go through the development set and find the mistakes that the algorithm made in the development set. through this process, you can then carry out error analysis to figure out what fraction of errors are due to beam search versus the RNN model. if you find that beam search is responsible for a lot of errors, then maybe is weâ€™re working hard to increase the beam width. Whereas in contrast, if you find that the RNN model is at fault, then you could do a deeper layer of analysis to try to figure out if you want to add regularization, or get more training data, or try a different network architecture, or something else. Bleu score One of the challenges of machine translation is that, given a French sentence, there could be multiple English translations that are equally good translations of that French sentence. So how do you evaluate a machine translation system if there are multiple equally good answers, unlike, say, image recognition where thereâ€™s one right answer? You just measure accuracy. If there are multiple great answers, how do you measure accuracy? The way this is done conventionally is through something called the BLEU score. Attention Model itâ€™s for long sentece translation because encoder-decoder algorithm is hard to remember whole sentence if it is too long. Speech Recognition how do you build a speech recognition system? One method that seems to work well is to use the CTC cost for speech recognition. CTC stands for Connection is Temporal Classification the basic rule for the CTC cost function is to collapse repeated characters not separated by â€œblankâ€. So, to be clear, Iâ€™m using this underscore to denote a special blank character and thatâ€™s different than the space character Trigger word system Transformer The major innovation of the transformer architecture is combining the use of attention based representations and a CNN convolutional neural network style of processing. Self Attention the main difference is that for every word, say for lâ€™Afrique, you have three values called the query, key, and value. These vectors are the key inputs to computing the attention value for each words. what are these query key and value vectors supposed to do? They were indeed using a loose analogy to a concert and databases where you can have queries and also key-value pairs. To recap, associated with each of the five words you end up with a query, a key, and a value. The query lets you ask a question about that word, such as whatâ€™s happening in Africa. The key looks at all of the other words, and by the similarity to the query, helps you figure out which words gives the most relevant answer to that question. In this case, visite is whatâ€™s happening in Africa, someoneâ€™s visiting Africa. Then finally, the value allows the representation to plug in how visite should be represented within A^3, within the representation of Africa. This allows you to come up with a representation for the word Africa that says this is Africa and someone is visiting Africa. multi-head attention Transformer","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Coursera","slug":"Coursera","permalink":"https://jmj3047.github.io/tags/Coursera/"},{"name":"AI Studies","slug":"AI-Studies","permalink":"https://jmj3047.github.io/tags/AI-Studies/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}]},{"title":"Convolutional Neural Networks_Quiz","slug":"DL4_Quiz","date":"2023-05-20T15:00:00.000Z","updated":"2023-06-22T15:28:27.227Z","comments":true,"path":"2023/05/21/DL4_Quiz/","link":"","permalink":"https://jmj3047.github.io/2023/05/21/DL4_Quiz/","excerpt":"","text":"ê°œìš” Coursera Deep Learning Course 4 Quiz 1. The Basics of ConvNets Link: https://www.coursera.org/learn/neural-networks-deep-learning/home/module/1 2.Deep Convolutional Models Link: https://www.coursera.org/learn/convolutional-neural-networks/home/module/2 3. Detection Algorithms Link: https://www.coursera.org/learn/convolutional-neural-networks/home/module/3 4. Special Applications: Face Recognition &amp; Neural Style Transfer Link: https://www.coursera.org/learn/convolutional-neural-networks/home/module/4","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"Coursera","slug":"Coursera","permalink":"https://jmj3047.github.io/tags/Coursera/"},{"name":"Quiz","slug":"Quiz","permalink":"https://jmj3047.github.io/tags/Quiz/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}]},{"title":"ARIMA model","slug":"ARIMA","date":"2023-05-15T15:00:00.000Z","updated":"2023-05-17T11:51:58.842Z","comments":true,"path":"2023/05/16/ARIMA/","link":"","permalink":"https://jmj3047.github.io/2023/05/16/ARIMA/","excerpt":"","text":"ì •ìƒì„±(stationarity) ì‹œê³„ì—´ì€ ì‹œê³„ì—´ì˜ íŠ¹ì§•ì´ í•´ë‹¹ ì‹œê³„ì—´ì´ ê´€ì¸¡ëœ ì‹œê°„ì— ë¬´ê´€ ì¶”ì„¸ë‚˜ ê³„ì ˆì„±ì´ ìˆëŠ” ì‹œê³„ì—´ì€ ì •ìƒì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ì‹œê³„ì—´ì´ ì•„ë‹˜ â†’ ì¶”ì„¸ì™€ ê³„ì ˆì„±ì€ ì„œë¡œ ë‹¤ë¥¸ ì‹œê°„ì— ì‹œê³„ì—´ì˜ ê°’ì— ì˜í–¥ì„ ì¤„ ê²ƒì´ê¸° ë•Œë¬¸. ë°±ìƒ‰ ì¡ìŒ(white noise) ì‹œê³„ì—´: ì •ìƒì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ì‹œê³„ì—´ â†’ ì–¸ì œ ê´€ì°°í•˜ëŠ”ì§€ì— ìƒê´€ì´ ì—†ê³  ì‹œê°„ì— ë”°ë¼ ì–´ë–¤ ì‹œì ì—ì„œ ë³´ë”ë¼ë„ ë˜‘ê°™ì´ ë³´ì¼ê²ƒì´ê¸° ë•Œë¬¸. (a) 200 ê±°ë˜ì¼ ë™ì•ˆì˜ êµ¬ê¸€ ì£¼ì‹ ê°€ê²©; (b) 200 ê±°ë˜ì¼ ë™ì•ˆì˜ êµ¬ê¸€ ì£¼ì‹ ê°€ê²©ì˜ ì¼ì¼ ë³€ë™; (c) ë¯¸êµ­ì˜ ì—°ê°„ íŒŒì—… ìˆ˜; (d) ë¯¸êµ­ì—ì„œ íŒë§¤ë˜ëŠ” ìƒˆë¡œìš´ ë‹¨ë… ì£¼íƒì˜ ì›”ë³„ íŒë§¤ì•¡; (e) ë¯¸êµ­ì—ì„œ ê³„ë€ 12ê°œì˜ ì—°ê°„ ê°€ê²© (ê³ ì • ë‹¬ëŸ¬); (f) í˜¸ì£¼ ë¹…í† ë¦¬ì•„ ì£¼ì—ì„œ ë§¤ì›” ë„ì‚´í•œ ë¼ì§€ì˜ ì „ì²´ ìˆ˜; (g) ìºë‚˜ë‹¤ ë¶ì„œë¶€ì˜ ë§¥í‚¨ì§€ ê°• ì§€ì—­ì—ì„œ ì—°ê°„ í¬íšëœ ìŠ¤ë¼ì†Œë‹ˆì˜ ì „ì²´ ìˆ˜; (h) í˜¸ì£¼ ì›”ë³„ ë§¥ì£¼ ìƒì‚°ëŸ‰; (i) í˜¸ì£¼ ì›”ë³„ ì „ê¸° ìƒì‚°ëŸ‰. ë¶„ëª…í•˜ê²Œ ê³„ì ˆì„±ì´ ë³´ì´ëŠ” (d), (h), (i)ëŠ” í›„ë³´ê°€ ë˜ì§€ ëª»í•©ë‹ˆë‹¤. ì¶”ì„¸ê°€ ìˆê³  ìˆ˜ì¤€ì´ ë³€í•˜ëŠ” (a), (c), (e), (f), (i)ë„ í›„ë³´ê°€ ë˜ì§€ ëª»í•©ë‹ˆë‹¤. ë¶„ì‚°ì´ ì¦ê°€í•˜ëŠ” (i)ë„ í›„ë³´ê°€ ë˜ì§€ ëª»í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë©´ (b)ì™€ (g)ë§Œ ì •ìƒì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ì‹œê³„ì—´ í›„ë³´ë¡œ ë‚¨ì•˜ìŠµë‹ˆë‹¤. ì–¸ëœ» ë³´ë©´ ì‹œê³„ì—´ (g)ì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” ëšœë ·í•œ ì£¼ê¸°(cycle) ë•Œë¬¸ì— ì •ìƒì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ì‹œê³„ì—´ì´ ì•„ë‹Œ ê²ƒì²˜ëŸ¼ ë³´ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ëŸ¬í•œ ì£¼ê¸°ëŠ” ë¶ˆê·œì¹™ì (aperiodic)ì…ë‹ˆë‹¤ â€” ë¨¹ì´ë¥¼ êµ¬í•˜ê¸° í˜ë“¤ë§Œí¼ ì‚´ì¾¡ì´ ê°œì²´ìˆ˜ê°€ ë„ˆë¬´ ë§ì´ ëŠ˜ì–´ë‚˜ ë²ˆì‹ì„ ë©ˆì¶°ì„œ, ê°œì²´ìˆ˜ê°€ ì‘ì€ ìˆ«ìë¡œ ì¤„ì–´ë“¤ê³ , ê·¸ ë‹¤ìŒ ë¨¹ì´ë¥¼ êµ¬í•  ìˆ˜ ìˆê²Œ ë˜ì–´ ê°œì²´ìˆ˜ê°€ ë‹¤ì‹œ ëŠ˜ì–´ë‚˜ëŠ” ì‹ì´ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì¥ê¸°ì ìœ¼ë¡œ ë³¼ ë•Œ, ì´ëŸ¬í•œ ì£¼ê¸°ì˜ ì‹œì‘ì´ë‚˜ ëì€ ì˜ˆì¸¡í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì´ ì‹œê³„ì—´ì€ ì •ìƒì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ì‹œê³„ì—´ì…ë‹ˆë‹¤. ACF (Autocorrelation Function) ì •ìƒì„± í™•ì¸í•˜ëŠ” ë°©ë²• ì¤‘ í•˜ë‚˜ xì¶•ì€ lag, yì¶•ì€ ACF lag&#x3D;1: í•œ ì‹œì  ë¯¸ë£¬ ë°ì´í„°ì™€ì˜ ì°¨ì´ë¥¼ ì˜ë¯¸ ìê¸° ìì‹ ê³¼ ìê¸° ìì‹  ì´ì „ ë°ì´í„°ì™€ì˜ correlation &#x3D; Autocorrelation lag 1ì´ë‹¤. lag&#x3D;2: ìì‹  ìì‹  ë°ì´í„°ì™€ ë‘ ì‹œì  ë¯¸ë£¬ ë°ì´í„°ì™€ì˜ correlation &#x3D; Autocorrelation lag 2 lag&#x3D;20: í˜„ì¬ ë°ì´í„°ì™€ 20 ì‹œì ì„ shiftí•œ ë°ì´í„°ì™€ì˜ correlation &#x3D; Autocorrelation lag 20 ì´ ê·¸ë˜í”„ì—ì„œëŠ” 5ì‹œì  shiftí•œ ê²ƒê³¼ autocorrelationì´ ê½¤ ìˆëŠ” ê²ƒìœ¼ë¡œ ë³´ì„ ACFë¥¼ í†µí•´ì„œ ì •ìƒì„±ì„ ì•Œì•„ë³´ëŠ” ë°©ë²• ì •ë¦¬ ì¼ì •í•œ íŒ¨í„´ì´ ì—†ê±°ë‚˜ ê°‘ìê¸° ë–¨ì–´ì§€ëŠ” íŒ¨í„´ &#x3D;&gt; stationary ì¼ì •í•˜ê²Œ ë–¨ì–´ì§€ê±°ë‚˜ ì˜¬ë¼ê°”ë‹¤ ë‚´ë ¤ê°”ë‹¤í•˜ë©´ì„œ êµ‰ì¥íˆ ì²œì²œíˆ ë–¨ì–´ì§€ëŠ” íŒ¨í„´ &#x3D;&gt; nonstationary Autoregressive (AR) Models dependent variableì¸ Yì˜ lagë¥¼ independent variablesì¸ Xë¡œ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ ì²«ë²ˆì§¸ Xê°€ ë°”ë¡œ ytâˆ’1, ì¦‰ yì˜ í•œ ì‹œì ì „ ë°ì´í„°, ytâˆ’2ëŠ” ë‘ ì‹œì  ì „ ë°ì´í„°,â€¦, ytâˆ’pëŠ” p ì‹œì  ì „ ë°ì´í„°ë¥¼ ì˜ë¯¸í•œë‹¤. ìê¸°ìì‹ ì„ Xë¡œ ì‚¼ê¸°ë•Œë¬¸ì— X1 &#x3D; ytâˆ’1, X2 &#x3D;ytâˆ’2,..,Xp &#x3D; ytâˆ’pë¡œ ìƒê°í•˜ë©´ ëœë‹¤. Ï•0ëŠ” ì¸í„¸ì…‰ ytë¥¼ ëª¨ë¸ë§í•  ë•Œ ytì˜ lagëœ ë³€ìˆ˜ë“¤(ìì‹ ì˜ ê³¼ê±° ë°ì´í„°)ì„ X ì‚¼ì•„ì„œ íšŒê·€ëª¨ë¸ì„ ë§Œë“¦ .(Auto &#x3D; selfë¼ê³  ìƒê°í•˜ë©´ ë¨) multiple regression modelê³¼ ë‹¤ë¥¸ ì  ìê¸°ìì‹ ì„ ê°–ê³  ëª¨ë¸ë§ì„ í•˜ê¸° ë•Œë¬¸ì— ë…ë¦½ì„±ì´ ì—†ë‹¤. Ï•0 (ê³„ìˆ˜)ë¥¼ ì¶”ì •í•  ë•Œ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©í–ˆë˜ ìµœì†Œì œê³±ë²•ì€ ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤. Moving Average (MA) Models ytë¥¼ Îµ(ì—¡ì‹¤ë¡ &#x3D;error)ìœ¼ë¡œ ëª¨ë¸ë§ tì‹œì ì˜ ë°ì´í„°(yt)ë¥¼ t ì‹œì ì˜ ì—ëŸ¬(Îµt)ì™€ ê³¼ê±°ì˜ ì—ëŸ¬ë“¤ë¡œ í‘œí˜„ ì—°ì†ì ì¸ ì—ëŸ¬ termìœ¼ë¡œ yì™€dì˜ ê´€ê³„ë¥¼ ëª¨ë¸ë§í•˜ëŠ” ë°©ë²• Autoregressive and Moving Average (ARMA) ARê³¼ MAì„ í•©ì¹œ ëª¨ë¸ tì‹œì ì˜ ë°ì´í„°(yt)ë¥¼ ìê¸°ìì‹ ì˜ lagëœ ê°’ë“¤, tì‹œì ì˜ errorì™€ ì „ ì‹œì ì˜ errorë“¤ë¡œ í‘œí˜„í•¨ Autoregressive Integrated Moving Average (ARIMA) p: order of the AR part of the model d: order of differencing q: order of the MA part of the model differencingì„ í–ˆë‹¤ëŠ” ê²ƒì„ â€œintegratedâ€ë¡œ í‘œí˜„í•¨ I ë¼ëŠ” ê²ƒì€ differencingì„ ëª‡ë²ˆ í–ˆëŠ”ì§€ë¥¼ ì˜ë¯¸ p,d,q AR â†’ p I â†’ d MA â†’ q ARëª¨ë¸ì—ì„œ p &#x3D; independent variableì˜ ê°œìˆ˜ë¥¼ ë‚˜íƒ€ëƒ„ MRëª¨ë¸ì—ì„œ q, Î¸ì˜ ê°œìˆ˜, ì¦‰ íŒŒë¼ë¯¸í„°ì˜ ê°œìˆ˜ë¥¼ ë‚˜íƒ€ëƒ„ d &#x3D; ëª‡ë²ˆ differencingì„ í–ˆëƒ ì‹œê³„ì—´ ë°ì´í„° ëª¨ë¸ì„ êµ¬í˜„í•  ë•Œ ì£¼ì˜í•´ì•¼ í•  ìƒí™© AR, MA, ARMA ì´ ëª¨ë¸ë“¤ì„ êµ¬í˜„í•˜ê¸° ìœ„í•´ì„œëŠ” ë¶„ì„í•´ì•¼ë˜ëŠ” ë°ì´í„°ê°€ stationaryí•´ì•¼ëœë‹¤. nonstationaryì¸ ê²½ìš°, ì´ ëª¨ë¸ë“¤ì„ ì ìš©í•  ìˆ˜ ì—†ë‹¤. ì¼ìƒìƒí™œì—” nonstationaryí•œ ë°ì´í„°ë“¤ì´ í›¨ì”¬ ë” ë§ë‹¤. ë”°ë¼ì„œ stationaryí•œ ë°ì´í„°ë¡œ ë°”ê¾¼ ë’¤ì— ì´ ëª¨ë¸ë§ì„ í•  ìˆ˜ ìˆë‹¤. ì–´ë–»ê²Œ nonstationaryë¥¼ stationaryë¡œ ë°”ê¾¸ëŠ” ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•ì´ ë°”ë¡œ differencing(ì°¨ë¶„) ì°¨ë¶„(differencing) í˜„ ì‹œì  ë°ì´í„°ì—ì„œ d ì‹œì  ì´ì „ ë°ì´í„°ë¥¼ ëº€ ê²ƒ ì—°ì´ì€ ê´€ì¸¡ê°’ë“¤ì˜ ì°¨ì´ë¥¼ ê³„ì‚°í•˜ëŠ” ê²ƒ ì›ë˜ ë°ì´í„°ì™€ ì›ë˜ ë°ì´í„°ë¥¼ í•œë²ˆ shiftí•œ ê²ƒì„ ë¹¼ì£¼ë©´ ê²°ê³¼ê°€ ë‚˜ì˜¤ëŠ”ë° ì´ê²ƒì´ ë°”ë¡œ ì²«ë²ˆì§¸ differencingí•œ ê²°ê³¼ì´ë‹¤. 1ì°¨ ì°¨ë¶„ì´ë€ tì‹œì ì˜ ë°ì´í„°ì™€ t-1ì‹œì ì˜ ë°ì´í„°ì˜ ì°¨ì´ 2ì°¨ ì°¨ë¶„ì´ë€ tì‹œì ì˜ ë°ì´í„°ì™€ t-2ì‹œì ì˜ ë°ì´í„°ì˜ ì°¨ì´ dì°¨ ì°¨ë¶„ì´ë€ tì‹œì ì˜ ë°ì´í„°ì™€ t-dì‹œì ì˜ ë°ì´í„°ì˜ ì°¨ì´ X(ì›ë˜ ë°ì´í„°)ëŠ” nonstationaryì—¬ë„ differencingì„ í•œ ê²°ê³¼(Y)ëŠ” stationaryë¡œ ë°”ë€” í™•ë¥ ì´ ë§¤ìš° í¬ë‹¤ (a)ì˜ êµ¬ê¸€(Google) ì£¼ì‹ ê°€ê²©ì´ ì •ìƒì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ì‹œê³„ì—´ì´ ì•„ë‹ˆì—ˆì§€ë§Œ íŒ¨ë„ (b)ì˜ ì¼ë³„ ë³€í™”ëŠ” ì •ìƒì„±ì„ ë‚˜íƒ€ëƒˆë‹¤ëŠ” ê²ƒì— ì£¼ëª©í•©ì‹œë‹¤. ì´ ê·¸ë¦¼ì€ ì •ìƒì„±ì„ ë‚˜íƒ€ë‚´ì§€ ì•ŠëŠ” ì‹œê³„ì—´ì„ ì •ìƒì„±ì„ ë‚˜íƒ€ë‚´ë„ë¡ ë§Œë“œëŠ” í•œ ê°€ì§€ ë°©ë²•ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ë¡œê·¸ ê°™ì€ ë³€í™˜ì€ ì‹œê³„ì—´ì˜ ë¶„ì‚° ë³€í™”ë¥¼ ì¼ì •í•˜ê²Œ ë§Œë“œëŠ”ë° ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì°¨ë¶„(differencing)ì€ ì‹œê³„ì—´ì˜ ìˆ˜ì¤€ì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” ë³€í™”ë¥¼ ì œê±°í•˜ì—¬ ì‹œê³„ì—´ì˜ í‰ê·  ë³€í™”ë¥¼ ì¼ì •í•˜ê²Œ ë§Œë“œëŠ”ë° ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ ì¶”ì„¸ë‚˜ ê³„ì ˆì„±ì´ ì œê±°(ë˜ëŠ” ê°ì†Œ)ë©ë‹ˆë‹¤. ì •ìƒì„±ì„ ë‚˜íƒ€ë‚´ì§€ ì•ŠëŠ” ì‹œê³„ì—´ì„ ì°¾ì•„ë‚¼ ë•Œ ë°ì´í„°ì˜ ì‹œê°„ ê·¸ë˜í”„ë¥¼ ì‚´í´ë³´ëŠ” ê²ƒë§Œí¼, ACF ê·¸ë˜í”„ë„ ìœ ìš©í•©ë‹ˆë‹¤. ì •ìƒì„±ì„ ë‚˜íƒ€ë‚´ì§€ ì•ŠëŠ” ë°ì´í„°ì—ì„œëŠ” ACFê°€ ëŠë¦¬ê²Œ ê°ì†Œí•˜ì§€ë§Œ, ì •ìƒì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ì‹œê³„ì—´ì—ì„œëŠ”, ACFê°€ ë¹„êµì  ë¹ ë¥´ê²Œ 0ìœ¼ë¡œ ë–¨ì–´ì§ˆ ê²ƒì…ë‹ˆë‹¤. ê·¸ë¦¬ê³  ì •ìƒì„±ì„ ë‚˜íƒ€ë‚´ì§€ ì•ŠëŠ” ë°ì´í„°ì—ì„œ r1 ì€ ì¢…ì¢… í° ì–‘ìˆ˜ ê°’ì„ ê°–ìŠµë‹ˆë‹¤. ARIMA - Order of Differencing ë§Œì•½ original ë°ì´í„°ê°€ stationaryì´ë©´ differencingì€ í•„ìš”ì—†ë‹¤. ë§Œì•½ original ë°ì´í„°ê°€ constant average trend(ì¼ì •í•˜ê²Œ ì¦ê°€í•˜ê±°ë‚˜ ê°ì†Œí•˜ëŠ” íŒ¨í„´)ì´ë©´ 1ì°¨ ì°¨ë¶„ì´ë©´ ì¶©ë¶„í•˜ë‹¤. ì˜¤ë¥¸ìª½ì˜ ê·¸ë˜í”„ì™€ ê°™ì´ ë” ë³µì¡í•œ íŒ¨í„´ì„ ê°€ì§€ê³  ìˆë‹¤ë©´ 2ì°¨ ì°¨ë¶„ê¹Œì§€ ê°€ì•¼ëœë‹¤. ëŒ€ë¶€ë¶„ì˜ ë°ì´í„°ê°€ 2ì°¨ ì°¨ë¶„ìœ¼ë¡œ ì¶©ë¶„í•˜ë‹¤. 3ì°¨ ì°¨ë¶„ê¹Œì§€ í–ˆì„ ë•Œ stationaryê°€ ë˜ëŠ” ë°ì´í„°ëŠ” AR,MA,ARMA ëª¨ë¸ë¡œëŠ” ì í•©í•˜ì§€ ì•Šì€ ë°ì´í„°ë¼ê³  ìƒê°í•˜ë©´ ë¨ 1st Differencing (1ì°¨ ì°¨ë¶„) 2nd Differencing (2ì°¨ ì°¨ë¶„) 1ì°¨ì™€ 2ì°¨ì˜ ì°¨ì´ê°€ ì—†ìœ¼ë¯€ë¡œ 2ì°¨ ì°¨ë¶„ê¹Œì§€ í•  í•„ìš”ê°€ ì—†ì–´ë³´ì„. nonstationaryê°€ stationaryë¡œ ë³€í–ˆëŠ”ì§€ ê·¸ëƒ¥ ë´¤ì„ ë•ŒëŠ” ì˜ ëª¨ë¥´ë¯€ë¡œ ACFë¥¼ í™•ì¸í•˜ì. ì›ë˜ ë°ì´í„°ëŠ” ACFì—ì„œ ì¼ì •í•˜ê²Œ ê°ì†Œí•˜ëŠ” íŒ¨í„´ 1ì°¨ ì°¨ë¶„í•œ ê²ƒì€ ì¼ì •í•œ íŒ¨í„´ì´ ì—†ë‹¤. Reference https://otexts.com/fppkr/stationarity.html https:&#x2F;&#x2F;velog.io&#x2F;@sjina0722&#x2F;ì‹œê³„ì—´ë¶„ì„-ARIMA-ëª¨ë¸","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}],"tags":[{"name":"ARIMA","slug":"ARIMA","permalink":"https://jmj3047.github.io/tags/ARIMA/"},{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}]},{"title":"Transfer Learning for Speech Emotion Recognition","slug":"Transfer_Learning_SER","date":"2023-05-10T15:00:00.000Z","updated":"2023-06-14T05:46:15.955Z","comments":true,"path":"2023/05/11/Transfer_Learning_SER/","link":"","permalink":"https://jmj3047.github.io/2023/05/11/Transfer_Learning_SER/","excerpt":"","text":"Journal&#x2F;Conference : IEEE 5th Intl Conference on Big Data Security on Cloud (BigDataSecurity), IEEE Intl Conference on High Performance and Smart Computing,(HPSC) and IEEE Intl Conference on Intelligent Data and Security (IDS)Year(published year): 2019Author: Han, Zhijie, Huijuan Zhao, and Ruchuan WangSubject: transfer learning, speech emotion recognition, domain adaption, cross-domain Transfer Learning for Speech Emotion Recognition Summary ì´ ë…¼ë¬¸ì€ ê°ì •ì´ ì¸ê°„ì˜ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ì—ì„œ ì¤‘ìš”í•œ ì—­í• ì„ í•˜ëŠ” ê²ƒì„ ê³ ë ¤í•˜ì—¬, transfer learningì˜ ì´ë¡ ê³¼ ì£¼ìš” ë²”ì£¼ë¥¼ ì¡°ì‚¬í•˜ê³  speech emotion recognitionì—ì„œì˜ ì ìš©ì„ ì—°êµ¬í•˜ë©°, ì´ë¥¼ í†µí•´ ê°ì • ì¸ì‹ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê°œì„ í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê°ì • ì¸ì‹ì„ ìœ„í•œ ë ˆì´ë¸” ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ê²ƒì€ ì–´ë ¤ìš¸ ìˆ˜ ìˆì§€ë§Œ, ì „ì´ í•™ìŠµì€ ë‹¤ë¥¸ ë¶„ì•¼ì—ì„œ í•™ìŠµëœ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ê°ì • ì¸ì‹ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ì´ë ‡ê²Œ í•˜ë©´ ë ˆì´ë¸” ë°ì´í„°ì— ëŒ€í•œ ì˜ì¡´ë„ë¥¼ ë‚®ì¶”ê³  ê°ì • ì¸ì‹ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. I. Introduction ìŒì„± ê°ì • ì¸ì‹ì˜ ë³µì¡ì„±ìœ¼ë¡œ ì¸í•´, ê³ ì„±ëŠ¥ì˜ ê°•ë ¥í•œ ì¸ì‹ì€ ì—¬ì „íˆ ë§¤ìš° ì–´ë ¤ìš´ ê³¼ì œì…ë‹ˆë‹¤.Due to the complexity of the speech emotion recognition, high performance and robust recognition is still very challenging. ì£¼ëœ ì´ìœ ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: ì¸ê°„ì˜ ì˜¤ë””ì˜¤ ìƒì„±ì€ ë§í•˜ëŠ” ì¥ë©´, ë§í•˜ëŠ” ë°©ì‹, í™”ìì˜ ë‚˜ì´, ì„±ë³„, ë§í•˜ê¸° ìŠµê´€ ë“± ë§í•˜ê¸°ì˜ ë§¥ë½ê³¼ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ì´ ìˆìŠµë‹ˆë‹¤[11]. human audio generation is directly related to the context of speech, such as the speaking scene, the way of speaking, and the age, gender, and speaking habits of the speaker [11]. ìŒì„± ë°ì´í„° ìˆ˜ì§‘ì€ ë³µì¡í•˜ë©° ë°±ê·¸ë¼ìš´ë“œ ë…¸ì´ì¦ˆë¥¼ ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤. The collection of speech data is complex, and need deal with the back ground noise. ê°ì •ì€ ì£¼ê´€ì ì´ë©° ê³µì‹ì ì¸ ê°ì • ì •ì˜ê°€ ì—†ìŠµë‹ˆë‹¤. Emotion is subjective, and there is no formal emotion definition. ë°ì´í„°ì— ë ˆì´ë¸”ì„ ì§€ì •í•˜ëŠ” ì‚¬ëŒì˜ ê°ì • ì¸ì‹ ëŠ¥ë ¥ì€ ê°ì • ë°ì´í„°ì˜ ì£¼ì„ì— ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤[12]. annotationsì€ ì™„ì „í•œ ìŒì„± ì •ë³´ í‘œí˜„ì— ì˜ì¡´í•˜ë¯€ë¡œ ì£¼ì„ ì‘ì—…ì— ì‹œê°„ì´ ë§ì´ ê±¸ë¦¬ë¯€ë¡œ ì£¼ì„ì´ ë‹¬ë¦° ê³µê³µ ìŒì„± ê°ì • ì½”í¼ìŠ¤ì˜ ìˆ˜ê°€ ì œí•œë˜ì–´ ìˆìŠµë‹ˆë‹¤. The emotional perception ability of the person who labels the data affects the annotation of the emotion data[12]. Annotation relies on the complete speech information presentation, so the annotation work is time consuming.Therefore, the number of the annotated public speech emotion corpora is limited. í˜„ì¬ ë¨¸ì‹ ëŸ¬ë‹ ì—°êµ¬ëŠ” í›ˆë ¨ ì„¸íŠ¸ì™€ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ê°€ ë™ì¼í•œ íŠ¹ì§•ê³µê°„ì— ì†í•˜ê³  ë™ì¼í•œ ë¶„í¬ë¥¼ ê°–ëŠ”ë‹¤ëŠ” ì „ì œí•˜ì— ì´ë£¨ì–´ì§€ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì‹¤ì œ ì–´í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ í›ˆë ¨ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì´ ì¡°ê±´ì„ ì¶©ì¡±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê°ì •ì¸ì‹ì€ ì¼ë°˜ì ìœ¼ë¡œ ë„ë©”ì¸ ê°„ ë˜ëŠ” ì‹¬ì§€ì–´ ì–¸ì–´ ê°„ì— ì´ë£¨ì–´ ì§‘ë‹ˆë‹¤.At present, the research of machine learning is on the premise that train sets and test sets belong to the same feature space and have the same distribution. However, train and test data generally do not meet this condition in practical applications. Emotion recognition is usually cross-domain, or even cross-language. ì „ì´ í•™ìŠµì€ í•œ ë„ë©”ì¸ì—ì„œ í•™ìŠµí•œ ì§€ì‹ì„ ë‹¤ë¥¸ ìœ ì‚¬í•œ ë¶„ì•¼ì— ì ìš©í•˜ì—¬ í•™ìŠµ ì‘ì—…ì„ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•œí¸, annotationì— ëŒ€í•œ ì˜ì¡´ë„ë¥¼ ì¤„ì—¬ ìŒì„± ê°ì • ì¸ì‹ì˜ ì„±ëŠ¥ì„ ë”ìš± í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.Transfer learning uses the knowledge learned in one domain and apply to another similar field, which can reduce the training work. Meanwhile, reduce the dependence on the annotation data, which can better promote the performance of speech emotion recognition. annotation data: Annotation dataëŠ” ê°ì • ì¸ì‹ì„ ìœ„í•´ ë ˆì´ë¸”ë§ëœ ë°ì´í„°ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ì¦‰, ì‚¬ëŒë“¤ì´ ì–´ë–¤ ê°ì •ì„ í‘œí˜„í•˜ê³  ìˆëŠ”ì§€ë¥¼ ë¯¸ë¦¬ ì•Œë ¤ì¤€ ë°ì´í„°ë¥¼ ë§í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë°ì´í„°ëŠ” ëŒ€ê·œëª¨ë¡œ ìˆ˜ì§‘í•˜ê¸° ì–´ë µê¸° ë•Œë¬¸ì—, transfer learningì€ ì´ëŸ¬í•œ annotation dataì— ëŒ€í•œ ì˜ì¡´ë„ë¥¼ ë‚®ì¶”ë©´ì„œ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë°©ë²•ìœ¼ë¡œ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤. ê°ì • ì¸ì‹ì„ ìœ„í•´ ë ˆì´ë¸”ë§ ëœ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ê²ƒì€ ë§¤ìš° ì–´ë µê³  ì‹œê°„ì´ ë§ì´ ì†Œìš”ë˜ëŠ” ì‘ì—…ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ë ˆì´ë¸”ë§í•˜ëŠ” ê²ƒì€ ê°ì • ì¸ì‹ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° ì¤‘ìš”í•˜ì§€ë§Œ, ì´ëŸ¬í•œ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ì˜ëª»ëœ ë ˆì´ë¸”ë§ìœ¼ë¡œ ì¸í•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì— ë¶€ì •ì ì¸ ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ transfer learningì€ ë‹¤ë¥¸ ìœ ì‚¬í•œ ë¶„ì•¼ì—ì„œ í•™ìŠµëœ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ê°ì • ì¸ì‹ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë°©ë²•ìœ¼ë¡œ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ annotation dataì— ëŒ€í•œ ì˜ì¡´ë„ë¥¼ ë‚®ì¶”ë©´ì„œë„ ê°ì • ì¸ì‹ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. II. Related WorkTransfer Learning ì „ì´ í•™ìŠµì€ ë°ì´í„°, ê³¼ì œ, ëª¨ë¸ì—ì„œ ë‘ ê³¼ì œì˜ ìœ ì‚¬ì„±ì„ í™œìš©í•˜ì—¬ íŠ¹ì • ë¶„ì•¼ì—ì„œ í•™ìŠµí•œ ëª¨ë¸ì„ ìƒˆë¡œìš´ ë¶„ì•¼ì˜ í•™ìŠµ ê³¼ì •ì— ì ìš©í•˜ëŠ” ê²ƒì„ ë§í•©ë‹ˆë‹¤[14]. ì¼ë°˜ì ìœ¼ë¡œ ë„ë©”ì¸ê³¼ íƒœìŠ¤í¬ëŠ” ê³¼ì œë¥¼ ì„¤ëª…í•˜ëŠ” ë‘ ê°€ì§€ ê°œë…ì…ë‹ˆë‹¤. ë„ë©”ì¸ì€ ë°ì´í„°ì™€ ë°ì´í„° distributionë¼ëŠ” ë‘ ê°€ì§€ êµ¬ì„± ìš”ì†Œë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. ë˜í•œ íƒœìŠ¤í¬ì—ëŠ” ë ˆì´ë¸” ê³µê°„ê³¼ ì˜ˆì¸¡ í•¨ìˆ˜ì˜ ë‘ ê°€ì§€ êµ¬ì„± ìš”ì†Œê°€ ìˆìŠµë‹ˆë‹¤. í•¨ìˆ˜ëŠ” ê·¸ë¦¼ 1ê³¼ ê°™ì´ ë¨¸ì‹ ëŸ¬ë‹ì„ í†µí•´ ì–»ì–´ì§€ë©°, ì§€ì‹ì€ ì†ŒìŠ¤ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•¨ìœ¼ë¡œì¨ í•™ìŠµë©ë‹ˆë‹¤.Tansfer learning refers to apply a model learned in a certain field to a learning process in a new field utilizing the similarities of two task in data, tasks and models [14]. Usually, Domain and task are two concepts to describe the tasks. A domain consists of two components: data and data distribution. The task also has two components: label space and prediction function. The function is got through machine learning, as shown in Fig.1, the knowledge is learned through training the model using the source data. ì—¬ê¸°ì„œ ë§í•˜ëŠ” ë°ì´í„° distributionì´ë€, ë°ì´í„°ê°€ ì–´ë–»ê²Œ ë¶„í¬ë˜ì–´ìˆëŠ”ì§€ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ì˜ˆë¥¼ë“¤ì–´ ì–´ë–¤ íšŒì‚¬ì—ì„œ ê³ ê°ë“¤ì˜ êµ¬ë§¤ ê¸°ë¡ì„ ìˆ˜ì§‘í•˜ê³  ìˆë‹¤ê³  ê°€ì •í•´ ë´…ì‹œë‹¤. ì´ íšŒì‚¬ê°€ ë³´ìœ í•œ ë°ì´í„°ëŠ” ê³ ê°ì˜ êµ¬ë§¤ ë‚´ì—­, ì§€ë¶ˆ ë°©ë²•, ë°°ì†¡ì§€ ë“±ì´ ìˆì„ ê²ƒ ì…ë‹ˆë‹¤. ê³ ê°ì˜ êµ¬ë§¤ ë‚´ì—­ì€ ìƒí’ˆì˜ ê°€ê²©, ìˆ˜ëŸ‰, ì¹´í…Œê³ ë¦¬ ë“±ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆìœ¼ë©° ì´ëŸ¬í•œ ë°ì´í„°ë¥¼ ì¼ë°˜ì ìœ¼ë¡œ ì—°ì†í˜• ë³€ìˆ˜ë¡œ í‘œí˜„ë©ë‹ˆë‹¤. ë°˜ë©´ì— ì§€ë¶ˆ ë°©ë²•ì€ ì‹ ìš©ì¹´ë“œ, í˜„ê¸ˆ, ëª¨ë°”ì¼ ê²°ì œ ë“±ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆìœ¼ë©° ë²”ì£¼í˜• ë³€ìˆ˜ë¡œ í‘œí˜„ë©ë‹ˆë‹¤. ë˜í•œ ë°°ì†¡ì§€ëŠ” ì§€ì—­ë³„ë¡œ ë‹¤ë¥¸ ë¶„í¬ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆìœ¼ë©° ì´ëŸ¬í•œ ë°ì´í„°ëŠ” ì§€ë¦¬ì ì¸ ì •ë³´ë¥¼ í¬í•¨í•˜ë¯€ë¡œ ê³µê°„ ë°ì´í„°ë¡œ ë¶„ë¥˜ê°€ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë°ì´í„°ë“¤ì€ ê°ê° ë‹¤ë¥¸ íŠ¹ì„±ê³¼ ë¶„í¬ë¥¼ ê°€ì§€ê³  ìˆìœ¼ë©° ì´ëŸ¬í•œ íŠ¹ì„±ê³¼ ë¶„í¬ë¥¼ íŒŒì•…í•˜ì—¬ ì ì ˆí•œ ëª¨ë¸ë§Œë“¤ì–´ì•¼ í•©ë‹ˆë‹¤. Domainì€ ë°ì´í„°ê°€ ìˆ˜ì§‘ëœ í™˜ê²½ì´ë‚˜ ìƒí™©ì„ ì˜ë¯¸í•˜ë©°, taskëŠ” ë°ì´í„°ë¥¼ ì´ìš©í•˜ì—¬ ìˆ˜í–‰í•˜ê³ ì í•˜ëŠ” ì‘ì—…ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ììœ¨ì£¼í–‰ ìë™ì°¨ì˜ ê²½ìš° ë„ë¡œ ì£¼í–‰(domain)ê³¼ ì¥ì• ë¬¼ íšŒí”¼(task)ì™€ ê°™ì€ ë‹¤ì–‘í•œ ì‘ì—…ì´ ìˆìŠµë‹ˆë‹¤. ëª©í‘œ ê³¼ì œì— ë ˆì´ë¸”ì´ ì§€ì •ëœ ë°ì´í„°ì¸ì§€ ì—¬ë¶€ì— ë”°ë¼ ì „ì´ í•™ìŠµì€ ê·€ë‚©ì  ì „ì´ í•™ìŠµ, ì „ì´ì  ì „ì´ í•™ìŠµ ë° ë¹„ì§€ë„ ì „ì´ í•™ìŠµì˜ ì„¸ ê°€ì§€ ë²”ì£¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆìŠµë‹ˆë‹¤.According to whether the data labeled in the target tasks, transfer learning can divided into the following three categories: inductive transfer learning, transductive transfer learning and unsupervised transfer learning. Source data: ì „ì´ í•™ìŠµì—ì„œ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ì´ˆê¸° ë°ì´í„° ì„¸íŠ¸ì…ë‹ˆë‹¤. ì´ ë°ì´í„° ì„¸íŠ¸ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ëŒ€ê·œëª¨ ë°ì´í„° ì„¸íŠ¸ì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤. Source domain: ì†ŒìŠ¤ ë°ì´í„°ê°€ ê°€ì ¸ì˜¨ ë„ë©”ì¸ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì†ŒìŠ¤ ë°ì´í„°ê°€ ìì—°ì–´ ì²˜ë¦¬(NLP) ë¶„ì•¼ì˜ í…ìŠ¤íŠ¸ ë°ì´í„°ë¼ë©´, ì†ŒìŠ¤ ë„ë©”ì¸ì€ NLP ë¶„ì•¼ì…ë‹ˆë‹¤. Source task: ì†ŒìŠ¤ ë°ì´í„°ë¡œ ìˆ˜í–‰í•˜ëŠ” ì‘ì—…ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì†ŒìŠ¤ ë°ì´í„°ê°€ ê°ì • ë¶„ì„ ì‘ì—…ì— ì‚¬ìš©ëœë‹¤ë©´, ì†ŒìŠ¤ ì‘ì—…ì€ ê°ì • ë¶„ì„ì…ë‹ˆë‹¤. Target data: ì „ì´ í•™ìŠµì—ì„œ ìƒˆë¡œìš´ ì‘ì—…ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë˜ëŠ” ì¶”ê°€ì ì¸ ë°ì´í„° ì„¸íŠ¸ì…ë‹ˆë‹¤. Target domain: íƒ€ê²Ÿ ë°ì´í„°ê°€ ê°€ì ¸ì˜¨ ë„ë©”ì¸ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, íƒ€ê²Ÿ ë°ì´í„°ê°€ ì˜ë£Œ ë¶„ì•¼ì˜ ì´ë¯¸ì§€ë¼ë©´, íƒ€ê²Ÿ ë„ë©”ì¸ì€ ì˜ë£Œ ì´ë¯¸ì§€ ë¶„ì•¼ì…ë‹ˆë‹¤. Target task: ì „ì´ í•™ìŠµì—ì„œ ìˆ˜í–‰í•˜ë ¤ëŠ” ìƒˆë¡œìš´ ì‘ì—…ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, íƒ€ê²Ÿ ì‘ì—…ì´ ì˜ë£Œ ì´ë¯¸ì§€ì—ì„œ ì§ˆë³‘ì„ ê°ì§€í•˜ëŠ” ê²ƒì´ë¼ë©´, íƒ€ê²Ÿ ì‘ì—…ì€ ì§ˆë³‘ ê°ì§€ì…ë‹ˆë‹¤. target data, target domain, target taskëŠ” ê°ê° ë‹¤ë¥¸ ê°œë…ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì†ŒìŠ¤ ë„ë©”ì¸ì€ ìŒì•… ë°ì´í„°ì´ê³  ì†ŒìŠ¤ ì‘ì—…ì€ ìŒì•… ì¥ë¥´ ë¶„ë¥˜ì…ë‹ˆë‹¤. ì´ ê²½ìš°, íƒ€ê²Ÿ ë„ë©”ì¸ì€ ì˜í™” ë°ì´í„°ì´ê³  íƒ€ê²Ÿ ì‘ì—…ì€ ì˜í™” ì¥ë¥´ ë¶„ë¥˜ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ íƒ€ê²Ÿ ë°ì´í„°ëŠ” ì˜í™” ë°ì´í„° ì„¸íŠ¸ ìì²´ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ì „ì´ í•™ìŠµì—ì„œëŠ” ì†ŒìŠ¤ ë„ë©”ì¸ê³¼ ì‘ì—…ì—ì„œ í•™ìŠµí•œ ì§€ì‹ì„ ì‚¬ìš©í•˜ì—¬ íƒ€ê²Ÿ ë„ë©”ì¸ê³¼ ì‘ì—…ì— ëŒ€í•œ ëª¨ë¸ì„ êµ¬ì¶•í•˜ê³  ì¡°ì •í•©ë‹ˆë‹¤. ì¦‰, ì „ì´ í•™ìŠµì—ì„œ â€œtarget dataâ€ëŠ” ìƒˆë¡œìš´ ë°ì´í„° ì„¸íŠ¸ë¥¼ ì˜ë¯¸í•˜ë©° â€œtarget domainâ€ì€ í•´ë‹¹ ë°ì´í„°ê°€ ì†í•œ ë¶„ì•¼ ë˜ëŠ” ë„ë©”ì¸ì„ ë‚˜íƒ€ë‚´ë©° â€œtarget taskâ€ëŠ” ìƒˆë¡œìš´ ì‘ì—… ë˜ëŠ” ë¬¸ì œë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ë”°ë¼ì„œ, ì „ì´ í•™ìŠµì—ì„œëŠ” ì†ŒìŠ¤ ë°ì´í„°ì™€ ì†ŒìŠ¤ ë„ë©”ì¸ì—ì„œ ëª¨ë¸ì„ í•™ìŠµí•œ í›„, íƒ€ê²Ÿ ë°ì´í„°ì™€ íƒ€ê²Ÿ ë„ë©”ì¸ì—ì„œ ëª¨ë¸ì„ ì¡°ì •í•˜ì—¬ ìƒˆë¡œìš´ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ finetune ë°©ë²•ê³¼ ê°™ì€ ê¸°ìˆ ì´ ì‚¬ìš©ë©ë‹ˆë‹¤. ì„¸ ì „ì´ í•™ìŠµ ëª¨ë‘ source domainì—ì„œ í•™ìŠµí•œ ëª¨ë¸ì„ target domainìœ¼ë¡œ ì „ì´ì‹œì¼œ ìƒˆë¡œìš´ taskë¥¼ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. Label spaceëŠ” ëª¨ë¸ì´ ì˜ˆì¸¡í•˜ë ¤ëŠ” ê²°ê³¼ê°’ì˜ ì§‘í•©ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì´ë©”ì¼ ë¶„ë¥˜ ëª¨ë¸ì—ì„œ label spaceëŠ” â€œìŠ¤íŒ¸â€ê³¼ â€œìŠ¤íŒ¸ì´ ì•„ë‹˜â€ê³¼ ê°™ì€ ë‘ ê°€ì§€ ê°’ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. Inductive transfer learningì€ source domainê³¼ target domainì´ ì„œë¡œ ë‹¤ë¥¸ label spaceë¥¼ ê°€ì§€ê³  ìˆì„ ë•Œ ì‚¬ìš©ë©ë‹ˆë‹¤. Inductive transfer learningì€ target domainì—ì„œ labeled dataê°€ ì ê±°ë‚˜ ì—†ëŠ” ê²½ìš°ì— ìœ ìš©í•©ë‹ˆë‹¤. Transductive transfer learningì€ source domainê³¼ target domainì´ ë™ì¼í•œ label spaceë¥¼ ê°€ì§€ê³  ìˆì„ ë•Œ ì‚¬ìš©ë©ë‹ˆë‹¤. Transductive transfer learningì€ target domainì—ì„œ labeled dataê°€ ìˆëŠ” ê²½ìš°ì— ìœ ìš©í•©ë‹ˆë‹¤. Unsupervised transfer learningì€ source domainê³¼ target domainì´ ë™ì¼í•œ label spaceë¥¼ ê°€ì§€ê³  ìˆì§€ ì•Šì„ ë•Œ ì‚¬ìš©ë©ë‹ˆë‹¤. Unsupervised transfer learningì€ target domainì—ì„œ labeled dataê°€ ì—†ëŠ” ê²½ìš°ì— ìœ ìš©í•©ë‹ˆë‹¤. Transfer approachì— ë”°ë¼ ë„¤ê°€ì§€ ìœ í˜•ì˜ ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤. instance-transfer learning: source domainê³¼ target domainì´ ë™ì¼í•œ feature spaceë¥¼ ê°€ì§€ê³  ìˆì§€ë§Œ, ë°ì´í„° ë¶„í¬ê°€ ë‹¤ë¥¸ ê²½ìš°ì— ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ source domainì—ì„œ í•™ìŠµí•œ ëª¨ë¸ì„ target domainìœ¼ë¡œ ì „ì´ì‹œì¼œ ìƒˆë¡œìš´ taskë¥¼ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ë°ì´í„° ë¶„í¬ê°€ ë‹¤ë¥¸ ê²½ìš°ì—ë„ ì˜ ì‘ë™í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. The data in the train set will have different weight to show the similarity between source task and target task. train setì˜ ë°ì´í„°ëŠ” source taskì™€ target task ê°„ì˜ ìœ ì‚¬ì„±ì„ ë‚˜íƒ€ë‚´ê¸° ìœ„í•´ ì„œë¡œ ë‹¤ë¥¸ ê°€ì¤‘ì¹˜ë¥¼ ê°€ì§‘ë‹ˆë‹¤. ì´ëŠ” source domainê³¼ target domainì´ ë™ì¼í•œ feature spaceë¥¼ ê°€ì§€ê³  ìˆì§€ë§Œ, ë°ì´í„° ë¶„í¬ê°€ ë‹¤ë¥¸ ê²½ìš°ì— ì‚¬ìš©ë˜ë©°, ì´ëŸ¬í•œ ê°€ì¤‘ì¹˜ëŠ” ë°ì´í„° ë¶„í¬ì˜ ì°¨ì´ë¥¼ ë³´ìƒí•˜ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤. feature-representaion transfer learning: source domainê³¼ target domainì´ ë‹¤ë¥¸ feature spaceë¥¼ ê°€ì§€ê³  ìˆì„ ë•Œ ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ë‘ ë„ë©”ì¸ ê°„ì˜ ê³µí†µëœ featureë¥¼ ì°¾ì•„ë‚´ê³ , ì´ë¥¼ ì´ìš©í•˜ì—¬ ë°ì´í„° ë¶„í¬ì˜ ì°¨ì´ë¥¼ ì¤„ì´ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ë‘ ë„ë©”ì¸ ê°„ì˜ ìœ ì‚¬ì„±ì„ ì°¾ì•„ë‚´ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. The feature space of source task and target task are different, both the feature space of the source and the target task will be transformed to a shared subspace. ë§Œì•½ source taskì™€ target taskì˜ feature spaceê°€ ì„œë¡œ ë‹¤ë¥´ë‹¤ë©´, ë‘ feature spaceë¥¼ ê³µìœ í•˜ëŠ” subspaceë¡œ ë³€í™˜ì‹œì¼œì•¼ í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ feature-representation transfer learningì„ ì‚¬ìš©í•©ë‹ˆë‹¤. parameter-transfer learning: ëª¨ë¸ ê¸°ë°˜ì˜ transfer learning ë°©ë²• ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ë¨¼ì € source taskì—ì„œ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê³ , ê·¸ ë‹¤ìŒ target taskì—ì„œ ëª¨ë¸ì„ fine-tuningí•˜ì—¬ target taskì— ì ìš©í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ source taskì™€ target taskëŠ” ì„œë¡œ ë‹¤ë¥¸ ë°ì´í„° ë¶„í¬ë¥¼ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸ì—, fine-tuningì´ í•„ìš”í•©ë‹ˆë‹¤. ë”°ë¼ì„œ parameter-transfer learningì€ source taskì™€ target task ê°„ì˜ ìœ ì‚¬ì„±ì„ ì°¾ì•„ë‚´ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. Firstly, the model is trained in the source task, and then it can be used in the target task, because of the diffference of the two task, the target task will need finetune. relational-knowledge transfer learning: source taskì™€ target task ê°„ì˜ ê´€ê³„ê°€ ìœ ì‚¬í•œ ê²½ìš°, source taskì—ì„œ í•™ìŠµí•œ ê´€ê³„ë¥¼ target taskë¡œ ì „ë‹¬í•  ìˆ˜ ìˆëŠ” transfer learning ë°©ë²•ì…ë‹ˆë‹¤. If there is a relation in the source task is similar to the relation in the target task, then we can transfer the relation from source to target. ëª¨ë“  transfer learning ë°©ë²•ì€ source taskì™€ target task ê°„ì˜ ìœ ì‚¬ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ë©°, ì´ ìœ ì‚¬ì„±ì€ feature, parameter, relationship ë“± ì—¬ëŸ¬ ê°€ì§€ ìš”ì†Œë“¤ì—ì„œ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ transfer learningì˜ í•µì‹¬ì€ ë‘ task ê°„ì˜ ìœ ì‚¬ì„±ì„ ì°¾ì•„ë‚´ëŠ” ê²ƒì…ë‹ˆë‹¤.In summary, each kind of transfer method is on the basic condition, which is the similarity between source and destination task, whether they are features, parameters, or relationships. Therefore, the key to transfer learning is to find the similarities between tasks. ìœ ì‚¬ì„±ì´ ì—†ëŠ” ê²½ìš°ì—ë„ transfer learningì„ í•  ìˆ˜ ìˆì§€ë§Œ, ì´ ê²½ìš°ì—ëŠ” transfer learningì˜ ì„±ëŠ¥ì´ ì €í•˜ë  ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ transfer learningì„ ì ìš©í•˜ê¸° ì „ì— ë‘ task ê°„ì˜ ìœ ì‚¬ì„±ì„ ë¶„ì„í•˜ê³ , ê°€ëŠ¥í•œí•œ ìœ ì‚¬í•œ taskë¥¼ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. transfer learningê³¼ ê´€ë ¨í•˜ì—¬ ì–¸ê¸‰í•´ì•¼ í•  ë‘ê°€ì§€ í•µì‹¬ ì´ìŠˆëŠ” domain adaptionê³¼ task relateness ì…ë‹ˆë‹¤. There are two key issues to mention about transfer learning: domain adaption and task relateness. Domain adaptation transfer learningì˜ í•œ ë¶„ì•¼ë¡œ, source domainê³¼ target domain ê°„ì˜ ì°¨ì´ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•œ ë°©ë²•ì…ë‹ˆë‹¤. ì´ ë°©ë²•ì€ source domainì—ì„œ í•™ìŠµí•œ ëª¨ë¸ì„ target domainì—ì„œ ì ìš©í•  ë•Œ ë°œìƒí•˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤. ì£¼ë¡œ feature spaceì™€ class space ê°„ì˜ ì°¨ì´ë¥¼ ì¤„ì´ëŠ” ê²ƒì´ ëª©ì ì…ë‹ˆë‹¤. transfer learningì—ì„œ domain adaptationì´ í•„ìš”í•œ ì´ìœ : source domainì—ì„œëŠ” ì´ë¯¸ labelì´ ìˆëŠ” ë°ì´í„°ê°€ ì¡´ì¬í•˜ì§€ë§Œ, target domainì—ì„œëŠ” labelì´ ì—†ëŠ” ë°ì´í„°ë§Œ ì¡´ì¬í•˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. ì´ ë•Œ, transfer learningì„ ì‚¬ìš©í•˜ì—¬ source domainì˜ ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê³ , ì´ ëª¨ë¸ì„ ì´ìš©í•˜ì—¬ target domainì˜ ì…ë ¥ ë°ì´í„°ì˜ ê°ì • ì¹´í…Œê³ ë¦¬ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ ëª©ì ì…ë‹ˆë‹¤. In the source domain the data have label, but the data in the target domain do not have label. Here the objective of transfer learning is to use the data from the source domain to train the model and then use the model to predict the emotion category of the input target data. Task relateness transfer learningì—ì„œ ê³ ë ¤í•´ì•¼ í•  ë˜ ë‹¤ë¥¸ ì¤‘ìš”í•œ ìš”ì†Œì…ë‹ˆë‹¤. ì´ëŠ” source taskì™€ target task ê°„ì˜ ìœ ì‚¬ì„±ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ë‘ taskê°€ ìœ ì‚¬í• ìˆ˜ë¡ transfer learningì˜ ì„±ëŠ¥ì´ í–¥ìƒë©ë‹ˆë‹¤. ì´ëŸ¬í•œ ìœ ì‚¬ì„±ì€ feature, parameter, relationship ë“± ì—¬ëŸ¬ ê°€ì§€ ì¸¡ë©´ì—ì„œ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ transfer learningì„ ì ìš©í•˜ê¸° ì „ì— ë‘ task ê°„ì˜ ìœ ì‚¬ì„±ì„ ë¶„ì„í•˜ê³ , ê°€ëŠ¥í•œí•œ ìœ ì‚¬í•œ taskë¥¼ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. ì „ì´ í•™ìŠµì—ì„œ ì‘ì—… ê°„ì˜ ìœ ì‚¬ì„±ì€ ì„±ê³µì ì¸ ì „ì´ë¥¼ ìœ„í•œ ê¸°ì´ˆì´ë©°, ì´ ìœ ì‚¬ì„±ì„ ì°¾ì•„ ì˜¬ë°”ë¥´ê²Œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì€ í•µì‹¬ì ì¸ ë¬¸ì œì…ë‹ˆë‹¤. In transfer learning, the similarity between tasks is the basis for successful transfer and how to find and correctly use this similarity is a key issue. ìŒì„± ê°ì • ì¸ì‹ ì—°êµ¬ì—ëŠ” ì£¼ë¡œ ì–¸ì–´ ê°„, ë°ì´í„°ë² ì´ìŠ¤ ê°„, ëª¨ë‹¬ ê°„, ì• í”Œë¦¬ì¼€ì´ì…˜ ê°„ ì „ì´(ì˜ˆ: ìŒì„± ì¸ì‹ì—ì„œ ìŒì„± ê°ì • ì¸ì‹ìœ¼ë¡œì˜ ì „ì´)ê°€ ìˆìŠµë‹ˆë‹¤. í‘œ IIëŠ” ì „ì†¡ ë²”ìœ„ì™€ ë‹¤ì–‘í•œ ì „ì†¡ì— ëŒ€í•œ ê°„ëµí•œ ì„¤ëª…ì„ ì œê³µí•©ë‹ˆë‹¤.In the research of speech emotion recognition, there are mainly cross-language, cross-database, cross-modal, cross-application transfer (for example, transfer from speech recognition to speech emotion recognition). Table II give the transfer scope and the brief description about the different transfer. III. Transfer Learning Methods For Speech Emotion Recognition ìŒì„± ê°ì •ì¸ì‹ì˜ íŠ¹ì„±ì— ë”°ë¼ ì´ ë¶„ì•¼ì—ì„œ ì „ì´í•™ìŠµì˜ ì ìš©ì€ ë‘ê°€ì§€ ë²”ì£¼ë¡œ ë‚˜ë‰©ë‹ˆë‹¤.Based on the characteristics of speech emotion recognition, the application of transfer learning in this field mainly has the following two categories A. Feature based Transfer Learning Feature based transfer learningì€ source domainê³¼ target domainì´ ë™ì¼í•œ featureë¥¼ ê³µìœ í•œë‹¤ëŠ” ê°€ì •ì— ê¸°ë°˜í•˜ì—¬, ë‘ domain ê°„ì˜ ì°¨ì´ë¥¼ ì¤„ì´ê¸° ìœ„í•´ feature transformationì„ í†µí•´ ì„œë¡œ ì „ë‹¬í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.Based on the hypothesis that the source and target domain share the same features, we can use feature based transfer learning. Feature based transfer learning refers to that transfer to each other through feature transformation, to reduce the gap between the source and target domains. ë˜ ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œëŠ” source domainê³¼ target domainì˜ ë°ì´í„°ë¥¼ í†µí•©ëœ feature spaceë¡œ ë³€í™˜í•œ í›„ ì „í†µì ì¸ ê¸°ê³„ í•™ìŠµ ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ ë¶„ë¥˜í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.Other method is to transform the data both in source and target domains into a unified feature space and then use traditional machine learning methods for classification [25]. SiddiqueëŠ” ì—°êµ¬ì—ì„œ í™”ìì˜ ì–¼êµ´ê³¼ ìŒì„± ê°„ì˜ ê´€ê³„ë¥¼ í™œìš©í•˜ì—¬, ì‹œê°ì ì¸ ì˜ì—­(faces)ì—ì„œ í‘œí˜„ì˜ ì£¼ì„(annotation)ì„ ìŒì„± ì˜ì—­ìœ¼ë¡œ ì „ë‹¬í•˜ì—¬, labelì´ ì—†ëŠ” ì˜¤ë””ì˜¤ ë°ì´í„° ë¬¸ì œë¥¼ í•´ê²°í•˜ì˜€ìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œ transfer learningì„ ì ìš©í•¨ìœ¼ë¡œì¨, labelì´ ì—†ëŠ” ë°ì´í„°ì—ì„œë„ ìœ ìš©í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. DengëŠ” autoencoderë¥¼ ì‚¬ìš©í•˜ì—¬ source domainê³¼ target domain ê°„ì— featureë¥¼ ì „ë‹¬í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•˜ì˜€ìŠµë‹ˆë‹¤. AutoencoderëŠ” ì…ë ¥ ë°ì´í„°ë¥¼ ì••ì¶•í•˜ê³  ì¬ìƒì„±í•˜ëŠ” ì¸ê³µ ì‹ ê²½ë§ êµ¬ì¡°ë¡œ, ì´ë¥¼ í†µí•´ source domainì˜ featureë“¤ì„ ì••ì¶•í•˜ê³  target domainìœ¼ë¡œ ì „ë‹¬í•˜ì—¬, ë‘ ë„ë©”ì¸ ê°„ì˜ ì°¨ì´ë¥¼ ì¤„ì¼ ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤. AutoencoderëŠ” ì…ë ¥ ë°ì´í„°ë¥¼ ì••ì¶•í•˜ê³  ì¬ìƒì„±í•˜ëŠ” ì¸ê³µ ì‹ ê²½ë§ êµ¬ì¡°ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ source domainì˜ featureë“¤ì„ ì••ì¶•í•˜ê³  target domainìœ¼ë¡œ ì „ë‹¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. AutoencoderëŠ” ë‘ ê°œì˜ ì£¼ìš” êµ¬ì„± ìš”ì†Œë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤. í•˜ë‚˜ëŠ” ì¸ì½”ë”(encoder)ì´ê³ , ë‹¤ë¥¸ í•˜ë‚˜ëŠ” ë””ì½”ë”(decoder)ì…ë‹ˆë‹¤. ì¸ì½”ë”ëŠ” ì…ë ¥ ë°ì´í„°ë¥¼ ì €ì°¨ì›ì˜ latent spaceë¡œ ì••ì¶•í•˜ê³ , ë””ì½”ë”ëŠ” latent spaceì—ì„œ ì›ë˜ì˜ ì…ë ¥ ë°ì´í„°ë¥¼ ì¬ìƒì„±í•©ë‹ˆë‹¤. ë”°ë¼ì„œ, transfer learningì—ì„œ autoencoderë¥¼ ì‚¬ìš©í•˜ì—¬ feature transferë¥¼ ìˆ˜í–‰í•  ë•Œ, ë¨¼ì € source domainì˜ ë°ì´í„°ë¥¼ ì¸ì½”ë”ì— ì…ë ¥í•˜ì—¬ latent spaceë¡œ ì••ì¶•í•©ë‹ˆë‹¤. ê·¸ë¦¬ê³  ì´ë ‡ê²Œ ì••ì¶•ëœ featureë“¤ì„ ë””ì½”ë”ë¥¼ í†µí•´ target domainìœ¼ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ ì „ë‹¬ëœ featureë“¤ì€ target domainì—ì„œ ìƒˆë¡œìš´ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ë° ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¸ì½”ë”ì—ì„œ ì••ì¶•ë˜ê¸° ì „ì˜ featureì™€ ë””ì½”ë”ì—ì„œ ì••ì¶•ë˜ì–´ ë‚˜ì˜¨ featureëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë‹¤ë¦…ë‹ˆë‹¤. ì¸ì½”ë”ëŠ” ì…ë ¥ ë°ì´í„°ë¥¼ ì €ì°¨ì›ì˜ latent spaceë¡œ ì••ì¶•í•˜ê³ , ë””ì½”ë”ëŠ” ì´ latent spaceì—ì„œ ì›ë˜ì˜ ì…ë ¥ ë°ì´í„°ë¥¼ ì¬êµ¬ì„±í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì—ì„œ, ì¸ì½”ë”ëŠ” ì…ë ¥ ë°ì´í„°ì˜ ì¤‘ìš”í•œ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ê³ , ë””ì½”ë”ëŠ” ì´ëŸ¬í•œ íŠ¹ì§•ì„ ì‚¬ìš©í•˜ì—¬ ì…ë ¥ ë°ì´í„°ë¥¼ ì¬êµ¬ì„±í•©ë‹ˆë‹¤. ë”°ë¼ì„œ, ì¸ì½”ë”ì—ì„œ ì¶”ì¶œëœ featureì™€ ë””ì½”ë”ì—ì„œ ì¬êµ¬ì„±ëœ featureëŠ” ì„œë¡œ ë‹¤ë¥¸ í˜•íƒœë¥¼ ê°€ì§€ë©°, ì¼ë°˜ì ìœ¼ë¡œ ì°¨ì›ì´ ë‹¤ë¦…ë‹ˆë‹¤. B. Model&#x2F;Parameter based Transfer Learning Model based transfer learningì€ source domainê³¼ target domainì´ ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ ê³µìœ í•  ìˆ˜ ìˆë‹¤ëŠ” ì•„ì´ë””ì–´ì— ê¸°ë°˜í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ì£¼ë¡œ ë§ì€ hidden layerë¥¼ ê°€ì§„ ë”¥ëŸ¬ë‹ ëª¨ë¸ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ ë°©ë²•ì„ ì‚¬ìš©í•˜ë©´, source domainì—ì„œ í•™ìŠµëœ ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ target domainì—ì„œ ìƒˆë¡œìš´ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ë° ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.The main idea of model based transfer learning is that the source and target domain can share the model parameters. This method is mainly used in deep learning model, which has many hidden layers [28]. ZhaoëŠ” ì—°ë ¹ ë° ì„±ë³„ ë¶„ë¥˜ ëª¨ë¸ì„ ê°ì • ì¸ì‹ ì‘ì—…ì— ì „ì´í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí–ˆìŠµë‹ˆë‹¤. ì´ ë°©ë²•ì€ model based transfer learningì˜ í•œ ì˜ˆì…ë‹ˆë‹¤.Zhao proposed transfer age and gender classification model to the emotion recogntion task [11]. ZhaoëŠ” ì—°ë ¹ ë° ì„±ë³„ ë¶„ë¥˜ ëª¨ë¸ì„ ê°ì • ì¸ì‹ ì‘ì—…ì— ì „ì´í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí–ˆìŠµë‹ˆë‹¤. ì´ ë°©ë²•ì€ transfer learningì˜ í•œ ì˜ˆì…ë‹ˆë‹¤. ì´ ë…¼ë¬¸ì—ì„œëŠ” hierarchical deep learningì„ ì‚¬ìš©í•˜ì—¬ ëŒ€ê·œëª¨ ìŒì„± ë°ì´í„°ì—ì„œ ì—°ë ¹, ì„±ë³„ ë° ê°ì • ì¹´í…Œê³ ë¦¬ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ë¨¼ì € ì—°ë ¹ê³¼ ì„±ë³„ ì†ì„±ì„ ì¶”ì¶œí•˜ê³ , ê·¸ ë‹¤ìŒì— ê°ì • ì¹´í…Œê³ ë¦¬ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤. ì—°ë ¹ê³¼ ì„±ë³„ì€ ê°ì • ì¹´í…Œê³ ë¦¬ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°ì— ì¤‘ìš”í•œ íŠ¹ì§•ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì—°ë ¹ì´ ì–´ë¦° ì‚¬ëŒì¼ìˆ˜ë¡ í–‰ë³µí•œ ê°ì •ì„ ë” ë§ì´ ë‚˜íƒ€ë‚´ëŠ” ê²½í–¥ì´ ìˆê³ , ì—¬ì„±ì¼ìˆ˜ë¡ ë‚¨ì„±ë³´ë‹¤ ìŠ¬í”ˆ ê°ì •ì„ ë” ë§ì´ ë‚˜íƒ€ë‚´ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì—°ë ¹ê³¼ ì„±ë³„ë§Œìœ¼ë¡œëŠ” ê°ì • ì¹´í…Œê³ ë¦¬ë¥¼ ì™„ë²½í•˜ê²Œ ì˜ˆì¸¡í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì´ ë…¼ë¬¸ì—ì„œëŠ” hierarchical deep learning ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì…ë ¥ ë°ì´í„°ì—ì„œ ë‹¤ì–‘í•œ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ê³ , ì´ë¥¼ í™œìš©í•˜ì—¬ ë³´ë‹¤ ì •í™•í•œ ê°ì • ì¹´í…Œê³ ë¦¬ë¥¼ ì˜ˆì¸¡í•˜ì˜€ìŠµë‹ˆë‹¤. ì´ ë…¼ë¬¸ì—ì„œëŠ” transfer learningì˜ ê°œë…ì„ í™œìš©í•˜ì—¬, ì´ë¯¸ ì—°ë ¹ ë° ì„±ë³„ ë¶„ë¥˜ ì‘ì—…ì—ì„œ í•™ìŠµëœ ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ í™œìš©í•˜ì—¬ ê°ì • ì¸ì‹ ì‘ì—…ì— ì ìš©í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´, ì ì€ ì–‘ì˜ labeled dataë¡œë„ íš¨ê³¼ì ì¸ ê°ì • ì¸ì‹ ëª¨ë¸ì„ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. IV. Deep Transfer Learning For Speech Emotion Recognition ë”¥ëŸ¬ë‹ê³¼ ì „ì´í•™ìŠµì„ ê²°í•©í•˜ë©´ ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ë”¥ëŸ¬ë‹ì—ëŠ” ë‘ ê°€ì§€ ë°©ë²•ì´ ì‚¬ìš©ë©ë‹ˆë‹¤. í•˜ë‚˜ëŠ” Multi-task learningì´ê³ , ë‹¤ë¥¸ í•˜ë‚˜ëŠ” finetune ì…ë‹ˆë‹¤.The combination of deep learning and transfer learning can achieve better results. Usually there are two methods used in deep learning. One is multi-task learning and the other is finetune. A. Multi-Task Learning Multi-task learningê³¼ Transfer learningì˜ ì°¨ì´ì ê³¼ ê³µí†µì  Multi-task learningì€ ì—¬ëŸ¬ ê°œì˜ ê´€ë ¨ëœ ì‘ì—…ì„ í•¨ê»˜ í•™ìŠµí•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•˜ë©°, Transfer learningì€ knowledgeë¥¼ source domainì—ì„œ target domainìœ¼ë¡œ ì „ë‹¬í•˜ëŠ” ê³¼ì •ì— ì¤‘ì ì„ ë‘¡ë‹ˆë‹¤. Multi-task learningì€ Transfer learningì˜ í•œ ìœ í˜•ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Transfer learningì˜ í•µì‹¬ ë¬¸ì œëŠ” ë‘ ì‘ì—… ê°„ì˜ ìœ ì‚¬ì„±ì„ ì°¾ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ë¥¼ ì°¾ì§€ ëª»í•˜ë©´ í•™ìŠµ ê³¼ì •ì—ì„œ ë¶€ì •ì ì¸ ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°˜ë©´, Multi-task learningì˜ í•µì‹¬ ë¬¸ì œëŠ” related tasksë¥¼ ì°¾ëŠ” ê²ƒì…ë‹ˆë‹¤. ë”°ë¼ì„œ, ë‘ ê°€ì§€ ë°©ë²• ëª¨ë‘ ì‘ì—… ê°„ì˜ ìœ ì‚¬ì„±ì„ ì°¾ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. B. Finetuning ë¨¼ì € ì†ŒìŠ¤ ë°ì´í„°ì— ëŒ€í•´ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¨ ë‹¤ìŒ, íƒ€ê²Ÿ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ì¡°ì •í•˜ì—¬ íƒ€ê²Ÿ ì‘ì—…ì— ì ì‘ì‹œí‚¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì†ŒìŠ¤ì™€ íƒ€ê²Ÿ ì‘ì—…ì€ ìƒí˜¸ ì—°ê´€ì„±ì´ ìˆì§€ë§Œ, ì¼ë°˜ì ìœ¼ë¡œ ì‘ì—…ì˜ ë°ì´í„° ë¶„í¬ê°€ ë™ì¼í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ëª¨ë¸ì€ í•™ìŠµëœ ì†ŒìŠ¤ ëª¨ë¸ì— ë”°ë¼ ì¡°ì •ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. finetune ë°©ë²•ê³¼ ì „ì´ í•™ìŠµì˜ ì¥ì  Finetune ë°©ë²•ì€ ì†ŒìŠ¤ ë°ì´í„°ë¡œë¶€í„° ëª¨ë¸ì„ í•™ìŠµí•œ í›„, íƒ€ê²Ÿ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ì¡°ì •í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ì†ŒìŠ¤ ì‘ì—…ê³¼ íƒ€ê²Ÿ ì‘ì—… ê°„ì˜ ì°¨ì´ë¥¼ ê·¹ë³µí•  ìˆ˜ ìˆìœ¼ë©°, ë”¥ ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬ëŠ” ë¬´ì‘ìœ„ ì´ˆê¸°í™” ê°€ì¤‘ì¹˜ë³´ë‹¤ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ë°©ë²•ì€ í›ˆë ¨ ì‹œê°„ì„ ì ˆì•½í•˜ë©´ì„œë„ ëª¨ë¸ì˜ ì¼ë°˜ì„±ê³¼ ê²¬ê³ ì„±ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, ì‚¬ì „ í›ˆë ¨(pre-train)ì´ ì¼ë°˜ì ìœ¼ë¡œ ëŒ€ê·œëª¨ ë°ì´í„° ì„¸íŠ¸ì—ì„œ ìˆ˜í–‰ë©ë‹ˆë‹¤. ì´ë ‡ê²Œ í•¨ìœ¼ë¡œì¨ í›ˆë ¨ ë°ì´í„°ë¥¼ í™•ì¥í•˜ì—¬ ëª¨ë¸ì˜ ì¼ë°˜ì„±ê³¼ ê²¬ê³ ì„±ì„ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. V. Conclusionsì¸ê³µ ì§€ëŠ¥, ì‚¬ë¬¼ ì¸í„°ë„· ë° Fog ì»´í“¨íŒ…ì˜ ê¸‰ì†í•œ ë°œì „ìœ¼ë¡œ ì¸í•´ ì—°êµ¬ìë“¤ì˜ ê´€ì‹¬ì´ ë†’ì•„ì§€ê³  ìˆë‹¤ëŠ” ê²ƒì„ ì–¸ê¸‰í•˜ë©°, ì „ì´ í•™ìŠµ(transfer learning)ì´ ê¸°ê³„ í•™ìŠµ(machine learning)ì˜ ì¤‘ìš”í•œ ì—°êµ¬ ë°©í–¥ ì¤‘ í•˜ë‚˜ì„ì„ ê°•ì¡°í•©ë‹ˆë‹¤. ì´ì— ë”°ë¼, ì´ ë…¼ë¬¸ì—ì„œëŠ” ì „ì´ í•™ìŠµì˜ ê¸°ë³¸ ì§€ì‹, ë²”ì£¼ ë° ê¸°ë³¸ì ì¸ ë°©ë²•ì„ ì¡°ì‚¬í•˜ê³ , ìŒì„± ê°ì • ì¸ì‹ì— ëŒ€í•œ ì „ì´ í•™ìŠµ ì‘ìš©ì„ ì—°êµ¬í•©ë‹ˆë‹¤. ì´ ì‘ìš©ì—ì„œëŠ” ì‹¤ì œ ì‘ì—…ê³¼ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ë¶€ì •ì ì¸ ì „ì´ë¥¼ í”¼í•˜ê¸° ìœ„í•´ ì£¼ì˜í•´ì•¼ í•˜ë©°, ì „ì´ í•™ìŠµì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ë‹¤ì¤‘ ëª¨ë¸ ê°ì • ì¸ì‹ ë° ë„ë©”ì¸ ì ì‘(domain adaption)ì€ ì¤‘ìš”í•œ ì£¼ì œì…ë‹ˆë‹¤.","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speech Emotion Recognition","slug":"Paper/Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/categories/Paper/Speech-Emotion-Recognition/"}],"tags":[{"name":"Speech Emotion Recognition","slug":"Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/tags/Speech-Emotion-Recognition/"},{"name":"Transfer Learning","slug":"Transfer-Learning","permalink":"https://jmj3047.github.io/tags/Transfer-Learning/"},{"name":"Domain Adaptation","slug":"Domain-Adaptation","permalink":"https://jmj3047.github.io/tags/Domain-Adaptation/"},{"name":"Cross-domain","slug":"Cross-domain","permalink":"https://jmj3047.github.io/tags/Cross-domain/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speech Emotion Recognition","slug":"Paper/Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/categories/Paper/Speech-Emotion-Recognition/"}]},{"title":"Convolutional Neural Networks","slug":"DL_Part4","date":"2023-05-07T15:00:00.000Z","updated":"2023-06-22T15:34:36.923Z","comments":true,"path":"2023/05/08/DL_Part4/","link":"","permalink":"https://jmj3047.github.io/2023/05/08/DL_Part4/","excerpt":"","text":"Course Link Lecture 4 in Deep Learning CNN By convention, in machine learning, we usually do not bother with this flipping operation. Technically this operation is maybe better called cross-correlation. But most of the deep learning literature just causes the convolution operator. CNN Case LeNet-5 AlexNet this neural network actually had a lot of similarities to LeNet, but it was much bigger. So whereas the LeNet-5 from previous slide had about 60,000 parameters, this AlexNet that had about 60 million parameters. And the fact that they could take pretty similar basic building blocks that have a lot more hidden units and training on a lot more data, they trained on the image that dataset that allowed it to have a just remarkable performance. Another aspect of this architecture that made it much better than LeNet was using the relu activation function. VGG-16 the 16 in the VGG-16 refers to the fact that this has 16 layers that have weights. And this is a pretty large network, this network has a total of about 138 million parameters. And thatâ€™s pretty large even by modern standards. ResNet Residual block: using residual blocks allows you to train much deeper neural networks. And the way you build a ResNet is by taking many of these residual blocks, blocks like these, and stacking them together to form a deep network. the theory, in theory, having a deeper network should only help. But in practice or in reality, having a plain network, so no ResNet, having a plain network that is very deep means that all your optimization algorithm just has a much harder time training. And so, in reality, your training error gets worse if you pick a network thatâ€™s too deep. But what happens with ResNet is that even as the number of layers gets deeper, you can have the performance of the training error kind of keep on going down. Even if we train a network with over a hundred layers. Network in network(1 X 1 Convolutions) one way to think about the 32 numbers you have in this one, a one by 32 filter is as if you have one neuron that is taking us input 32 numbers. Multiplying each of these 32 numbers in one slice in the same position, height, and width, but these 32 different channels, multiplying them by 32 weights. One way to think about a one-by-one convolution is that it is basically having a fully connected neural network that applies to each of the 62 different positions. shrinking channels Inception Network what the inception network does, is, more or less, put a lot of these modules together. It turns out that thereâ€™s one last detail to the inception network if we read the optional research paper. Which is that there are these additional side-branches that I just added. So what do they do? Well, the last few layers of the network is a fully connected layer followed by a softmax layer to try to make a prediction. What these side branches do is it takes some hidden layer and it tries to use that to make a prediction. So this is actually a softmax output and so is that. And this other side branch, again it is a hidden layer passes through a few layers like a few connected layers. And then has the softmax try to predict whatâ€™s the output label. And you should think of this as maybe just another detail of the inception thatâ€™s worked. But what is does is it helps to ensure that the features computed. Even in the heading units, even at intermediate layers. That theyâ€™re not too bad for protecting the output cause of a image. And this appears to have a regularizing effect on the inception network and helps prevent this network from overfitting. Depth-wise seperable convolution youâ€™ll learn about MobileNets, which is another foundational convolutional neural network architecture used for computer vision. Using MobileNets will allow you to build and deploy new networks that work even in low compute environment, such as a mobile phone. MobileNet In MobileNet v2, there are two main changes. One is the addition of a residual connection. This is just a residual connections that you learned about in the ResNet videos. This residual connection or skip connection, takes the input from the previous layer and sums it or passes it directly to the next layer, does allow ingredients to propagate backward more efficiently. The second change is that it also as an expansion layer, which you learn more about on the next slide, before the depthwise convolution, followed by the pointwise convolution, which weâ€™re going to call projection in a point-wise convolution. The block with red line is called bottleneck block why do we meet these bottleneck blocks? It turns out that the bottleneck block accomplishes two things, One, by using the expansion operation, it increases the size of the representation within the bottleneck block. This allows the neural network to learn a richer function. Thereâ€™s just more computation over here. But when deploying on a mobile device, on edge device, you will often be heavy memory constraints. The bottleneck block uses the pointwise convolution or the projection operation in order to project it back down to a smaller set of values, so that when you pass this the next block, the amount of memory needed to store these values is reduced back down. EfficientNet With MobileNet, youâ€™ve learned how to build more computationally efficient layers, and with EfficientNet, you can also find a way to scale up or down these neural networks based on the resources of a device you may be working on. Object Detection in this example the ideal bx might be about 0.5 because this is about halfway to the right to the image. by might be about 0.7 since itâ€™s about maybe 70% to the way down to the image. bh might be about 0.3 because the height of this red square is about 30% of the overall height of the image. And bw might be about 0.4 letâ€™s say because the width of the red box is about 0.4 of the overall width of the entire image. to implement sliding windows, previously, what you do is you crop out a region. Letâ€™s say this is 14 by 14 and run that through your convnet and do that for the next region over, then do that for the next 14 by 14 region, then the next one, then the next one, then the next one, then the next one and so on, until hopefully that one recognizes the car. But now, instead of doing it sequentially, with this convolutional implementation that you saw in the previous slide, you can implement the entire image, all maybe 28 by 28 and convolutionally make all the predictions at the same time by one forward pass through this big convnet and hopefully have it recognize the position of the car. So thatâ€™s how you implement sliding windows convolutionally and it makes the whole thing much more efficient. Application Face recognition One of the challenges of face recognition is that you need to solve the one-shot learning problem. What that means is that for most face recognition applications you need to be able to recognize a person given just one single image, or given just one example of that personâ€™s face. And, historically, deep learning algorithms donâ€™t work well if you have only one training example. to input two faces and tell you how similar or how different they are. A good way to do this is to use a Siamese network. this idea of running two identical, convolutional neural networks on two different inputs and then comparing them, sometimes thatâ€™s called a Siamese neural network architecture. Neural Style Transfer just to wrap this up, you can now define the overall cost function as alpha times the content cost between c and G plus beta times the style cost between s and G and then just create in the sense or a more sophisticated optimization algorithm if you want in order to try to find an image G that normalize, that tries to minimize this cost function j of G. And if you do that, you can generate pretty good looking neural artistic and if you do that youâ€™ll be able to generate some pretty nice novel artwork.","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Coursera","slug":"Coursera","permalink":"https://jmj3047.github.io/tags/Coursera/"},{"name":"AI Studies","slug":"AI-Studies","permalink":"https://jmj3047.github.io/tags/AI-Studies/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}]},{"title":"Structuring Machine Learning Projects_Quiz","slug":"DL3_Quiz","date":"2023-05-06T15:00:00.000Z","updated":"2023-06-03T13:54:32.965Z","comments":true,"path":"2023/05/07/DL3_Quiz/","link":"","permalink":"https://jmj3047.github.io/2023/05/07/DL3_Quiz/","excerpt":"","text":"ê°œìš”Coursera Deep Learning Course 3 Quiz 1. Bird Recognition in the City of Peacetopia (Case Study) Link: https://www.coursera.org/learn/machine-learning-projects/home/module/1 2. Autonomous Driving (Case Study) Link: https://www.coursera.org/learn/machine-learning-projects/home/module/2","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"Coursera","slug":"Coursera","permalink":"https://jmj3047.github.io/tags/Coursera/"},{"name":"Quiz","slug":"Quiz","permalink":"https://jmj3047.github.io/tags/Quiz/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}]},{"title":"(Error solved) Accessing non-existent property 'lineno' of module exports inside circular dependency","slug":"Error_node_14126","date":"2023-05-06T15:00:00.000Z","updated":"2023-05-07T09:49:23.606Z","comments":true,"path":"2023/05/07/Error_node_14126/","link":"","permalink":"https://jmj3047.github.io/2023/05/07/Error_node_14126/","excerpt":"","text":"(node:14126) Warning, Accessing non-existent property â€˜linenoâ€™ of module exports inside circular dependencyì–´ëŠë‚  ë¶€í„°ì¸ê°€ ë¸”ë¡œê·¸ ì—…ë¡œë“œ ì „ì— hexo sever ë¥¼ ì…ë ¥í•˜ë©´ ë°‘ì— node ê´€ë ¨ëœ warningì´ ëœ¨ê¸° ì‹œì‘í–ˆë‹¤. For some reason, when I type hexo sever before uploading a blog, a warning about nodes appears below. 1234567891011MacBookPro myblog % hexo serverINFO Validating configINFO Start processingINFO Hexo is running at http://localhost:4000/ . Press Ctrl+C to stop.(node:14126) Warning: Accessing non-existent property &#x27;lineno&#x27; of module exports inside circular dependency(Use `node --trace-warnings ...` to show where the warning was created)(node:14126) Warning: Accessing non-existent property &#x27;column&#x27; of module exports inside circular dependency(node:14126) Warning: Accessing non-existent property &#x27;filename&#x27; of module exports inside circular dependency(node:14126) Warning: Accessing non-existent property &#x27;lineno&#x27; of module exports inside circular dependency(node:14126) Warning: Accessing non-existent property &#x27;column&#x27; of module exports inside circular dependency(node:14126) Warning: Accessing non-existent property &#x27;filename&#x27; of module exports inside circular dependency ê²Œì‹œë¬¼ì´ ì—…ë¡œë“œ ë˜ëŠ”ë°ì—ëŠ” ë¬¸ì œê°€ ì—†ì—ˆì§€ë§Œ, ì°ì°í•´ì„œ í•´ê²°í•˜ë ¤ê³  ì°¾ì•„ë³¸ ê²°ê³¼: I didnâ€™t have any issues with the post uploading, but when I went to troubleshoot, I found that this code: 1rm -rf node_modules package-lock.json &amp;&amp; npm install &amp;&amp; npm run ìœ„ ëª…ë ¹ì–´ë¥¼ ì…ë ¥í–ˆë”ë‹ˆ í•´ê²° ë˜ì—ˆë‹¤. ëŒ€ì¶© ì°¾ì•„ë´¤ë”ë‹ˆ npmê³¼ nodeì˜ ë²„ì „ ë¬¸ì œ ì•„ë‹ˆë©´ package-lock.jsonì˜ ë¬¸ì œ ë‘˜ì¤‘ì— í•˜ë‚˜ ì¸ ê±° ê°™ì•˜ë‹¤. I entered the above command and it worked. After a quick search, I realized it was either a version issue with npm and node or a problem with package-lock.json. Reference: https://github.com/nodejs/help/issues/2347","categories":[{"name":"Setting","slug":"Setting","permalink":"https://jmj3047.github.io/categories/Setting/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://jmj3047.github.io/tags/Hexo/"},{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"}],"keywords":[{"name":"Setting","slug":"Setting","permalink":"https://jmj3047.github.io/categories/Setting/"}]},{"title":"Structuring Machine Learning Projects","slug":"DL_Part3","date":"2023-05-04T15:00:00.000Z","updated":"2023-06-19T12:49:03.236Z","comments":true,"path":"2023/05/05/DL_Part3/","link":"","permalink":"https://jmj3047.github.io/2023/05/05/DL_Part3/","excerpt":"","text":"Course Lecture 3 in Deep Learning Why ML Strategy F1 score Classifier A is 90% recall: That of all of the images in, say, your dev sets that really are cats, classifier A accurately pulled out 90% of them. It turns out that thereâ€™s often a tradeoff between precision and recall, and you care about both. â†’ When the classifier says something is a cat, thereâ€™s a high chance it really is a cat. But of all the images that are cats, you also want it to pull a large fraction of them as cats. (ë¶„ë¥˜ê¸°ê°€ ê³ ì–‘ì´ë¼ê³  ë§í•˜ë©´ ê·¸ê²Œ ì‹¤ì œë¡œ ê³ ì–‘ì´ì¼ í™•ë¥ ì´ ë†’ê¸°ë¥¼ ì›í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê³ ì–‘ì´ ì´ë¯¸ì§€ ì¤‘ì—ì„œë„ ë§ì€ ì´ë¯¸ì§€ë¥¼ ê³ ì–‘ì´ë¡œ ë¶„ë¥˜í•  ìˆ˜ ìˆê¸°ë¥¼ ì›í•©ë‹ˆë‹¤.) So it might be reasonable to try to evaluate the classifiers in terms of its precision and its recall. The problem with using precision recall as your evaluation metric is that if classifier A does better on recall, which it does here, the classifier B does better on precision, then youâ€™re not sure which classifier is better. F1 score: In the machine learning literature, the standard way to combine precision and recall. The details of F1 score arenâ€™t too important, but informally, you can think of this as the average of precision, P, and recall, R. Formally, the F1 score is defined by this formula, itâ€™s 2&#x2F; 1&#x2F;P + 1&#x2F;R. In mathematics, this function is called the harmonic mean of precision P and recall R. But less formally, you can think of this as some way that averages precision and recall. To summarize, if there are multiple things you care about by say thereâ€™s one as the optimizing metric that you want to do as well as possible on and one or more as satisficing metrics were youâ€™ll be satisfice. So long as it does better than some threshold you can now have an almost automatic way of quickly looking at multiple core size and picking the, quote, best one. Avoidale bias In machine learning, avoidable bias is the difference between the training error and the Bayes error. The Bayes error is the theoretical minimum error rate that is possible for a given machine learning model. Avoidable bias is the error that is due to the machine learning model, rather than the data that it is trained on. There are a number of factors that can contribute to avoidable bias, including: The choice of machine learning algorithm The size and quality of the training data The way that the model is trained There are a number of things that can be done to reduce avoidable bias, including: Choosing a machine learning algorithm that is appropriate for the task at hand Using a large and high-quality training dataset Using a variety of techniques to train the model, such as cross-validation and regularization It is important to note that it is not always possible to eliminate avoidable bias entirely. However, by taking steps to reduce avoidable bias, it is possible to improve the accuracy and fairness of machine learning models. Here are some examples of avoidable bias in machine learning: A model that is trained on a dataset that is biased towards men may be more likely to predict that a job applicant is a man, even if the applicantâ€™s qualifications are equal to those of a woman. A model that is trained on a dataset that is biased towards white people may be more likely to predict that a criminal is white, even if the crime rate is equal for all races. A model that is trained on a dataset that is biased towards wealthy people may be more likely to predict that a person is wealthy, even if the personâ€™s income is equal to that of a person from a lower socioeconomic status. Avoidable bias can have a significant impact on the fairness and accuracy of machine learning models. It is important to be aware of the potential for avoidable bias and to take steps to reduce it. what you want is to maybe keep improving your training performance until you get down to Bayes error but you donâ€™t actually want to do better than Bayes error. You canâ€™t actually do better than Bayes error unless youâ€™re overfitting. And this, the difference between your training area and the dev error, thereâ€™s a measure still of the variance problem of your algorithm. The term avoidable bias acknowledges that thereâ€™s some bias or some minimum level of error that you just cannot get below which is that if Bayes error is 7.5%, you donâ€™t actually want to get below that level of error. So rather than saying that if youâ€™re training error is 8%, then the 8% is a measure of bias in this example, youâ€™re saying that the avoidable bias is maybe 0.5% or 0.5% is a measure of the avoidable bias whereas 2% is a measure of the variance and so thereâ€™s much more room in reducing this 2% than in reducing this 0.5%. Relationship between human level performance, bayes error and variance If youâ€™re trying to understand bias and variance where you have an estimate of human-level error for a task that humans can do quite well, you can use human-level error as a proxy or as a approximation for Bayes error. so the difference between your estimate of Bayes error tells you how much avoidable bias is a problem, how much avoidable bias there is. And the difference between training error and dev error, that tells you how much variance is a problem, whether your algorithmâ€™s able to generalize from the training set to the dev set. And the big difference between our discussion here and what we saw in an earlier course was that instead of comparing training error to 0%. And just calling that the estimate of the bias. In contrast, in this video we have a more nuanced analysis in which there is no particular expectation that you should get 0% error. Because sometimes Bayes error is non zero and sometimes itâ€™s just not possible for anything to do better than a certain threshold of error. Human-level performance is a measure of how well humans can perform a task. Bayes error is the lowest possible error that a machine learning model can achieve on a task, given the amount of data that is available. Variance is a measure of how much the modelâ€™s performance varies on different datasets The relationship between human-level performance, Bayes error, and variance can be understood in the following way: Human-level performance is a lower bound on Bayes error. This is because humans have access to all of the information that is available to the machine learning model, and they are able to use their knowledge and experience to make better decisions. Variance is a measure of how much the modelâ€™s performance is affected by noise in the data. The more noise there is in the data, the higher the variance will be. As the modelâ€™s variance increases, the gap between its performance and human-level performance will also increase. This is because the model will be more likely to make mistakes on data that is not representative of the training data. In general, the goal of machine learning is to develop models that have low bias and low variance. Low bias means that the model is not making systematic errors, and low variance means that the modelâ€™s performance is not affected by noise in the data. There are a number of techniques that can be used to reduce bias and variance in machine learning models. These techniques include: Data augmentation: This involves creating new data points by modifying existing data points. This can help to reduce the amount of noise in the data. Regularization: This involves adding a penalty to the loss function that is being minimized. This can help to reduce the modelâ€™s complexity and make it more robust to noise in the data. Ensembling: This involves combining the predictions of multiple models. This can help to reduce the variance of the modelâ€™s predictions. Structured data in ML All four of these examples are actually learning from structured data, where you might have a database of what ads users have clicked on, database of products youâ€™ve bought before, databases of how long it takes to get from A to B, database of previous loan applications and their outcomes. These are not natural perception problems, so these are not computer vision, or speech recognition, or natural language processing tasks. Humans tend to be very good in natural perception task. So it is possible, but itâ€™s just a bit harder for computers to surpass human-level performance on natural perception tasks. Finally, all of these are problems where there are teams that have access to huge amounts of data. So for example, the best systems for all four of these applications have probably looked at far more data of that application than any human could possibly look at. And so, thatâ€™s also made it relatively easy for a computer to surpass human-level performance. Reducing bias and variance The process weâ€™ve seen in the last several videos, if you want to improve the performance of your machine learning system, I would recommend looking at the difference between your training error and your proxy for Bayes error and just gives you a sense of the avoidable bias. In other words, just how much better do you think you should be trying to do on your training set. Then look at the difference between your dev error and your training error as an estimate of how much of a variance problem you have. In other words, how much harder you should be working to make your performance generalized from the training set to the dev set that it wasnâ€™t trained on explicitly. How to build ML Strategy Carrying Out Error Analysis: you should find a set of mislabeled examples, either in your dev set, or in your development set. And look at the mislabeled examples for false positives and false negatives. And just count up the number of errors that fall into various different categories. what I would recommend you do, if youâ€™re starting on building a brand new machine learning application, is to build your first system quickly and then iterate. What I mean by that is I recommend that you first quickly set up a dev&#x2F;test set and metric. So this is really deciding where to place your target. And if you get it wrong, you can always move it later, but just set up a target somewhere. Then I recommend you build an initial machine learning system quickly. Find the training set, train it and see. Start to see and understand how well youâ€™re doing against your dev&#x2F;test set and your valuation metric. When you build your initial system, you will then be able to use bias&#x2F;variance analysis which we talked about earlier as well as error analysis which we talked about just in the last several videos, to prioritize the next steps. Transfer Learning If you retrain all the parameters in the neural network, then this initial phase of training on image recognition is sometimes called pre-training, because youâ€™re using image recognitions data to pre-initialize or really pre-train the weights of the neural network. And then if you are updating all the weights afterwards, then training on the radiology data sometimes thatâ€™s called fine tuning. speech recognition model to wake word&#x2F;trigger word detection model youâ€™ve trained a speech recognition system to output your transcripts. And letâ€™s say that you now want to build a â€œwake wordsâ€ or a â€œtrigger wordsâ€ detection system. So, recall that a wake word or the trigger word are the words we say in order to wake up speech control devices in our houses such as saying â€œAlexaâ€ to wake up an Amazon Echo or â€œOK Googleâ€ to wake up a Google device or â€œhey Siriâ€ to wake up an Apple device or saying â€œNi hao baiduâ€ to wake up a baidu device. In order to do this, you might take out the last layer of the neural network again and create a new output node. But sometimes another thing you could do is actually create not just a single new output, but actually create several new layers to your neural network to try to predict the labels Y for your wake word detection problem. Then again, depending on how much data you have, you might just retrain the new layers of the network or maybe you could retrain even more layers of this neural network. When does transfer learning make sense? Transfer learning makes sense when you have a lot of data for the problem youâ€™re transferring from and usually relatively less data for the problem youâ€™re transferring to. For speech recognition, maybe youâ€™ve trained the speech recognition system on 10000 hours of data. So, youâ€™ve learned a lot about what human voices sounds like from that 10000 hours of data, which really is a lot. But for your trigger word detection, maybe you have only one hour of data. So, thatâ€™s not a lot of data to fit a lot of parameters. So in this case, a lot of what you learn about what human voices sound like, what are components of human speech and so on, that can be really helpful for building a good wake word detector, even though you have a relatively small dataset or at least a much smaller dataset for the wake word detection task. So in both of these cases, youâ€™re transferring from a problem with a lot of data to a problem with relatively little data. Transfer learning has been most useful if youâ€™re trying to do well on some Task B, usually a problem where you have relatively little data. So for example, in radiology, you know itâ€™s difficult to get that many x-ray scans to build a good radiology diagnosis system. In that case, you might find a related but different task, such as image recognition, where you can get maybe a million images and learn a lot of load-over features from that, so that you can then try to do well on Task B on your radiology task despite not having that much data for it. Multi task Learning In multi-task learning, you start off simultaneously, trying to have one neural network do several things at the same time. And then each of these task helps hopefully all of the other task. when does multi-task learning make sense? One is if your training on a set of tasks that could benefit from having shared low-level features. So for the autonomous driving example, it makes sense that recognizing traffic lights and cars and pedestrians, those should have similar features that could also help you recognize stop signs, because these are all features of roads. Second, this is less of a hard and fast rule, so this isnâ€™t always true. But what I see from a lot of successful multi-task learning settings is that the amount of data you have for each task is quite similar. When you recall from transfer learning, you learn from some task A and transfer it to some task B. So if you have a million examples for task A then and 1,000 examples for task B, then all the knowledge you learned from that million examples could really help augment the much smaller data set you have for task B. In multi-task learning you usually have a lot more tasks than just two. So maybe you have, previously we had 4 tasks but letâ€™s say you have 100 tasks. And youâ€™re going to do multi-task learning to try to recognize 100 different types of objects at the same time. So what you may find is that you may have 1,000 examples per task and so if you focus on the performance of just one task, letâ€™s focus on the performance on the 100th task, you can call A100. If you are trying to do this final task in isolation, you would have had just a thousand examples to train this one task, this one of the 100 tasks that by training on these 99 other tasks. These in aggregate have 99,000 training examples which could be a big boost, could give a lot of knowledge to argument this otherwise, relatively small 1,000 example training set that you have for task A100. And symmetrically every one of the other 99 tasks can provide some data or provide some knowledge that help every one of the other tasks in this list of 100 tasks. So the second bullet isnâ€™t a hard and fast rule but what I tend to look at is if you focus on any one task, for that to get a big boost for multi-task learning, the other tasks in aggregate need to have quite a lot more data than for that one task. And so one way to satisfy that is if a lot of tasks like we have in this example on the right, and if the amount of data you have in each task is quite similar. But the key really is that if you already have 1,000 examples for 1 task, then for all of the other tasks you better have a lot more than 1,000 examples if those other other task are meant to help you do better on this final task. And finally multi-task learning tends to make more sense when you can train a big enough neural network to do well on all the tasks. The alternative to multi-task learning would be to train a separate neural network for each task. Rather than training one neural network for pedestrian, car, stop sign, and traffic light detection, you could have trained one neural network for pedestrian detection, one neural network for car detection, one neural network for stop sign detection, and one neural network for traffic light detection. So what a researcher, Rich Carona, found many years ago was that the only times multi-task learning hurts performance compared to training separate neural networks is if your neural network isnâ€™t big enough. But if you can train a big enough neural network, then multi-task learning certainly should not or should very rarely hurt performance. And hopefully it will actually help performance compared to if you were training neural networks to do these different tasks in isolation. End to end deep learning Briefly, there have been some data processing systems, or learning systems that require multiple stages of processing. And what end-to-end deep learning does, is it can take all those multiple stages, and replace it usually with just a single neural network. One of the challenges of end-to-end deep learning is that you might need a lot of data before it works well. For example, if youâ€™re training on 3,000 hours of data to build a speech recognition system, then the traditional pipeline, the full traditional pipeline works really well. Itâ€™s only when you have a very large data set, you know one could say 10,000 hours of data, anything going up to maybe 100,000 hours of data that the end-to end-approach then suddenly starts to work really well. When you have a smaller data set, the more traditional pipeline approach actually works just as well. Often works even better. And you need a large data set before the end-to-end approach really shines. And if you have a medium amount of data, then there are also intermediate approaches where maybe you input audio and bypass the features and just learn to output the phonemes of the neural network, and then at some other stages as well. Reference avoidable bias, variance, human level performance","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Coursera","slug":"Coursera","permalink":"https://jmj3047.github.io/tags/Coursera/"},{"name":"AI Studies","slug":"AI-Studies","permalink":"https://jmj3047.github.io/tags/AI-Studies/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}]},{"title":"An ongoing review of speech emotion recognition","slug":"SER_Review","date":"2023-04-25T15:00:00.000Z","updated":"2023-04-27T14:20:29.636Z","comments":true,"path":"2023/04/26/SER_Review/","link":"","permalink":"https://jmj3047.github.io/2023/04/26/SER_Review/","excerpt":"","text":"Journal&#x2F;Conference : Science DirectYear(published year): 2023Author: Javier de Lope, Manuel GraÃ±aSubject: Speech Emtion Recognition, Speech feature extraction Summary This paper provides an overview of recent and classical approaches to speech emotion recognition (SER) using machine learning and deep learning techniques. SER is an active area of research that involves recognizing emotional states from speech signals, which can have applications in fields such as human-computer interaction, psychology, and healthcare. Classical machine learning approaches for SER include support vector machines (SVMs), k-nearest neighbors (kNN), decision trees, Gaussian mixture models (GMMs), among others. These approaches typically involve extracting features from speech signals using techniques such as Mel Frequency Cepstral Coefficients (MFCCs) or prosodic features like pitch or energy. Recent deep learning approaches for SER include convolutional neural networks (CNNs), recurrent neural networks (RNNs), long short-term memory (LSTM) networks, among others. These approaches usually encompass both the feature extraction and classification phases and have shown promising results in some datasets. Transfer learning is a technique used in DL that involves reusing a pre-trained neural network model on a new task or dataset. In SER, transfer learning has been applied to CNNs and RNNs to leverage pre-trained models on related tasks and reduce the amount of training data required. Dataset 12ê°œ ë°ì´í„° ì…‹, 2004ë…„~2022ë…„ë„ì— ë°œí–‰ëœ 93ê°œì˜ ë…¼ë¬¸ Table1ì—ëŠ” VAMì´ ë¹ ì ¸ì‡ì–´ 11ê°œì˜ ë°ì´í„° ì…‹ë§Œ ë¹„êµí•¨ DES ë´ë§ˆí¬ì–´ emotion Database 4ëª…ì˜ ì „ë¬¸ ë°°ìš°(ì—¬ì„± 2ëª…, ë‚¨ì„± 2ëª…)ì˜ ë…¹ìŒ 30ë¶„ ë¶„ëŸ‰ì˜ ì—°ì„¤ë¡œ êµ¬ì„± 2ê°œì˜ ë…ë¦½ëœ ë‹¨ì–´ (ì˜ˆ, ì•„ë‹ˆì˜¤), 9ê°œì˜ ì§§ì€ êµ¬ë¬¸(4ê°œ ì§ˆë¬¸, 5ê°œ í‰ì„œë¬¸), ë‘ê°œì˜ êµ¬ì ˆë¡œ êµ¬ì„± ì¤‘ë¦½, ë†€ëŒ, í–‰ë³µ, ìŠ¬í””, ë¶„ë…¸ ì´ 5ê°œì˜ ê°ì • EMODB ë…ì¼ì–´ emotion Database 10ëª…ì˜ ë°°ìš°(ì—¬ì„± 5ëª…, ë‚¨ì„± 5ëª…) 1487ì´ˆ(í‰ê·  2.77ì´ˆ) ì§§ì€ ë¬¸ì¥ 5ê°œ, ê¸´ ë¬¸ì¥ 5ê°œ ì¼ë¶€ ê°ì • í‘œí˜„ì—ëŠ” ë™ì¼í•œ ë°œí™”ìê°€ ê¸°ë¡í•œ ë‘ê°€ì§€ ë²„ì „ì´ ìˆìŒ ì•½ 800ê°œì˜ ë¬¸ì¥ ì œê³µ(ì¤‘ë³µ í¬í•¨), 535ê°œ ë°œí™”(ì¤‘ë³µì œê±°) ë¶„ë…¸, ì¤‘ë¦½, í™”ë‚¨, ì§€ë£¨í•¨, í–‰ë³µ, ìŠ¬í””, í˜ì˜¤ ì´ 7ê°€ì§€ ê°ì • eNTERFACE 89 ì˜ì–´ audio-visual emotion Database 14ê°œì˜ êµ­ì ì˜ 42ëª…(ì—¬ì„± 19%, ë‚¨ì„± 81%) 6ê°œì˜ ë‹¨í¸ ìŠ¤í† ë¦¬ì— ëŒ€í•œ ë°˜ì‘ì„ í‘œí˜„ í–‰ë³µ, ìŠ¬íŒ, ë†€ë¼ì›€, ë¶„ë…¸, í˜ì˜¤ê°, ë‘ë ¤ì›€ ì´ 6ê°€ì§€ ê°ì • IEMOCAP Interactive Emotional Dyadic Motion Capture Database collected by University of South California 10ëª…ì˜ ë°°ìš°ê°€ ëŒ€ë³¸ ì‹œë‚˜ë¦¬ì˜¤ì™€ ì¦‰í¥ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ì–¼êµ´, ë¨¸ë¦¬, ì† ë“±ì— ë§ˆì»¤ë¥¼ ë¶€ì°©í•œì±„ ë…¹ìŒì´ ì§„í–‰ë¨ 12ì‹œê°„ë¶„ëŸ‰ì˜ ë°ì´í„° ì˜¤ë””ì˜¤ëŠ” 3-15ì´ˆ ì‚¬ì´ë¡œ ë‚˜ëˆ ì§€ë©° 3-4ëª…ì˜ ì‚¬ëŒì´ ë¼ë²¨ë§ì„ í•¨ ë¶„ë…¸, ìŠ¬í””, í–‰ë³µ, í˜ì˜¤ê°, ë‘ë ¤ì›€, ì¢Œì ˆ, í¥ë¶„, ì¤‘ë¦½ ë“± 10ê°€ì§€ ê°ì •ìœ¼ë¡œ í™•ì¥ë¨(ì›ë˜ ì„¤ê³„ëŠ” ë¶„ë…¸, ìŠ¬í””, í–‰ë³µ, ì¢Œì ˆ, ì¤‘ë¦½ 5ê°œ ì˜€ìŒ) SAVEE ì˜ì–´ Audio-Visual Expressed Emotion Database 4ëª…ì˜ ì˜êµ­ ë‚¨ì„± ë°°ìš°ì˜ ì˜¤ë””ì˜¤ ë° ë¹„ë””ì˜¤ ë…¹ìŒ 120ê°œì˜ ë°œí™”ë¥¼ ì—°ê¸°, ì´ 480ê°œì˜ ë¬¸ì¥ ê°ì •ë³„ë¡œ 15ê°œì˜ ë¬¸ì¥(ê³µí†µ ë¬¸ì¥ 3ê°œ, ê°ì •ë³„ ë¬¸ì¥ 2ê°œ, ì¼ë°˜ ë¬¸ì¥ 10ê°œ) ì¤‘ë¦½, ë¶„ë…¸, í˜ì˜¤, ê³µí¬, í–‰ë³µ, ìŠ¬í””, ë†€ëŒ ì´ 7ê°€ì§€ ê°ì • Thai DB Audiovisual Thai Emotion Database 6ëª…ì˜ ë“œë¼ë§ˆ ì „ê³µìƒë“¤ 1~7ìŒì ˆë¡œ êµ¬ì„±ëœ ì¼ë°˜ì ì¸ íƒœêµ­ì–´ ë‹¨ì–´ 1000ê°œ â†’ ìµœì¢…ìœ¼ë¡œëŠ” 972ê°œ ë‹¨ì–´ í–‰ë³µ, ìŠ¬í””, ë†€ë¼ì›€, ë¶„ë…¸, ë‘ë ¤ì›€, í˜ì˜¤ê° ì´ 6ê°€ì§€ ê°ì • INTER1SP ìŠ¤í˜ì¸ì–´ emotion database ë‚¨ì„± 1ëª…ê³¼ ì—¬ì„± 1ëª…ì˜ Spanish ì „ë¬¸ ë°°ìš°ì˜ ë…¹ìŒ ë‹¨ì–´, ìˆ«ì, ë¬¸ì¥ì„ í¬í•¨í•˜ëŠ” 184ê°œ ë°œí™”ë¥¼ í¬í•¨. ê° í™”ì ë§ˆë‹¤ 4ì‹œê°„ ë¶„ëŸ‰ì˜ ë°ì´í„°, 6040ê°œì˜ ìƒ˜í”Œì´ í¬í•¨ ë¶„ë…¸, í˜ì˜¤, ê³µí¬, í–‰ë³µ, ìŠ¬í””, ë†€ë¼ì›€, ì¤‘ë¦½ë“±ì˜ ê°ì •ì´ ê³ ë ¤ë¨ TESS Toronto emotion database 2ëª…ì˜ ì „ë¬¸ ë°°ìš° 200ê°œì˜ ë‹¨ì–´, 2800ê°œì˜ ìƒ˜í”Œ ë¶„ë…¸, í˜ì˜¤, ë‘ë ¤ì›€, í–‰ë³µ, ìœ ì¾Œí•¨, ìŠ¬í””, ì¤‘ë¦½ ì´ 7ê°€ì§€ ê°ì • RAVDESS ì˜ì–´ Audio-Visual Database of Emotional Speech and Song 24ëª…ì˜ ì „ë¬¸ë°°ìš°(ì—¬ì„± 12ëª…, ë‚¨ì„± 12ëª…) 2ê°œì˜ ë¬¸êµ¬, 1440ê°œ ìŒì„± ì˜¤ë””ì˜¤ ìƒ˜í”Œ(ì˜¤ë””ì˜¤ í•˜ë‚˜ë‹¹ ì•½ 3ì´ˆ) 7356ê°œì˜ ì˜¤ë””ì˜¤ ë° ë¹„ë””ì˜¤ í´ë¦½ìœ¼ë¡œ êµ¬ì„±(25GB) ì¤‘ë¦½, í‰ì˜¨, í–‰ë³µ, ìŠ¬í””, ë¶„ë…¸, ë‘ë ¤ì›€, í˜ì˜¤, ë†€ë¼ì›€ ì´ 8ê°€ì§€ ê°ì • JL-Corpus ë‰´ì§ˆëœë“œ emotion database 5ê°œì˜ ê¸°ë³¸ê°ì •ê³¼ 5ê°œì˜ ë³´ì¡° ê°ì • [58]ë…¼ë¬¸ ì™¸ì—ëŠ” ë‹¤ë¥¸ê³³ì—ì„œ ì‚¬ìš©í•œ ì–¸ê¸‰ì„ ì°¾ì„ìˆ˜ ì—†ìŒ MSP-PODCAST ì˜ì–´ emotion database ì˜¤í”ˆ ì˜¤ë””ì˜¤ ë°ì´í„° ì†ŒìŠ¤ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ (ì¼ë°˜ì¸ì˜) ë…¹ìŒ 27ì‹œê°„(2.75~11ì´ˆ), 18000ê°œì˜ ìì—°ìŠ¤ëŸ¬ìš´ ê°ì • ë¬¸ì¥ 300ëª…ì˜ í‰ê°€ìê°€ ë¼ë²¨ë§ 8ê°œì˜ ê°ì •(ê¸°ë³¸ arousal, valence, dominance + extended list of emotions) Machine Learning Conventional machine learning approaches found in the literature. VK SVM &#x3D; various kernels for the SVM RSVM &#x3D; radial basis function kernel SVM TSVM &#x3D; twins SVM, LR &#x3D; logistic regression MLP &#x3D; multilayer perceptron HuM &#x3D; Humoments ELMDT &#x3D; extreme learning machine decision tree BN &#x3D; Bayes networks GMM &#x3D; Gaussian mixture model EDT &#x3D; Ensembles of decision trees kNN &#x3D; k nearest neighbors NNMF &#x3D; non negative matrix factorization. Deep Learning ê·¸ë¦¼ 1ì—ëŠ” CNNê³¼ LSTMì„ ê²°í•©í•œ ìŒì„± ê°ì • ì¸ì‹ì„ ìœ„í•œ ì¼ë°˜ì ì¸ ë”¥ëŸ¬ë‹ êµ¬ì¡°ì— ëŒ€í•´ ì„¤ëª…í•©ë‹ˆë‹¤. CNNì„ ì‚¬ìš©í•´ ê°ì • íŠ¹ì§•ì„ í•™ìŠµí•˜ê±°ë‚˜ ê¸°ì¡´ì˜ ìˆ˜ì‘ì—… íŠ¹ì§• ì—†ì´ CNNê³¼ RNNì„ ì—°ê²°í•´ ê¸°ì¡´ ë°©ì‹ë³´ë‹¤ ë” ë†’ì€ ì •í™•ë„ë¥¼ ë‹¬ì„±í•œ ì´ˆê¸° ì ‘ê·¼ë²•ì˜ ì˜ˆë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. ì´ ë¬¸ì¥ë“¤ì€ CNN ë° LSTMê³¼ ê°™ì€ ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìŒì„±ì—ì„œ ê°ì •ì„ ì¸ì‹í•˜ëŠ” ë‹¤ì–‘í•œ ì—°êµ¬ë“¤ì„ ì„¤ëª…í•©ë‹ˆë‹¤. ì—°êµ¬ìë“¤ì€ ì´ëŸ¬í•œ ëª¨ë¸ì„ ë‹¤ì–‘í•œ ë°©ì‹ìœ¼ë¡œ ê²°í•©í•˜ì—¬ ì‚¬ìš©í•˜ì˜€ìœ¼ë©°, CNNì„ ì‚¬ìš©í•˜ì—¬ affective featureë¥¼ í•™ìŠµí•˜ê±°ë‚˜, CNNê³¼ RNNì„ ê²°í•©í•˜ì—¬ ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜ë¥¼ ë§Œë“¤ì–´ ê°ì •ì„ ì¸ì‹í•˜ëŠ” ë“± ë‹¤ì–‘í•œ ë°©ë²•ì„ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. ë§ì€ ì—°êµ¬ì—ì„œëŠ” ê¸°ì¡´ì˜ ë°©ë²•ê³¼ ë¹„êµí•˜ì—¬ ë†’ì€ ì •í™•ë„ë¥¼ ë³´ê³ í•˜ì˜€ìŠµë‹ˆë‹¤. ì—°êµ¬ë“¤ì€ MFCC, LFE(Log Mel-Filterbank Energies*)*, spectrograms ë° Mel spectrograms ë“± ë‹¤ì–‘í•œ ìœ í˜•ì˜ ë°ì´í„°ë¥¼ ê²°í•©í•˜ê³ , ë³‘ë ¬í™”ëœ convolutional recurrent neural networks, multi-CNN, 1D convolutional layers ë“± ë‹¤ì–‘í•œ ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì‹¤í—˜ë“¤ì€ ë‹¤ì–‘í•œ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìˆ˜í–‰ë˜ì—ˆìŠµë‹ˆë‹¤. CNN, LSTM, transformers ë“± ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìŒì„±ì—ì„œ ê°ì •ì„ ì¸ì‹í•˜ëŠ” ë‹¤ì–‘í•œ ì—°êµ¬ë“¤ì„ ì„¤ëª…í•©ë‹ˆë‹¤. ì—°êµ¬ìë“¤ì€ í›ˆë ¨ ë° í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ë©°, ë°ì´í„° ì¦ê°• ë° ê¹Šì€ ë©”íŠ¸ë¦­ ëŸ¬ë‹ê³¼ ê°™ì€ ë‹¤ì–‘í•œ ê¸°ìˆ ì„ ì ìš©í•˜ì—¬ ê°ì • ì¸ì‹ ê²°ê³¼ë¥¼ ê°œì„ í•˜ë ¤ê³  ë…¸ë ¥í•©ë‹ˆë‹¤. DL network, recurrent neural networks(RNN) ë° gated recurrent units (GRU) ë“±ì˜ ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìŒì„±ì—ì„œ ê°ì •ì„ ë¶„ë¥˜í•˜ê¸° ìœ„í•œ ë‹¤ì–‘í•œ ì—°êµ¬ë“¤ì„ ì„¤ëª…í•©ë‹ˆë‹¤. ì—°êµ¬ë“¤ì€ spectrograms ë° MFCCì™€ ê°™ì€ ë‹¤ì–‘í•œ ê¸°ìˆ ì„ ì‚¬ìš©í•˜ë©°, ê¸°ì¡´ì˜ ìµœì‹  ë°©ë²•ë³´ë‹¤ ë” ë‚˜ì€ ì •í™•ë„ë¥¼ ë³´ê³ í•˜ê³  ìˆìŠµë‹ˆë‹¤. í•˜ë‚˜ì˜ ì—°êµ¬ì—ì„œëŠ” ì „í†µì ì¸ ë°ì´í„° ì¦ê°• ê¸°ìˆ ì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” DL networkë¥¼ ì œì•ˆí•˜ë©°, ë” í° ì´ë¯¸ì§€ ì°¨ì›ì´ ë” ë†’ì€ ì •í™•ë„ë¥¼ ë³´ì´ì§€ë§Œ ê³„ì‚° ë³µì¡ì„±ì´ ì¦ê°€í•œë‹¤ëŠ” ê²ƒì„ ë³´ê³ í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ ìŠ¤í™íŠ¸ëŸ¼ì— ì ìš©ë˜ëŠ” ì˜ ì•Œë ¤ì§„ DL ì•„í‚¤í…ì²˜ë¥¼ transfer learningí•˜ëŠ” ê²ƒì´ ë§ì€ DL ì ‘ê·¼ ë°©ë²•ì— ì‚¬ìš©ë©ë‹ˆë‹¤. ë§ì€ ë…¼ë¬¸ë“¤ì´ CNNì„ ì´ìš©í•˜ì—¬ ìŠ¤í™íŠ¸ëŸ¼ ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•˜ë©°, ê·¸ ì¤‘ Stolar et al. [130], Badshah et al. [12]ì€ AlexNet ëª¨ë¸ì„ transfer learning í•˜ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ë˜í•œ, Huang and Bao [52], Zhang et al. [163], Gerczuk et al. [42], Popova et al. [106], Wang et al.[143] ë“±ì€ ìŠ¤í™íŠ¸ëŸ¼ ì´ë¯¸ì§€ë‚˜ MFCCë¥¼ íŠ¹ì§•ìœ¼ë¡œ í•˜ëŠ” DL transfer learning ë°©ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. Tripathi et al.ì€ ResNet ê¸°ë°˜ì˜ ì‹ ê²½ë§ì„ ì œì•ˆí•˜ì—¬ ì–´ë ¤ìš´ ìƒ˜í”Œì— ë” ë§ì€ ì¤‘ìš”ë„ë¥¼ ë¶€ì—¬í•˜ëŠ” focal lossë¡œ ê°ì • ì¸ì‹ì„ í›ˆë ¨ì‹œì¼°ìŠµë‹ˆë‹¤. ì´ëŠ” ë‹¤ì–‘í•œ í´ë˜ìŠ¤ ê°„ì— ì¤‘ìš”í•œ í´ë˜ìŠ¤ ë¶ˆê· í˜•ì´ ì¡´ì¬í•  ë•Œ ì •í™•ë„ë¥¼ í–¥ìƒì‹œí‚¤ë ¤ëŠ” ê²ƒì…ë‹ˆë‹¤. Park et al.ì€ íŠ¹ì„± ì…ë ¥ì— ì§ì ‘ ì ìš©ë˜ëŠ” ìŒì„± ì¸ì‹ìš© ë°ì´í„° ì¦ê°• ë°©ë²•ì„ ì œì•ˆí•˜ì—¬, ì£¼íŒŒìˆ˜ ì±„ë„ ë¸”ë¡ ë° ì‹œê°„ ë‹¨ê³„ ë¸”ë¡ì„ ë§ˆìŠ¤í‚¹í•˜ëŠ” ë°©ë²• ë“±ìœ¼ë¡œ íŠ¹ì„±ì„ ì™œê³¡í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ ì„¸íŠ¸ë¥¼ ë³´ê°•í•˜ì—¬ ì–¸ì–´ ëª¨ë¸ì˜ ë„ì›€ ì—†ì´ë„ ìƒíƒœê°€ ì¢‹ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆì—ˆë‹¤ê³  ë³´ê³ í•˜ê³  ìˆìŠµë‹ˆë‹¤. Yi et al.ëŠ” ìƒì„± ì ëŒ€ ì‹ ê²½ë§(GAN)ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ì¦ê°•ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. Shilandari et al. ë° Latif et al.ë„ ë°ì´í„° ì¦ê°•ì„ ìœ„í•´ GANì„ ì œì•ˆí•©ë‹ˆë‹¤. Bakhshi et al.ì€ CNNì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì˜¤ë””ì˜¤ ë…¹ìŒì—ì„œ ì§ˆê° ìˆëŠ” ì´ë¯¸ì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. Zeng et al. [156]ì€ ì˜¤ë””ì˜¤ íŒŒì¼ì—ì„œ ìƒì„± ëœ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ì„ ê¸°ëŠ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ë©° LSTMì—ì„œ ì‚¬ìš©í•˜ëŠ” ê²Œì´íŠ¸ ë©”ì»¤ë‹ˆì¦˜ê³¼ ìœ ì‚¬í•œ ResNet ì•„í‚¤í…ì²˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ DL ì ‘ê·¼ ë°©ì‹ì„ ì œì•ˆí•©ë‹ˆë‹¤. Jannat et al. [58]ì€ ì˜¤ë””ì˜¤ ê¸°ëŠ¥ë§Œ ì‚¬ìš©í•˜ì—¬ Inception-v3 ë”¥ëŸ¬ë‹ ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í–‰ë³µê³¼ ìŠ¬í””ì— ëŒ€í•´ 66.41 %ì˜ ì •í™•ë„ (êµì°¨ ê²€ì¦)ë¥¼ ë‹¬ì„±í•˜ëŠ” ë‹¤ì¤‘ ëª¨ë‹¬ ì ‘ê·¼ ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. Sanchez-Gutierrez ë° Gonzalez-Perez [113]ëŠ” ë”¥ëŸ¬ë‹ ë„¤íŠ¸ì›Œí¬ì—ì„œ ìœ ìš©í•œ ë‰´ëŸ° ë…¸ë“œë¥¼ ì‹ë³„í•˜ê³  ì œê±°í•˜ì—¬ ì˜¤ë¥˜ìœ¨ì„ ê°ì†Œì‹œí‚¤ê¸° ìœ„í•´ ì—¬ëŸ¬ íŒë³„ì  ì¸¡ì • ë°©ë²•ì„ ì ìš©í•˜ë©°, Manohar ë° Logashanmugam [84]ì€ ê°ì • ë¶„ë¥˜ì— ëŒ€í•œ ë”¥ëŸ¬ë‹ ë„¤íŠ¸ì›Œí¬ì˜ ì„±ëŠ¥ì„ ë†’ì´ê¸° ìœ„í•œ ê¸°ëŠ¥ ì„ íƒ ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. Wang et al. [145]ëŠ” ì´ë¯¸ì§€ì™€ ì˜¤ë””ì˜¤ ë…¹ìŒìœ¼ë¡œë¶€í„° ê°ì •ì„ ì¸ì‹í•˜ëŠ” ë©€í‹°ëª¨ë‹¬ ì‹œìŠ¤í…œì„ ì œì•ˆí•œë‹¤. íŠ¹íˆ, ì˜¤ë””ì˜¤ ì„œë¸Œì‹œìŠ¤í…œì€ CNNê³¼ LSTM ë„¤íŠ¸ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ë©°, ë…¹ìŒì—ì„œ ìƒì„±ëœ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ì´ë¯¸ì§€ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤. Heredia et al. [50]ëŠ” ì†Œì…œ ë¡œë´‡ì—ì„œ ê°ì •ì„ ê°ì§€í•˜ê¸° ìœ„í•œ ë©€í‹°ëª¨ë‹¬ (ë¹„ë””ì˜¤, ì˜¤ë””ì˜¤ ë° í…ìŠ¤íŠ¸) DL ì•„í‚¤í…ì²˜ë¥¼ ì œì•ˆí•˜ê³  ìˆë‹¤. Conclusion ì´ì œëŠ” ì˜ˆì¸¡ ëª¨ë¸ì˜ ë¹„êµ ë° ìƒˆë¡œìš´ ë°œì „ì„ ì§€íƒ±í•  ë°ì´í„°ì˜ ê°€ìš©ì„±ì´ ê³¼í•™ì˜ í•µì‹¬ ì¤‘ í•˜ë‚˜ì´ë‹¤. SER ë¶„ì•¼ì—ì„œëŠ” ì§€ì—­ì ì¸ ì‘ì€ ê·œëª¨ì˜ ë°ì´í„°ì…‹ì´ ë§ë‹¤. ìµœê·¼ ë°ì´í„°ì…‹ ì¤‘ ì¼ë¶€ëŠ” ì•„ì§ í™œìš©ë˜ì§€ ì•Šì•˜ìœ¼ë©°, ì˜¤ë˜ëœ ë°ì´í„°ì…‹ ì¤‘ ì¼ë¶€ë§Œ í™œìš©ë˜ì–´ ìƒˆë¡œìš´ ë°ì´í„°ì…‹ì´ ì œì•ˆë  ë•Œë§ˆë‹¤ ì˜¤ë˜ë˜ì–´ ìƒˆë¡œìš´ ê²°ë¡ ì´ ë˜ì§€ ì•Šì„ ìˆ˜ ìˆë‹¤. SER ë¶„ì•¼ì—ì„œ ì¤‘ìš”í•œ ì‹ í˜¸ íŠ¹ì„±ê³¼ DL ì•„í‚¤í…ì²˜ì˜ ë³´ê¸‰ì´ ì´ë¯¸ ì‹œì‘ë˜ì—ˆì§€ë§Œ, ì´ëŸ¬í•œ ì ‘ê·¼ ë°©ì‹ì€ ë°ì´í„° ìƒ˜í”Œë§ì˜ ë¯¼ê°ì„± ë° ìœ íš¨ì„± ê²€ì‚¬ì— ëŒ€í•œ ë¶„ì„ê³¼ ê²€ì¦ì´ í•„ìš”í•˜ë‹¤. ë˜í•œ, ì´ ë¶„ì•¼ì—ì„œëŠ” ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜ ë° ê¸°ëŠ¥ì´ ë¹ ë¥´ê²Œ ë‚˜íƒ€ë‚˜ê³  ìˆìœ¼ë¯€ë¡œ, ê³ ê°ê³¼ì˜ ê°ì„±ì  ìƒí˜¸ì‘ìš©ì˜ ë†’ì€ ê°€ì¹˜ ë•Œë¬¸ì— ì„±ëŠ¥ì˜ í° í–¥ìƒì´ ê³§ ë‚˜íƒ€ë‚  ê²ƒìœ¼ë¡œ ê¸°ëŒ€ëœë‹¤","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speech Emotion Recognition","slug":"Paper/Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/categories/Paper/Speech-Emotion-Recognition/"}],"tags":[{"name":"Speech Emotion Recognition","slug":"Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/tags/Speech-Emotion-Recognition/"},{"name":"Speech Feature Extraction","slug":"Speech-Feature-Extraction","permalink":"https://jmj3047.github.io/tags/Speech-Feature-Extraction/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speech Emotion Recognition","slug":"Paper/Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/categories/Paper/Speech-Emotion-Recognition/"}]},{"title":"Imporoving Deep Neural Networks, Hyper parameter tuning, Regularization and Optimization_Quiz","slug":"DL2_Quiz","date":"2023-04-22T15:00:00.000Z","updated":"2023-06-03T13:33:05.137Z","comments":true,"path":"2023/04/23/DL2_Quiz/","link":"","permalink":"https://jmj3047.github.io/2023/04/23/DL2_Quiz/","excerpt":"","text":"ê°œìš”Coursera Deep Learning Course 2 Quiz 1.Practical aspects of Deep Learning Link: https://www.coursera.org/learn/deep-neural-network/home/module/1 2.Optimization Algorithms Link: https://www.coursera.org/learn/deep-neural-network/home/module/2 3.Hyperparameter tuning, Batch Normalization, Programming Frameworks Link: https://www.coursera.org/learn/deep-neural-network/home/module/3","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"Coursera","slug":"Coursera","permalink":"https://jmj3047.github.io/tags/Coursera/"},{"name":"Quiz","slug":"Quiz","permalink":"https://jmj3047.github.io/tags/Quiz/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}]},{"title":"Neural Networks and Deep Learning_Quiz","slug":"DL1_Quiz","date":"2023-04-20T15:00:00.000Z","updated":"2023-06-03T13:33:11.015Z","comments":true,"path":"2023/04/21/DL1_Quiz/","link":"","permalink":"https://jmj3047.github.io/2023/04/21/DL1_Quiz/","excerpt":"","text":"ê°œìš”Coursera Deep Learning Course 1 Quiz 1.Introduction to Deep Learning Link: https://www.coursera.org/learn/neural-networks-deep-learning/home/module/1 2.Neural Network Basics Link: https://www.coursera.org/learn/neural-networks-deep-learning/home/module/2 3.Shallow Neural Networks Link: https://www.coursera.org/learn/neural-networks-deep-learning/home/module/3 4.Key Concepts on Deep Neural Networks Link: https://www.coursera.org/learn/neural-networks-deep-learning/home/module/4","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"Coursera","slug":"Coursera","permalink":"https://jmj3047.github.io/tags/Coursera/"},{"name":"Quiz","slug":"Quiz","permalink":"https://jmj3047.github.io/tags/Quiz/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}]},{"title":"Imporoving Deep Neural Networks, Hyper parameter tuning, Regularization and Optimization","slug":"DL_Part2","date":"2023-04-20T15:00:00.000Z","updated":"2023-06-03T13:34:01.660Z","comments":true,"path":"2023/04/21/DL_Part2/","link":"","permalink":"https://jmj3047.github.io/2023/04/21/DL_Part2/","excerpt":"","text":"Course Lecture 2 in Deep Learning Regularization useful technique for reducing variance. There is a little bit of a bias variance tradeoff when you use regularization. It might increase the bias a little bit, although often not too much if you have a huge enough network. If you suspect your neural network is over fitting your data, that is, you have a high variance problem, one of the first things you should try is probably regularization. The other way to address high variance is to get more training data thatâ€™s also quite reliable. But you canâ€™t always get more training data, or it could be expensive to get more data. But adding regularization will often help to prevent overfitting, or to reduce variance in your network. Why does regularization help with overfitting? Why does it help with reducing variance problems? Simplicity and Generalization: Regularization encourages the model to be simpler by penalizing large weights. Simpler models tend to generalize better to new data because they are less likely to fit to the noise in the training data. This is achieved by adding a penalty term to the loss function that is proportional to the magnitude of the weights (parameters). This penalty term could be the L1 norm (sum of absolute values of weights) or L2 norm (sum of squares of weights) of the model parameters. The L1 norm leads to Lasso regularization and the L2 norm leads to Ridge regularization. Reducing Model Complexity: By penalizing large weights, regularization effectively reduces the modelâ€™s complexity by discouraging learning a highly flexible model, which would lead to overfitting. In this way, it helps in reducing the variance of the model. High variance is a sign of overfitting where the model is too sensitive to the small fluctuations in the training set and hence may not perform well on the unseen data. Feature Selection (for L1 regularization): In the case of L1 regularization, it can shrink some of the weights to zero, effectively performing feature selection. This means the model becomes simpler and more interpretable, and can improve generalization by reducing overfitting to irrelevant features. Dropout Regularization the thing to remember is that drop out is a regularization technique, it helps prevent overfitting. And so unless my avram is overfitting, I wouldnâ€™t actually bother to use drop out. So as you somewhat less often in other application areas, thereâ€™s just a computer vision, you usually just donâ€™t have enough data so you almost always overfitting, which is why they tend to be some computer vision researchers swear by drop out by the intuition. One big downside of drop out is that the cost function J is no longer well defined on every iteration. Youâ€™re randomly, calling off a bunch of notes. Other Regularization: Data Augmentation, Early Stopping Normalizing Inputs: If your input features came from very different scales, maybe some features are from 0-1, sum from 1-1000, then itâ€™s important to normalize your features. If your features came in on similar scales, then this step is less important although performing this type of normalization pretty much never does any harm. Gradient Checking Implementation Notes Optimization exponentially weighted averages bias correction Adam Optimization Algorithm Adam: Adaptive moment estimation Beta_1 is computing the mean of the derivatives. This is called the first moment Beta_2 is used to compute exponentially weighted average of the squares, and thatâ€™s called the second moment. Local Optima first, youâ€™re actually pretty unlikely to get stuck in bad local optima so long as youâ€™re training a reasonably large neural network, save a lot of parameters, and the cost function J is defined over a relatively high dimensional space. But second, that plateaus are a problem and you can actually make learning pretty slow. And this is where algorithms like momentum or RmsProp or Adam can really help your learning algorithm as well. And these are scenarios where more sophisticated observation algorithms, such as Adam, can actually speed up the rate at which you could move down the plateau and then get off the plateau. Hyperparameter Batch norm means that, especially from the perspective of one of the later layers of the neural network, the earlier layers donâ€™t get to shift around as much, because theyâ€™re constrained to have the same mean and variance. And so this makes the job of learning on the later layers easier. It turns out batch norm has a second effect, it has a slight regularization effect. So one non-intuitive thing of a batch norm is that each mini-batch, I will say mini-batch X_t, has the values Z_t, has the values Z_l, scaled by the mean and variance computed on just that one mini-batch.","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Coursera","slug":"Coursera","permalink":"https://jmj3047.github.io/tags/Coursera/"},{"name":"AI Studies","slug":"AI-Studies","permalink":"https://jmj3047.github.io/tags/AI-Studies/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}]},{"title":"wav2vec 2.0, A Framework for Self-Supervised Learning of Speech Representations","slug":"Wav2Vec","date":"2023-04-20T15:00:00.000Z","updated":"2023-04-24T00:29:44.647Z","comments":true,"path":"2023/04/21/Wav2Vec/","link":"","permalink":"https://jmj3047.github.io/2023/04/21/Wav2Vec/","excerpt":"","text":"Journal&#x2F;Conference : IEEEYear(published year): 2020Author: Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael AuliSubject: self-supervised learning, speech representation Self-supervised learningì€ ë ˆì´ë¸”ì´ ì—†ëŠ” ë°ì´í„°ì—ì„œ ì¼ë°˜ì ì¸ ë°ì´í„° í‘œí˜„ì„ í•™ìŠµí•˜ê³ , ì´ë¥¼ ë ˆì´ë¸”ì´ ìˆëŠ” ë°ì´í„°ì—ì„œ ì„¸ë¶€ ì¡°ì •í•˜ëŠ” íŒ¨ëŸ¬ë‹¤ì„ì…ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ìì—°ì–´ ì²˜ë¦¬ ë¶„ì•¼ì—ì„œ íŠ¹íˆ ì„±ê³µì ìœ¼ë¡œ ì ìš©ë˜ì–´ ì™”ìœ¼ë©°, ì»´í“¨í„° ë¹„ì „ ë¶„ì•¼ì—ì„œë„ í™œë°œí•œ ì—°êµ¬ ì£¼ì œ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. Self-supervised learningì€ ì§€ë„ í•™ìŠµê³¼ ë‹¬ë¦¬ ë ˆì´ë¸”ë§ ì‘ì—…ì— ëŒ€í•œ ì¸ë ¥ê³¼ ì‹œê°„ì„ ì ˆì•½í•  ìˆ˜ ìˆìœ¼ë©°, ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì—ì„œ íš¨ê³¼ì ìœ¼ë¡œ ì‘ë™í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. wav2vec 2.0ì€ ìŒì„± ì²˜ë¦¬ ë¶„ì•¼ì—ì„œ self-supervised learningì„ ì‚¬ìš©í•˜ì—¬ ê°•ë ¥í•œ ìŒì„± í‘œí˜„ì„ í•™ìŠµí•˜ëŠ” ë°©ë²• ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. WERëŠ” â€˜ë‹¨ì–´ ì˜¤ë¥˜ ë¹„ìœ¨â€™ì„ ì˜ë¯¸í•˜ëŠ” ìš©ì–´ë¡œ, ìŒì„± ì¸ì‹ ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ë° ìì£¼ ì‚¬ìš©ë©ë‹ˆë‹¤. ì²« í˜ì´ì§€ì˜ ì´ˆë¡ì—ì„œëŠ” wav2vec 2.0ì´ ëª¨ë“  ë ˆì´ë¸” ë°ì´í„°ë¥¼ ì‚¬ìš©í•  ë•Œ Librispeechì˜ ê¹¨ë—í•œ&#x2F;ë‹¤ë¥¸ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ ê°ê° 1.8&#x2F;3.3 WERì„ ë‹¬ì„±í•œë‹¤ê³  ì–¸ê¸‰ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ê²ƒì€ ì‹œìŠ¤í…œì´ ê¹¨ë—í•œ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ 100 ë‹¨ì–´ë‹¹ í‰ê·  1.8ê°œì˜ ì˜¤ë¥˜ì™€ ë‹¤ë¥¸ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ 100 ë‹¨ì–´ë‹¹ í‰ê·  3.3ê°œì˜ ì˜¤ë¥˜ë¥¼ ë§Œë“ ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. Latent representationsì€ ì…ë ¥ ë°ì´í„°ì˜ íŠ¹ì§•ì„ ë‚˜íƒ€ë‚´ëŠ” ë²¡í„°ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ë²¡í„°ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì¸ê³µ ì‹ ê²½ë§ì˜ ì¤‘ê°„ ê³„ì¸µì—ì„œ ì¶”ì¶œë©ë‹ˆë‹¤. wav2vec 2.0ì—ì„œëŠ” ìŒì„± ì…ë ¥ì„ ì ì¬ ê³µê°„ì—ì„œ ë§ˆìŠ¤í‚¹í•˜ê³ , ì´ë¥¼ í•¨ê»˜ í•™ìŠµí•˜ëŠ” ì–‘ìí™”ëœ ì ì¬ í‘œí˜„ì„ ì‚¬ìš©í•˜ì—¬ ëŒ€ì¡°ì ì¸ ì‘ì—…ì„ í•´ê²°í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ wav2vec 2.0ì€ ë ˆì´ë¸”ì´ ì—†ëŠ” ìŒì„± ë°ì´í„°ë¡œë¶€í„° ê°•ë ¥í•œ ìŒì„± í‘œí˜„ì„ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì ì¬ ê³µê°„ì€ ì¸ê³µ ì‹ ê²½ë§ì˜ hidden layerì™€ ìœ ì‚¬í•œ ê°œë…ì…ë‹ˆë‹¤. ì¸ê³µ ì‹ ê²½ë§ì—ì„œ ì…ë ¥ ë°ì´í„°ëŠ” ì—¬ëŸ¬ ê°œì˜ hidden layerë¥¼ ê±°ì³ ì¶œë ¥ì¸µìœ¼ë¡œ ì „ë‹¬ë©ë‹ˆë‹¤. ì´ë•Œ hidden layerì—ì„œ ì¶”ì¶œëœ ë²¡í„°ë¥¼ ì ì¬ ê³µê°„ì´ë¼ê³  ë¶€ë¦…ë‹ˆë‹¤. wav2vec 2.0ì—ì„œë„ ìŒì„± ì…ë ¥ì€ ì—¬ëŸ¬ ê°œì˜ hidden layerë¥¼ ê±°ì³ ì ì¬ ê³µê°„ì—ì„œ ë§ˆìŠ¤í‚¹ë˜ê³ , ì´ë¥¼ í•¨ê»˜ í•™ìŠµí•˜ëŠ” ì–‘ìí™”ëœ ì ì¬ í‘œí˜„ì„ ì‚¬ìš©í•˜ì—¬ ëŒ€ì¡°ì ì¸ ì‘ì—…ì„ í•´ê²°í•©ë‹ˆë‹¤. wav2vec 2.0ì—ì„œëŠ” ìŒì„± ì…ë ¥ì„ ì ì¬ ê³µê°„ì—ì„œ ë§ˆìŠ¤í‚¹í•˜ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ë ˆì´ë¸”ì´ ì—†ëŠ” ë°ì´í„°ë¡œë¶€í„° ê°•ë ¥í•œ ìŒì„± í‘œí˜„ì„ í•™ìŠµí•˜ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ wav2vec 2.0ì€ ì…ë ¥ ìŒì„± ì‹ í˜¸ë¥¼ ì ì¬ ê³µê°„ìœ¼ë¡œ ë³€í™˜í•˜ê³ , ì¼ë¶€ ì ì¬ ë²¡í„°ë¥¼ ë¬´ì‘ìœ„ë¡œ ë§ˆìŠ¤í‚¹í•©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ, ëª¨ë¸ì€ ë§ˆìŠ¤í‚¹ëœ ë²¡í„°ë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í›ˆë ¨ë©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œ wav2vec 2.0ì€ ë ˆì´ë¸”ì´ ì—†ëŠ” ë°ì´í„°ì—ì„œë„ ìœ íš¨í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. wav2vec 2.0 ëª¨ë¸ì€ ë‹¤ìŒê³¼ ê°™ì€ ë‹¨ê³„ë¡œ ë°ì´í„°ê°€ í˜ëŸ¬ê°‘ë‹ˆë‹¤: ì…ë ¥ ë°ì´í„°ì¸ raw audio XëŠ” multi-layer convolutional feature encoder fë¥¼ í†µí•´ ì ì¬ ê³µê°„ì˜ ë²¡í„° z1, â€¦, zTë¡œ ë³€í™˜ë©ë‹ˆë‹¤. ì ì¬ ë²¡í„°ë“¤ z1, â€¦, zTëŠ” Transformer gë¥¼ í†µí•´ c1, â€¦, cTë¡œ ë³€í™˜ë©ë‹ˆë‹¤. ì´ë•Œ, c1, â€¦, cTëŠ” ì „ì²´ ì‹œí€€ìŠ¤ [9, 5, 4]ì—ì„œ ì •ë³´ë¥¼ ìº¡ì²˜í•˜ëŠ” ë²¡í„°ì…ë‹ˆë‹¤. feature encoderì˜ ì¶œë ¥ì€ ì–‘ìí™” ëª¨ë“ˆ Zë¥¼ í†µí•´ qtë¡œ ì´ì‚°í™”ë©ë‹ˆë‹¤. ì´ë ‡ê²Œ ì–»ì–´ì§„ qtëŠ” self-supervised objective (Â§ 3.2)ì—ì„œ targetì„ ë‚˜íƒ€ë‚´ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œ wav2vec 2.0ì€ ë ˆì´ë¸”ì´ ì—†ëŠ” ë°ì´í„°ì—ì„œë„ ìœ íš¨í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, wav2vec 2.0ì€ vq-wav2vec [5]ì™€ ë¹„êµí•˜ì—¬ ì—°ì†ì ì¸ ìŒì„± í‘œí˜„ì— ëŒ€í•œ context representationsì„ êµ¬ì¶•í•˜ê³  self-attentionì„ ì‚¬ìš©í•˜ì—¬ ì „ì²´ ì‹œí€€ìŠ¤ì˜ ì¢…ì†ì„±ì„ ìº¡ì²˜í•©ë‹ˆë‹¤. ì „ì²´ ì‹œí€€ìŠ¤ì—ì„œ ì •ë³´ë¥¼ ìº¡ì²˜í•œë‹¤ëŠ” ê²ƒì€, ì…ë ¥ ë°ì´í„°ì˜ ì „ì²´ ì‹œí€€ìŠ¤ì— ëŒ€í•œ ì •ë³´ë¥¼ ëª¨ë¸ì´ í•™ìŠµí•˜ê³  ì´ë¥¼ ì˜ ë°˜ì˜í•˜ì—¬ ì¶œë ¥ì„ ìƒì„±í•œë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. wav2vec 2.0 ëª¨ë¸ì—ì„œëŠ” ì…ë ¥ ë°ì´í„°ì¸ raw audio Xê°€ feature encoderë¥¼ í†µí•´ ì ì¬ ê³µê°„ì˜ ë²¡í„° z1, â€¦, zTë¡œ ë³€í™˜ë˜ê³ , ì´ ë²¡í„°ë“¤ì´ Transformerë¥¼ í†µí•´ c1, â€¦, cTë¡œ ë³€í™˜ë©ë‹ˆë‹¤. ì´ë•Œ, c1, â€¦, cTëŠ” ì „ì²´ ì‹œí€€ìŠ¤ [9, 5, 4]ì—ì„œ ì •ë³´ë¥¼ ìº¡ì²˜í•˜ëŠ” ë²¡í„°ì…ë‹ˆë‹¤. ë”°ë¼ì„œ wav2vec 2.0 ëª¨ë¸ì€ ì…ë ¥ ë°ì´í„°ì˜ ì „ì²´ ì‹œí€€ìŠ¤ì— ëŒ€í•œ ì •ë³´ë¥¼ ì˜ ë°˜ì˜í•˜ì—¬ ì¶œë ¥ì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì–‘ìí™” ëª¨ë“ˆ(Quantization module)ì€ wav2vec 2.0 ëª¨ë¸ì—ì„œ self-supervised trainingì„ ìœ„í•´ ì‚¬ìš©ë˜ëŠ”ë°, ì´ ëª¨ë“ˆì€ feature encoderì˜ ì¶œë ¥ì¸ z ë²¡í„°ë¥¼ ìœ í•œí•œ speech representations ì§‘í•©ìœ¼ë¡œ ì´ì‚°í™”(discretize)í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì´ì‚°í™”ëœ ë²¡í„°ë“¤ì€ self-supervised objective (Â§ 3.2)ì—ì„œ íƒ€ê²Ÿì„ ë‚˜íƒ€ë‚´ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. wav2vec 2.0 ëª¨ë¸ì—ì„œëŠ” product quantization [25]ì´ë¼ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ z ë²¡í„°ë¥¼ ì´ì‚°í™”í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ë²¡í„° ê³µê°„ì„ ì—¬ëŸ¬ ê°œì˜ ì„œë¸Œ ê³µê°„ìœ¼ë¡œ ë¶„í• í•˜ê³ , ê° ì„œë¸Œ ê³µê°„ì— ëŒ€í•´ centroidë¥¼ ê³„ì‚°í•˜ì—¬ í•´ë‹¹ ì„œë¸Œ ê³µê°„ ë‚´ì˜ ì ë“¤ì„ ê°€ì¥ ê°€ê¹Œìš´ centroidì™€ ë§¤ì¹­ì‹œí‚¤ëŠ” ë°©ì‹ìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ ìƒì„±ëœ ë§¤ì¹­ëœ centroidë“¤ì˜ ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ z ë²¡í„°ë¥¼ ì´ì‚°í™”í•©ë‹ˆë‹¤. z ë²¡í„°ë¥¼ ìœ í•œí•œ speech representations ì§‘í•©ìœ¼ë¡œ ì´ì‚°í™”í•œë‹¤ëŠ” ê²ƒì€, ì—°ì†ì ì¸ ê°’ì„ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ë‚˜ëˆ„ì–´ì„œ ìœ í•œí•œ speech representations ì§‘í•©ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë³€í™˜ì€ self-supervised objective (Â§ 3.2)ì—ì„œ íƒ€ê²Ÿì„ ë‚˜íƒ€ë‚´ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. Feature encoderëŠ” ì…ë ¥ ë°ì´í„°ì¸ raw audio Xë¥¼ ì ì¬ ê³µê°„ì˜ ë²¡í„° z1, â€¦, zTë¡œ ë³€í™˜í•˜ëŠ”ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ encoderëŠ” ì—¬ëŸ¬ ë¸”ë¡ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, ê° ë¸”ë¡ì€ ì‹œê°„ì  ì»¨ë³¼ë£¨ì…˜(temporal convolution)ì„ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´í›„ layer normalization [1]ê³¼ GELU activation function [21]ì´ ì ìš©ë©ë‹ˆë‹¤. ë˜í•œ, Feature encoderì— ì…ë ¥ë˜ëŠ” raw waveformì€ zero meanê³¼ unit varianceë¡œ ì •ê·œí™”ë©ë‹ˆë‹¤. ì´ë ‡ê²Œ ì •ê·œí™”ëœ ì…ë ¥ ë°ì´í„°ëŠ” Encoderì˜ ì´ strideì— ë”°ë¼ Tê°œì˜ time-stepsë¡œ Transformer (Â§ 4.2)ì— ì…ë ¥ë©ë‹ˆë‹¤. ë”°ë¼ì„œ Feature encoderëŠ” ì…ë ¥ ë°ì´í„°ë¥¼ ì˜ ì²˜ë¦¬í•˜ì—¬ Transformerì— ì „ë‹¬í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. Contextualized representations with TransformersëŠ” wav2vec 2.0 ëª¨ë¸ì—ì„œ ì‚¬ìš©ë˜ëŠ” ê¸°ìˆ  ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. ì´ ê¸°ìˆ ì€ feature encoderì˜ ì¶œë ¥ì¸ z ë²¡í„°ë¥¼ context networkë¡œ ì „ë‹¬í•˜ì—¬, ì…ë ¥ ë°ì´í„°ì˜ ì „ì²´ ì‹œí€€ìŠ¤ì— ëŒ€í•œ ì •ë³´ë¥¼ ì˜ ë°˜ì˜í•˜ëŠ” ë²¡í„° c1, â€¦, cTë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì´ë•Œ, context networkëŠ” Transformer architecture [55, 9, 33]ì„ ë”°ë¦…ë‹ˆë‹¤. TransformerëŠ” ì…ë ¥ ë°ì´í„°ì˜ ì „ì²´ ì‹œí€€ìŠ¤ì— ëŒ€í•œ ì •ë³´ë¥¼ ì˜ ë°˜ì˜í•  ìˆ˜ ìˆëŠ” êµ¬ì¡°ë¡œ, ì´ì „ì˜ RNNê³¼ ê°™ì€ ëª¨ë¸ë³´ë‹¤ ë”ìš± íš¨ê³¼ì ìœ¼ë¡œ ì…ë ¥ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Transformer architectureëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸ ì¤‘ í•˜ë‚˜ë¡œ, ì…ë ¥ ë°ì´í„°ì˜ ì „ì²´ ì‹œí€€ìŠ¤ì— ëŒ€í•œ ì •ë³´ë¥¼ ì˜ ë°˜ì˜í•  ìˆ˜ ìˆëŠ” êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì´ êµ¬ì¡°ëŠ” RNNê³¼ ê°™ì€ ëª¨ë¸ë³´ë‹¤ ë”ìš± íš¨ê³¼ì ìœ¼ë¡œ ì…ë ¥ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Transformer architectureëŠ” self-attention mechanismì„ ì‚¬ìš©í•˜ì—¬ ì…ë ¥ ë°ì´í„°ì˜ ì „ì²´ ì‹œí€€ìŠ¤ì— ëŒ€í•œ ì •ë³´ë¥¼ ì˜ ë°˜ì˜í•©ë‹ˆë‹¤. Self-attention mechanismì€ ì…ë ¥ ë°ì´í„° ë‚´ì—ì„œ ì„œë¡œ ë‹¤ë¥¸ ìœ„ì¹˜ì˜ ì •ë³´ë¥¼ ìƒí˜¸ì‘ìš©ì‹œì¼œì„œ, í•´ë‹¹ ìœ„ì¹˜ì˜ ì •ë³´ê°€ ë‹¤ë¥¸ ìœ„ì¹˜ì—ì„œ ì–´ë–»ê²Œ ì‚¬ìš©ë˜ëŠ”ì§€ í•™ìŠµí•©ë‹ˆë‹¤. Transformer architectureëŠ” í¬ê²Œ ë‘ ê°€ì§€ ë¶€ë¶„ìœ¼ë¡œ ë‚˜ë‰©ë‹ˆë‹¤. ì²« ë²ˆì§¸ ë¶€ë¶„ì€ encoderì´ë©°, ì…ë ¥ ë°ì´í„°ë¥¼ ì„ë² ë”©í•˜ê³  ì—¬ëŸ¬ ì¸µì˜ self-attentionê³¼ feed-forward network layerë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. ë‘ ë²ˆì§¸ ë¶€ë¶„ì€ decoderì´ë©°, encoderì—ì„œ ìƒì„±ëœ ë²¡í„°ë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ ì¶œë ¥ ì‹œí€€ìŠ¤ë¥¼ ìƒì„±í•˜ëŠ”ë° ì‚¬ìš©ë©ë‹ˆë‹¤. Transformer architectureëŠ” ìì—°ì–´ ì²˜ë¦¬ ë¶„ì•¼ì—ì„œ ë§ì´ ì‚¬ìš©ë˜ë©°, íŠ¹íˆ ê¸°ê³„ ë²ˆì—­ ë¶„ì•¼ì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. wav2vec 2.0 ëª¨ë¸ì—ì„œë„ Transformer architectureê°€ feature encoderì™€ context networkì— ì ìš©ë˜ì–´, ì…ë ¥ ë°ì´í„°ì˜ ì „ì²´ ì‹œí€€ìŠ¤ì— ëŒ€í•œ ì •ë³´ë¥¼ ì˜ ë°˜ì˜í•˜ëŠ” ë²¡í„°ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤. ë˜í•œ, wav2vec 2.0 ëª¨ë¸ì—ì„œëŠ” absolute positional informationì„ ì¸ì½”ë”©í•˜ëŠ” ê³ ì •ëœ positional embeddings ëŒ€ì‹  relative positional embeddingì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ relative positional embeddingì€ convolutional layerì™€ ìœ ì‚¬í•˜ê²Œ ë™ì‘í•˜ë©° [37, 4, 57], ì…ë ¥ ë°ì´í„°ì˜ ìƒëŒ€ì ì¸ ìœ„ì¹˜ ì •ë³´ë¥¼ ì¸ì½”ë”©í•©ë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, convolutional layerì˜ ì¶œë ¥ê°’ì— GELU activation functionì„ ì ìš©í•˜ê³  ì…ë ¥ê°’ì— ë”í•´ì¤€ í›„ layer normalizationì„ ì ìš©í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ê³¼ì •ì„ ê±°ì¹˜ë©´ì„œ ìƒì„±ëœ ë²¡í„° c1, â€¦, cTëŠ” ì…ë ¥ ë°ì´í„°ì˜ ì „ì²´ ì‹œí€€ìŠ¤ì— ëŒ€í•œ ì •ë³´ë¥¼ ì˜ ë°˜ì˜í•˜ê³  ìˆìŠµë‹ˆë‹¤. Quantization moduleì€ wav2vec 2.0 ëª¨ë¸ì—ì„œ ì‚¬ìš©ë˜ëŠ” ê¸°ìˆ  ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. ì´ ëª¨ë“ˆì€ self-supervised trainingì„ ìœ„í•´ feature encoderì˜ ì¶œë ¥ì¸ z ë²¡í„°ë¥¼ ì´ì‚°í™”(discretize)í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤ Quantization moduleì€ product quantization [25]ì´ë¼ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ z ë²¡í„°ë¥¼ ì´ì‚°í™”í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ Gê°œì˜ codebook(ë˜ëŠ” group)ì„ ì‚¬ìš©í•˜ë©°, ê° codebookì€ Vê°œì˜ entryë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. ë”°ë¼ì„œ, ê° codebookì—ì„œ í•˜ë‚˜ì˜ entryë¥¼ ì„ íƒí•˜ê³ , ì„ íƒëœ entryë“¤ì„ concatenateí•˜ì—¬ í•˜ë‚˜ì˜ ë²¡í„° që¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ê³¼ì •ìœ¼ë¡œ ìƒì„±ëœ q ë²¡í„°ëŠ” ì…ë ¥ ë°ì´í„° Xì— ëŒ€í•œ ì ì¬ì ì¸ í‘œí˜„(representation)ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ë•Œ, q ë²¡í„°ëŠ” ì—°ì†ì ì¸ ê°’ì„ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ë‚˜ëˆ„ì–´ì„œ ìœ í•œí•œ speech representations ì§‘í•©ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. Quantization moduleì€ wav2vec 2.0 ëª¨ë¸ì—ì„œ self-supervised objective (Â§ 3.2)ì—ì„œ íƒ€ê²Ÿì„ ë‚˜íƒ€ë‚´ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ì¦‰, ì…ë ¥ ë°ì´í„° Xì™€ q ë²¡í„° ì‚¬ì´ì˜ ê´€ê³„ë¥¼ í•™ìŠµí•˜ì—¬, ì…ë ¥ ë°ì´í„° Xì— ëŒ€í•œ ì ì¬ì ì¸ í‘œí˜„(representation)ì„ ìƒì„±í•˜ëŠ” ë° í™œìš©ë©ë‹ˆë‹¤. wav2vec 2.0 ëª¨ë¸ì˜ trainingì€ í¬ê²Œ pre-trainingê³¼ fine-tuning ë‘ ë‹¨ê³„ë¡œ ë‚˜ë‰©ë‹ˆë‹¤. ì´ ë‘ ë‹¨ê³„ì—ì„œ ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ëŠ” ëª¨ë‘ self-supervised learningì„ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤. Pre-training ë‹¨ê³„ì—ì„œëŠ”, ì…ë ¥ ë°ì´í„° Xë¥¼ ì´ìš©í•˜ì—¬ feature encoderì™€ context networkë¥¼ í•™ìŠµí•©ë‹ˆë‹¤. ì´ë•Œ, ì…ë ¥ ë°ì´í„° XëŠ” ì¼ì •í•œ ë¹„ìœ¨ë¡œ time stepsê°€ maskë˜ì–´ ìˆìŠµë‹ˆë‹¤. Maskingëœ time stepsì— ëŒ€í•´ì„œ, ëª¨ë¸ì€ í•´ë‹¹ time stepì˜ quantized latent audio representationì„ ì‹ë³„í•´ì•¼ í•©ë‹ˆë‹¤. Fine-tuning ë‹¨ê³„ì—ì„œëŠ”, labeled dataë¥¼ ì´ìš©í•˜ì—¬ pre-trained modelì„ fine-tuningí•©ë‹ˆë‹¤. Fine-tuningì— ì‚¬ìš©ë˜ëŠ” labeled dataëŠ” speech recognition taskë‚˜ speaker identification task ë“±ê³¼ ê°™ì€ downstream taskì—ì„œ ìˆ˜ì§‘ëœ ë°ì´í„°ì…ë‹ˆë‹¤. ë”°ë¼ì„œ, wav2vec 2.0 ëª¨ë¸ì˜ training ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. Pre-training: ì…ë ¥ ë°ì´í„° Xë¥¼ ì´ìš©í•˜ì—¬ feature encoderì™€ context networkë¥¼ í•™ìŠµí•©ë‹ˆë‹¤. Masked language modeling ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬, ì¼ë¶€ time stepsê°€ maskë©ë‹ˆë‹¤. Maskingëœ time stepsì— ëŒ€í•´ì„œ, ëª¨ë¸ì€ í•´ë‹¹ time stepì˜ quantized latent audio representationì„ ì‹ë³„í•´ì•¼ í•©ë‹ˆë‹¤. Fine-tuning: labeled dataë¥¼ ì´ìš©í•˜ì—¬ pre-trained modelì„ fine-tuningí•©ë‹ˆë‹¤. Downstream taskì—ì„œ ìˆ˜ì§‘ëœ labeled dataë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. Fine-tuned modelì€ downstream taskì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. quantized latent audio representationì„ ì‹ë³„í•´ì•¼ í•œë‹¤ëŠ” ê²ƒì€, masked time stepì—ì„œ ì…ë ¥ ë°ì´í„° Xì— ëŒ€í•œ feature encoderì˜ ì¶œë ¥ì¸ z ë²¡í„°ë¥¼ ì´ì‚°í™”í•˜ì—¬ ìƒì„±ëœ q ë²¡í„° ì¤‘ì—ì„œ, ì •í™•íˆ ì–´ë–¤ q ë²¡í„°ê°€ í•´ë‹¹ masked time stepì— ëŒ€í•œ ì˜¬ë°”ë¥¸ í‘œí˜„(representation)ì¸ì§€ë¥¼ ì‹ë³„í•´ì•¼ í•œë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì¦‰, wav2vec 2.0 ëª¨ë¸ì€ pre-training ë‹¨ê³„ì—ì„œ ì…ë ¥ ë°ì´í„° Xì˜ ì¼ë¶€ time stepsë¥¼ maskí•˜ê³ , ì´ëŸ¬í•œ masked time stepsì— ëŒ€í•´ì„œ ì˜¬ë°”ë¥¸ quantized latent audio representationì„ ì‹ë³„í•˜ëŠ” ë° ì´ˆì ì„ ë‘¡ë‹ˆë‹¤. ì´ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œ ëª¨ë¸ì€ ì…ë ¥ ë°ì´í„° Xì— ëŒ€í•œ ì ì¬ì ì¸ í‘œí˜„(representation)ì„ í•™ìŠµí•˜ê²Œ ë©ë‹ˆë‹¤. ì´ëŸ¬í•œ pre-training ë‹¨ê³„ì—ì„œ í•™ìŠµëœ ëª¨ë¸ì€ fine-tuning ë‹¨ê³„ì—ì„œ downstream taskì— ì ìš©ë©ë‹ˆë‹¤. Fine-tuning ë‹¨ê³„ì—ì„œëŠ” labeled dataë¥¼ ì´ìš©í•˜ì—¬ pre-trained modelì„ fine-tuningí•˜ë©°, ì´ë•Œ ëª¨ë¸ì€ downstream taskì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ë„ë¡ í•™ìŠµë©ë‹ˆë‹¤ ì…ë ¥ ë°ì´í„°ê°€ context networkë¥¼ í•™ìŠµí•œë‹¤ëŠ” ê²ƒì€, wav2vec 2.0 ëª¨ë¸ì—ì„œ ì…ë ¥ ë°ì´í„° Xì˜ ì ì¬ì ì¸ í‘œí˜„(representation)ì„ ìƒì„±í•˜ê¸° ìœ„í•´ context networkê°€ ì‚¬ìš©ëœë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. Context networkëŠ” feature encoderì˜ ì¶œë ¥ì¸ z ë²¡í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„, ì „ì²´ ì‹œí€€ìŠ¤ì— ëŒ€í•œ ì •ë³´ë¥¼ ì˜ ë°˜ì˜í•˜ëŠ” c ë²¡í„°ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤. ì´ë•Œ, c ë²¡í„°ë“¤ì€ Transformer architectureì™€ í•¨ê»˜ ì‚¬ìš©ë˜ì–´ ì…ë ¥ ë°ì´í„° Xì˜ ì „ì²´ ì‹œí€€ìŠ¤ì— ëŒ€í•œ ì •ë³´ë¥¼ ì˜ ë°˜ì˜í•˜ëŠ” ë²¡í„°ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤. ë”°ë¼ì„œ, wav2vec 2.0 ëª¨ë¸ì—ì„œ ì…ë ¥ ë°ì´í„° XëŠ” feature encoderì™€ context networkë¥¼ í†µí•´ ì ì¬ì ì¸ í‘œí˜„(representation)ìœ¼ë¡œ ë³€í™˜ë©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œ ëª¨ë¸ì€ self-supervised learningì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ì—¬ ì…ë ¥ ë°ì´í„° Xì— ëŒ€í•œ ì¢‹ì€ í‘œí˜„(representation)ì„ í•™ìŠµí•˜ê²Œ ë©ë‹ˆë‹¤. link : https://arxiv.org/abs/2006.11477","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speech Representations","slug":"Paper/Speech-Representations","permalink":"https://jmj3047.github.io/categories/Paper/Speech-Representations/"}],"tags":[{"name":"Self-Supervised Learning","slug":"Self-Supervised-Learning","permalink":"https://jmj3047.github.io/tags/Self-Supervised-Learning/"},{"name":"Speech Representations","slug":"Speech-Representations","permalink":"https://jmj3047.github.io/tags/Speech-Representations/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speech Representations","slug":"Paper/Speech-Representations","permalink":"https://jmj3047.github.io/categories/Paper/Speech-Representations/"}]},{"title":"The Geneva Minimalistic Acoustic Parameter Set (GeMAPS) for Voice Research andAffective Computing","slug":"eGeMAPS","date":"2023-04-20T15:00:00.000Z","updated":"2023-04-20T21:06:58.521Z","comments":true,"path":"2023/04/21/eGeMAPS/","link":"","permalink":"https://jmj3047.github.io/2023/04/21/eGeMAPS/","excerpt":"","text":"Journal&#x2F;Conference : IEEEYear(published year): 2016Author: Florian Eyben, Klaus R. Scherer, Bjorn W. Schuller, Johan Sundberg, Elisabeth Andre, Carlos Busso, Laurence Y. Devillers, Julien Epps, Petri Laukka, Shrikanth S. Narayanan, and Khiet P. TruongSubject: Acoustic Parameter Set, eGeMAPS Summary ì´ ë…¼ë¬¸ì€ ìŒì„± íŠ¹ì§• ì¶”ì¶œì„ ìœ„í•œ ìµœì†Œí•œì˜ ë§¤ê°œ ë³€ìˆ˜ ì„¸íŠ¸ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì œì•ˆëœ ë§¤ê°œ ë³€ìˆ˜ ì„¸íŠ¸ëŠ” ë‹¤ì–‘í•œ ê°ì • ë¶„ë¥˜ ì‘ì—…ì—ì„œ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. binary arousal ë° binary valence ë¶„ë¥˜ì— ëŒ€í•œ ìš”ì•½ ê²°ê³¼ê°€ Table 2ì— ë‚˜ì™€ ìˆìŠµë‹ˆë‹¤. FAU AIBOë¥¼ ì œì™¸í•œ ëª¨ë“  ë°ì´í„°ë² ì´ìŠ¤ì™€ 9ê°œì˜ ìµœê³  SVM ë³µì¡ë„(C&#x3D;0.0025)ì—ì„œ UAR(Unweighted Average Recall)ì„ í‰ê· í™”í•©ë‹ˆë‹¤. ê° í™”ìì— ëŒ€í•œ í‘œì¤€í™”ì™€ ê· í˜• ì¡íŒ í›ˆë ¨ ì„¸íŠ¸ë¥¼ ìœ„í•´ ì¸ìŠ¤í„´ìŠ¤ ì—…ìƒ˜í”Œë§ì´ ìˆ˜í–‰ë©ë‹ˆë‹¤. 88ê°œ íŒŒë¼ë¯¸í„° ì •ë¦¬ ê¸°ë³¸ GeMAPS, 62ê°œ íŒŒë¼ë¯¸í„° Pitch Jitter Formant 1 frequency Formant 2 frequency Formant 3 frequency Formant 1 Shimmer Loudness Harmonics-to-noise ratio (HNR) Alpha Ratio Hammarberg Index Spectral Slope 0-500 Hz Spectral Slope 500-1500 Hz Formant 1 relative energy Formant 2 relative energy Formant 3 relative energy Harmonic difference H1-H2 Harmonic difference H1-A3 ìœ„ 18ê°œì˜ ì‚°ìˆ í‰ê· ê³¼ ë³€ë™ê³„ìˆ˜ë¥¼ ì ìš© -&gt; 36ê°œ + loudness 8ê°€ì§€ í•¨ìˆ˜ ì¶”ê°€ ì ìš© íŒŒë¼ë¯¸í„°+ pitch 8ê°€ì§€ í•¨ìˆ˜ ì¶”ê°€ ì ìš© íŒŒë¼ë¯¸í„° -&gt; 52ê°œ Alpha Ratioì˜ ì‚°ìˆ í‰ê·  Hammarberg Indexì˜ ì‚°ìˆ í‰ê·  0-500 Hz ìŠ¤í™íŠ¸ëŸ¼ ê¸°ìš¸ê¸°ì˜ ì‚°ìˆ í‰ê·  (ë¬´ì„±ìŒ êµ¬ê°„ ì „ì²´) 500-1500 Hz ìŠ¤í™íŠ¸ëŸ¼ ê¸°ìš¸ê¸°ì˜ ì‚°ìˆ í‰ê·  (ë¬´ì„±ìŒ êµ¬ê°„ ì „ì²´) the rate of loudness peaks continuously voiced regionsì˜ í‰ê·  ê¸¸ì´(F0 &gt; 0) continuously voiced regionsì˜ í‘œì¤€í¸ì°¨(F0 &gt; 0) unvoiced regions (approximating pauses)ì˜ í‰ê·  ê¸¸ì´(F0 &#x3D; 0) unvoiced regions (approximating pauses)ì˜ í‘œì¤€í¸ì°¨(F0 &#x3D; 0) continuous voiced regionsì˜ ì´ˆë‹¹ ê°œìˆ˜ (pseudo syllable rate) extended GeMAPS(eGeMAPS), ì¶”ê°€ 26ê°œ íŒŒë¼ë¯¸í„° 7ê°œì˜ LLDì— ëŒ€í•´ ì‚°ìˆ í‰ê· ê³¼ ë³€ë™ê³„ìˆ˜ ì ìš© -&gt; 14ê°œì˜ discriptor ì¶”ê°€ MFCC 1 (MFCCì˜ ì²«ë²ˆì§¸ ê³„ìˆ˜) MFCC 2 MFCC 3 MFCC 4 Spectral flux Formant 2 bandwidth Formant 2 bandwidth 11ê°œì˜ discriptorê°€ ì¶”ê°€ ë¬´ì„±ì˜ì—­ì—ì„œë§Œ spectral fluxì˜ ì‚°ìˆ  í‰ê·  ìœ ì„±ì˜ì—­ì—ì„œë§Œ spectral fluxì˜ ì‚°ìˆ  í‰ê·  ìœ ì„±ì˜ì—­ì—ì„œë§Œ spectral fluxì˜ ë³€ë™ ê³„ìˆ˜ ìœ ì„± ì˜ì—­ì—ì„œë§Œ MFCC 1ì˜ ë³€ë™ ê³„ìˆ˜ ìœ ì„± ì˜ì—­ì—ì„œë§Œ MFCC 2ì˜ ë³€ë™ ê³„ìˆ˜ ìœ ì„± ì˜ì—­ì—ì„œë§Œ MFCC 3ì˜ ë³€ë™ ê³„ìˆ˜ ìœ ì„± ì˜ì—­ì—ì„œë§Œ MFCC 4ì˜ ë³€ë™ ê³„ìˆ˜ ìœ ì„± ì˜ì—­ì—ì„œë§Œ MFCC 1ì˜ ì‚°ìˆ  í‰ê·  ìœ ì„± ì˜ì—­ì—ì„œë§Œ MFCC 2ì˜ ì‚°ìˆ  í‰ê·  ìœ ì„± ì˜ì—­ì—ì„œë§Œ MFCC 3ì˜ ì‚°ìˆ  í‰ê·  ìœ ì„± ì˜ì—­ì—ì„œë§Œ MFCC 4ì˜ ì‚°ìˆ  í‰ê·  equivalent soud level ì´ë ‡ê²Œ í•˜ë©´ í™•ì¥ëœ íŒŒë¼ë¯¸í„°ì—ëŠ” 14+11+1 26ê°œì˜ íŒŒë¼ë¯¸í„°ê°€ ì¶”ê°€ ë¨ 0.abstract GeMAPSëŠ” ìŒì„±ê³¼ ê°ì • ì»´í“¨íŒ… ë¶„ì•¼ì—ì„œ ì‚¬ìš©ë˜ëŠ” ê¸°ë³¸ í‘œì¤€ ìŒí–¥ íŒŒë¼ë¯¸í„° ì„¸íŠ¸ì…ë‹ˆë‹¤. ì´ ì„¸íŠ¸ëŠ” ë‹¤ì–‘í•œ ìë™ ìŒì„± ë¶„ì„ ì˜ì—­ì— ì ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. GeMAPSëŠ” ê°ì •ì  ìƒë¦¬ì  ë³€í™”ë¥¼ ìƒ‰ì¸í™”í•˜ëŠ” ë° ì ì¬ë ¥ì´ ìˆìŠµë‹ˆë‹¤. ì´ì „ ì—°êµ¬ì—ì„œ ì…ì¦ëœ ê°€ì¹˜ì™€ ì´ë¡ ì  ì¤‘ìš”ì„±ì„ ê°€ì§€ë©°, ìë™ ì¶”ì¶œ ê°€ëŠ¥í•©ë‹ˆë‹¤. GeMAPSëŠ” ë¯¸ë˜ ì—°êµ¬ í‰ê°€ì˜ ê³µí†µ ê¸°ì¤€ì„ ì œê³µí•˜ê³ , ë‹¤ë¥¸ íŒŒë¼ë¯¸í„° ì„¸íŠ¸ ë˜ëŠ” ë™ì¼í•œ íŒŒë¼ë¯¸í„°ì˜ ë‹¤ë¥¸ êµ¬í˜„ìœ¼ë¡œ ì¸í•œ ì°¨ì´ë¥¼ ì œê±°í•˜ê¸° ìœ„í•´ ë§Œë“¤ì–´ì¡ŒìŠµë‹ˆë‹¤. 1. Introduction ì´ ë‹¨ë½ì—ì„œëŠ” ë‹¤ì–‘í•œ ê°ì • ìƒíƒœì˜ ìŒì„± í‘œí˜„ì— ëŒ€í•œ ê´€ì‹¬ì´ ì˜¤ë«ë™ì•ˆ ì§€ì†ë˜ì–´ ì™”ìœ¼ë©°, ë‹¤ì–‘í•œ ë¶„ì•¼ì˜ ì—°êµ¬ìë“¤ì´ ì´ ì£¼ì œì— ëŒ€í•´ ì—°êµ¬í•´ ì™”ë‹¤ê³  ì„¤ëª…í•©ë‹ˆë‹¤. ì •ì‹ ê³¼ ì˜ì‚¬ëŠ” ê°ì • ìƒíƒœë¥¼ ì§„ë‹¨í•˜ê¸° ìœ„í•´ ë…¸ë ¥í•´ ì™”ìœ¼ë©°, ì‹¬ë¦¬í•™ìì™€ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ì—°êµ¬ìë“¤ì€ ê°ì •ì˜ ì‹ í˜¸ë¥¼ ì „ë‹¬í•˜ëŠ” ëª©ì†Œë¦¬ì˜ ëŠ¥ë ¥ì„ íƒêµ¬í•´ ì™”ìŠµë‹ˆë‹¤. ì–¸ì–´í•™ìì™€ ìŒì„±í•™ìë“¤ì€ ì–¸ì–´ ìƒì‚°ê³¼ ì§€ê°ì—ì„œ ì •ì„œì  ì‹¤ìš©ì  ì •ë³´ì˜ ì—­í• ì„ ë°œê²¬í•´ ì™”ìŠµë‹ˆë‹¤. ìµœê·¼ì—ëŠ” ì»´í“¨í„° ê³¼í•™ìì™€ ì—”ì§€ë‹ˆì–´ë“¤ì´ í™”ìì˜ íƒœë„ì™€ ê°ì •ì„ ìë™ìœ¼ë¡œ ì¸ì‹í•˜ê³  ì¡°ì‘í•˜ì—¬ ì¸ê°„ ì‚¬ìš©ìê°€ ì •ë³´ ê¸°ìˆ ì— ë” ì‰½ê²Œ ì ‘ê·¼í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆë„ë¡ í•˜ë ¤ëŠ” ì‹œë„ë¥¼ í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì—°êµ¬ì˜ ëŒ€ë¶€ë¶„ì€ ìŒì„± ì‹ í˜¸ì—ì„œ ìŒí–¥ ë§¤ê°œë³€ìˆ˜ë¥¼ ì¶”ì¶œí•˜ì—¬ ë‹¤ì–‘í•œ ê°ì •ê³¼ ê¸°íƒ€ ì •ì„œì  ì„±í–¥ì´ ë°œì„± íŒ¨í„´ì„ í†µí•´ ì–´ë–»ê²Œ í‘œí˜„ë˜ëŠ”ì§€ ì´í•´í•˜ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ê¸°ë³¸ ì´ë¡ ì  ê°€ì •ì€ ì •ì„œì  ê³¼ì •ìœ¼ë¡œ ì¸í•œ ììœ¨ì‹ ê²½ í¥ë¶„ê³¼ ê·¼ìœ¡ ê¸´ì¥ì˜ ë³€í™”ë¥¼ ìŒí–¥ íŒŒí˜•ì˜ ë‹¤ì–‘í•œ íŒŒë¼ë¯¸í„°ë¡œ ì¶”ì •í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. GeMAPSëŠ” ìŒì„±ê³¼ ê°ì • ì»´í“¨íŒ… ë¶„ì•¼ì—ì„œ ì‚¬ìš©ë˜ëŠ” ê¸°ë³¸ í‘œì¤€ ìŒí–¥ íŒŒë¼ë¯¸í„° ì„¸íŠ¸ë¡œ, ì´ì „ ì—°êµ¬ì—ì„œëŠ” ë‹¤ì–‘í•œ íŒŒë¼ë¯¸í„°ë“¤ì´ ì‚¬ìš©ë˜ì—ˆì§€ë§Œ ì¼ê´€ëœ ë°©ì‹ìœ¼ë¡œ ì¶”ì¶œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ GeMAPSëŠ” ë‹¤ì–‘í•œ ìë™ ìŒì„± ë¶„ì„ ì˜ì—­ì— ì ìš©ë  ìˆ˜ ìˆëŠ” ê¸°ë³¸ í‘œì¤€ ì„¸íŠ¸ë¡œì„œ, ê°ì •ì  ìƒë¦¬ì  ë³€í™”ë¥¼ ìƒ‰ì¸í™”í•˜ëŠ” ë° ì ì¬ë ¥ì´ ìˆìœ¼ë©°, ì´ì „ ì—°êµ¬ì—ì„œ ì…ì¦ëœ ê°€ì¹˜ì™€ ì´ë¡ ì  ì¤‘ìš”ì„±ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. 2.Related Work CEICESëŠ” ë” ì—”ì§€ë‹ˆì–´ë§ ì¤‘ì‹¬ì ì¸ â€œìˆ˜ì§‘ê¸°(collector)â€ ì ‘ê·¼ ë°©ì‹ìœ¼ë¡œ, ë¶„ë¥˜ ì‹¤í—˜ì—ì„œ ì„±ê³µì ì¸ íŒŒë¼ë¯¸í„°ë“¤ì„ ëª¨ë‘ í¬í•¨í•©ë‹ˆë‹¤. ë°˜ë©´, GeMAPSëŠ” ë‹¤ì–‘í•œ ì¶œì²˜ì˜ êµì°¨í•™ì œì  ì¦ê±°ì™€ ì´ë¡ ì  ì¤‘ìš”ì„± ë˜ëŠ” ëª‡ ê°€ì§€ íŒŒë¼ë¯¸í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ìµœì†Œí•œì˜ íŒŒë¼ë¯¸í„° ì„¸íŠ¸ë¥¼ í•©ì˜í•˜ëŠ” ë³´ë‹¤ êµì°¨í•™ì œì ì¸ ì‹œë„ì…ë‹ˆë‹¤. ì´ˆê¸° ì¡°ì‚¬[15]ì™€ ìµœê·¼ ê°œìš”[17]ëŠ” ì •ì„œì  ìŒì„± ì—°êµ¬ì— ê´€í•œ ìˆ˜ì‹­ ë…„ê°„ì˜ ì‹¬ë¦¬í•™ ë¬¸í—Œì„ ì˜ ìš”ì•½í•˜ê³  ìˆìœ¼ë©°, ì œì‹œëœ ê²½í—˜ì  ë°ì´í„°ë¥¼ í†µí•´ ê°•ë„(ìŒëŸ‰) F0(ê¸°ë³¸ ì£¼íŒŒìˆ˜) í‰ê·  ë³€ë™ì„± ë° ë²”ìœ„ ìŒì„± ì‹ í˜¸ì˜ ê³ ì£¼íŒŒ ì½˜í…ì¸ &#x2F;ì—ë„ˆì§€ê°€ ìŠ¤íŠ¸ë ˆìŠ¤(ê°•ë„, F0 í‰ê· ) ë¶„ë…¸ ë° ìŠ¬í””(ëª¨ë“  ë§¤ê°œë³€ìˆ˜) ì§€ë£¨í•¨(F0 ë³€ë™ì„± ë° ë²”ìœ„)ê³¼ ê°™ì€ ì „í˜•ì ì¸ ìŒì„± ì •ì„œ í‘œí˜„ê³¼ ìƒê´€ê´€ê³„ë¥¼ ë³´ì¸ë‹¤ëŠ” ê²°ë¡ ì„ ë‚´ë ¸ìŠµë‹ˆë‹¤. speechì™€ articulation rateì€ ê°ì •í‘œí˜„ì— ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. [16]ì€ F0 ë° ìŠ¤í™íŠ¸ëŸ¼ ë¶„í¬ì™€ ê´€ë ¨ëœ ë§¤ê°œë³€ìˆ˜ê°€ ê°ì •ì  ë°œí™” ë‚´ìš©ì— ëŒ€í•œ ì¤‘ìš”í•œ ë‹¨ì„œì„ì„ í™•ì¸í•¨ [17]ê³¼ ê°™ì´ ëŒ€ë¶€ë¶„ì˜ ì—°êµ¬ëŠ” ì²­ê°ì  ê°ì •ë¶„ì„ì„ ë‹¤ë£¨ë©° accoustic arousalì— ëŒ€í•œ ë‹¨ì„œì¸ ìƒë‹¹íˆ ì¼ê´€ëœ ë§¤ê°œë³€ìˆ˜ë¥¼ ë³´ê³ í•¨ [24]ì— ë³´ë©´, ê·¸ëƒ¥ ìŒëŸ‰ì„ ì¸¡ì •í•˜ëŠ”ê±°ë³´ë‹¤ ë‹¤ì–‘í•œ ì£¼íŒŒìˆ˜ ëŒ€ì—­ì˜ ì‹ í˜¸ì—ë„ˆì§€ì— human-hearingâ€™s frequency sensitivityì— ë”°ë¼ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ê³¼í•˜ë©´ vocal affect dimensionê³¼ ë” ë§ì€ ìƒê´€ê´€ê³„ê°€ ìˆëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŒ. ë˜í•œ spectral fluxê°€ ë‹¨ì¼ í”¼ì²˜ì— ëŒ€í•´ ì „ë°˜ì ìœ¼ë¡œ ê°€ì¥ ë†’ì€ ìƒê´€ê´€ê³„ë¥¼ ë³´ì˜€ìŒ. [17],[25] : particular valence(ê°ì •ì˜ ê¸ì •ì„± ë˜ëŠ” ë¶€ì •ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œ) [25]ì—ì„œëŠ” LOI(Level of Interest, ëŒ€í™” ì°¸ì—¬ìì˜ ê´€ì‹¬ ìˆ˜ì¤€ì„ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œì…ë‹ˆë‹¤. LOIëŠ” boredom(ì§€ë£¨í•¨), overneutral(ì¤‘ë¦½), joyful interaction(ì¦ê±°ìš´ ìƒí˜¸ì‘ìš©)ìœ¼ë¡œ êµ¬ë¶„) ê°™ì€ ê²½ìš° MFCCì˜ ì¤‘ìš”ì„±ì´ í¬ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤Œ, lower order MFCCëŠ” ìŠ¤í™íŠ¸ëŸ¼ ì „ì²´ë²”ìœ„(first coefficient, ì²«ë²ˆì§¸ ê³„ìˆ˜) ë˜ëŠ” ë‹¤ì–‘í•œ ì‘ì€ sub-bands(second and higher coefficient, ë‘ë²ˆì§¸ ë° ìƒìœ„ê³„ìˆ˜)ì—ì„œ ì–´ëŠì •ë„ spectral tilt(slope)ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤. spectral slope(Spectral slopeëŠ” ì£¼íŒŒìˆ˜ ëŒ€ì—­ì—ì„œ íŒŒì›Œ ìŠ¤í™íŠ¸ëŸ¼ì˜ ê¸°ìš¸ê¸°ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œì…ë‹ˆë‹¤. ì´ ë…¼ë¬¸ì—ì„œëŠ” 0-500 Hz ë° 500-1500 Hz ëŒ€ì—­ì—ì„œì˜ ìŠ¤í™íŠ¸ëŸ¼ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ì„¤ëª…í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ìŠ¤í™íŠ¸ëŸ¼ ê¸°ìš¸ê¸°ëŠ” ìŒì„± íŠ¹ì§• ì¶”ì¶œ ë° ìŒì„± ê°ì • ì¸ì‹ê³¼ ê°™ì€ ë‹¤ì–‘í•œ ì—°êµ¬ ë¶„ì•¼ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤.) ì´ ì—°êµ¬ì˜ ëª©í‘œëŠ” ì´ì „ì˜ ê´€ë ¨ ì—°êµ¬ ê²°ê³¼ì— ë”°ë¼ ê´€ë ¨ íŒŒë¼ë¯¸í„°ë¥¼ ì„ íƒí•˜ëŠ” ê²ƒ ë§ì€ automatically extracted brute-force íŒŒë¼ë¯¸í„° ì„¸íŠ¸ëŠ” formant íŒŒë¼ë¯¸í„°ë¥¼ ì•ˆì •ì ìœ¼ë¡œ ì¶”ì¶œí•˜ê¸° ì–´ë µê¸° ë•Œë¬¸ì— ì´ë¥¼ ë¬´ì‹œí•˜ì§€ë§Œ, voice research and automatic classificationë¥¼ ìœ„í•´ì„œëŠ” formant íŒŒë¼ë¯¸í„°ê°€ ë§¤ìš° ì¤‘ìš”. formantëŠ” ë‹¤ì–‘í•œ í˜•íƒœì˜ ê°ì •ê³¼ ì •ì‹ ìƒíƒœì— ë¯¼ê°í•œ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìœ¼ë©° ê±°ì˜ sota ìˆ˜ì¤€ì˜ cognitive load classification result[27]ì™€ ìš°ìš¸ì¦ ì¸ì‹ ë° í‰ê°€ ê²°ê³¼[31],[38]ë¥¼ ì œê³µí•˜ë©° ë‹¤ë¥¸ ì‹œìŠ¤í…œì˜ feature dimensionì— ë¹„í•´ í›¨ì”¬ ì ì€ìˆ˜ì˜ formantë¡œë„ ê²½ìŸë ¥ ìˆëŠ” ê°ì •ì¸ì‹ ì„±ëŠ¥ì„ ì œê³µí• ìˆ˜ ìˆê¸° ë•Œë¬¸ cognitive load: ì¸ê°„ì˜ ì¸ì§€ ëŠ¥ë ¥ì´ ì²˜ë¦¬í•´ì•¼ í•˜ëŠ” ì •ë³´ì˜ ì–‘ê³¼ ë³µì¡ì„±ì— ë”°ë¼ ë°œìƒí•˜ëŠ” ì •ì‹ ì ì¸ ë¶€ë‹´ì„ ì˜ë¯¸ ê·¸ë˜ì„œ ì´ ë…¼ë¬¸ì— ì œì•ˆëœ íŒŒë¼ë¯¸í„°ì„¸íŠ¸ì—ëŠ” formantë„ í¬í•¨ë˜ì–´ ìˆìŒ. fundamental frequency(ê¸°ë³¸ ì£¼íŒŒìˆ˜[6])ì™€ ì§„í­&#x2F;ê°•ë„ì˜ ì¤‘ìš”ì„±ì´ ì…ì¦ë˜ì—ˆê¸° ë•Œë¬¸ì—, ê°•ë ¥í•œ fundamental frequency ì¸¡ì •ê³¼ pseudo-auditory loudness(ìœ ì‚¬ ì²­ê°ì  ìŒëŸ‰) ì¸¡ì •ì´ ì œì•ˆëœ ì„¸íŠ¸ì— í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë¶„í¬ ë³€í™”ë¥¼ í¬ì°©í•˜ê¸° ìœ„í•´ ì‹œê°„ì— ë”°ë¥¸ ë‹¤ì–‘í•œ í†µê³„ê°€ ë‘ ë§¤ê°œë³€ìˆ˜ì— ì ìš©ë©ë‹ˆë‹¤. ê³ ì£¼íŒŒ ì½˜í…ì¸ ì™€ ìŠ¤í™íŠ¸ëŸ¼ ë°¸ëŸ°ìŠ¤ë¥¼ ê°•ë ¥í•˜ê²Œ í‘œí˜„í•˜ê¸° ìœ„í•´ alpha ë¹„ìœ¨, Hammerberg ì§€ìˆ˜, ìŠ¤í™íŠ¸ëŸ¼ ê¸°ìš¸ê¸°ë¥¼ ì„¤ëª…ìë¡œ ê³ ë ¤í•©ë‹ˆë‹¤. Alpha RatioëŠ” ìŒì„± ì‹ í˜¸ì˜ ì €ì£¼íŒŒ ëŒ€ì—­ê³¼ ê³ ì£¼íŒŒ ëŒ€ì—­ ê°„ì˜ ì—ë„ˆì§€ ë¹„ìœ¨ì„ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œì…ë‹ˆë‹¤. ì´ ë…¼ë¬¸ì—ì„œëŠ” Alpha Ratioê°€ 50-1000 Hzì™€ 1-5 kHz ëŒ€ì—­ì—ì„œì˜ ì—ë„ˆì§€ í•©ì˜ ë¹„ìœ¨ë¡œ ì •ì˜ë˜ë©°, Hammarberg indexì™€ ìœ ì‚¬í•œ ë°©ì‹ìœ¼ë¡œ ê³„ì‚°ë©ë‹ˆë‹¤. Hammarberg indexëŠ” ìŒì„± ì‹ í˜¸ì˜ ì €ì£¼íŒŒ ëŒ€ì—­ê³¼ ê³ ì£¼íŒŒ ëŒ€ì—­ ê°„ì˜ ì—ë„ˆì§€ ë¹„ìœ¨ì„ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œì…ë‹ˆë‹¤. ì´ ë…¼ë¬¸ì—ì„œëŠ” Hammarberg indexê°€ 0-2 kHzì™€ 2-5 kHz ëŒ€ì—­ì—ì„œ ê°€ì¥ í° ì—ë„ˆì§€ í”¼í¬ ê°„ì˜ ë¹„ìœ¨ë¡œ ì •ì˜ë˜ë©°, ê³ ì •ëœ pivot pointì¸ 2 kHzë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚° vocal timbre(ë³´ì»¬ ìŒìƒ‰)ì€ MFCCë¡œ ì¸ì½”ë”©ë˜ê³ , F0ì˜ period-to-period jitterì™€ shimmerë¥¼ ì‚¬ìš©í•˜ì—¬ ìŒì„± ë°œí™” ì‹ í˜¸ì˜ ì§ˆì„ í‰ê°€. Vocal timbreëŠ” ìŒì„±ì˜ í†¤ ìƒ‰ê¹”ì´ë‚˜ ìŒìƒ‰ì„ ë‚˜íƒ€ë‚´ëŠ” ìš©ì–´ F0ì˜ period-to-period jitterì™€ shimmerëŠ” ìŒì„± ë°œí™” ì‹ í˜¸ì˜ ì•ˆì •ì„±ê³¼ ê·œì¹™ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œì…ë‹ˆë‹¤. JitterëŠ” ì—°ì†ëœ F0 ì£¼ê¸° ê°„ ìµœê³ ì ì˜ ì°¨ì´ë¥¼ ë‚˜íƒ€ë‚´ë©°, shimmerëŠ” ì—°ì†ëœ F0 ì£¼ê¸° ê°„ ìµœê³ ì ì˜ í¬ê¸° ì°¨ì´ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì´ëŸ¬í•œ ì§€í‘œë“¤ì€ ìŒì„± ë°œí™” ì‹ í˜¸ì˜ ì§ˆì„ í‰ê°€í•˜ëŠ” ë° ì‚¬ìš© ëª¨ìŒ ê¸°ë°˜ ìŒì„± ì—°êµ¬ë¥¼ í—ˆìš©í•˜ê³  íŠ¹ì • ì‘ì—…ì— ëŒ€í•œ ê´€ë ¨ì„±ì´ ì…ì¦ëœ formant íŒŒë¼ë¯¸í„°ë„ ì„¸íŠ¸ì— í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. 3.Acoustic Parameter Recommendation ì—¬ê¸°ì— ì œì‹œëœ ê¶Œì¥ ì‚¬í•­ì€ ì œë„¤ë°”ì—ì„œ ì—´ë¦° ìŒì„± ë° ì–¸ì–´ ê³¼í•™ìë“¤ì˜ í•™ì œ ê°„ íšŒì˜ì—ì„œ ê³ ì•ˆë˜ì—ˆìœ¼ë©° ë®Œí—¨ê³µê³¼ëŒ€í•™êµ(TUM)ì—ì„œ ë”ìš± ë°œì „ì‹œì¼°ìŠµë‹ˆë‹¤. ì„¸ê°€ì§€ ê¸°ì¤€ì— ë”°ë¼ ë§¤ê°œë³€ìˆ˜ì˜ ì„ íƒì´ ì´ë£¨ì–´ì¡ŒìŒ ì •ì„œì  ê³¼ì • ì¤‘ ìŒì„± ìƒì„±ì˜ ìƒë¦¬ì  ë³€í™”ë¥¼ ì§€í‘œí™” í• ìˆ˜ ìˆëŠ” ìŒí–¥ ë§¤ê°œë³€ìˆ˜ì˜ ì ì¬ë ¥ voice production ê³¼ì •ì—ì„œ ê°ì •ì  ë³€í™”ê°€ ìƒê¸°ë©´ì„œ ìƒë¦¬í•™ì  ë³€í™”ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ìŒí–¥ íŒŒë¼ë¯¸í„°ì˜ potential. ê³¼ê±° ë¬¸í—Œì—ì„œ í•´ë‹¹ ë§¤ê°œë³€ìˆ˜ê°€ ì‚¬ìš©ëœ ë¹ˆë„ ë° ì„±ê³µ ì—¬ë¶€(related works ì°¸ì¡°) ì´ë¡ ì  ì¤‘ìš”ì„±([1],[2]) ë‘ê°€ì§€ ë²„ì „ì˜ acoustic parameter setê°€ ê¶Œì¥ë¨ ì €ìì˜ ì´ì „ ì—°êµ¬ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ê²ƒìœ¼ë¡œ ë°í˜€ì§„ prosodic, excitation, vocal tract, and spectral descriptorsë¥¼ êµ¬í˜„í•˜ëŠ” ìµœì†Œí•œì˜ íŒŒë¼ë¯¸í„° ì„¸íŠ¸ ë¬¸í•™([40])ì—ì„œ pure prosodic ë° ìŠ¤í™íŠ¸ëŸ¼ ë§ˆë¼ë¯¸í„° ì„¸íŠ¸ë³´ë‹¤ automatic affect recognitionì˜ ì •í™•ë„ë¥¼ ë†’ì´ëŠ” ê²ƒìœ¼ë¡œ ì¼ê´€ë˜ê²Œ ì•Œë ¤ì§„ ì†ŒëŸ‰ì˜ cepstral descriptorsë¥¼ í¬í•¨í•˜ëŠ” ìµœì†Œí•œì˜ ì„¸íŠ¸ì— ëŒ€í•œ í™•ì¥ ì œì•ˆí•˜ëŠ” ìŒí–¥ íŒŒë¼ë¯¸í„° ì„¸íŠ¸: ì—¬ê¸°ì„œëŠ” ë‘ ê°€ì§€ ë²„ì „ì˜ ìŒí–¥ íŒŒë¼ë¯¸í„° ì„¸íŠ¸ë¥¼ ì œì•ˆí•˜ê³  ìˆìŠµë‹ˆë‹¤. ì²« ë²ˆì§¸ëŠ” ì €ìë“¤ì˜ ì´ì „ ì—°êµ¬ì—ì„œ ê°€ì¥ ì¤‘ìš”í•˜ë‹¤ê³  íŒë‹¨ëœ prosodic, excitation, vocal tract ë° spectral descriptorsë¥¼ êµ¬í˜„í•œ ìµœì†Œí•œì˜ íŒŒë¼ë¯¸í„° ì„¸íŠ¸ì…ë‹ˆë‹¤. ë‘ ë²ˆì§¸ëŠ” ìµœì†Œí•œì˜ íŒŒë¼ë¯¸í„° ì„¸íŠ¸ì— cepstral descriptorsë¥¼ ì¶”ê°€í•œ í™•ì¥ëœ ë²„ì „ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ cepstral descriptorsëŠ” [40]ê³¼ ê°™ì€ literatureì—ì„œ ìì£¼ ì–¸ê¸‰ë˜ë©°, ìˆœìˆ˜í•œ prosodic ë° spectral parameter setë³´ë‹¤ ìë™ ê°ì • ì¸ì‹ì˜ ì •í™•ë„ë¥¼ ë†’ì´ëŠ” ê²ƒìœ¼ë¡œ ì•Œë ¤ì ¸ ìˆìŠµë‹ˆë‹¤. ìë™ íŒŒë¼ë¯¸í„° ì„ íƒì— ëŒ€í•œ ì—°êµ¬ ê²°ê³¼ì™€ MFCCsì˜ ê¸°ë³¸ í•¨ìˆ˜ì— ëŒ€í•œ ì„¤ëª…, [23], [24]ì™€ ê°™ì€ ìë™ íŒŒë¼ë¯¸í„° ì„ íƒì— ê´€í•œ ì—°êµ¬ë“¤ì€, ê°ì • ë° ì–¸ì–´ì  ìŒì„± ë¶„ì„ ì‘ì—…ì—ì„œ ë‚®ì€ ì°¨ìˆ˜ì˜ MFCCsê°€ ë” ì¤‘ìš”í•˜ë‹¤ëŠ” ê²ƒì„ ì‹œì‚¬í•©ë‹ˆë‹¤. MFCCsë¥¼ ê³„ì‚°í•  ë•Œ ì‚¬ìš©ë˜ëŠ” ì´ì‚° ì½”ì‚¬ì¸ ë³€í™˜(DCT-II) ê¸°ë³¸ í•¨ìˆ˜ë¥¼ ì‚´í´ë³´ë©´, ë‚®ì€ ì°¨ìˆ˜ì˜ MFCCsê°€ ìŠ¤í™íŠ¸ëŸ¼ ê¸°ìš¸ê¸°ì™€ ìŠ¤í™íŠ¸ëŸ¼ ì—ë„ˆì§€ì˜ ì „ì²´ ë¶„í¬ì™€ ê´€ë ¨ì´ ìˆë‹¤ëŠ” ê²ƒì´ ë¶„ëª…í•©ë‹ˆë‹¤. ë†’ì€ ì°¨ìˆ˜ì˜ MFCCsëŠ” ë³´ë‹¤ ì •êµí•œ ì—ë„ˆì§€ ë¶„í¬ë¥¼ ë°˜ì˜í•˜ë©°, ìŒì„± ì†ì„±(non-verbal voice attribute)ë³´ë‹¤ëŠ” ìŒìš´ ë‚´ìš©(phonetic content)ì„ ì‹ë³„í•˜ëŠ” ë° ë” ì¤‘ìš”í•  ê²ƒìœ¼ë¡œ ì¶”ì •ë©ë‹ˆë‹¤. 3.1 Minimalistic Parameter Set 18ê°œì˜ Low-Level descriptors Frequency related parameters(1~6): Pitch, logarithmic F0 on a semitone frequency scale, starting at 27.5 Hz (semitone 0). í”¼ì¹˜, 27.5Hz(ë°˜ìŒ 0)ì—ì„œ ì‹œì‘í•˜ëŠ” ë°˜ìŒ ì£¼íŒŒìˆ˜ ìŠ¤ì¼€ì¼ì—ì„œ ë¡œê·¸ F0ì…ë‹ˆë‹¤. ì´ ë¬¸ì¥ì€ F0 ê°’ì— ëŒ€í•œ ì„¤ëª…ì…ë‹ˆë‹¤. F0 ê°’ì€ ìŒì„± ì‹ í˜¸ì—ì„œ ê¸°ë³¸ ì£¼íŒŒìˆ˜ë¥¼ ë‚˜íƒ€ë‚´ë©°, ì´ ê°’ì€ ì„¸ë¯¸í†¤ ì£¼íŒŒìˆ˜ ìŠ¤ì¼€ì¼ì—ì„œ ë¡œê·¸í•¨ìˆ˜ë¡œ ë³€í™˜ë©ë‹ˆë‹¤. ì´ ìŠ¤ì¼€ì¼ì€ 27.5 Hz (ì„¸ë¯¸í†¤ 0)ì—ì„œ ì‹œì‘í•˜ë©°, ê° ì„¸ë¯¸í†¤ ê°„ì˜ ì£¼íŒŒìˆ˜ ì°¨ì´ëŠ” 2^(1&#x2F;12)ë¡œ ê³„ì‚°ë©ë‹ˆë‹¤. ë”°ë¼ì„œ, ì˜ˆë¥¼ ë“¤ì–´, ì„¸ë¯¸í†¤ 1ì€ 29.136 Hzì´ê³ , ì„¸ë¯¸í†¤ 2ëŠ” 30.868 Hzì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ë³€í™˜ì„ í†µí•´ F0 ê°’ì„ ë³´ë‹¤ ì§ê´€ì ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Jitter, deviations in individual consecutive F0 period lengths. ì§€í„°ëŠ” ê°œë³„ ì—°ì† F0 ì£¼ê¸° ê¸¸ì´ì˜ í¸ì°¨ì…ë‹ˆë‹¤. ì´ ë¬¸ì¥ì€ ìŒì„± íŒŒë¼ë¯¸í„° ì¤‘ í•˜ë‚˜ì¸ Jitterì— ëŒ€í•œ ì„¤ëª…ì…ë‹ˆë‹¤. JitterëŠ” ì—°ì†ì ì¸ F0 ì£¼ê¸°ì˜ ê¸¸ì´ì—ì„œ ë°œìƒí•˜ëŠ” ë³€ë™ì„ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œì…ë‹ˆë‹¤. ì¦‰, F0 ì£¼ê¸°ì˜ ê¸¸ì´ê°€ ì¼ì •í•˜ì§€ ì•Šê³  ë³€ë™ì´ í¬ë©´ Jitter ê°’ì´ ë†’ì•„ì§‘ë‹ˆë‹¤. ì´ëŸ¬í•œ ë³€ë™ì€ ìŒì„± ì‹ í˜¸ì—ì„œ ë°œìƒí•˜ëŠ” êµ­ì†Œì ì¸ ì„±ëŒ€ ìš´ë™ ë¶ˆê·œì¹™ì„±ìœ¼ë¡œ ì¸í•´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Jitter ê°’ì€ ìŒì„± ì¥ì•  ì§„ë‹¨ ë° ê°ì • ë¶„ì„ ë“±ì— ì‚¬ìš©ë©ë‹ˆë‹¤. Formant 1 frequency, Formant 2 frequency, Formant 3 frequency, centre frequency of first, second, and third formant 1,2,3 ë²ˆì§¸ formantì˜ ì¤‘ì‹¬ ì£¼íŒŒìˆ˜ Formant 1, bandwidth of first formant. ì²«ë²ˆì§¸ formantì˜ ì¤‘ì‹¬ íŒŒë¼ë¯¸í„° Energy&#x2F;Amplitude related parameters(7~9): Shimmer, difference of the peak amplitudes of consecutive F0 periods. ì‰¬ë¨¸ëŠ” ì—°ì†ëœ F0 ê¸°ê°„ì˜ í”¼í¬ ì§„í­ì˜ ì°¨ì´ì…ë‹ˆë‹¤. ì´ ë¬¸ì¥ì€ ìŒì„± íŒŒë¼ë¯¸í„° ì¤‘ í•˜ë‚˜ì¸ Shimmerì— ëŒ€í•œ ì„¤ëª…ì…ë‹ˆë‹¤. ShimmerëŠ” ì—°ì†ì ì¸ F0 ì£¼ê¸°ì—ì„œ peak amplitudeì˜ ì°¨ì´ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œì…ë‹ˆë‹¤. ì¦‰, F0 ì£¼ê¸°ì—ì„œ peak amplitudeì˜ ì°¨ì´ê°€ í¬ë©´ Shimmer ê°’ì´ ë†’ì•„ì§‘ë‹ˆë‹¤. ì´ëŸ¬í•œ ë³€ë™ì€ ìŒì„± ì‹ í˜¸ì—ì„œ ë°œìƒí•˜ëŠ” êµ­ì†Œì ì¸ ì„±ëŒ€ ìš´ë™ ë¶ˆê·œì¹™ì„±ìœ¼ë¡œ ì¸í•´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Shimmer ê°’ì€ ìŒì„± ì¥ì•  ì§„ë‹¨ ë° ê°ì • ë¶„ì„ ë“±ì— ì‚¬ìš©ë©ë‹ˆë‹¤. Loudness, estimate of perceived signal intensity from an auditory spectrum. ì²­ê° ìŠ¤í™íŠ¸ëŸ¼ì—ì„œ ê°ì§€ëœ ì‹ í˜¸ê°•ë„ì˜ estimate Harmonics-to-noise ratio (HNR), relation of energy in harmonic components to energy in noise-like components. Harmonics ëŒ€ ì¡ìŒë¹„(HNR)ëŠ” Harmonics ì„±ë¶„ì˜ ì—ë„ˆì§€ì™€ ì¡ìŒ ì„±ë¶„ì˜ ì—ë„ˆì§€ë¥¼ ê³ ì¡°íŒŒ ì„±ë¶„ì˜ ì—ë„ˆì§€ì™€ ì¡ìŒ ì„±ë¶„ì˜ ì—ë„ˆì§€ì˜ ê´€ê³„ì…ë‹ˆë‹¤. ì´ ë¬¸ì¥ì€ ìŒì„± íŒŒë¼ë¯¸í„° ì¤‘ í•˜ë‚˜ì¸ Harmonics-to-noise ratio (HNR)ì— ëŒ€í•œ ì„¤ëª…ì…ë‹ˆë‹¤. HNRì€ ìŒì„± ì‹ í˜¸ì—ì„œ harmonic componentsì™€ noise-like components ê°„ì˜ ì—ë„ˆì§€ ë¹„ìœ¨ì„ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œì…ë‹ˆë‹¤. ì¦‰, HNR ê°’ì´ ë†’ì„ìˆ˜ë¡ ìŒì„± ì‹ í˜¸ì—ì„œ harmonic componentsì˜ ë¹„ì¤‘ì´ ë†’ì•„ì§€ê³ , HNR ê°’ì´ ë‚®ì„ìˆ˜ë¡ noise-like componentsì˜ ë¹„ì¤‘ì´ ë†’ì•„ì§‘ë‹ˆë‹¤. ì´ëŸ¬í•œ ì§€í‘œëŠ” ìŒì„± ì‹ í˜¸ì—ì„œ ë°œìƒí•˜ëŠ” ì¡ìŒê³¼ ì™œê³¡ ë“±ì„ ë¶„ì„í•˜ê³ , ìŒì„± ì¥ì•  ì§„ë‹¨ ë° ê°ì • ë¶„ì„ ë“±ì— ì‚¬ìš©ë©ë‹ˆë‹¤. Spectral (balance) parameters(10~18): Alpha Ratio, ratio of the summed energy from 50-1000 Hz and 1-5 kHz 50-1000Hz, 1-5kHzì—ì„œ í•©ì‚°ëœ ì—ë„ˆì§€ì˜ ë¹„ìœ¨ ìŒì„± ì‹ í˜¸ì˜ ì €ì£¼íŒŒ ëŒ€ì—­ê³¼ ê³ ì£¼íŒŒ ëŒ€ì—­ ê°„ì˜ ì—ë„ˆì§€ ë¹„ìœ¨ì„ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œ Alpha RatioëŠ” ì €ì£¼íŒŒ ì˜ì—­ê³¼ ê³ ì£¼íŒŒ ì˜ì—­ì˜ ì—ë„ˆì§€ ë¹„ìœ¨ì„ ë‚˜íƒ€ë‚´ëŠ” íŒŒë¼ë¯¸í„°ì…ë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ, 50-1000 Hzì™€ 1-5 kHzì—ì„œì˜ ì´ ì—ë„ˆì§€ ë¹„ìœ¨ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. Alpha RatioëŠ” ìŒì„± ì‹ í˜¸ì˜ ì„±ì§ˆì„ ë¶„ì„í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, Alpha Ratioê°€ ë†’ì€ ê²½ìš° ì €ì£¼íŒŒ ì„±ë¶„ì´ ë§ì€ ìŒì„± ì‹ í˜¸ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì •ë³´ëŠ” ìŒì„± ì§ˆí™˜ ì§„ë‹¨ ë° ì¹˜ë£Œì— ìœ ìš©í•˜ê²Œ ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Hammarberg Index, ratio of the strongest energy peak in the 0-2 kHz region to the strongest peak in the 2â€“5kHz region. 0-2kHz ì˜ì—­ì—ì„œ ê°€ì¥ ê°•í•œ ì—ë„ˆì§€ í”¼í¬ì™€ 2-5kHz ì˜ì—­ì—ì„œ ê°€ì¥ ê°•í•œ í”¼í¬ì˜ ë¹„ìœ¨ ì´ ë¬¸ì¥ì€ ìŒì„± íŒŒë¼ë¯¸í„° ì¤‘ í•˜ë‚˜ì¸ Hammarberg Indexì— ëŒ€í•œ ì„¤ëª…ì…ë‹ˆë‹¤. Hammarberg IndexëŠ” ìŒì„± ì‹ í˜¸ì—ì„œ 0-2 kHz ì˜ì—­ê³¼ 2-5 kHz ì˜ì—­ì—ì„œ ê°€ì¥ ê°•í•œ ì—ë„ˆì§€ peak ê°„ì˜ ë¹„ìœ¨ì„ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œì…ë‹ˆë‹¤. ì¦‰, Hammarberg Index ê°’ì´ ë†’ì„ìˆ˜ë¡ 0-2 kHz ì˜ì—­ì—ì„œì˜ ì—ë„ˆì§€ peakê°€ ë†’ì•„ì§€ê³ , Hammarberg Index ê°’ì´ ë‚®ì„ìˆ˜ë¡ 2-5 kHz ì˜ì—­ì—ì„œì˜ ì—ë„ˆì§€ peakê°€ ë†’ì•„ì§‘ë‹ˆë‹¤. ì´ëŸ¬í•œ ì§€í‘œëŠ” ìŒì„± ì‹ í˜¸ì—ì„œ ë°œìƒí•˜ëŠ” ê³ ìŒê³¼ ì €ìŒ ë“±ì„ ë¶„ì„í•˜ê³ , ìŒì„± ì¥ì•  ì§„ë‹¨ ë° ê°ì • ë¶„ì„ ë“±ì— ì‚¬ìš©ë©ë‹ˆë‹¤. Spectral Slope 0-500 Hz Spectral Slope 500-1500 Hz, linear regression slope of the logarithmic power spectrum within the two given bands. 0-500 Hz ê³¼ 500-1500 Hz, ì£¼ì–´ì§„ ë‘ ëŒ€ì—­ ë‚´ì—ì„œ ë¡œê·¸ íŒŒì›Œ ìŠ¤í™íŠ¸ëŸ¼ì˜ ì„ í˜• íšŒê·€ ê¸°ìš¸ê¸° ì´ ë¬¸ì¥ì€ ìŒì„± íŒŒë¼ë¯¸í„° ì¤‘ í•˜ë‚˜ì¸ Spectral Slope 0-500 Hz and 500-1500 Hzì— ëŒ€í•œ ì„¤ëª…ì…ë‹ˆë‹¤. Spectral SlopeëŠ” ì£¼ì–´ì§„ ë‘ ê°œì˜ ì£¼íŒŒìˆ˜ ë²”ìœ„ (0-500 Hz ë° 500-1500 Hz) ë‚´ì—ì„œ ë¡œê·¸ ìŠ¤ì¼€ì¼ì˜ íŒŒì›Œ ìŠ¤í™íŠ¸ëŸ¼ì˜ ì„ í˜• íšŒê·€ ê¸°ìš¸ê¸°ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œì…ë‹ˆë‹¤. ì¦‰, Spectral Slope ê°’ì´ ë†’ì„ìˆ˜ë¡ íŒŒì›Œ ìŠ¤í™íŠ¸ëŸ¼ì´ ë¹ ë¥´ê²Œ ê°ì†Œí•˜ê³ , Spectral Slope ê°’ì´ ë‚®ì„ìˆ˜ë¡ íŒŒì›Œ ìŠ¤í™íŠ¸ëŸ¼ì´ ì²œì²œíˆ ê°ì†Œí•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì§€í‘œëŠ” ìŒì„± ì‹ í˜¸ì—ì„œ ë°œìƒí•˜ëŠ” ì €ì—­ëŒ€ì™€ ê³ ì—­ëŒ€ì˜ ì—ë„ˆì§€ ë¶„í¬ë¥¼ ë¶„ì„í•˜ê³ , ìŒì„± ì¥ì•  ì§„ë‹¨ ë° ê°ì • ë¶„ì„ ë“±ì— ì‚¬ìš©ë©ë‹ˆë‹¤. íŒŒì›Œ ìŠ¤í™íŠ¸ëŸ¼ì€ ì‹œê°„ ë„ë©”ì¸ì˜ ì‹ í˜¸ë¥¼ ì£¼íŒŒìˆ˜ ë„ë©”ì¸ìœ¼ë¡œ ë³€í™˜í•œ ê²ƒì…ë‹ˆë‹¤. ì¦‰, íŒŒì›Œ ìŠ¤í™íŠ¸ëŸ¼ì€ ì£¼íŒŒìˆ˜ë³„ë¡œ ì‹ í˜¸ì˜ ì—ë„ˆì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê·¸ë˜í”„ì…ë‹ˆë‹¤. íŒŒì›Œ ìŠ¤í™íŠ¸ëŸ¼ì„ ê³„ì‚°í•˜ë©´ ì£¼íŒŒìˆ˜ ì˜ì—­ì—ì„œ ì‹ í˜¸ì˜ ì„±ë¶„ì„ ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë¶„ì„ì€ ìŒì„± ì¸ì‹, ìŒì„± í•©ì„±, ìŒì„± ë³€ì¡° ë° ìŒì„± ê°ì • ë¶„ì„ ë“±ì— ì‚¬ìš©ë©ë‹ˆë‹¤. Formant 1 relative energy Formant 2 relative energy Formant 3 relative energy, as well as the ratio of the energy of the spectral harmonic peak at the first, second, third formantâ€™s centre frequency to the energy of the spectral peak at F0. Formant 1, 2, 3 ìƒëŒ€ ì—ë„ˆì§€ëŠ” ì²« ë²ˆì§¸, ë‘ ë²ˆì§¸, ì„¸ ë²ˆì§¸ í¬ë¨¼íŠ¸ì˜ ì¤‘ì‹¬ ì£¼íŒŒìˆ˜ì—ì„œ spectral harmonic peakì˜ ì—ë„ˆì§€ì™€ F0ì—ì„œ spectral peakì˜ ì—ë„ˆì§€ì˜ ë¹„ìœ¨ì…ë‹ˆë‹¤. Harmonic difference H1-H2, ratio of energy of the first F0 harmonic (H1) to the energy of the second F0 harmonic (H2). ì²« ë²ˆì§¸ F0 Harmonic(H1)ì˜ ì—ë„ˆì§€ì™€ ë‘ ë²ˆì§¸ F0 Harmonic(H2)ì˜ ì—ë„ˆì§€ ë‘˜ ì‚¬ì´ì˜ ë¹„ìœ¨ì…ë‹ˆë‹¤. Harmonic difference H1-A3, ratio of energy of the first F0 harmonic (H1) to the energy of the highest harmonic in the third formant range (A3). Harmonic ì°¨ì´ H1-A3ì€ ì²« ë²ˆì§¸ F0 harmonic(H1)ì˜ ì—ë„ˆì§€ì™€ ì„¸ ë²ˆì§¸ formant ë²”ìœ„(A3)ì—ì„œ ê°€ì¥ ë†’ì€ harmonicì˜ ì—ë„ˆì§€ì˜ ë¹„ìœ¨ì…ë‹ˆë‹¤. ì²« ë²ˆì§¸ F0 harmonic(H1)ì™€ ì„¸ ë²ˆì§¸ formant ì—ë„ˆì§€(A3) ë¹„ìœ¨ì„ ê³„ì‚° LLD(Low-Level Descriptors)ëŠ” ìŒì„± ì‹ í˜¸ì—ì„œ ì¶”ì¶œëœ ì €ìˆ˜ì¤€ íŠ¹ì§•ì„ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ì§€í‘œë“¤ì€ ì‹œê°„ì— ë”°ë¼ ë³€í™”í•˜ë¯€ë¡œ, 3 í”„ë ˆì„ ê¸¸ì´ì˜ ëŒ€ì¹­ ì´ë™ í‰ê·  í•„í„°(symmetric moving average filter*)*ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œê°„ì ìœ¼ë¡œ smoothing(í‰í™œí™”)ë©ë‹ˆë‹¤. smoothingì€ ë°œì„± ì˜ì—­ ë‚´ì—ì„œë§Œ ìˆ˜í–‰ë˜ê¸° ë•Œë¬¸ì— pitch, jitter ë° shimmerì™€ ê°™ì€ ê²½ìš°, 0(ë¬´ì„±ìŒ)ì—ì„œ 0ì´ ì•„ë‹˜ìœ¼ë¡œì˜ ì „í™˜ì€ smoothingí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. Smoothingì€ ì‹œê³„ì—´ ë°ì´í„°ì—ì„œ ì¡ìŒì´ë‚˜ ë¶ˆê·œì¹™ì„±ì„ ì œê±°í•˜ê³ , ë°ì´í„°ì˜ ì „ë°˜ì ì¸ ì¶”ì„¸ë¥¼ ë¶€ë“œëŸ½ê²Œ ë§Œë“œëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ëŒ€í‘œì ìœ¼ë¡œ ì´ë™ í‰ê·  í•„í„°ì™€ ê°™ì€ í•„í„°ë§ ê¸°ë²•ì´ ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ë™ í‰ê·  í•„í„°ëŠ” ì¼ì •í•œ ê¸¸ì´ì˜ ìœˆë„ìš°ë¥¼ ì„¤ì •í•˜ê³ , í•´ë‹¹ ìœˆë„ìš° ë‚´ì˜ ë°ì´í„°ë“¤ì˜ í‰ê· ê°’ì„ êµ¬í•˜ì—¬ ê°ê°ì˜ ë°ì´í„°ì— ëŒ€í•´ ì ìš©í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ smoothingì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ LLDì— ëŒ€í•´ ì‚°ìˆ í‰ê· (arithmetic mean)ê³¼ ë³€ë™ ê³„ìˆ˜ (coefficient of variation, í‘œì¤€ í¸ì°¨ë¥¼ ì‚°ìˆ  í‰ê· ìœ¼ë¡œ ì •ê·œí™” í•œ ê²ƒ)ê°€ ëª¨ë“  18ê°œì˜ LLDì— ëŒ€í•´ ì ìš©ë˜ì–´ 36ê°œì˜ íŒŒë¼ë¯¸í„°ê°€ ìƒì„±ë©ë‹ˆë‹¤. ìŒì„± íŒŒë¼ë¯¸í„° ì¶”ì¶œ ê³¼ì •ì—ì„œ LLDì— ëŒ€í•œ í†µê³„ì ì¸ ì§€í‘œ ê³„ì‚° ë°©ë²• ì‚°ìˆ í‰ê· ì€ ê° LLDì˜ ê°’ë“¤ì„ ëª¨ë‘ ë”í•œ í›„, LLDì˜ ê°œìˆ˜ë¡œ ë‚˜ëˆ„ì–´ í‰ê· ê°’ì„ êµ¬í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ë³€ë™ ê³„ìˆ˜ëŠ” í‘œì¤€ í¸ì°¨ë¥¼ ì‚°ìˆ  í‰ê· ìœ¼ë¡œ ì •ê·œí™”í•˜ì—¬, ë°ì´í„°ì˜ ë³€ë™ì„±ì„ ìƒëŒ€ì ìœ¼ë¡œ ë¹„êµí•  ìˆ˜ ìˆëŠ” ì§€í‘œì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ì‚°ìˆ í‰ê· ê³¼ ë³€ë™ ê³„ìˆ˜ë¥¼ ê°ê° 18ê°œì˜ LLDì— ëŒ€í•´ ì ìš©í•˜ì—¬, 36ê°œì˜ íŒŒë¼ë¯¸í„°ê°€ ìƒì„±ë©ë‹ˆë‹¤. ìŒëŸ‰ê³¼ í”¼ì¹˜ì—ëŠ” 20ë²ˆì§¸, 50ë²ˆì§¸, 80ë²ˆì§¸ ë°±ë¶„ìœ„ìˆ˜, 20~80ë²ˆì§¸ ë°±ë¶„ìœ„ìˆ˜ ë²”ìœ„, ìƒìŠ¹&#x2F;í•˜ê°• ì‹ í˜¸ ë¶€ë¶„ì˜ ê¸°ìš¸ê¸° í‰ê· ê³¼ í‘œì¤€í¸ì°¨ ë“± 8ê°€ì§€ í•¨ìˆ˜ê°€ ì¶”ê°€ë¡œ ì ìš©ë©ë‹ˆë‹¤. ì´ ë¬¸ì¥ì€ ìŒì„± íŒŒë¼ë¯¸í„° ì¶”ì¶œ ê³¼ì •ì—ì„œ ìŒëŸ‰ê³¼ í”¼ì¹˜ì— ëŒ€í•´ ì ìš©ë˜ëŠ” ì¶”ê°€ì ì¸ í•¨ìˆ˜ë“¤ì— ëŒ€í•œ ì„¤ëª…ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ í•¨ìˆ˜ë“¤ì€ 20ë²ˆì§¸, 50ë²ˆì§¸, 80ë²ˆì§¸ ë°±ë¶„ìœ„ìˆ˜, 20~80ë²ˆì§¸ ë°±ë¶„ìœ„ìˆ˜ ë²”ìœ„, ìƒìŠ¹&#x2F;í•˜ê°• ì‹ í˜¸ ë¶€ë¶„ì˜ ê¸°ìš¸ê¸° í‰ê· ê³¼ í‘œì¤€í¸ì°¨ ë“± ì´ 8ê°€ì§€ í•¨ìˆ˜ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. ì´ëŸ¬í•œ í•¨ìˆ˜ë“¤ì€ ìŒì„± ì‹ í˜¸ì—ì„œ ì¶”ì¶œëœ ì €ìˆ˜ì¤€ íŠ¹ì§•ì„ ë”ìš± ìƒì„¸í•˜ê²Œ ë¶„ì„í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ë°±ë¶„ìœ„ìˆ˜ëŠ” ë°ì´í„°ì˜ ë¶„í¬ë¥¼ íŒŒì•…í•˜ëŠ” ë° ì‚¬ìš©ë˜ë©°, ìƒìŠ¹&#x2F;í•˜ê°• ì‹ í˜¸ ë¶€ë¶„ì˜ ê¸°ìš¸ê¸° í‰ê· ê³¼ í‘œì¤€í¸ì°¨ëŠ” ìŒì„±ì˜ ê°•ë„ë‚˜ ë†’ë‚®ì´ ë³€í™”ë¥¼ ë¶„ì„í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ëŸ¬í•œ íŒŒë¼ë¯¸í„°ë“¤ì€ ìŒì„± ì¸ì‹ ë° ê°ì • ë¶„ì„ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í™œìš©ë©ë‹ˆë‹¤. 20,50,80 ë²ˆì§¸ ë°±ë¶„ìœ„ìˆ˜: ê°ê° ì „ì²´ ë°ì´í„°ì˜ 20%, 50%, 80%ì— í•´ë‹¹í•˜ëŠ” ê°’ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ë§Œì•½ ì–´ë–¤ ë°ì´í„° ì§‘í•©ì—ì„œ 50ë²ˆì§¸ ë°±ë¶„ìœ„ìˆ˜ê°€ 10ì´ë¼ë©´, ì´ëŠ” í•´ë‹¹ ë°ì´í„° ì§‘í•©ì—ì„œ ì¤‘ê°„ê°’(median)ì´ 10ì´ë¼ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë°±ë¶„ìœ„ìˆ˜ë“¤ì€ ë°ì´í„°ì˜ ë¶„í¬ë¥¼ íŒŒì•…í•˜ê³ , ëŒ€í‘œê°’ì„ ì¶”ì •í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ë”°ë¼ì„œ ìŒì„± ì‹ í˜¸ì—ì„œ ì¶”ì¶œëœ íŒŒë¼ë¯¸í„°ë“¤ì— ëŒ€í•´ ì ìš©ë˜ëŠ” ë°±ë¶„ìœ„ìˆ˜ë“¤ì€ í•´ë‹¹ íŒŒë¼ë¯¸í„°ë“¤ì˜ ë¶„í¬ë¥¼ íŒŒì•…í•˜ê³ , ëŒ€í‘œê°’ì„ ì¶”ì •í•˜ëŠ” ë° ì‚¬ìš© ìƒìŠ¹&#x2F;í•˜ê°• ì‹ í˜¸ ë¶€ë¶„ì˜ ê¸°ìš¸ê¸° í‰ê· ê³¼ í‘œì¤€í¸ì°¨: ìŒì„± ì‹ í˜¸ì—ì„œ ì¶”ì¶œëœ íŒŒë¼ë¯¸í„°ë“¤ ì¤‘ í•˜ë‚˜ë¡œ, ìŒì„±ì˜ ê°•ë„ë‚˜ ë†’ë‚®ì´ ë³€í™”ë¥¼ ë¶„ì„í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ëŸ¬í•œ íŒŒë¼ë¯¸í„°ëŠ” ìƒìŠ¹&#x2F;í•˜ê°• ì‹ í˜¸ ë¶€ë¶„ì—ì„œì˜ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•˜ì—¬, í•´ë‹¹ êµ¬ê°„ì—ì„œì˜ ìŒì„± ë³€í™” ì •ë„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œì…ë‹ˆë‹¤. ê¸°ìš¸ê¸° í‰ê· ì€ í•´ë‹¹ êµ¬ê°„ì—ì„œì˜ ê¸°ìš¸ê¸° ê°’ë“¤ì„ ëª¨ë‘ ë”í•œ í›„, êµ¬ê°„ ê¸¸ì´ë¡œ ë‚˜ëˆ„ì–´ í‰ê· ê°’ì„ êµ¬í•˜ëŠ” ê²ƒì´ë©°, í‘œì¤€í¸ì°¨ëŠ” í•´ë‹¹ êµ¬ê°„ì—ì„œì˜ ê¸°ìš¸ê¸° ê°’ë“¤ì˜ í©ì–´ì§„ ì •ë„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œì…ë‹ˆë‹¤. ì´ëŸ¬í•œ íŒŒë¼ë¯¸í„°ë“¤ì€ ìŒì„± ì¸ì‹ ë° ê°ì • ë¶„ì„ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í™œìš© ìƒìŠ¹&#x2F;í•˜ê°• ì‹ í˜¸ ë¶€ë¶„ì˜ ê¸°ìš¸ê¸° êµ¬í•˜ëŠ” ë°©ë²•: ìƒìŠ¹&#x2F;í•˜ê°• ì‹ í˜¸ ë¶€ë¶„ì˜ ê¸°ìš¸ê¸°ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë¯¸ë¶„ì„ í†µí•´ êµ¬í•©ë‹ˆë‹¤. ìŒì„± ì‹ í˜¸ì—ì„œëŠ” ì¼ë°˜ì ìœ¼ë¡œ Short-Time Energy(STE) ë˜ëŠ” Zero Crossing Rate(ZCR)ì™€ ê°™ì€ ì €ìˆ˜ì¤€ íŠ¹ì§•ì„ ì‚¬ìš©í•˜ì—¬, ìƒìŠ¹&#x2F;í•˜ê°• ì‹ í˜¸ ë¶€ë¶„ì„ ê²€ì¶œí•œ í›„, í•´ë‹¹ êµ¬ê°„ì—ì„œì˜ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, STEë¥¼ ì‚¬ìš©í•˜ì—¬ ìŒì„± ì‹ í˜¸ì—ì„œ ìƒìŠ¹&#x2F;í•˜ê°• ì‹ í˜¸ ë¶€ë¶„ì„ ê²€ì¶œí•œ í›„, í•´ë‹¹ êµ¬ê°„ì—ì„œì˜ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ì„œëŠ” í•´ë‹¹ êµ¬ê°„ ë‚´ STE ê°’ë“¤ì˜ ì°¨ì´ë¥¼ ê³„ì‚°í•˜ì—¬, ì´ë¥¼ êµ¬ê°„ ê¸¸ì´ë¡œ ë‚˜ëˆ„ì–´ ê¸°ìš¸ê¸° ê°’ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. loudnessì— ì ìš©ë˜ëŠ” í•¨ìˆ˜ë“¤ì„ ì œì™¸í•œ ëª¨ë“  í•¨ìˆ˜ëŠ” ìŒì„±ì˜ì—­(F0ê°€ 0ì´ ì•„ë‹Œê²ƒ)ì—ì„œë§Œ ì ìš©ë¨. ì´ë ‡ê²Œ 36ê°œ íŒŒë¼ë¯¸í„° + loudness 8ê°€ì§€ í•¨ìˆ˜ ì¶”ê°€ ì ìš© íŒŒë¼ë¯¸í„° + pitch 8ê°€ì§€ í•¨ìˆ˜ ì¶”ê°€ ì ìš© íŒŒë¼ë¯¸í„° = 52ê°œì˜ íŒŒë¼ë¯¸í„°ê°€ ìµœì¢… ë˜í•œ ëª¨ë“  ë¬´ì„±ìŒ ì„¸ê·¸ë¨¼íŠ¸ì— ëŒ€í•œ ì•ŒíŒŒ ë¹„ìœ¨ì˜ ì‚°ìˆ  í‰ê· , í•˜ë§ˆë²„ê·¸ ì§€ìˆ˜, 0-500Hz ë° 500-1500Hzì˜ ìŠ¤í™íŠ¸ëŸ¼ ìŠ¬ë¡œí”„ê°€ í¬í•¨ë˜ì–´ ì´ 56ê°œì˜ íŒŒë¼ë¯¸í„°ê°€ ìˆìŠµë‹ˆë‹¤. ë˜í•œ 6ê°œì˜ ì‹œê°„ì  íŠ¹ì§•ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. Alpha Ratioì˜ ì‚°ìˆ í‰ê·  Hammarberg Indexì˜ ì‚°ìˆ í‰ê·  0-500 Hz ìŠ¤í™íŠ¸ëŸ¼ ê¸°ìš¸ê¸°ì˜ ì‚°ìˆ í‰ê·  (ë¬´ì„±ìŒ êµ¬ê°„ ì „ì²´) 500-1500 Hz ìŠ¤í™íŠ¸ëŸ¼ ê¸°ìš¸ê¸°ì˜ ì‚°ìˆ í‰ê·  (ë¬´ì„±ìŒ êµ¬ê°„ ì „ì²´) ìŒì„± ì‹ í˜¸ì—ì„œ voiced ë˜ëŠ” unvoiced êµ¬ê°„ì˜ ìµœì†Œ ê¸¸ì´ê°€ ì •í•´ì ¸ ìˆì§€ ì•Šìœ¼ë¯€ë¡œ ì´ëŸ¬í•œ êµ¬ê°„ì´ í•œ í”„ë ˆì„ë§Œí¼ ì§§ì„ ìˆ˜ë„ ìˆë‹¤. ê·¸ëŸ¬ë‚˜ F0 contourì˜ Viterbi-based smoothingì€ ì—ëŸ¬ë¡œ ì¸í•´ ë‹¨ì¼ voiced frameì´ ëˆ„ë½ë˜ëŠ” ê²ƒì„ íš¨ê³¼ì ìœ¼ë¡œ ë°©ì§€í•©ë‹ˆë‹¤. the rate of loudness peaks: ìŒì„± ì‹ í˜¸ì—ì„œ ì´ˆë‹¹ loudness peakì˜ ìˆ˜ë¥¼ ë‚˜íƒ€ë‚´ëŠ” íŒŒë¼ë¯¸í„° continuously voiced regionsì˜ í‰ê·  ê¸¸ì´(F0 &gt; 0), continuously voiced regionsì˜ í‘œì¤€í¸ì°¨(F0 &gt; 0), unvoiced regions (approximating pauses)ì˜ í‰ê·  ê¸¸ì´(F0 &#x3D; 0), unvoiced regions (approximating pauses)ì˜ í‘œì¤€í¸ì°¨(F0 &#x3D; 0), continuous voiced regionsì˜ ì´ˆë‹¹ ê°œìˆ˜ (pseudo syllable rate) ìŒì„± ì‹ í˜¸ì—ì„œ ì—°ì†ì ìœ¼ë¡œ ë°œìƒí•˜ëŠ” voiced êµ¬ê°„ì˜ ê°œìˆ˜ë¥¼ ì´ˆë‹¹ ë‹¨ìœ„ë¡œ ë‚˜íƒ€ë‚´ëŠ” íŒŒë¼ë¯¸í„°ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ íŒŒë¼ë¯¸í„°ëŠ” ìŒì„± ì‹ í˜¸ì—ì„œ ë°œìƒí•˜ëŠ” ì–¸ì–´ì ì¸ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ë° ì‚¬ìš© íŒŒë¼ë¯¸í„°ë“¤ì€ ìŒì„± ì‹ í˜¸ì—ì„œ ë°œìƒí•˜ëŠ” pauseë‚˜ ê°•ì„¸ ë“±ê³¼ ê°™ì€ ë‹¤ì–‘í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ë° ì‚¬ìš© 3.2 Extended Parameter Set 3.1ì˜ íŒŒë¼ë¯¸í„°ì—ëŠ” cepstral parameterë‚˜ dynamic parameterê°€ ê±°ì˜ ë“¤ì–´ê°€ì§€ ì•ŠìŒ Cepstral parameterëŠ” ìŒì„± ì‹ í˜¸ì˜ ì£¼íŒŒìˆ˜ íŠ¹ì„±ì„ ë¶„ì„í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” íŒŒë¼ë¯¸í„°ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ íŒŒë¼ë¯¸í„°ëŠ” Mel-Frequency Cepstral Coefficients (MFCCs)ì™€ ê°™ì€ ë³€í™˜ì„ í†µí•´ ì¶”ì¶œë©ë‹ˆë‹¤. MFCCsëŠ” ìŒì„± ì‹ í˜¸ë¥¼ ì¼ë ¨ì˜ ì£¼íŒŒìˆ˜ ëŒ€ì—­ìœ¼ë¡œ ë¶„í• í•˜ê³ , ê° ëŒ€ì—­ì—ì„œ ì ì ˆí•œ ê³„ìˆ˜ë¥¼ ì¶”ì¶œí•˜ì—¬ êµ¬ì„±ë©ë‹ˆë‹¤. ì´ëŸ¬í•œ ê³„ìˆ˜ë“¤ì€ ìŒì„± ì‹ í˜¸ì˜ ì£¼íŒŒìˆ˜ íŠ¹ì„±ì„ ì˜ ë‚˜íƒ€ë‚´ë©°, ìŒì„± ì¸ì‹ ë° ê°ì • ì¸ì‹ê³¼ ê°™ì€ ë‹¤ì–‘í•œ ì–´í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ìœ ìš©í•˜ê²Œ ì‚¬ìš©ë©ë‹ˆë‹¤. Dynamic parameterëŠ” ì‹œê°„ì ì¸ ë³€í™”ë¥¼ ë‚˜íƒ€ë‚´ëŠ” íŒŒë¼ë¯¸í„°ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ íŒŒë¼ë¯¸í„°ëŠ” ìŒì„± ì‹ í˜¸ì˜ ë™ì ì¸ íŠ¹ì§•ì„ ë¶„ì„í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, Delta ë° Delta-Delta ê³„ìˆ˜ì™€ ê°™ì€ ë™ì  íŒŒë¼ë¯¸í„°ëŠ” MFCCsì™€ ê°™ì€ ì •ì  íŒŒë¼ë¯¸í„°ì— ì¶”ê°€í•˜ì—¬ ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë™ì  íŒŒë¼ë¯¸í„°ëŠ” ìŒì„± ì‹ í˜¸ì˜ ë¹ ë¥¸ ë³€í™”ë¥¼ ì˜ ë‚˜íƒ€ë‚´ë©°, ìŒì„± ì¸ì‹ ë° ê°ì • ì¸ì‹ê³¼ ê°™ì€ ë‹¤ì–‘í•œ ì–´í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ìœ ìš©í•˜ê²Œ ì‚¬ìš©ë©ë‹ˆë‹¤. ì¦‰, ì´ minimal Parameter Setì€ Delta íšŒê·€ ê³„ìˆ˜ ë° ì°¨ì´ íŠ¹ì§•ê³¼ ê°™ì€ ë™ì  íŒŒë¼ë¯¸í„°ë¥¼ í¬í•¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ëŒ€ì‹ , ìƒìŠ¹ ë° í•˜ê°•í•˜ëŠ” F0ì™€ ìŒëŸ‰ ì„¸ê·¸ë¨¼íŠ¸ì˜ ê¸°ìš¸ê¸°ë§Œì´ ì¼ë¶€ ë™ì  ì •ë³´ë¥¼ í¬í•¨í•©ë‹ˆë‹¤. ì¦‰, ì´ Parameter Setì—ì„œëŠ” ë‘ ê°œì˜ ì—°ì†ëœ í”„ë ˆì„ ê°„ ì°¨ì´ë¥¼ ë‚˜íƒ€ë‚´ëŠ” íŒŒë¼ë¯¸í„°ê°€ í¬í•¨ë˜ì–´ ìˆì§€ ì•Šìœ¼ë©°, ìƒìŠ¹ ë° í•˜ê°•í•˜ëŠ” F0ì™€ ìŒëŸ‰ ì„¸ê·¸ë¨¼íŠ¸ì˜ ê¸°ìš¸ê¸°ë§Œì´ ì¼ë¶€ ë™ì  ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ìˆëŠ” íŒŒë¼ë¯¸í„°ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤. [23],[40],[41]ì—ì„œëŠ” affective stateë¥¼ ëª¨ë¸ë§í•˜ëŠ”ë° cepstral íŒŒë¼ë¯¸í„°ê°€ ë§¤ìš° ì„±ê³µì ì¸ ê²ƒìœ¼ë¡œ ì…ì¦ë˜ì—ˆë‹¤. ë”°ë¼ì„œ í™•ì¥ëœ setì—ì„œëŠ” minimalistic setì— ì¶”ê°€ë¡œ 7ê°œì˜ LLDë¥¼ í¬í•¨í•˜ëŠ” ê²ƒìœ¼ë¡œ ì œì•ˆí•¨ Spectral (balance&#x2F;shape&#x2F;dynamics) parameters: MFCC 1 (MFCCì˜ ì²«ë²ˆì§¸ ê³„ìˆ˜) MFCC 2 MFCC 3 MFCC 4, Mel-Frequency Cepstral Coefficients 1-4. MFCCëŠ” ì—¬ëŸ¬ ê°œì˜ ê³„ìˆ˜ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ MFCCëŠ” 12ê°œì—ì„œ 40ê°œê¹Œì§€ì˜ ê³„ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ê³„ìˆ˜ëŠ” ìŒì„± ì‹ í˜¸ì—ì„œ ì¶”ì¶œëœ íŠ¹ì§•ì„ ë‚˜íƒ€ë‚´ë©°, ì¼ë°˜ì ìœ¼ë¡œ ìŒì„± ì¸ì‹, ê°ì • ì¸ì‹ ë° í™”ì ì¸ì‹ê³¼ ê°™ì€ ì‘ì—…ì— ì‚¬ìš©ë©ë‹ˆë‹¤. ë” ë‚®ì€ ìˆœì„œì˜ MFCCëŠ” ì£¼íŒŒìˆ˜ ìŠ¤í™íŠ¸ëŸ¼ì˜ ê¸°ìš¸ê¸°ì™€ ì „ì²´ ìŠ¤í™íŠ¸ëŸ¼ ì—ë„ˆì§€ ë¶„í¬ì™€ ê´€ë ¨ì´ ìˆìœ¼ë©°, ë” ë†’ì€ ìˆœì„œì˜ MFCCëŠ” ë³´ë‹¤ ì„¸ë¶€ì ì¸ ì—ë„ˆì§€ ë¶„í¬ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì´ëŸ¬í•œ ë‹¤ì–‘í•œ ê³„ìˆ˜ë¥¼ ì¡°í•©í•˜ì—¬ ìŒì„± ì²˜ë¦¬ ì‘ì—…ì—ì„œ ë³´ë‹¤ ì •í™•í•œ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Spectral flux, difference of the spectra of two consecutive frames. Spectral fluxëŠ” ìŒì„± ì‹ í˜¸ì˜ ìŠ¤í™íŠ¸ëŸ¼ ë³€í™”ë¥¼ ì¸¡ì •í•˜ëŠ” íŒŒë¼ë¯¸í„° ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. ì´ íŒŒë¼ë¯¸í„°ëŠ” ì—°ì†ëœ í”„ë ˆì„ ê°„ ìŠ¤í™íŠ¸ëŸ¼ ì°¨ì´ì˜ ì œê³±í•©ì„ ê³„ì‚°í•˜ì—¬ êµ¬í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ê³„ì‚°ì€ ìŒì„± ì‹ í˜¸ì˜ ì£¼íŒŒìˆ˜ ì„±ë¶„ì´ ì–¼ë§ˆë‚˜ ë¹ ë¥´ê²Œ ë³€í™”í•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ë©°, ìŒì„± ì‹ í˜¸ì˜ ì—ë„ˆì§€ ë¶„í¬ê°€ ì–´ë–»ê²Œ ë³€í™”í•˜ëŠ”ì§€ë¥¼ ì¶”ì í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Spectral fluxëŠ” ìŒì„± ì¸ì‹ ë° ê°ì • ì¸ì‹ê³¼ ê°™ì€ ë‹¤ì–‘í•œ ì–´í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ìœ ìš©í•˜ê²Œ ì‚¬ìš© Frequency related parameters: Formant 2 bandwidth Formant 3 bandwidth, added for completeness of Formant 1-3 parameters. Formant 1-3 íŒŒë¼ë¯¸í„°ì˜ ì™„ì„±ë„ë¥¼ ìœ„í•´ Formant2,3 ëŒ€ì—­í­ì´ ì¶”ê°€ í•¨ìˆ˜ë¡œì¨ ì‚°ìˆ í‰ê· ê³¼ ë³€ë™ê³„ìˆ˜ëŠ” ì´ 7ê°€ì§€ ì¶”ê°€ LLDì— ëª¨ë‘ ì ìš©ë¨. ì„¸ê·¸ë¨¼íŠ¸(ìœ ,ë¬´ì„± ëª¨ë‘)ë¥¼ í¬í•¨í•˜ì—¬ ìœ ì„±ì˜ì—­ì—ë§Œ í•¨ìˆ˜ê°€ ì ìš©ë˜ëŠ” ëŒ€ì—­í­ì€ ì œì™¸ë¨. â†’ ìµœì¢…ì ìœ¼ë¡œ 14ê°œì˜ discriptorê°€ ì¶”ê°€ê°€ ë¨ ë˜í•œ ë¬´ì„± ì˜ì—­ì—ì„œë§Œ spectral fluxì˜ ì‚°ìˆ  í‰ê· , ìœ ì„±ì˜ì—­ì—ì„œë§Œ spectral fluxì˜ ì‚°ìˆ  í‰ê· ê³¼ ë³€ë™ ê³„ìˆ˜ ë° MFCC 1-4ê°€ í¬í•¨ë¨. â†’ ì´ ê²°ê³¼ 11ê°œì˜ discriptorê°€ ì¶”ê°€ë¨. ì¶”ê°€ì ìœ¼ë¡œ equivalent soud levelì´ í¬í•¨ë¨. Equivalent sound level (LEq)ì€ í”„ë ˆì„ë‹¹ RMS ì—ë„ˆì§€ì˜ í‰ê· ê°’ì„ ë¡œê·¸ ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜í•˜ì—¬ ê³„ì‚°ë©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ìŒì„± ì‹ í˜¸ì˜ í‰ê· ì ì¸ ì—ë„ˆì§€ ë ˆë²¨ì„ ì¸¡ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. LEqëŠ” ì£¼ë¡œ ì†ŒìŒ ë° ìŒí–¥ ê´€ë ¨ ì–´í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ì‚¬ìš©ë˜ë©°, ìŒì„± ê°ì • ì¸ì‹ê³¼ ê°™ì€ ë¶„ì•¼ì—ì„œë„ ìœ ìš©í•˜ê²Œ ì‚¬ìš©ë©ë‹ˆë‹¤. RMS (Root Mean Square) ì—ë„ˆì§€ í‰ê· ê°’ì€ ìŒì„± ì‹ í˜¸ì˜ ì—ë„ˆì§€ë¥¼ ì¸¡ì •í•˜ëŠ” ë°©ë²• ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. ì´ ê°’ì€ ê° í”„ë ˆì„ì—ì„œ ìŒì„± ì‹ í˜¸ì˜ ì§„í­ ê°’ì„ ì œê³±í•œ í›„, í‰ê· ì„ êµ¬í•œ ê°’ì…ë‹ˆë‹¤. RMS ì—ë„ˆì§€ëŠ” ìŒì„± ì‹ í˜¸ì˜ ì „ì²´ì ì¸ ì—ë„ˆì§€ ë ˆë²¨ ì´ë ‡ê²Œ í•˜ë©´ ì´ 26ê°œì˜ ì¶”ê°€ íŒŒë¼ë¯¸í„°ê°€ ìƒì„± ê²°ê³¼ì ìœ¼ë¡œ extended Geneva Minimalistic Acoustic Parameter Set(eGeMAPS)ëŠ” 88ê°œì˜ íŒŒë¼ë¯¸í„°ê°€ í¬í•¨ë˜ì–´ ìˆìŒ. 4.Baseline Evaluation ìœ„ì—ì„œ ì œì•ˆëœ ë‘ê°€ì§€ íŒŒë¼ë¯¸í„°ì„¸íŠ¸ëŠ” ê°ê° binary arousalì™€ binary valence dimensionsì—ì„œ ìë™ ì¸ì‹ ì‘ì—…ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ í‰ê°€ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´, ì—¬ëŸ¬ ê°€ì§€ ê°ì •ì ì¸ ìŒì„± ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì œê³µëœ ì›ë˜ ë¼ë²¨ì„ binary dimensional labels (Arousal&#x2F;Valence)ë¡œ ë§¤í•‘í•˜ì—¬ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. Binary arousalì™€ binary valence dimensionsëŠ” ê°ì • ì¸ì‹ ë¶„ì•¼ì—ì„œ ì‚¬ìš©ë˜ëŠ” ë‘ ê°€ì§€ ì´ì§„ ì°¨ì›ì…ë‹ˆë‹¤. Arousal ì°¨ì›ì€ ê°ì •ì˜ í™œì„±í™” ìˆ˜ì¤€ì„ ë‚˜íƒ€ë‚´ë©°, ë‚®ì€ ìˆ˜ì¤€ì˜ í™œì„±í™”ëŠ” ì¹¨ì°©í•˜ê³  ì§‘ì¤‘ë ¥ì´ ë†’ì€ ìƒíƒœë¥¼ ë‚˜íƒ€ë‚´ê³ , ë†’ì€ ìˆ˜ì¤€ì˜ í™œì„±í™”ëŠ” í¥ë¶„í•˜ê±°ë‚˜ ê³µí¬ì™€ ê°™ì€ ê°•í•œ ê°ì •ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. Valence ì°¨ì›ì€ ê°ì •ì˜ ê¸ì •ì„±&#x2F;ë¶€ì •ì„± ì •ë„ë¥¼ ë‚˜íƒ€ë‚´ë©°, ê¸ì •ì ì¸ ê°ì •ì€ ë†’ì€ valence ê°’ì„ ê°€ì§€ê³  ë¶€ì •ì ì¸ ê°ì •ì€ ë‚®ì€ valence ê°’ì„ ê°€ì§‘ë‹ˆë‹¤. ì´ëŸ¬í•œ ì´ì§„ ì°¨ì›ë“¤ì„ ì‚¬ìš©í•˜ì—¬ ìŒì„± ì‹ í˜¸ì—ì„œ íŠ¹ì •í•œ ê°ì •ì„ ìë™ìœ¼ë¡œ ì¸ì‹í•˜ëŠ” ì‘ì—…ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. ì›ë˜ ë°ì´í„° ì„¸íŠ¸ì— ë¼ë²¨ë§ ëœ ê°ì • TUM AVIC ë°ì´í„°ë² ì´ìŠ¤ì˜ Levels of Interest Geneva Multimodal Emotion Portrayals (GEMEP) corpus ë° German Berlin Emotional Speech database (EMO-DB) professional opera singersì˜ ë…¸ë˜ì—ì„œ ë‚˜íƒ€ë‚œ ê°ì • (GeSiE) FAU AIBO corpusì˜ ì–´ë¦°ì´ë“¤ì˜ ë°œí™”ì—ì„œ ë‚˜íƒ€ë‚œ valence German talk-show recordings (Vera-am-Mittag corpus)ì—ì„œ ë‚˜íƒ€ë‚œ ì‹¤ì œ ê°ì • ì œì•ˆëœ minimal setì€ INTERSPEECH 2009 Emotion Challenge, INTERSPEECH 2010 Paralinguistic Challenge, INTERSPEECH 2011 Speaker State Challenge, INTERSPEECH 2012 Speaker Trait Challenge ë° INTERSPEECH 2013 Computational Paralingusitics ChallengE (ComParE)ì™€ ê°™ì€ ëŒ€ê·œëª¨ brute-forced baseline acoustic feature setê³¼ ë¹„êµë˜ì—ˆìŠµë‹ˆë‹¤. 4.1 Database 4.1.1 FAU AIBO: FAU AIBOëŠ” ì„¸ê³„ ìµœì´ˆì˜ êµ­ì œ ê°ì • ì±Œë¦°ì§€ì˜ ê³µì‹ ë§ë­‰ì¹˜ë¡œ ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ë°ì´í„°ë² ì´ìŠ¤ëŠ” Sony ì• ì™„ë™ë¬¼ ë¡œë´‡ Aiboì™€ ìƒí˜¸ì‘ìš©í•˜ëŠ” ì–´ë¦°ì´ë“¤ì˜ ë°œí™” ë…¹ìŒì„ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ spontaneous í•˜ë©°, ê°ì •ì ìœ¼ë¡œ ìƒ‰ì¹ ëœ ë…ì¼ì–´ ë°œí™”ë¥¼ í¬í•¨í•©ë‹ˆë‹¤. ì–´ë¦°ì´ë“¤ì€ Aibo ë¡œë´‡ì´ ë°©í–¥ì— ëŒ€í•œ ëª©ì†Œë¦¬ ëª…ë ¹ì— ë°˜ì‘í•œë‹¤ê³  ì•Œë ¤ì¡Œì§€ë§Œ, ì‹¤ì œë¡œ ë¡œë´‡ì€ ë•Œë¡œëŠ” ë¶ˆë³µì¢…ì ìœ¼ë¡œ í–‰ë™í•˜ì—¬ ì–´ë¦°ì´ë“¤ë¡œë¶€í„° ê°•í•œ ê°ì • ë°˜ì‘ì„ ìœ ë„í•˜ê¸° ìœ„í•´ ì¸ê°„ ì¡°ì‘ìì— ì˜í•´ ì œì–´ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ë…¹ìŒì€ MONTì™€ OHMì´ë¼ëŠ” ë‘ ê°œì˜ í•™êµì—ì„œ ì´ 51ëª…ì˜ ì–´ë¦°ì´ (10-13ì„¸, 21ëª… ë‚¨ì„±, 30ëª… ì—¬ì„±)ìœ¼ë¡œë¶€í„° ìˆ˜ì§‘ë˜ì—ˆìœ¼ë©°, ì‰¬ëŠ” ì‹œê°„ì„ ì œì™¸í•˜ê³  ì•½ 9.2ì‹œê°„ì˜ ë°œí™”ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. 4.1.2 TUM Audiovisual Interest Corpus (TUM-AVIC): TUM Audiovisual Interest Corpus (TUM-AVIC)ëŠ” spontaneousí•œ affective interactionsì„ í¬í•¨í•˜ëŠ” audiovisual ë…¹ìŒì„ ë‹´ê³  ìˆëŠ” ë°ì´í„°ë² ì´ìŠ¤ì…ë‹ˆë‹¤. ì´ ë°ì´í„°ë² ì´ìŠ¤ëŠ” INTERSPEECH 2010 Paralinguistics Challengeë¥¼ ìœ„í•œ ë°ì´í„°ì…‹ìœ¼ë¡œ ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ë°ì´í„°ì…‹ì€ ì œí’ˆ í”„ë ˆì  í„°ê°€ ëŒ€ìƒìë¥¼ ìƒí’ˆ í”„ë ˆì  í…Œì´ì…˜ì„ í†µí•´ ì•ˆë‚´í•˜ëŠ” ê³¼ì •ì—ì„œ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤. ì‚¬ìš©ëœ ì–¸ì–´ëŠ” ì˜ì–´ì´ì§€ë§Œ, ëŒ€ë¶€ë¶„ì˜ ì œí’ˆ í”„ë ˆì  í„°ëŠ” ë…ì¼ì–´ ì›ì–´ë¯¼ì…ë‹ˆë‹¤. ëŒ€ìƒìë“¤ì€ ì£¼ë¡œ ìœ ëŸ½ ë° ì•„ì‹œì•„ êµ­ì ì…ë‹ˆë‹¤. ì´ ë°ì´í„°ë² ì´ìŠ¤ì—ëŠ” 21ëª…ì˜ ëŒ€ìƒì (ì—¬ì„± 10ëª…)ì˜ ë…¹ìŒì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. LOIëŠ” ê° sub-turnë§ˆë‹¤ (í™”ì turnì˜ ìˆ˜ë™ì ì¸ pause ê¸°ë°˜ sub-divisionì„ í†µí•´ ì°¾ì•„ë‚¸) ì„¸ ê°€ì§€ ë¼ë²¨ë¡œ í‘œì‹œë©ë‹ˆë‹¤. ì´ ì„¸ ê°€ì§€ ë¼ë²¨ì€ boredom (ëŒ€ìƒìê°€ ëŒ€í™”ë‚˜ ì£¼ì œì— ì§€ë£¨í•¨ì„ ëŠë¼ë©° ë§¤ìš° ìˆ˜ë™ì ì´ë©° ëŒ€í™”ë¥¼ ë”°ë¥´ì§€ ì•ŠìŒ; loi1ë¡œë„ ì•Œë ¤ì§), over neutral (ëŒ€ìƒìê°€ ëŒ€í™”ë¥¼ ë”°ë¥´ê³  ì°¸ì—¬í•˜ì§€ë§Œ ì£¼ì œì— ê´€ì‹¬ì´ ìˆëŠ”ì§€ ì•„ë‹ˆë©´ ë¬´ê´€ì‹¬í•œì§€ íŒë‹¨í•  ìˆ˜ ì—†ìŒ; loi2ë¡œë„ ì•Œë ¤ì§), joyful interaction (ëŒ€ìƒìê°€ ëŒ€í™”í•˜ê³  ì£¼ì œì— ëŒ€í•´ ë” ë°°ìš°ê³  ì‹¶ì–´í•˜ëŠ” ê°•í•œ ìš•êµ¬ë¥¼ ë³´ì—¬ì£¼ë©°, ì¦‰, ê·¸/ê·¸ë…€ëŠ” í† ë¡ ì— í° ê´€ì‹¬ì„ ê°€ì§; loi3ìœ¼ë¡œë„ ì•Œë ¤ì§)ì…ë‹ˆë‹¤. ì´ í‰ê°€ì—ì„œëŠ” [47]ì˜ 3,002ê°œì˜ êµ¬ë¬¸(sub-turns)ì´ ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŠ” ì˜ˆë¥¼ ë“¤ì–´ [46]ì—ì„œ ì‚¬ìš©ëœ high interlabeller agreementë¥¼ ê°€ì§„ 996ê°œì˜ êµ¬ë¬¸ë³´ë‹¤ ë” ë§ìŠµë‹ˆë‹¤. high interlabeller agreementëŠ” ì„œë¡œ ë‹¤ë¥¸ ë¼ë²¨ëŸ¬ë“¤ì´ ë™ì¼í•œ ë¼ë²¨ì„ ë¶€ì—¬í•˜ëŠ” ì •ë„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œì…ë‹ˆë‹¤. ì¦‰, ì„œë¡œ ë‹¤ë¥¸ ë¼ë²¨ëŸ¬ë“¤ì´ ë™ì¼í•œ êµ¬ë¬¸ì— ëŒ€í•´ ë™ì¼í•œ ë¼ë²¨ì„ ë¶€ì—¬í•˜ëŠ” ê²½ìš°, ê·¸ êµ¬ë¬¸ì€ high interlabeller agreementë¥¼ ê°€ì§„ë‹¤ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ê²ƒì€ ë°ì´í„°ì…‹ì˜ ì‹ ë¢°ì„±ê³¼ ì¼ê´€ì„±ì„ ë³´ì¥í•˜ê¸° ìœ„í•´ ì¤‘ìš”í•œ ì§€í‘œ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. 4.1.3 Berlin Emotional Speech Database: Berlin Emotional Speech Database ë˜ëŠ” EMO-DBëŠ” ìë™ ê°ì • ë¶„ë¥˜ì˜ íš¨ê³¼ë¥¼ í…ŒìŠ¤íŠ¸í•˜ê¸° ìœ„í•´ ë§¤ìš° ì˜ ì•Œë ¤ì ¸ ìˆê³  ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ë² ì´ìŠ¤ì…ë‹ˆë‹¤. ì´ ë°ì´í„°ë² ì´ìŠ¤ëŠ” Levels of Emotion Annotation Space (LEAS)ì™€ í•¨ê»˜ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤. EMO-DBì—ëŠ” 10ëª…ì˜ ë°°ìš°ê°€ 7ê°€ì§€ ê°ì •(ë¶„ë…¸, ê²½ë©¸, ë‘ë ¤ì›€, ê¸°ì¨, ìŠ¬í””, ìˆ˜ì¹˜ì‹¬, ì¤‘ë¦½)ì„ ë‚˜íƒ€ë‚´ëŠ” ë…ì¼ì–´ ë‹¨ì–´ ë° ë¬¸ì¥ì„ ë°œí™”í•œ ë…¹ìŒì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ ë°ì´í„°ë² ì´ìŠ¤ì—ëŠ” ëŒ€ëµ 5ì‹œê°„ì˜ ì˜¤ë””ì˜¤ ë…¹ìŒì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. 4.1.4 The Geneva Multimodal Emotion Portrayals: The Geneva Multimodal Emotion Portrayals(GEMEP)ì€ 10ëª…ì˜ í”„ë‘ìŠ¤ì–´ ë°°ìš°ê°€ ì—°ê¸°í•œ 1,260ê°œì˜ ë©€í‹°ëª¨ë‹¬ ê°ì • í‘œí˜„ì„ ìˆ˜ì§‘í•œ ë°ì´í„°ë² ì´ìŠ¤ì…ë‹ˆë‹¤. ì´ ë°ì´í„°ë² ì´ìŠ¤ëŠ” ì–¼êµ´ í‘œì •, ìŒì„±, ì œìŠ¤ì²˜ ë“± ë‹¤ì–‘í•œ ëª¨ë‹¬ë¦¬í‹°ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°ì •ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. GEMEP ë°ì´í„°ë² ì´ìŠ¤ëŠ” ë§ì€ ì—°êµ¬ì—ì„œ ì‚¬ìš©ë˜ë©°, ì´ ì—°êµ¬ì—ì„œë„ ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œëŠ” 12ê°€ì§€ ê°ì •ì´ ìˆìœ¼ë©° ì´ 24ê°œëŸ¬ ë¶„ë¥˜ ëœë‹¤. 4.1.5 Geneva Singing Voice Emotion Database(GeSiE): 3ëª…ì˜ ê°€ìˆ˜ê°€ ë…¹ìŒí•œ ê°ì •ì ì¸ ë…¸ë˜ë¥¼ ìˆ˜ì§‘í•œ ë°ì´í„°ë² ì´ìŠ¤ì…ë‹ˆë‹¤. ì´ ë°ì´í„°ë² ì´ìŠ¤ëŠ” 5ëª…ì˜ ì „ë¬¸ ì˜¤í˜ë¼ ê°€ìˆ˜ê°€ ì¶”ê°€ë¡œ ë…¹ìŒí•œ ê²ƒìœ¼ë¡œ í™•ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ 8ëª…ì˜ ê°€ìˆ˜ë“¤ì´ 10ê°€ì§€ ê°ì • ë²”ì£¼ì—ì„œ ì„¸ ê°€ì§€ ë‹¤ë¥¸ êµ¬ì ˆê³¼ ìŒê³„ë¥¼ ë¶€ë¥´ë©° ë…¹ìŒë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ë°ì´í„°ë² ì´ìŠ¤ëŠ” ë§ì€ ì—°êµ¬ì—ì„œ ì‚¬ìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤. Vera-Am-Mittag: ë…ì¼ TV ì‡¼ â€œVera am Mittagâ€ì—ì„œ ì¶”ì¶œí•œ 947ê°œì˜ ê°ì •ì ì¸ ë°œí™”ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ ì‡¼ì—ì„œ ì£¼ì¸ê³µì¸ VeraëŠ” ê²ŒìŠ¤íŠ¸ë“¤ ê°„ì˜ í† ë¡ ì„ ì£¼ê´€í•©ë‹ˆë‹¤. ì´ ë°ì´í„°ë² ì´ìŠ¤ëŠ” ë§¤ìš° ìë°œì ì´ê³  ê°ì •ì ìœ¼ë¡œ ë§¤ìš° ë‹¤ì–‘í•œ ìƒíƒœë¥¼ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ ë°ì´í„°ë² ì´ìŠ¤ì˜ ê°ì •ì€ í™œì„±í™”, ê°€ì¹˜ ë° ì§€ë°°ë ¥&#x2F;ê¶Œë ¥ ì„¸ ê°€ì§€ ì°¨ì›ìœ¼ë¡œ ì„¤ëª…ë©ë‹ˆë‹¤.ë¹„ë””ì˜¤ë¡œ êµ¬ì„±ëœ ë°ì´í„°ë² ì´ìŠ¤ì…ë‹ˆë‹¤. ì´ ì‡¼ì—ì„œ ì£¼ì¸ê³µì¸ VeraëŠ” ê²ŒìŠ¤íŠ¸ë“¤ ê°„ì˜ í† ë¡ ì„ ì£¼ê´€í•©ë‹ˆë‹¤. ì´ ë°ì´í„°ë² ì´ìŠ¤ëŠ” ë§ì€ ì—°êµ¬ì—ì„œ ì‚¬ìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ ë¶€ë¶„ì—ì„œëŠ” EmoReact ë°ì´í„°ë² ì´ìŠ¤ì˜ ì£¼ì„ ë°©ë²•ì— ëŒ€í•œ ì •ë³´ê°€ ì œê³µë©ë‹ˆë‹¤. ì£¼ì„ìë“¤ì€ ê°ê°ì˜ ê°ì • ì°¨ì›ì— ëŒ€í•´ ë‹¤ì„¯ ê°œì˜ ì´ë¯¸ì§€ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•  ìˆ˜ ìˆëŠ” ì•„ì´ì½˜ ê¸°ë°˜ ë°©ë²•ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ì£¼ì„ìë“¤ì€ ë¨¼ì € ìˆ˜ë™ìœ¼ë¡œ ë¶„í• ëœ ë°œí™”ë¥¼ ë“£ê³ , ê·¸ ë°œí™”ì—ì„œ ê°€ì¥ ì˜ ì„¤ëª…í•˜ëŠ” ê°ì • ì°¨ì›ì— ëŒ€í•œ ì•„ì´ì½˜ì„ ì„ íƒí•´ì•¼ í–ˆìŠµë‹ˆë‹¤. ì´ ì•„ì´ì½˜ì˜ ì„ íƒì€ í›„ì— ê° ì°¨ì›ë§ˆë‹¤ [-1;1] ë²”ìœ„ ë‚´ì—ì„œ ê· ë“±í•˜ê²Œ ë¶„í¬ëœ ë‹¤ì„¯ ê°€ì§€ ë²”ì£¼ë¡œ ë§¤í•‘ë˜ì—ˆìœ¼ë©°, ì£¼ì„ìì˜ í™•ì‹ ë„ë¥¼ ê³ ë ¤í•˜ëŠ” ê°€ì¤‘ì¹˜ í•¨ìˆ˜ë¥¼ ì ìš©í•˜ì—¬ í‰ê· ê°’ì„ ê³„ì‚°í–ˆìŠµë‹ˆë‹¤. ë¹„êµì  í‰ê°€ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•˜ê¸° ìœ„í•´ ì—°ì†ì ì¸ ê°€ì¹˜ì™€ í™œì„±í™” ë¼ë²¨ì€ ë„¤ ê°œì˜ í´ë˜ìŠ¤ë¡œ ì´ì‚°í™”ë˜ì—ˆìœ¼ë©°, ì´ëŠ” activation-valence ê³µê°„ì˜ ë„¤ ì‚¬ë¶„ë©´(q1:positive-active, q2:positive-passive, q3:negative-passive, and q4:negative-active)ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. 4.2 Common Mapping of Emotions ì´ ë¶€ë¶„ì—ì„œëŠ” ëª¨ë“  ë°ì´í„°ì…‹ì—ì„œ ê²°ê³¼ì™€ íŠ¹ì§• ì§‘í•©ì˜ ì„±ëŠ¥ì„ ë¹„êµí•  ìˆ˜ ìˆë„ë¡ í•˜ê¸° ìœ„í•´, ê°ê°ì˜ ë°ì´í„°ì…‹ì— ëŒ€í•œ íŠ¹ì •í•œ ê°ì • ë¼ë²¨ì„ ê³µí†µì  binary arousal and valence representationìœ¼ë¡œ ë§¤í•‘í–ˆë‹¤ëŠ” ê²ƒì„ ì„¤ëª…í•©ë‹ˆë‹¤. ì´ ë§¤í•‘ì€ [43], [47], [49] (GEMEPì˜ ê²½ìš°)ì—ì„œ ì œì•ˆëœ ëŒ€ë¡œ [53]ë¥¼ ì°¸ê³ í•˜ì—¬ ìˆ˜í–‰ë˜ì—ˆìŠµë‹ˆë‹¤. GeSiEì˜ ê²½ìš° GEMEPì—ì„œ ì‚¬ìš©ëœ ì ˆì°¨ì™€ ìœ ì‚¬í•˜ê²Œ ë§¤í•‘ì´ ìˆ˜í–‰ë˜ì—ˆìŠµë‹ˆë‹¤. Table 1ì€ ê°ì • ë²”ì£¼ë¥¼ ë°”ì´ë„ˆë¦¬ í™œì„±í™” ë° ê°€ì¹˜ ë¼ë²¨ë¡œ ë§¤í•‘í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. Note, that for FAU AIBO: ì›ë˜ 5ê°œì˜ í´ë˜ìŠ¤ ë ˆì´ë¸” íŠ¹ì„±ìƒ binary valenceì— ëŒ€í•œ ë§¤í•‘ë§Œ ê°€ëŠ¥ 4.3 Experimental Protocol AIBOì— ëŒ€í•œ ì‹¤í—˜ì„ ì œì™¸í•œ ëª¨ë“  ì‹¤í—˜ì€ LOSO(Leave-One-Speaker(Group)-Out) êµì°¨ ê²€ì¦ì„ ì‚¬ìš©í•˜ì—¬ ìˆ˜í–‰ë©ë‹ˆë‹¤. LOSOëŠ” â€œLeave-One-Speaker-Outâ€ì˜ ì•½ì–´ë¡œ, ê° ì‹¤í—˜ì—ì„œ í•˜ë‚˜ì˜ í™”ìë¥¼ ì œì™¸í•œ ëª¨ë“  í™”ì ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•˜ê³ , ë‚˜ë¨¸ì§€ í™”ìì˜ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í‰ê°€í•˜ëŠ” êµì°¨ ê²€ì¦ ë°©ë²•ì…ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ëª¨ë¸ì´ ë‹¤ë¥¸ í™”ìë“¤ì— ëŒ€í•´ì„œë„ ì¼ë°˜í™”ë  ìˆ˜ ìˆëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤. ì¦‰, ê°ê°ì˜ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ëŠ” í•œ ëª…ì˜ í™”ìì— ëŒ€í•œ ë°ì´í„°ë§Œ í¬í•¨í•˜ë©°, ì´ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œ ëª¨ë“  í™”ìì— ëŒ€í•œ í‰ê°€ê°€ ìˆ˜í–‰ë©ë‹ˆë‹¤. (êµì°¨ê²€ì¦ì˜ ì¼ì¢…) GeSiE ë°ì´í„°ì…‹ì²˜ëŸ¼ í™”ì ìˆ˜ê°€ 8ëª… ì´í•˜ì¸ ê²½ìš°, ê° í™”ìì˜ ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ êµì°¨ ê²€ì¦ í´ë“œë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ 8ëª… ì´ìƒì¸ ê²½ìš°, í™”ì IDê°€ ë¬´ì‘ìœ„ë¡œ 8ê°œì˜ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ„ì–´ì§€ê³ , ì´ì— ë”°ë¼ ë°ì´í„°ê°€ 8ê°œì˜ í´ë“œë¡œ ë¶„í• ë©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ êµì°¨ ê²€ì¦ì€ ê°ê° 7ê°œ í´ë“œì—ì„œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ 8ê°œì˜ ë‹¤ë¥¸ ëª¨ë¸ì„ í›ˆë ¨í•˜ê³ , ì²« ë²ˆì§¸ ëª¨ë¸ì— ëŒ€í•œ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì²« ë²ˆì§¸ í´ë“œë¥¼ ì œì™¸í•˜ê³  ë‘ ë²ˆì§¸ ëª¨ë¸ì— ëŒ€í•œ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ë‘ ë²ˆì§¸ í´ë“œë¥¼ ì œì™¸í•˜ëŠ” ì‹ìœ¼ë¡œ ìˆ˜í–‰ë©ë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ì „ì²´ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì˜ˆì¸¡ì´ í›ˆë ¨ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤‘ë³µ ì—†ì´ ìƒì„±ë©ë‹ˆë‹¤. FAU AIBOì˜ ê²½ìš°, OHMì—ì„œ í•™ìŠµí•˜ê³  MONTì—ì„œ í‰ê°€í•˜ëŠ” ë°©ì‹ê³¼ MONTì—ì„œ í•™ìŠµí•˜ê³  OHMì—ì„œ í‰ê°€í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë‘ ê°œì˜ êµì°¨ ê²€ì¦ í´ë“œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. êµì°¨ ê²€ì¦ í´ë“œëŠ” êµì°¨ ê²€ì¦ì—ì„œ ì‚¬ìš©ë˜ëŠ” ë°ì´í„° ì„¸íŠ¸ì˜ í•˜ìœ„ ì§‘í•©ì…ë‹ˆë‹¤. ì „ì²´ ë°ì´í„° ì„¸íŠ¸ë¥¼ ì—¬ëŸ¬ ê°œì˜ í´ë“œë¡œ ë‚˜ëˆ„ì–´ ê°ê°ì„ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì™€ í•™ìŠµ ì„¸íŠ¸ë¡œ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë°˜ë³µì ìœ¼ë¡œ í•™ìŠµí•˜ê³  í‰ê°€í•˜ëŠ” ë°©ë²•ì—ì„œ, ê°ê°ì˜ í´ë“œëŠ” ì„œë¡œ ë‹¤ë¥¸ ë°ì´í„°ë¥¼ í¬í•¨í•˜ë©°, ì „ì²´ ë°ì´í„° ì„¸íŠ¸ë¥¼ ëŒ€í‘œí•  ìˆ˜ ìˆëŠ” í¬ê¸°ì™€ ë¶„í¬ë¥¼ ê°€ì§€ë„ë¡ êµ¬ì„±ë©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, 10ê°œì˜ í´ë“œë¡œ ë‚˜ëˆ„ì–´ êµì°¨ ê²€ì¦ì„ ìˆ˜í–‰í•˜ëŠ” ê²½ìš°, 10ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ í•˜ìœ„ ì§‘í•©ìœ¼ë¡œ ë°ì´í„°ê°€ ë¶„í• ë˜ë©°, ê°ê°ì˜ í´ë“œëŠ” 10%ì”©ì˜ ë°ì´í„°ë¥¼ í¬í•¨í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œ ëª¨ë“  ë°ì´í„°ê°€ í…ŒìŠ¤íŠ¸ ë° í•™ìŠµì— ì‚¬ìš©ë˜ë©°, ëª¨ë¸ì´ ì¼ë°˜í™”ë  ìˆ˜ ìˆëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. paralinguistics ë¶„ì•¼ì—ì„œ ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ì •ì  ë¶„ë¥˜ê¸°ë¡œì„œ, support-vector machines(SVMs)ê°€ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤. SVMsëŠ” WEKA [54]ì—ì„œ êµ¬í˜„ëœ ìˆœì°¨ ìµœì†Œ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ í•™ìŠµë©ë‹ˆë‹¤. ëª¨ë¸ ë³µì¡ë„ Cì˜ ê°’ ë²”ìœ„ê°€ í‰ê°€ë˜ë©°, ê²°ê³¼ëŠ” ë§¤ê°œ ë³€ìˆ˜ ì„¸íŠ¸ì˜ ì„±ëŠ¥ê³¼ ê´€ë ¨í•˜ì—¬ ë” ì•ˆì •ì ì¸ ê²°ê³¼ë¥¼ ì–»ê¸° ìœ„í•´ ì „ì²´ ë²”ìœ„ì—ì„œ í‰ê· í™”ë©ë‹ˆë‹¤. cê°’ì˜ ë²”ìœ„ëŠ” C1&#x3D;0.000025, C2&#x3D;0.00005, C3&#x3D;0.000075, C4&#x3D;0.0001, â€¦, C15&#x3D;0.075, C16&#x3D;0.1, C17&#x3D;0.25ìœ¼ë¡œ ì´ 17ê°œì˜ ê°’ì´ í‰ê°€ ëŒ€ìƒì…ë‹ˆë‹¤. ì´ ë²”ìœ„ì—ì„œ ëª¨ë¸ ë³µì¡ë„ë¥¼ í‰ê°€í•˜ê³  ê²°ê³¼ë¥¼ í‰ê· í™”í•˜ì—¬ ë§¤ê°œ ë³€ìˆ˜ ì„¸íŠ¸ì˜ ì„±ëŠ¥ì„ ë” ì•ˆì •ì ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤. SVMsë¥¼ êµ¬í˜„í•˜ëŠ” ë° í•„ìš”í•œ ë°ì´í„° ê· í˜• ë¬¸ì œì— ëŒ€í•´ ì„¤ëª…í•˜ê³  ìˆìŠµë‹ˆë‹¤. SVMsë¥¼ ì‚¬ìš©í•  ë•Œ, ë‹¤ìˆ˜ í´ë˜ìŠ¤ì— ëŒ€í•œ ì‚¬ì „ í¸í–¥ì„ í”¼í•˜ê¸° ìœ„í•´ ê° í´ë˜ìŠ¤ì— ëŒ€í•´ ë™ì¼í•œ ìˆ˜ì˜ ì¸ìŠ¤í„´ìŠ¤ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ê°ê°ì˜ í›ˆë ¨ íŒŒí‹°ì…˜ì€ ê· í˜•ì„ ë§ì¶”ì–´ì•¼ í•©ë‹ˆë‹¤. Up-samplingì€ ì†Œìˆ˜ í´ë˜ìŠ¤ì˜ ë°ì´í„°ë¥¼ ë³µì œí•˜ì—¬ ë‹¤ìˆ˜ í´ë˜ìŠ¤ì™€ ë™ì¼í•œ ìˆ˜ì˜ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ìˆ˜í–‰ë©ë‹ˆë‹¤. ì´ë ‡ê²Œ í•¨ìœ¼ë¡œì¨, ëª¨ë¸ì´ ë‹¤ìˆ˜ í´ë˜ìŠ¤ì— ëŒ€í•´ ì‚¬ì „ í¸í–¥ì„ í•™ìŠµí•˜ì§€ ì•Šë„ë¡ í•˜ê³ , ì†Œìˆ˜ í´ë˜ìŠ¤ì˜ ì •ë³´ë¥¼ ë” ì˜ ë°˜ì˜í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. Up-samplingì€ ì¼ë°˜ì ìœ¼ë¡œ ë¬´ì‘ìœ„ë¡œ ì„ íƒëœ ì†Œìˆ˜ í´ë˜ìŠ¤ ìƒ˜í”Œì„ ë³µì œí•˜ì—¬ ë°ì´í„° ì„¸íŠ¸ì˜ í¬ê¸°ë¥¼ ì¦ê°€ì‹œí‚¤ëŠ” ë°©ì‹ìœ¼ë¡œ ìˆ˜í–‰ë©ë‹ˆë‹¤. VMsë¥¼ ìˆ˜ì¹˜ì ìœ¼ë¡œ íš¨ìœ¨ì ìœ¼ë¡œ ë§Œë“¤ê¸° ìœ„í•´ ëª¨ë“  ìŒí–¥ ë§¤ê°œ ë³€ìˆ˜ê°€ ê³µí†µ ê°’ ë²”ìœ„ë¡œ ì •ê·œí™”ë˜ì–´ì•¼ í•¨ì„ ì„¤ëª…í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ z-ì •ê·œí™”, ì¦‰ í‰ê· ì´ 0ì´ê³  ë¶„ì‚°ì´ 1ì¸ ì •ê·œí™”ê°€ ìˆ˜í–‰ë©ë‹ˆë‹¤. ì´ ë…¼ë¬¸ì—ì„œëŠ” ì •ê·œí™” ë§¤ê°œ ë³€ìˆ˜ë¥¼ ê³„ì‚°í•˜ê³  ì ìš©í•˜ëŠ” ì„¸ ê°€ì§€ ë‹¤ë¥¸ ë°©ë²•ì„ ì¡°ì‚¬í•©ë‹ˆë‹¤. ì²«ì§¸, ì „ì²´ í›ˆë ¨ íŒŒí‹°ì…˜ì—ì„œ í‰ê· ê³¼ ë¶„ì‚°ì„ ê³„ì‚°í•˜ëŠ” ë°©ë²•(std)ì…ë‹ˆë‹¤. ë‘˜ì§¸, ê° í™”ìì— ëŒ€í•´ ê°œë³„ì ìœ¼ë¡œ í‰ê· ê³¼ ë¶„ì‚°ì„ ê³„ì‚°í•˜ëŠ” ë°©ë²•(spkstd)ì…ë‹ˆë‹¤. ì´ ë°©ë²•ì€ [55]ì™€ ìœ ì‚¬í•©ë‹ˆë‹¤. ì…‹ì§¸, í›ˆë ¨ ë° í…ŒìŠ¤íŠ¸ íŒŒí‹°ì…˜ ê°ê°ì— ëŒ€í•´ ê°œë³„ì ìœ¼ë¡œ í‰ê· ê³¼ ë¶„ì‚°ì„ ê³„ì‚°í•˜ëŠ” ë°©ë²•(stdI)ì…ë‹ˆë‹¤. 4.4 Result ì œì•ˆëœ ìµœì†Œí•œì˜ ë§¤ê°œ ë³€ìˆ˜ ì„¸íŠ¸ì™€ Interspeech Challenges ì‹œë¦¬ì¦ˆì—ì„œ ì‚¬ìš©ëœ ëŒ€ê·œëª¨ brute-forced ë§¤ê°œ ë³€ìˆ˜ ì„¸íŠ¸ ê°„ì˜ ê²°ê³¼ë¥¼ ë¹„êµí•©ë‹ˆë‹¤. brute-forced ë§¤ê°œ ë³€ìˆ˜ ì„¸íŠ¸ëŠ” Interspeech ChallengesëŠ” 2009ë…„ [43] (InterSp09)ì˜ ê°ì •, 2010ë…„ [36] (InterSp10)ì˜ ì—°ë ¹ ë° ì„±ë³„, ê´€ì‹¬ë„ ìˆ˜ì¤€, 2011ë…„ [44] (InterSp11)ì˜ í™”ì ìƒíƒœ, 2012ë…„ [45] (InterSp12)ì˜ í™”ì íŠ¹ì„± ë° 2013 ë° 2014ë…„ [12], [37] (ComParE)ì˜ ì „ì‚°ì–¸ì–´í•™ì— ëŒ€í•œ ì‹œë¦¬ì¦ˆë¡œ êµ¬ì„±ì´ ë˜ì–´ìˆìŠµë‹ˆë‹¤. binary arousal and binary valence classificationì˜ ê²°ê³¼ ë§¤ê°œ ë³€ìˆ˜ ì„¸íŠ¸ë¥¼ ì œì™¸í•œ ëª¨ë“  ë³€ìˆ˜ë¥¼ ì œê±°í•˜ê¸° ìœ„í•´ ê²°ê³¼ëŠ” ë‹¤ì„¯ ê°œì˜ ë°ì´í„°ë² ì´ìŠ¤(all, FAU AIBO ì œì™¸)ì™€ C&#x3D;0.0025ë¶€í„° ì‹œì‘í•˜ëŠ” ê°€ì¥ ë†’ì€ ì•„í™‰ ê°œì˜ SVM ë³µì¡ë„ ì„¤ì •ì—ì„œ í‰ê· í™”ë©ë‹ˆë‹¤. ë” ë†’ì€ ë³µì¡ë„ ì„¤ì •ì—ì„œë§Œ í‰ê· ì„ ê³„ì‚°í•˜ëŠ” ê²°ì •ì€ ì‘ì€ íŠ¹ì§• ì§‘í•©ì˜ ê²½ìš° ì´ ì„ê³„ê°’ë³´ë‹¤ ë‚®ì€ ë³µì¡ë„ì—ì„œ ì„±ëŠ¥ì´ í¬ê²Œ ì €í•˜ë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ì €í•˜ê°€ í‰ê· í™”ë¥¼ í¸í–¥ì‹œí‚¤ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. UARì€ Unweighted Average Recallì˜ ì•½ìë¡œ, ê° í´ë˜ìŠ¤ì— ëŒ€í•œ ì¬í˜„ìœ¨ì„ í‰ê· í™”í•œ ê°’ì…ë‹ˆë‹¤. ì´ ë¶€ë¶„ì—ì„œëŠ” FAU AIBOë¥¼ ì œì™¸í•œ ëª¨ë“  ë°ì´í„°ë² ì´ìŠ¤ì™€ 9ê°œì˜ ìµœê³  SVM ë³µì¡ë„(C&gt;&#x3D;0.0025)ì—ì„œ UARì„ í‰ê· í™”í•©ë‹ˆë‹¤. ì´ëŠ” ê°€ì¤‘ì¹˜ê°€ ì ìš©ë˜ì§€ ì•Šì€ í‰ê· ì…ë‹ˆë‹¤. ë˜í•œ, ê° í™”ìì— ëŒ€í•œ í‘œì¤€í™”ì™€ ê· í˜• ì¡íŒ í›ˆë ¨ ì„¸íŠ¸ë¥¼ ìœ„í•´ ì¸ìŠ¤í„´ìŠ¤ ì—…ìƒ˜í”Œë§ì´ ìˆ˜í–‰ë©ë‹ˆë‹¤. ê° í´ë˜ìŠ¤ì— ëŒ€í•œ ì¬í˜„ìœ¨ì€ ë¶„ë¥˜ê¸°ê°€ ì‹¤ì œ ì–‘ì„±ì¸ ìƒ˜í”Œì„ ì–¼ë§ˆë‚˜ ì˜ ì°¾ì•„ëƒˆëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œì…ë‹ˆë‹¤. ì¦‰, ë¶„ë¥˜ê¸°ê°€ ì°¸ ì–‘ì„±(True Positive)ìœ¼ë¡œ ë¶„ë¥˜í•œ ìƒ˜í”Œ ìˆ˜ë¥¼ ì‹¤ì œ ì–‘ì„±(True Positive + False Negative)ì¸ ì „ì²´ ìƒ˜í”Œ ìˆ˜ë¡œ ë‚˜ëˆˆ ê°’ì…ë‹ˆë‹¤. ì´ ê°’ì€ í•´ë‹¹ í´ë˜ìŠ¤ì˜ ë¶„ë¥˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. í‰ê·  ê²°ê³¼ì—ì„œ GeMAPS ì„¸íŠ¸ì˜ ë†’ì€ íš¨ìœ¨ì„±ì„ í™•ì¸í• ìˆ˜ ìˆìŠµë‹ˆë‹¤. eGeMAPS ì„¸íŠ¸ëŠ” arousalì—ì„œ ê±°ì˜ 80%ì˜ UARì— ë„ë‹¬í•˜ì—¬ ê°€ì¥ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. eGeMAPS ì„¸íŠ¸ëŠ” GEMEP ë°ì´í„°ë² ì´ìŠ¤ì˜ ì´ì§„ ê°ì„± ë¶„ë¥˜ì™€ GeSiE ë°ì´í„°ë² ì´ìŠ¤ì˜ ì´ì§„ ê°€ì¹˜ ë¶„ë¥˜ì—ì„œ ìµœìƒì˜ ê²°ê³¼ë¥¼ ë³´ì…ë‹ˆë‹¤. eGeMAPS ì„¸íŠ¸ëŠ” í•­ìƒ GeMAPS ì„¸íŠ¸ë³´ë‹¤ ìš°ìˆ˜í•˜ê±°ë‚˜ ë™ì¼í•˜ë©°, ì´ëŠ” ì¶”ê°€ íŒŒë¼ë¯¸í„°(MFCC ë° ìŠ¤í™íŠ¸ëŸ¼ í”ŒëŸ­ìŠ¤ íŠ¹íˆ)ì˜ ì¤‘ìš”ì„±ì„ ì‹œì‚¬í•©ë‹ˆë‹¤. ì´ëŠ” íŠ¹íˆ ê°€ì¹˜ì—ì„œ GeMAPSì™€ eGeMAPS ê°„ì˜ í‰ê·  ì°¨ì´ê°€ ë” í° ê²½ìš°ì— ê·¸ëŸ¬í•˜ë©°, ì´ë¡œì¨ ìŒí–¥ ê°€ì¹˜ì— ëŒ€í•œ ê·¸ëŸ¬í•œ íŒŒë¼ë¯¸í„°ì˜ ì¤‘ìš”ì„±ì„ ì œì•ˆí•©ë‹ˆë‹¤. í° ê·œëª¨ì˜ íŒŒë¼ë¯¸í„° ì„¸íŠ¸ë“¤ì— ë¹„í•´ ì „ë°˜ì ìœ¼ë¡œ GeMAPS ì„¸íŠ¸ëŠ” ìµœì†Œí•œì˜ í¬ê¸°ì„ì—ë„ ë¶ˆêµ¬í•˜ê³  ë†€ëê²Œë„ ë¹„ìŠ·í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. í–¥í›„ ì—°êµ¬ì—ì„œëŠ” ì œì•ˆëœ ìµœì†Œí•œì˜ ì„¸íŠ¸ê°€ êµì°¨ ë°ì´í„°ë² ì´ìŠ¤ ë¶„ë¥˜ ì‹¤í—˜ì—ì„œ ë” ë‚˜ì€ ì¼ë°˜í™”ë¥¼ ì–»ì„ ìˆ˜ ìˆëŠ”ì§€ ì¡°ì‚¬í•´ì•¼ í•©ë‹ˆë‹¤. 5.Discussion and Conclusion GeMAPSëŠ” ìˆ˜ë™ ìƒí˜¸ì‘ìš©ì´ë‚˜ ë³´ì • ì—†ì´ ì˜¤ë””ì˜¤ íŒŒí˜•ì—ì„œ ìŒí–¥ íŒŒë¼ë¯¸í„° ì„¸íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” ìë™ ì¶”ì¶œ ì‹œìŠ¤í…œì„ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ íŠ¹ì • í˜„ìƒê³¼ ê´€ë ¨ì„±ì´ ìˆê±°ë‚˜ ìƒê´€ê´€ê³„ê°€ ìˆëŠ” ê²ƒìœ¼ë¡œ ë°í˜€ì§„ ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ ìë™ìœ¼ë¡œ ì•ˆì •ì ìœ¼ë¡œ ì¶”ì¶œí•  ìˆ˜ ìˆëŠ” ê²ƒì€ ì•„ë‹™ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ëª¨ìŒê¸°ë°˜ í¬ë¨¼íŠ¸ ë¶„ì„ì—ëŠ” ì‹ ë¢°í• ìˆ˜ ìˆëŠ” ìë™ëª¨ìŒê°ì§€ë° ë¶„ë¥˜ ì‹œìŠ¤í…œì´ í•„ìš”í•©ë‹ˆë‹¤. ë”°ë¼ì„œ GeMAPSì—ì„œëŠ” ê¹¨ë—í•œ ìŒí–¥ ì¡°ê±´ì—ì„œ ê°ë…ì—†ì´ ì•ˆì •ì ìœ¼ë¡œ ì¶”ì¶œí•  ìˆ˜ ìˆëŠ” íŒŒë¼ë¯¸í„°ë§Œ í¬í•¨í–ˆìŠµë‹ˆë‹¤. ê²€ì¦ ì‹¤í—˜ì€ ë°ì´í„° ë² ì´ìŠ¤ê°„ ìµœìƒì˜ ë¹„êµ ê°€ëŠ¥ì„±ì„ ìœ„í•´ binary classification ì‹¤í—˜ìœ¼ë¡œ ì œí•œë˜ì—ˆìŠµë‹ˆë‹¤. ìë™ ì¶”ì¶œì„ í†µí•œ í‘œì¤€ íŒŒë¼ë¯¸í„° ì„¸íŠ¸ì˜ ì ì¬ì  ìœ„í—˜ ì¤‘ í•˜ë‚˜ëŠ” ë°œì„± í˜„ìƒê³¼ì˜ ì—°ê²°ì´ ë¬´ì‹œë  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. íŒŒë¼ë¯¸í„° ì„¸íŠ¸ë¥¼ ì„ íƒí•  ë•Œ, ì´ëŸ¬í•œ ì—°ê²°ì„ ê°•ì¡°í•˜ê³  ìˆ˜ì§‘ ê¸°ì¤€ ì¤‘ í•˜ë‚˜ë¡œ ê¸°ë³¸ ë°œì„± ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. í–¥í›„ ì—°êµ¬ë¥¼ í†µí•´ ì´ëŸ¬í•œ ê¸°ì´ˆë¥¼ ê°•í™”í•˜ê³  ìƒˆë¡œìš´ í†µì°°ë ¥ì„ ì œê³µí•  ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ê°ì„±ê³¼ ë¹ ë¥¸ ë°œì„± ë°&#x2F;ë˜ëŠ” ì¡°ìŒ ì œìŠ¤ì²˜ì™€ ê´€ë ¨ì´ ìˆë‹¤ê³  ì˜ˆìƒí•  ìˆ˜ ìˆìœ¼ë©°, í‰í™”ë¡œìš´ ì„±ê²©ì€ ëŠë¦° ì œìŠ¤ì²˜ë¡œë¶€í„° ë¹„ë¡¯ëœë‹¤ê³  í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ì•ìœ¼ë¡œëŠ” ë°œì„±ì˜ ìŒí–¥ ì¶œë ¥ì„ ì†Œë¦¬ í¬ê¸°, í”¼ì¹˜ ë“± ê¸°ë³¸ íŒŒë¼ë¯¸í„°ë¥¼ ë„˜ì–´ ìƒë¦¬í•™ì ìœ¼ë¡œ ê´€ë ¨ëœ íŒŒë¼ë¯¸í„°ë¡œ ì´í•´ë¥¼ í™•ì¥í•˜ëŠ” ê²ƒì´ ê°€ì¹˜ê°€ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë§¥ë½ì—ì„œ ì„±ëŒ€ ì ‘ì°©ì€ íŠ¹íˆ ê´€ë ¨ì„±ì´ ë†’ì€ íŒŒë¼ë¯¸í„°ì…ë‹ˆë‹¤. ì ‘ì°© ì¦ê°€ëŠ” ë‹«íŒ ë‹¨ê³„ë¥¼ ê¸¸ê²Œ í•˜ê³  íš¡ì„±ëŒ€ ê³µê¸° íë¦„ í„ìŠ¤ì˜ ì§„í­ì„ ì¤„ì…ë‹ˆë‹¤. ì´ë¡œ ì¸í•´ ìŒì„± ì›ì²œ ê¸°ë³¸ê°’ì˜ ê°ì‡„ ë˜ëŠ” ë” êµ¬ì²´ì ìœ¼ë¡œëŠ” ìŒì„± ì›ì²œ ë¶€ë¶„ ìŒ ë‘ ê°œ ì‚¬ì´ì˜ ë ˆë²¨ ì°¨ì´ë¥¼ ì¤„ì¼ ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë°©ì‚¬ìŒì—ì„œ ì´ ë ˆë²¨ ì°¨ì´ëŠ” ì£¼ë¡œ ì²« ë²ˆì§¸ í¬ë¨¼íŠ¸ì˜ ì£¼íŒŒìˆ˜ì— ì˜í–¥ì„ ë°›ìœ¼ë©°, ì´ëŠ” ë°œì„±ì˜ ê°ì • ìƒ‰ì±„ì— ì´ì°¨ì ì¸ ì¤‘ìš”ì„±ì„ ê°€ì§‘ë‹ˆë‹¤. GeMAPSì˜ í–¥í›„ ë°œì „ì€ ìŒì„± ì›ì²œ íŒŒë¼ë¯¸í„°ë¥¼ ì§ì ‘ ì¸¡ì •í•˜ê¸° ìœ„í•´ ìŒí–¥ ì¶œë ¥ ì‹ í˜¸ë¥¼ ì—­ í•„í„°ë§í•˜ëŠ” ê¸°ìˆ ì˜ ì¶”ê°€ë¥¼ í¬í•¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë¶„ì„ì€ ê°ì • í‘œí˜„ì˜ ë‹¤ì–‘í•œ ìŒí–¥ ì¶œë ¥ íŠ¹ì„±ì˜ ìƒë¦¬ì  ìƒê´€ ê´€ê³„ë¥¼ í™•ì¸í•  ìˆ˜ ìˆê²Œ í•˜ì—¬ ê°ì •ì  ê°ì„±ì´ ìŒì„± ìƒì‚°ì— ë¯¸ì¹˜ëŠ” ë©”ì»¤ë‹ˆì¦˜ì— ëŒ€í•œ ì§€ì‹ì„ ê°•í™”í•©ë‹ˆë‹¤.","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speech Emotion Recognition","slug":"Paper/Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/categories/Paper/Speech-Emotion-Recognition/"}],"tags":[{"name":"Speech Emotion Recognition","slug":"Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/tags/Speech-Emotion-Recognition/"},{"name":"Acoustic Parameter Set","slug":"Acoustic-Parameter-Set","permalink":"https://jmj3047.github.io/tags/Acoustic-Parameter-Set/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speech Emotion Recognition","slug":"Paper/Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/categories/Paper/Speech-Emotion-Recognition/"}]},{"title":"Neural Networks and Deep Learning","slug":"DL_Part1","date":"2023-04-18T15:00:00.000Z","updated":"2023-05-31T13:16:29.416Z","comments":true,"path":"2023/04/19/DL_Part1/","link":"","permalink":"https://jmj3047.github.io/2023/04/19/DL_Part1/","excerpt":"","text":"Course Lecture 1 of Deep Learning Course Derivatives https://community.deeplearning.ai/t/derivation-of-dl-dz/165 vectorizing logistic regression Why we like to use that cost function for logistic regression? maximum likelihood estimation by minimizing this cost function J(w,b) weâ€™re really carrying out maximum likelihood estimation with the logistic regression model. Under the assumption that our training examples were IID, or identically independently distributed. Getting your Matrix dimensions right Lecture note: https://community.deeplearning.ai/t/dls-course-1-lecture-notes/11862","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Coursera","slug":"Coursera","permalink":"https://jmj3047.github.io/tags/Coursera/"},{"name":"AI Studies","slug":"AI-Studies","permalink":"https://jmj3047.github.io/tags/AI-Studies/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}]},{"title":"Gen App Builder","slug":"Gen_App_Builder","date":"2023-04-17T15:00:00.000Z","updated":"2023-04-18T09:07:08.073Z","comments":true,"path":"2023/04/18/Gen_App_Builder/","link":"","permalink":"https://jmj3047.github.io/2023/04/18/Gen_App_Builder/","excerpt":"","text":"ê°œìš” êµ¬ê¸€ì—ì„œ ìƒˆë¡­ê²Œ ë°œí‘œí•œ gen app builderì— ëŒ€í•´ì„œ ì•Œì•„ë³´ì ì–´ë–¤ ê¸°ëŠ¥ì´ ìˆê³  official docì´ ë°œí‘œëœ ê²Œ ë§ì´ ì—†ì§€ë§Œ ìˆëŠ”ê±¸ ìµœëŒ€í•œ í™œìš©í•´ì„œ ì„¤ëª…í•œë‹¤(ìœ íˆ¬ë¸Œì— ë‚˜ì˜¨ ë°ëª¨ ì„¤ëª… í¬í•¨) ì›¹ì‚¬ì´íŠ¸ ì„¤ëª… ì˜¤í”¼ì…œ ë¬¸ì„œëŠ” ì•„ë‹ˆì§€ë§Œ ì‚¬ì´íŠ¸ì— ê°„ë‹¨í•˜ê²Œ ì„¤ëª…ì´ ì˜ ë˜ì–´ ìˆì–´ì„œ ì²¨ë¶€. Gen App Builderì˜ ë„ì›€ìœ¼ë¡œ ê¸°ê³„ í•™ìŠµ ê²½í—˜ì´ ì—†ëŠ” ê°œë°œìë„ ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ ìƒì„± AI ì•±ì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ê°•ë ¥í•œ ë„êµ¬ëŠ” ì½”ë“œ ì—†ëŠ” ì ‘ê·¼ ë°©ì‹ì„ ì œê³µí•˜ì—¬ ê°œë°œìê°€ ëª‡ ë¶„ ë˜ëŠ” ëª‡ ì‹œê°„ ë‚´ì— ê³ í’ˆì§ˆ í™˜ê²½ì„ êµ¬ì¶•í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. ì£¼ìš” ê¸°ëŠ¥ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. ì½”ë”© í•„ìš” ì—†ìŒ: Gen App Builderë¥¼ ì‚¬ìš©í•˜ë©´ ì‚¬ìš©ìëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ì§€ ì•Šê³ ë„ ë§ì¶¤í˜• ë¹„ì¦ˆë‹ˆìŠ¤ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ í”Œë«í¼ì€ ì‹œê°ì  ì¸í„°í˜ì´ìŠ¤ì™€ ë“œë˜ê·¸ ì•¤ ë“œë¡­ ê¸°ëŠ¥ì„ ì œê³µí•˜ì—¬ ì‚¬ìš©ìê°€ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì‰½ê²Œ ì„¤ê³„í•˜ê³  êµ¬ì¶• í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤ . ë‹¤ë¥¸ Google í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤ì™€ì˜ í†µí•©: Gen App BuilderëŠ” ****Cloud SQL ë° Cloud Storage ì™€ ê°™ì€ ë‹¤ë¥¸ Google Cloud ì„œë¹„ìŠ¤ì™€ í†µí•©ë˜ì–´ ë°ì´í„°ë¥¼ ì‰½ê²Œ ì €ì¥í•˜ê³  ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‚¬ìš©ì ì§€ì • ê°€ëŠ¥í•œ í…œí”Œë¦¿: ì´ í”Œë«í¼ì€ ì‚¬ìš©ìê°€ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë¹ ë¥´ê³  ì‰½ê²Œ ë§Œë“¤ ìˆ˜ ìˆë„ë¡ ì‚¬ìš©ì ì§€ì • ê°€ëŠ¥í•œ í…œí”Œë¦¿ì„ ì œê³µí•©ë‹ˆë‹¤. í…œí”Œë¦¿ì€ ê³ ê° ê´€ê³„ ê´€ë¦¬, ì¸ì‚¬ ë° í”„ë¡œì íŠ¸ ê´€ë¦¬ë¥¼ ë¹„ë¡¯í•œ ë‹¤ì–‘í•œ ì‚¬ìš© ì‚¬ë¡€ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‹¤ì‹œê°„ í˜‘ì—…: ì´ App Builderë¥¼ ì‚¬ìš©í•˜ë©´ ì‚¬ìš©ìê°€ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ì‹¤ì‹œê°„ìœ¼ë¡œ í˜‘ì—…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ íŒ€ì´ ì‰½ê²Œ í˜‘ë ¥í•˜ê³  í”¼ë“œë°±ì„ ê³µìœ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í™•ì¥ì„±: Gen App Builderë¡œ êµ¬ì¶•ëœ ì• í”Œë¦¬ì¼€ì´ì…˜ì€ í™•ì¥ì„±ì´ ë›°ì–´ë‚˜ê³  ë§ì€ ì–‘ì˜ ë°ì´í„°ì™€ ì‚¬ìš©ìë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë³´ì•ˆ: Google Cloudì˜ ë³´ì•ˆ ê¸°ëŠ¥ì€ App Builderì— ë‚´ì¥ë˜ì–´ ìˆì–´ ****ë³´ì•ˆ ìœ„í˜‘ìœ¼ë¡œë¶€í„° ì• í”Œë¦¬ì¼€ì´ì…˜ê³¼ ë°ì´í„°ë¥¼ ë³´í˜¸í•©ë‹ˆë‹¤ . êµ¬ê¸€ ì˜¤í”¼ì…œ ë¬¸ì„œ êµ¬ê¸€ ì˜¤í”¼ì…œ ë¬¸ì„œì— ìˆëŠ” ì„¤ëª…ì„ ê°„ëµí•˜ê²Œ ì¶”ë¦° ê²ƒì´ë‹¤. ìƒì„±í˜• ì•± ë¹Œë”ë¥¼ ì‚¬ìš©í•œ ìƒˆë¡œìš´ ìƒì„±í˜• AI ê¸°ë°˜ ê²€ìƒ‰ ë° ëŒ€í™”í˜• í™˜ê²½ ë¹Œë“œ ìƒì„±í˜• ì•± ë¹Œë”ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¨¸ì‹ ëŸ¬ë‹ ê¸°ìˆ ì„ í™œìš©í•œ ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ ìƒì„±í˜• AI ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ ë¹ ë¥´ê²Œ ê³ í’ˆì§ˆ í™˜ê²½ì„ êµ¬ì¶•í•˜ê³  ì• í”Œë¦¬ì¼€ì´ì…˜ ë° ì›¹ì‚¬ì´íŠ¸ì— í†µí•© ê¸°ë°˜ ëª¨ë¸ê³¼ ì •ë³´ ê²€ìƒ‰ì„ ê²°í•©í•´ ê´€ë ¨ì„± ë†’ì€ ë§ì¶¤ ì •ë³´ ì œê³µ í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ê¸°íƒ€ ë¯¸ë””ì–´ë¡œ ì‘ë‹µí•  ìˆ˜ ìˆëŠ” ë©€í‹°ëª¨ë‹¬ ì•± êµ¬ì¶• ì„œë“œ íŒŒí‹° ì•± ë° ì„œë¹„ìŠ¤ì™€ ì—°ê²°í•˜ê³  íŠ¸ëœì­ì…˜ ì§€ì› ê¸°ëŠ¥ ì œê³µ ì°¨ì„¸ëŒ€ ëŒ€í™”í˜• AI ê²½í—˜ ë° ì–´ì‹œìŠ¤í„´íŠ¸ ìƒì„±í˜• ì•± ë¹Œë”ë¥¼ ì‚¬ìš©í•´ ì›í™œí•œ ëŒ€í™”í˜• ë°©ì‹ìœ¼ë¡œ ê¸°ì—… ë°ì´í„° ì„¸íŠ¸ë¥¼ í™œìš©í•˜ê³  ì‚¬ìš©ì ê²½í—˜ ê°œì„  AI ê¸°ë°˜ ì•±ìœ¼ë¡œ ëª¨ë“  ì†ŒìŠ¤ì˜ ì •ë³´ ì¢…í•© ë° êµ¬ì²´ì ì¸ ì‹¤í–‰ ê°€ëŠ¥í•œ ë‹µë³€ ì œê³µ ê³ ê° ì„œë¹„ìŠ¤ ë¶„ì•¼ì—ì„œ ìˆ˜ìµ, ê³ ê° ë§Œì¡±ë„, ì¶©ì„±ë„ í–¥ìƒ ë„ëª¨ ë©€í‹°ëª¨ë‹¬ ê¸°ëŠ¥ê³¼ ëŒ€í™”í˜• UI íŠ¸ëœì­ì…˜ì„ ì‚¬ìš©í•˜ì—¬ ì œí’ˆ êµ¬ë§¤ ê³¼ì • ë•ê¸° ë‹¤ì–‘í•œ ì‚°ì—… ë° ì‚¬ìš© ì‚¬ë¡€ì— ì ìš© ê°€ëŠ¥, ì˜ˆ: ì†Œë¹„ì¬, ê³µê³µ ì„œë¹„ìŠ¤, ê¸ˆìœµ, ì¸íŠ¸ë¼ë„· ë“± Google í’ˆì§ˆ ìˆ˜ì¤€ì˜ ê²€ìƒ‰ê³¼ ê¸°ë°˜ ëª¨ë¸ì˜ ê²°í•© ì¡°ì§ ì „ì²´ ë°ì´í„°ì—ì„œ ì˜¬ë°”ë¥¸ ì •ë³´ ì°¾ê¸° ì¤‘ìš”, ê¸°ì¡´ ë„êµ¬ë¡œëŠ” ì–´ë ¤ì›€ ìƒì„±í˜• ì•± ë¹Œë”ë¡œ Google í’ˆì§ˆ ê²€ìƒ‰ ê¸°ëŠ¥ê³¼ ìƒì„±í˜• AI ê²°í•©í•˜ì—¬ ê´€ë ¨ì„± ë†’ì€ ë§ì¶¤ ì •ë³´ ì°¾ê¸° ì§€ì› ì½”ë”© ê²½í—˜ ì—†ì´ ëª‡ ë¶„~ì‹œê°„ ë§Œì— ëŒ€í™”í˜• ê²€ìƒ‰ í™˜ê²½ êµ¬ì¶• ê°€ëŠ¥ ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰ í™˜ê²½ì„ í†µí•´ í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ë™ì˜ìƒ ê²€ìƒ‰ ì§€ì› ìì—°ìŠ¤ëŸ½ê²Œ ì¸ìš© ì œê³µ ë° ë§ì¶¤ ê²°ê³¼ ì§€ì›ìœ¼ë¡œ ì‚¬ìš©ì ë§Œì¡±ë„ í–¥ìƒ ë°ì´í„° ì‚¬ìš©ë¥  ì¦ê°€, í”„ë¡œì„¸ìŠ¤ íš¨ìœ¨ì„± ê°œì„ , ì§ì›ê³¼ ê³ ê° ë§Œì¡±ë„ ë†’ì´ê¸° ë‹¤ì–‘í•œ ì†ŒìŠ¤ì˜ ë³µì¡í•œ ë°ì´í„°ì™€ ìƒí˜¸ì‘ìš© ëŠ¥ë ¥ìœ¼ë¡œ ê³ ê° ì„œë¹„ìŠ¤ ê°œì„  ê°€ëŠ¥ ëŒ€í™”í˜• ì²˜ë¦¬ ê¸°ëŠ¥ê³¼ ê²°í•©í•˜ì—¬ ê³ ê° ì°¸ì—¬ë„, ì§ì› ìƒì‚°ì„± ê°œì„  ê°€ëŠ¥ ê°œë°œìì™€ ê¸°ì—…ì´ ìƒˆë¡œìš´ ê²½í—˜ê³¼ ìˆ˜ìµ ê¸°íšŒë¥¼ ì‹¤í˜„í•  ê²ƒìœ¼ë¡œ ê¸°ëŒ€ ìœ íˆ¬ë¸Œ ë°ëª¨ ì„¤ëª… ìƒí™©: íˆ¬ì ì „ëµì„ ìˆ˜ë¦½í•˜ê¸° ìœ„í•´ ë°˜ë„ì²´ ì‹œì¥ì„ í‰ê°€í•´ì•¼ í•˜ëŠ” ì• ë„ë¦¬ìŠ¤íŠ¸, ì—”ì§€ë‹ˆì–´ë“¤ì´ íšŒì‚¬ í™ˆí˜ì´ì§€ì— gen app builderë¥¼ ì‚¬ìš©í•˜ì—¬ì„œ ê²€ìƒ‰ ì—”ì§„ì„ ë§Œë“¤ì–´ì¤Œ ì„¸ê³„ì ìœ¼ë¡œ ë°˜ë„ì²´ ë¶€ì¡± í˜„ìƒì´ ì¼ì–´ë‚¨ â†’ ì–´ë–¤ ì‚°ì—…ì—ì„œ ê°€ì¥ ì˜í–¥ì„ ë§ì´ ë¯¸ì³¤ëŠ”ì§€ë¥¼ ê²€ìƒ‰ ì—”ì§„ìœ¼ë¡œ ë¬¼ì–´ë´„ ê²°ê³¼: internal, externalì´ í‘œì‹œë˜ì–´ì„œ íšŒì‚¬ê°€ ì œê³µí•œ ë°ì´í„°ëƒ ì•„ë‹ˆëƒì— ë”°ë¼ì„œ ê²°ê³¼ê°€ ë‚˜ì˜´, ê°ê° ê²°ê³¼ë§ˆë‹¤ ai ì—”ì§„ì´ ìë™ì ìœ¼ë¡œ ìš”ì•½í•´ì„œ ë³´ì—¬ì¤Œ ì…‹ì¤‘ì— í•˜ë‚˜ë¥¼ ë” ì•Œê³  ì‹¶ë‹¤ë©´ í´ë¦­í•œë‹¤. ê·¸ëŸ¼ ê¸€ë“¤ì„ ìë™ì ìœ¼ë¡œ ì •ë¦¬í•´ì„œ ëª©ì°¨ë¥¼ ë§Œë“¤ì–´ì¤Œ ê¸°ì—…ì—ì„œ êµ¬ë…í•˜ê³  ìˆëŠ” ê¸ˆìœµ ì €ë„ë“¤ì— ì¤‘ìš”í•œ ì •ë³´ê°€ ë‹´ê²¨ ìˆìŒ. ê·¸ë˜ì„œ ì•± ì„¤ì •í•´ì„œ ì´ ì •ë³´ë“¤ì„ í¬í•¨í•´ ë‹¬ë¼ê³  ì„¤ì • ê°€ëŠ¥, ê·¸ë¦¬ê³  ì°¾ì€ ì •ë³´ë“¤ì„ ì „ë¶€ í•©ì¹˜ê³  ì •ë¦¬í•´ì„œ ë³´ì—¬ë‹¬ë¼ê³ ë„ ì„¤ì • ê°€ëŠ¥ ì´ë ‡ê²Œ ì •ë³´ë¥¼ ì¶”ê°€ í•œ í›„ interest rateì— ëŒ€í•´ì„œ follow-up ì§ˆë¬¸ì„ í•¨ ê²€ìƒ‰í•œ ì •ë³´ì™€ í•¨ê»˜ ê²€ìƒ‰ overall summaryë¥¼ ê°™ì´ ë³´ì—¬ì¤Œ ë‘ ì§ˆë¬¸ìœ¼ë¡œ ì‚°ì—…êµ°ì˜ ì´í•´ê°€ ë†’ì•„ì§ ì´ ëª¨ë“ ê²ƒì„ íšŒì‚¬ url, ë°ì´í„°ë¥¼ cloudstorageì— ì˜¬ë ¤ì„œ (êµ¬ê¸€ì€ ì´ ì •ë³´ë¥¼ íšŒì‚¬ í—ˆê°€ í•˜ì—ë§Œ ì‚¬ìš©) searchë¡œ í• ê±´ì§€ chatìœ¼ë¡œ í• ê±´ì§€ë¥¼ ì„ íƒí•œ ë‹¤ìŒ ë§Œë“¤ë©´ ëì´ë‚¨ â†’ ì„œì¹˜ ì—”ì§„ì„ì„ ì‰½ê²Œ ë§Œë“¤ì–´ì¤Œ ë§Œë“¤ê³  ë‚˜ì„œ ì»¤ìŠ¤í„°ë§ˆì´ì§•ë„ ê°€ëŠ¥ â†’ í•˜ë‹¨ ë„¤ë¹„ê²Œì´ì…˜ ë°”ì— ìˆëŠ” ê²ƒë“¤ì„ ìš°ë¦¬ê°€ ì¡°ì •í•´ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ deploy ì½”ë“œë¥¼ ë”°ë¡œ ë°°í¬ â†’ íšŒì‚¬ htmlì— ì‚½ì…í•˜ë©´ ì„œì¹˜ ì—”ì§„ì´ ë¨, apië¡œë„ ì œê³µ Reference https://www.analyticsvidhya.com/blog/2023/04/gen-app-builder-google-clouds-latest-generative-ai-tools/ https://cloud.google.com/blog/ko/products/ai-machine-learning/create-generative-apps-in-minutes-with-gen-app-builder?_ga&#x3D;2.206347863.-517135162.1681093488&amp;hl&#x3D;ko https://www.youtube.com/watch?v=kOmG83wGfTs","categories":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}],"tags":[{"name":"Gen App Builder","slug":"Gen-App-Builder","permalink":"https://jmj3047.github.io/tags/Gen-App-Builder/"},{"name":"AI tool","slug":"AI-tool","permalink":"https://jmj3047.github.io/tags/AI-tool/"}],"keywords":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}]},{"title":"Hi,KIA A Speech Emotion Recognition Dataset for Wake-Up Words","slug":"SER_Hi_KIA","date":"2023-04-16T15:00:00.000Z","updated":"2023-05-11T14:36:21.812Z","comments":true,"path":"2023/04/17/SER_Hi_KIA/","link":"","permalink":"https://jmj3047.github.io/2023/04/17/SER_Hi_KIA/","excerpt":"","text":"Journal&#x2F;Conference : IEEEYear(published year): 2022 SeptemberAuthor: Taesu Kim, SeungHeon Doh, Gyunpyo Lee, Hyungseok Jeon, Juhan Nam, Hyeon-Jeong SukSubject: Emotion Recognition, Wake-Up Words Hi,KIA: A Speech Emotion Recognition Dataset for Wake-Up Words Summary ìƒˆë¡œìš´ ê³µê°œ ë°ì´í„°ì…‹ì¸ Hi, KIA, ê°ì • ë ˆì´ë¸”ì´ ì§€ì •ëœ WUW ë°ì´í„°ì…‹ì„ ì œì•ˆí•¨ Hi, KIAë¼ëŠ” ìƒˆë¡œìš´ ê³µê°œ ë°ì´í„°ì…‹ì„ ì œì•ˆí•˜ê³  ì´ë¥¼ ì´ìš©í•œ ê°ì • ì¸ì‹ ëª¨ë¸ì„ ê°œë°œí•¨ ì œì•ˆëœ ë°ì´í„°ì…‹ì€ 488ê°œì˜ í•œêµ­ì–´ ë°œí™” ìƒ˜í”Œë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, ë¶„ë…¸, ê¸°ì¨, ìŠ¬í””, ì¤‘ë¦½ ë“± 4ê°€ì§€ ê°ì • ìƒíƒœë¡œ ë ˆì´ë¸”ë§ë˜ì–´ ìˆìŒ ê°œë°œëœ ë‘ê°€ì§€ ë¶„ë¥˜ ëª¨ë¸ì€ ì „í†µì ì¸ hand-craft featureì™€ pre-trained neural networkë¥¼ ì´ìš©í•œ transfer-learning ì ‘ê·¼ ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ êµ¬í˜„ë˜ì—ˆìœ¼ë©° ì´ ë°ì´í„°ì…‹ì—ì„œ ì§§ì€ ë°œí™” ìˆ˜ì¤€ì˜ ê°ì • ì¸ì‹ì— ëŒ€í•œ ê¸°ì¤€ ê²°ê³¼ë¥¼ ì œì‹œí•¨ ì´ëŸ¬í•œ ê²°ê³¼ë“¤ì€ ì•ìœ¼ë¡œ VUI ê¸°ë°˜ ì–´í”Œë¦¬ì¼€ì´ì…˜ì— í™œìš©ë  ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë¨ I. Introduction ìŒì„± ì¸ì‹ ê¸°ìˆ ì€ ìŒì„± ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤(VUIs)ë¥¼ í†µí•´ ë‹¤ì–‘í•œ ì‘ìš© í”„ë¡œê·¸ë¨ì—ì„œ ì‚¬ìš©ë˜ê³  ìˆë‹¤. VUIsëŠ” ì‚¬ìš©ìì—ê²Œ ë³´ì´ì§€ ì•ŠëŠ” ì¸í„°í˜ì´ìŠ¤ë¡œì„œ, ê¸°ì¡´ì˜ ì¸í„°í˜ì´ìŠ¤ë³´ë‹¤ ê°ì •ì ì¸ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ì„ ë” ì˜ ì „ë‹¬í•  ìˆ˜ ìˆë‹¤. íŠ¹íˆ, ì°¨ëŸ‰ ë‚´ VUIsëŠ” ìš´ì „ ì¤‘ ì•ˆì „ì„ ë³´ì¥í•˜ê³  ìš´ì „ìì˜ ê°ì •ì ì¸ ê²½í—˜ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° í° ê´€ì‹¬ì„ ë°›ê³  ìˆë‹¤. WUW(Wake-up words)ë¥¼ ë°œí™”í•˜ì—¬ VUIsë¥¼ í™œì„±í™”í•˜ëŠ” ê²ƒì´ ê°€ì¥ ì¼ë°˜ì ì¸ ì‚¬ìš© ë°©ë²• ì¤‘ í•˜ë‚˜ì´ë‹¤. ì´ ì—°êµ¬ì—ì„œëŠ” Hi,KIA ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ WUW ë°œí™” ì‹œ ì‚¬ìš©ìì˜ ê°ì • ìƒíƒœë¥¼ ë¶„ë¥˜í•˜ëŠ” ëª¨ë¸ì„ ê°œë°œí•˜ì˜€ë‹¤. Hi,KIA ë°ì´í„°ì…‹ì€ 488ê°œì˜ í•œêµ­ì–´ ë°œí™” ìƒ˜í”Œë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, 4ê°€ì§€ ê°ì • ìƒíƒœ(ë¶„ë…¸, ê¸°ì¨, ìŠ¬í””, ì¤‘ë¦½)ë¡œ ë ˆì´ë¸”ë§ë˜ì–´ ìˆë‹¤. ì´ ëª¨ë¸ì€ ì°¨ëŸ‰ ë‚´ VUIsì—ì„œ WUWë¥¼ ë°œí™”í•œ ì‚¬ìš©ìì˜ ê°ì • ìƒíƒœë¥¼ ì¸ì‹í•˜ëŠ” ë° í™œìš©ë  ìˆ˜ ìˆë‹¤. Wake up word emotion recognition í™”ìì˜ ìŒì„±ì„ ê¸°ë°˜ìœ¼ë¡œ í™”ìì˜ ê°ì • ìƒíƒœë¥¼ ê°ì§€í•˜ëŠ” í”„ë¡œì„¸ìŠ¤. ì—¬ê¸°ì—ëŠ” ê°ì • ìƒíƒœì™€ ê´€ë ¨ì´ ìˆëŠ” ê²ƒìœ¼ë¡œ ì•Œë ¤ì§„ í”¼ì¹˜, ê°•ë„ ë° ìŠ¤í™íŠ¸ëŸ¼ íŠ¹ì„±ê³¼ ê°™ì€ ë‹¤ì–‘í•œ ìŒí–¥ì  íŠ¹ì§•ì— ëŒ€í•´ í™”ìì˜ ìŒì„±ì„ ë¶„ì„í•˜ëŠ” ì‘ì—…ì´ í¬í•¨. ê°ì • ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ ì´ìœ  ì‚¬ìš©ìì™€ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ë¥¼ í•  ìˆ˜ ìˆëŠ” VUI(Voice User Interface) ê¸°ë°˜ ì–´í”Œë¦¬ì¼€ì´ì…˜ì˜ ê°œë°œì´ í•„ìš” ì´ëŸ¬í•œ ì–´í”Œë¦¬ì¼€ì´ì…˜ì—ì„œëŠ” ì‚¬ìš©ìì˜ ê°ì • ìƒíƒœë¥¼ íŒŒì•…í•˜ì—¬ ì ì ˆí•œ ëŒ€ì‘ì„ ì œê³µí•˜ëŠ” ê²ƒì´ ì¤‘ìš” ì˜ˆë¥¼ ë“¤ì–´, ìš´ì „ ì¤‘ì¸ ì°¨ëŸ‰ì—ì„œ ìŒì„± ì¸ì‹ ê¸°ë°˜ì˜ VUI ì–´í”Œë¦¬ì¼€ì´ì…˜ì„ ì‚¬ìš©í•  ë•Œ, ìš´ì „ìê°€ ë¶„ë…¸ ìƒíƒœì¼ ë•ŒëŠ” ì•ˆì „ ìš´ì „ì— ë°©í•´ê°€ ë˜ëŠ” ë©”ì‹œì§€ë¥¼ ë³´ì—¬ì£¼ì§€ ì•Šë„ë¡ í•˜ê±°ë‚˜, ê¸°ë¶„ ì¢‹ì€ ìƒíƒœì¼ ë•ŒëŠ” ë”ìš± ì¹œê·¼í•œ ë©”ì‹œì§€ë¥¼ ë³´ì—¬ì¤„ ìˆ˜ ìˆìŒ SER ë°ì´í„°ì…‹ë“¤ê³¼ ë¹„êµ SERëŠ” ê°ì • ë°œí™”ì™€ ë¼ë²¨ì´ í¬í•¨ëœ ë°ì´í„°ì…‹ì„ í•„ìš”ë¡œ í•¨ IEMOCAP, EmoDB, RAVDESS, TESS ë“±ì˜ SER ë°ì´í„°ì…‹ì´ ìˆìŒ ì´ ë°ì´í„° ì…‹ë“¤ì€ text-independent emotion recognitionì„ ê¸°ë°˜ìœ¼ë¡œ ë§Œë“¤ì–´ì§ ì´ ë§ì„ ë‹¤ë¥¸ë§ë¡œ í•˜ë©´ lexical informationê³¼ëŠ” ìƒê´€ ì—†ì´ ì‚¬ìš©ìì˜ ê°ì •ì„ ì¸ì‹í•˜ëŠ” ì‹œìŠ¤í…œì´ë¼ëŠ” ê²ƒ WUW SER ë°ì´í„°ì…‹ì€ ì§§ì€ ë°œí™”ì™€ í‚¤ì›Œë“œë¡œ ì œí•œë¨ ìµœê·¼ì— Releaseëœ Ok Auraì˜ ê²½ìš° ë°ì´í„° ì…‹ë„ ë§ê³  ìŠ¤í”¼ì»¤ë„ ë§ì§€ë§Œ labeledëœ ë°ì´í„°ê°€ 218ê°œ ë°–ì— ì—†ìŒ. II. DATASETA. Scenario Selection ì‹œë‚˜ë¦¬ì˜¤ëŠ” â€œHi, KIA (WUW)â€ë¡œ ì‹œì‘í•˜ëŠ” í…ìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ë¡œ ì œê³µë˜ë©°, ê°ì • ìƒíƒœë¥¼ ìœ ë°œí•˜ëŠ” ë¬¸ë§¥ ë¬¸ì¥ìœ¼ë¡œ ëë‚¨ affective computing ë¶„ì•¼ì—ì„œ ê²½í—˜ì´ ìˆëŠ” 8ëª…ì˜ ëŒ€í•™ì›ìƒë“¤ì´ VUIê°€ íŠ¹ì • ê°ì •ì—ì„œ ì‚¬ìš©ë˜ëŠ” ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì œì•ˆí•˜ë„ë¡ ìš”ì²­ ì´ì „ ì—°êµ¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìš´ì „ìì˜ ë‹¤ì„¯ ê°€ì§€ ê°ì •(í™”ë‚¨, ìŠ¤íŠ¸ë ˆìŠ¤, ê¸°ì¨, ë‘ë ¤ì›€, ìŠ¬í””)ì´ ì œì‹œ í•™ìƒë“¤ì€ ê°ê°ì˜ ê°ì •ì— ëŒ€í•´ ìµœì†Œí•œ ë‘ ê°œì˜ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì œì•ˆí•˜ì˜€ìœ¼ë©°, ì¤‘ë³µëœ ê²ƒë“¤ì„ ë³‘í•©í•˜ì—¬ ì´ 53ê°œì˜ ì‹œë‚˜ë¦¬ì˜¤ê°€ ìˆ˜ì§‘ ì‹œë‚˜ë¦¬ì˜¤ëŠ” ê·¸ë¦¼ 1(a)ì— í‘œì‹œëœ valence-arousal ì¢Œí‘œê³„ë¡œ ë§¤í•‘ë˜ì—ˆìŠµë‹ˆë‹¤. 4ì‚¬ë¶„ë©´ì— í•´ë‹¹í•˜ëŠ” ê°ì • ë²”ì£¼ëŠ” ì‹œë‚˜ë¦¬ì˜¤ê°€ ê±°ì˜ ì—†ê¸° ë•Œë¬¸ì— ì œì™¸ë˜ì—ˆìŠµë‹ˆë‹¤. arousal: ê°ì •ì˜ í¥ë¶„ ì •ë„ë¥¼ ì´ì•¼ê¸°í•˜ëŠ” ì²™ë„ ì´ ê°’ì´ ì‘ì„ìˆ˜ë¡ ì°¨ë¶„í•œ ê°ì •ì¸ë°, ì§€ë£¨í•¨ì´ë‚˜ í¸ì•ˆí•¨, ì¡¸ë¦¼ ë“±ì´ í•´ë‹¹ arousalì´ í° ê°ì •ì—ëŠ” í¥ë¶„ê°, ë¶„ë…¸, ê³µí¬ ë“± valence: ê°ì •ì˜ ê¸ì • í˜¹ì€ ë¶€ì •ì ì¸ ì •ë„ ê³µí¬ëŠ” ì•„ì£¼ ë¶€ì •ì ì¸ valenceë¥¼ ê°€ì§€ê³  ì§€ë£¨í•¨ì´ë‚˜ í¥ë¶„ê°ì€ ì¤‘ê°„ ì •ë„ì˜ valenceë¥¼, í–‰ë³µì´ë‚˜ í¸ì•ˆí•¨ì€ ê¸ì •ì ì¸ valenceë¥¼ ê°€ì§ ì‹œë‚˜ë¦¬ì˜¤ëŠ” valence-arousal ì¢Œí‘œê³„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í™”ë‚¨, ê¸°ì¨, ìŠ¬í”” ë° ì¤‘ë¦½ ì¹´í…Œê³ ë¦¬ë¡œ í´ëŸ¬ìŠ¤í„°ë§ë˜ì—ˆìŠµë‹ˆë‹¤. ê° ì¹´í…Œê³ ë¦¬ì—ì„œ ëŒ€í‘œì ì¸ 3ê°œì˜ ì‹œë‚˜ë¦¬ì˜¤ê°€ ì„ íƒë˜ì–´ â€œHi, KIAâ€ ë°ì´í„°ì…‹ êµ¬ì¶•ì— ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤. ì„ íƒëœ ì‹œë‚˜ë¦¬ì˜¤ì— ëŒ€í•œ ë…¹ìŒ ê°€ì´ë“œë¡œ ì‚¬ìš©ë  ë¬¸ì¥ê³¼ í•¨ê»˜ ì‚½í™”ê°€ ìˆëŠ” ì¹´ë“œë¥¼ ì¤€ë¹„ ê° ì¹´ë“œì—ëŠ” ì°¨ ë‚´ë¶€ ìœ„ì— ë…¹ìŒ ê°€ì´ë“œë¡œ ì‚¬ìš©ë  ë¬¸ì¥ì´ ì œì‹œ í•´ë‹¹ ë¬¸ì¥ì˜ ìƒí™©ì„ ì„¤ëª…í•˜ëŠ” ì°¸ì¡° ì´ë¯¸ì§€ë¥¼ ë°°ê²½ìœ¼ë¡œ ì œê³µ B. Recording and Post-processing 4ëª…ì˜ ëª©ì†Œë¦¬ ë°°ìš°ë“¤ì„ ì˜¨ë¼ì¸ìœ¼ë¡œ ëª¨ì§‘. í‰ê·  ì—°ë ¹ì€ 31.38ì„¸ì´ë©° í‘œì¤€ í¸ì°¨ëŠ” 3.90ì„¸ ë§ˆìš°ìŠ¤ ìœ„ì¹˜ë¥¼ ë§ˆì´í¬ë¡œë¶€í„° 30cm ì´ìƒ ë–¨ì–´ëœ¨ë¦¬ë„ë¡ 12ê°œ ì‹œë‚˜ë¦¬ì˜¤ì— ëŒ€í•´ ì´ 576ê°œì˜ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ìˆ˜ì§‘ C. Human Validation 8ëª…ì˜ ëŒ€í•™ì›ìƒë“¤ì´ 576ê°œì˜ ë…¹ìŒ íŒŒì¼ì„ ë¬´ì‘ìœ„ë¡œ ì„ ì •í•˜ì—¬, ê° ë…¹ìŒ íŒŒì¼ì´ â€˜í™”ë‚¨â€™, â€˜ê¸°ì¨â€™, â€˜ìŠ¬í””â€™, â€˜ì¤‘ë¦½â€™ ì¤‘ ì–´ë–¤ ê°ì •ì„ ë‚˜íƒ€ë‚´ëŠ”ì§€ ë¶„ë¥˜ ë§Œì•½ ë…¹ìŒ íŒŒì¼ì´ ì¸ì‹í•˜ê¸° ì–´ë ¤ì› ë‹¤ë©´ â€˜unknownâ€™ìœ¼ë¡œ ë¶„ë¥˜ ì´ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ëª¨ë“  ì¸ê°„ í‰ê°€ìê°€ ë‹¤ë¥¸ ì˜ˆì¸¡ì„ í•œ 88ê°œì˜ ë…¹ìŒ íŒŒì¼ì€ ì œê±° ì´ë¥¼ í†µí•´ ìµœì¢…ì ìœ¼ë¡œ 488ê°œì˜ ë…¹ìŒ íŒŒì¼ì´ â€œHi, KIAâ€ datasetì— í¬í•¨ë¨. ì¸ê°„ í‰ê°€ìë“¤ì˜ ì‘ë‹µì„ ê²°í•©í•˜ê³  Figure 2ì— ë‚˜ì™€ ìˆëŠ” í˜¼ë™ í–‰ë ¬ì„ ê³„ì‚° ìš°ë¦¬ëŠ” í‰ê°€ìë“¤ì´ â€˜ìŠ¬í”ˆâ€™ ê°ì •ì„ ë¶„ë¥˜í•˜ëŠ” ë° ë¹„êµì  ëŠ¥ìˆ™í•˜ë‹¤ëŠ” ê²ƒì„ ë°œê²¬ ë°˜ë©´ì—, â€˜í™”ë‚œâ€™ê³¼ â€˜ì¤‘ë¦½â€™ ëª©ì†Œë¦¬ë¥¼ ì‹ë³„í•˜ëŠ” ë° ì–´ë ¤ì›€ì´ ìˆì—ˆìœ¼ë©°, ê·¸ë“¤ì€ â€˜í™”ë‚œâ€™ì„ â€˜ì¤‘ë¦½â€™ìœ¼ë¡œ í‰ê°€í•˜ê³  â€˜ì¤‘ë¦½â€™ì„ â€˜ìŠ¬í”ˆâ€™ìœ¼ë¡œ í‰ê°€ high-arousals ëª©ì†Œë¦¬ë¥¼ ê°ì •ì ì¸ ê¸°ì € ìƒíƒœë¡œ ì¸ì‹: ì°¸ê°€ìë“¤ì€ â€˜angryâ€™ì™€ â€˜happyâ€™ë¥¼ â€˜neutralâ€™ ê°ì •ìœ¼ë¡œ ê´€ì°° ì´ëŠ” ì‚¬ëŒë“¤ì´ WUW(wake up words)ì—ì„œ high arousals ì¸ìƒì„ ì¸ì‹í•˜ëŠ” ë° ì–´ë ¤ì›€ì´ ìˆë‹¤ëŠ” ê²ƒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. III. WAKE-UP WORD EMOTION RECOGNITION ë°ì´í„°ì…‹ì˜ í¬ê¸°ê°€ ì‘ê¸° ë•Œë¬¸ì— ë‘ ê°€ì§€ í•™ìŠµ ì „ëµì„ íƒêµ¬ í•˜ë‚˜ëŠ” ë„ë©”ì¸ ì§€ì‹ì— ê¸°ë°˜í•œ ìˆ˜ì‘ì—… ì˜¤ë””ì˜¤ íŠ¹ì§•ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ê³ , ë‹¤ë¥¸ í•˜ë‚˜ëŠ” ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµëœ ëª¨ë¸ì˜ ì¼ë°˜í™” ëŠ¥ë ¥ì„ í™œìš©í•˜ì—¬ ì‘ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ ì‚¬ì „ í•™ìŠµëœ ì‹ ê²½ë§ ëª¨ë¸ì„ ì„¸ë°€í•˜ê²Œ ì¡°ì •í•˜ëŠ” ê²ƒ One is using hand-craft audio features based on domain knowledge. The other is fine-tuning a pretrainedneural network model with the small dataset by leveraging the generalization capability of the model trained with a largescale dataset. A. Hand-craft Features ì£¼íŒŒìˆ˜, ì—ë„ˆì§€ ë° ìŠ¤í™íŠ¸ëŸ¼ ë„ë©”ì¸ ê¸°ëŠ¥ì„ í¬í•¨í•˜ëŠ” í™•ì¥ëœ Geneva Minimalistic Acoustic Parameter Set(eGeMAPS) [22]ë¥¼ ì‚¬ìš© 88ì°¨ì›ì˜ eGeMAPS íŠ¹ì§•ì€ ê³ ì •ëœ í‰ê· ê³¼ í‘œì¤€ í¸ì°¨ ê°’ì„ ì‚¬ìš©í•˜ì—¬ z-ì ìˆ˜ë¡œ í‘œì¤€í™” Figure 3ì€ ì „ì²´ ë°ì´í„°ì…‹ì—ì„œ ì—ë„ˆì§€ì™€ ìŒë†’ì´ ë¶„í¬ì˜ ë‘ ë°”ì´ì˜¬ë¦° ê·¸ë¦¼ì„ ë³´ì—¬ì¤Œ ì¼ë°˜ì ì¸ ê²½í–¥ì€ ê³ ê¸°ë¶„ ìƒíƒœ ê·¸ë£¹(â€˜í™”ë‚œâ€™, â€˜ê¸°ìœâ€™)ì´ ì €ê¸°ë¶„ ìƒíƒœ ê·¸ë£¹(â€˜ìŠ¬í”ˆâ€™, â€˜ë³´í†µâ€™)ê³¼ ì˜ êµ¬ë³„ë¨ ë”¥ ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬ ê¸°ë°˜ì˜ ê¸°ëŠ¥ê³¼ ë¹„êµí•˜ê¸° ìœ„í•´, ë¬¸ì¥ ìˆ˜ì¤€ì˜ eGeMAPS ê¸°ëŠ¥ì„ ë¡œì§€ìŠ¤í‹± íšŒê·€ ë¶„ë¥˜ê¸°ì˜ inputìœ¼ë¡œ ì‚¬ìš© B. Fine-Tuning with Pretrained Wav2vec 2.0 ìµœê·¼ ì—°êµ¬ì—ì„œëŠ” ìŒì„± ê°ì • ì¸ì‹ì„ ìœ„í•´ ë”¥ ëŸ¬ë‹ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ annotationì´ ë‹¬ë¦° ë°ì´í„°ì˜ ë¶€ì¡±ìœ¼ë¡œ ì¸í•´ ì´ëŸ¬í•œ ë°©ë²•ë“¤ì€ ì œí•œë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, Wav2vecì™€ ê°™ì€ ëŒ€ê·œëª¨ ì‚¬ì „ í•™ìŠµëœ ì‹ ê²½ë§ì„ ì‚¬ìš©í•œ ì „ì´ í•™ìŠµì´ ê°ì • ì¸ì‹ ì •í™•ë„ë¥¼ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. Hi, KIAì™€ í•¨ê»˜, ìš°ë¦¬ëŠ” Wav2vec2.0ì„ ì‚¬ìš©í•˜ì—¬ ìœ ì‚¬í•œ ì „ì´ í•™ìŠµì„ ìˆ˜í–‰í•˜ì˜€ìŠµë‹ˆë‹¤. Pretrained Wav2vec2.0 Wav2vec2.0 [29]ì€ ì›ì‹œ ì˜¤ë””ì˜¤ ì‹ í˜¸ì—ì„œ ì˜ë¯¸ ìˆëŠ” í‘œí˜„ì„ ì¶”ì¶œí•˜ê¸° ìœ„í•´ í›ˆë ¨ëœ transformer ê¸°ë°˜ ëª¨ë¸ì…ë‹ˆë‹¤. Wav2vec2.0ì€ CNNì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ë¡œì»¬ ì¸ì½”ë”, transformerë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ì»¨í…ìŠ¤íŠ¸ ë„¤íŠ¸ì›Œí¬ ë° ì–‘ìí™” ëª¨ë“ˆë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë¡œì»¬ ì¸ì½”ë”ëŠ” ì›ì‹œ íŒŒí˜•ì—ì„œ ì§ì ‘ low-level representationì„ ì¶”ì¶œí•©ë‹ˆë‹¤. ì´ëŸ¬í•œ í‘œí˜„ì„ ê¸°ë°˜ìœ¼ë¡œ ì»¨í…ìŠ¤íŠ¸ ë„¤íŠ¸ì›Œí¬ëŠ” ëŒ€ì¡° ì†ì‹¤(contrastive loss)ì„ ì‚¬ìš©í•˜ì—¬ ê³¼ê±° í‘œí˜„ì—ì„œ ë¯¸ë˜ í‘œí˜„ì„ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµë©ë‹ˆë‹¤. ì»¨í…ìŠ¤íŠ¸ ë„¤íŠ¸ì›Œí¬ì˜ ì¶œë ¥ì€ í•™ìŠµëœ high-level representationsì…ë‹ˆë‹¤. Fine-tuning methods ìš°ë¦¬ëŠ” Wav2vec2.0ì„ ì‚¬ìš©í•˜ì—¬ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ê³  average poolingì„ í†µí•´ ë¬¸ì¥ ìˆ˜ì¤€ì˜ ê¸°ëŠ¥ì„ ì–»ì—ˆìŠµë‹ˆë‹¤. ì´ì „ ì—°êµ¬ë¥¼ ë”°ë¼[28], ìš°ë¦¬ëŠ” Wav2vec2.0ì˜ ëª¨ë“ˆì— ëŒ€í•œ ë‹¤ì–‘í•œ ì„¸ë¶€ ì¡°ì • ì „ëµì„ íƒêµ¬í•˜ì˜€ìŠµë‹ˆë‹¤. ì²«ì§¸, ê°ì • ì§€ë„ ì—†ì´ ê¸´ê¸‰í•œ libri-speech corpusë¡œ í•™ìŠµëœ vanila Wav2vec2.0ì—ì„œ ê°ì • ì¸ì‹ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤(ì„¸ë¶€ ì¡°ì • ì—†ìŒ). ë‘˜ì§¸, ë¡œì»¬ ì¸ì½”ë” ë˜ëŠ” ì»¨í…ìŠ¤íŠ¸ ë„¤íŠ¸ì›Œí¬(ê°ê° ì €ìˆ˜ì¤€ ë˜ëŠ” ê³ ìˆ˜ì¤€ í‘œí˜„ì— ëŒ€í•œ ì±…ì„)ë¥¼ ì„¸ë°€í•˜ê²Œ ì¡°ì •í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ ì „ì²´ ë„¤íŠ¸ì›Œí¬ë¥¼ ì„¸ë°€í•˜ê²Œ ì¡°ì •í•©ë‹ˆë‹¤ C. Experiment SetupData split and Metrics speaker independenceë¥¼ ìœ„í•´ 8-fold cross-validationì„ ì‹¤í–‰: 7ëª…ì„ train, validationì— ì‚¬ìš©í•˜ê³  1ëª…ì„ testë¡œ ì‚¬ìš© â†’ ê·¸ ê²°ê³¼ê°€ WA(Weighted Accuracy)ì™€ UA(Unweighted Accuracy)ë¡œ ë³´ê³  ë¨ WA: ëª¨ë“  í´ë˜ìŠ¤ë¥¼ ëŒ€ìƒìœ¼ë¡œ í•œ ì „ì²´ ì •í™•ë„ UA: ê° í´ë˜ìŠ¤ì˜ í‰ê·  ì •í™•ë„ Hyper-parameters ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì¸ wav2vec2.0-baseë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤í—˜ì„ ìˆ˜í–‰ 2ê°œì˜ transformer ë¸”ë¡ê³¼ 7ê°œì˜ ì»¨ë³¼ë£¨ì…˜ ë¸”ë¡ìœ¼ë¡œ êµ¬ì„± ê°ê° 512 ì±„ë„ Huggingface transformers repository [30]ë¥¼ ê¸°ë°˜ AdamW [31]ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ìµœì í™”í•˜ì˜€ìœ¼ë©°, í•™ìŠµë¥ ì€ 5e^5, epochëŠ” 200 full audio data: 16,000 Hz sampling rateì™€ 1 batch size IV. RESULTS Table IIIëŠ” ë¶„ë¥˜ ëª¨ë¸ê³¼ ì¸ê°„ ê²€ì¦ì˜ ë¶„ë¥˜ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. Human Validation: 8ëª…ì˜ í‰ê°€ìì˜ í‰ê·  ì„±ëŠ¥ìœ¼ë¡œ ê³„ì‚° Fine-tuningì´ ì—†ëŠ” Wav2vec2.0 íŠ¹ì§•ì€ hand-craft íŠ¹ì§•ë³´ë‹¤ ì„±ëŠ¥ì´ ì¢‹ì§€ ì•ŠìŒ. ì´ëŠ” ìê¸° ì§€ë„ í•™ìŠµë§Œìœ¼ë¡œ high-level emotion featureë¥¼ ì¶”ì¶œí•˜ëŠ” ê²ƒì´ ì–´ë µë‹¤ëŠ” ê²ƒì„ ë‚˜íƒ€ëƒ„ Wav2vec2.0ì„ fine-tuningí•˜ë©´ ë¶„ë¥˜ ì •í™•ë„ê°€ í¬ê²Œ í–¥ìƒ ì„¸ ê°€ì§€ ì„¤ì • ì¤‘ì—ì„œ Context Netì„ fine-tuningí•˜ëŠ” ê²ƒì´ ê°€ì¥ ì˜ ì‘ë™í•˜ë©°, WA(%)ì™€ UA(%)ì—ì„œ ê°ê° 68.64%ì™€ 68.51%ë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤. ì´ëŠ” ì‘ì€ ë°ì´í„°ì…‹ì˜ ê²½ìš° high-level representationê³¼ ê´€ë ¨ëœ ë§¤ê°œë³€ìˆ˜ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ê²ƒì´ ëª¨ë“  ë§¤ê°œë³€ìˆ˜ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ê²ƒë³´ë‹¤ ë” íš¨ìœ¨ì ì„ì„ ë‚˜íƒ€ëƒ„ Context Netë¥¼ fine-tuningí•˜ë©´ ì¸ê°„ ê²€ì¦ë³´ë‹¤ ìš°ìˆ˜í•œ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŒ â†’ ì´ê²ƒì€ ê°ì • ì¸ì‹ì´ ì£¼ê´€ì ì´ê¸° ë•Œë¬¸. 4ëª…ì˜ ë‚¨ì„±ê³¼ 4ëª…ì˜ ì—¬ì„±ì´ í¬í•¨ëœ 8ê°œì˜ í´ë“œì—ì„œ WA(ê°€ì¤‘ì¹˜ ì •í™•ë„)ë¥¼ ë³´ì—¬ì¤Œ ì—¬ê¸°ì„œ ì£¼ëª©í•  ì : wav2vec2.0 featureê°€ ëŒ€ë¶€ë¶„ì˜ ì—¬ì„± í´ë“œì—ì„œ ì¸ê°„ ê²€ì¦ ì„±ëŠ¥ì„ ëŠ¥ê°€í•œë‹¤ëŠ” ê²ƒ Human Validation PerformanceëŠ” ë‚¨ì„±ê³¼ ì—¬ì„± í´ë“œ ëª¨ë‘ì—ì„œ ë¹„êµì  ì•ˆì •ì  Hand-craft íŠ¹ì§•ê³¼ â€˜Wav2vec2.0 FTâ€™ëŠ” ë‚¨ì„±ê³¼ ì—¬ì„± í´ë“œ ì‚¬ì´ì— ì„±ëŠ¥ ì°¨ì´ë¥¼ ë³´ì´ë©°, íŠ¹íˆ M1, M2, M4 ë° F5 í´ë“œì—ì„œ ì¸ê°„ ê²€ì¦ë³´ë‹¤ ì„±ëŠ¥ì´ ë‚®ìŒ hand-craft íŠ¹ì§•ê³¼ Wav2vec2.0 Context Net fine-tuningì— ì˜í•œ í˜¼ë™ í–‰ë ¬ ë‘ ëª¨ë¸ ëª¨ë‘ â€˜happyâ€™, â€˜sadâ€™ì™€ ê°™ì€ ê°ì •ì˜ í™œì„±í™” ë° ê°€ì¹˜ ì°¨ì´ë¥¼ êµ¬ë³„í•˜ëŠ” ë° ì¢‹ìŒ ê·¸ëŸ¬ë‚˜ hand-craft íŠ¹ì§•ì€ high-arousals(â€˜angryâ€™, â€˜happyâ€™) ë° sad-neutral pair ë‚´ì—ì„œ ê°€ì¹˜ ì°¨ì´ë¥¼ ì´í•´í•˜ëŠ” ë° ì•½í•¨ â†’ Wav2vec2.0 ì»¨í…ìŠ¤íŠ¸ ë„¤íŠ¸ì›Œí¬ fine-tuningì—ì„œ ì™„í™” Figure 2ì™€ Figure 5ë¥¼ ë¹„êµí•˜ë©´, Wav2vec2.0 ì»¨í…ìŠ¤íŠ¸ ë„¤íŠ¸ì›Œí¬ fine-tuningì€ high arousals ê°ì •ê³¼ neutral ê°ì •ì„ êµ¬ë³„í•˜ëŠ” ë° ì¸ê°„ ê²€ì¦ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì„. V. CONCLUSIONS ì´ ë…¼ë¬¸ì€ ê°ì • ë¼ë²¨ì´ ì§€ì •ëœ WUW ë°ì´í„° ì„¸íŠ¸ì¸ Hi, KIAë¥¼ ì œì•ˆ ê°ì •ì ì¸ ì§§ì€ ë§ì„ ìˆ˜ì§‘í•˜ê¸° ìœ„í•´ ì‹ ì¤‘í•˜ê²Œ ì„¤ê³„ëœ ì ˆì°¨ë¥¼ ì„¤ëª… Human Validationì„ ìˆ˜í–‰í•œ í›„ â†’ 488ê°œì˜ ë…¹ìŒìœ¼ë¡œ êµ¬ì„±ëœ ë°ì´í„° ì„¸íŠ¸ë¥¼ ì™„ì„± ì´ëŠ” í•œêµ­ì–´ ì•…ì„¼íŠ¸ì™€ ë°œí™” ìˆ˜ì¤€ì˜ 4ê°€ì§€ ê°ì • í´ë˜ìŠ¤ ì£¼ì„ì´ í¬í•¨ëœ ìƒ· ê¸¸ì´ ìŒì„± ë°ì´í„° ì„¸íŠ¸ ì´ ë°ì´í„° ì„¸íŠ¸ì—ì„œ hand-crafted íŠ¹ì§•ê³¼ transfer learningì„ ì‚¬ìš©í•˜ì—¬ ì§§ì€ ë°œí™” ìˆ˜ì¤€ ìŒì„± ê°ì • ì¸ì‹ì— ëŒ€í•œ ê¸°ì¤€ ê²°ê³¼ë¥¼ ì œì‹œ ê²°ê³¼ëŠ” 4ê°€ì§€ ê°ì • ì¸ì‹ì—ì„œ ë†’ì€ ì •í™•ë„ë¥¼ ë‹¬ì„±í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speech Emotion Recognition","slug":"Paper/Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/categories/Paper/Speech-Emotion-Recognition/"}],"tags":[{"name":"Speech Emotion Recognition","slug":"Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/tags/Speech-Emotion-Recognition/"},{"name":"Wake-Up Words","slug":"Wake-Up-Words","permalink":"https://jmj3047.github.io/tags/Wake-Up-Words/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speech Emotion Recognition","slug":"Paper/Speech-Emotion-Recognition","permalink":"https://jmj3047.github.io/categories/Paper/Speech-Emotion-Recognition/"}]},{"title":"Ecommerce Reorder Prediction","slug":"Ecommerce_Reorder_Pred","date":"2023-04-05T15:00:00.000Z","updated":"2023-04-06T09:04:17.102Z","comments":true,"path":"2023/04/06/Ecommerce_Reorder_Pred/","link":"","permalink":"https://jmj3047.github.io/2023/04/06/Ecommerce_Reorder_Pred/","excerpt":"","text":"ê°œìš” ìœ ì €ë“¤ì˜ ì¬ì£¼ë¬¸ ì—¬ë¶€ ì˜ˆì¸¡í•˜ê¸° instacart kaggle: https://www.kaggle.com/competitions/instacart-market-basket-analysis/leaderboard 2ìœ„ í•œ ëª¨ë¸ github: https://github.com/KazukiOnodera/Instacart í•„ìš” ë©”ëª¨ë¦¬ ì•½ 300GB RAMì´ í•„ìš”. ê°œì¸ìš© LBì—ì„œëŠ” ì•½ 60GB RAMë§Œìœ¼ë¡œë„ 0.4073ì„ ì–»ì„ ìˆ˜ ìˆìŒì„ í™•ì¸. ë˜í•œ ë©”ëª¨ë¦¬ê°€ ì¶©ë¶„í•˜ì§€ ì•Šê³  ë†’ì€ ì ìˆ˜ë¥¼ ì–»ê³  ì‹¶ë‹¤ë©´ xgb.trainì˜ xgb_modelì„ ì‚¬ìš©í•˜ì—¬ ì—°ì† í›ˆë ¨ì„ ì‹œë„. ë°ì´í„° êµ¬ì¡° ì£¼ìš” approach ë‘ ê°€ì§€ ëª¨ë¸: ì¬ì£¼ë¬¸ ì˜ˆì¸¡ ë° ì¬ì£¼ë¬¸ ì—†ìŒ ì˜ˆì¸¡ ì¬ì£¼ë¬¸ ëª¨ë¸ì˜ í‚¤ëŠ” user_idì™€ product_id ì—†ìŒ ëª¨ë¸ì˜ í‚¤ëŠ” user_id ë” ë‚˜ì€ ì˜ˆì¸¡ì„ ìœ„í•´ ë” ë§ì€ í›ˆë ¨ ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ì•¼ í•œë‹¤ê³  ìƒê°í–ˆìŠµë‹ˆë‹¤. prior ë°ì´í„°ë¥¼ í›ˆë ¨ ë°ì´í„°ë¡œë¡œ ì‚¬ìš©í•˜ê¸°ë¡œ ê²°ì • íŠœë‹ ê²°ê³¼, ìµœì ì˜ ìœˆë„ìš° ìˆ˜ëŠ” 3ê°œ ì¬ì£¼ë¬¸ ì˜ˆì¸¡ ì¬ì£¼ë¬¸ ì—†ìŒ ì˜ˆì¸¡ ëª¨ë¸ êµ¬ì¡° Feature Engineering User: what this user like Item: what this item like User x Item: How do user feel about the item Datetime: What this day and hour like *For None model, I canâ€™t user above features except user and datetime, so I convert those to stats(min, mean, max, sum, stdâ€¦) Feature Importance for reorder Feature Importance for None Importance Findings for reorder ìƒì‹ì ìœ¼ë¡œ ê³¼ê±°ì— ì—¬ëŸ¬ ë²ˆ êµ¬ë§¤í•œ í’ˆëª©ì€ ì¬ì£¼ë¬¸ë  í™•ë¥ ì´ ë†’ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì¬ì£¼ë¬¸ë˜ì§€ ì•ŠëŠ” í’ˆëª©ì— ëŒ€í•œ íŒ¨í„´ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤ â†’ ì´ íŒ¨í„´ì„ íŒŒì•…í•˜ì—¬ ì‚¬ìš©ìê°€ ì–¸ì œ ìƒí’ˆì„ ì¬ì£¼ë¬¸í•˜ì§€ ì•ŠëŠ”ì§€ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. user_id : 54035 ì´ ìœ ì €ëŠ” ì½œë¼ë¥¼ í•­ìƒ ì¬ì£¼ë¬¸í•˜ì˜€ë‹¤. ê·¸ëŸ¬ë‚˜ 8ë²ˆ ì£¼ë¬¸ì„ ë³´ë©´ ìœ ì €ëŠ” ì¬ì£¼ë¬¸ í•˜ì§€ ì•Šì•˜ë‹¤ ê·¸ ì´ìœ ëŠ” fridge-pack-colaë¥¼ ëŒ€ì‹  ìƒ€ê¸° ë•Œë¬¸. ì´ëŸ¬í•œ í–‰ë™ ìœ í˜•ì„ ì¡ê¸° ìœ„í•´ì„œ í”¼ì²˜ë¥¼ ë§Œë“¤ì—ˆë‹¤. days_last_order_maxëŠ” days_since_last_order_this_itemê³¼ useritem_order_days_maxë¥¼ ëº€ ê²ƒ. days_since_last_order_this_itemëŠ” ìœ ì €ì™€ ì•„ì´í…œ ì‚¬ì´ì˜ í”¼ì²˜ì´ë‹¤. â†’ ì´ëŠ” ë§ˆì§€ë§‰ ì˜¤ë”ì—ì„œ ì–¼ë§ˆë‚˜ ë§ì€ ì¼ìˆ˜ê°€ ì§€ë‚˜ê°”ëŠ”ì§€ì— ëŒ€í•œ ê²ƒ useritem_order_days_max ëŠ” ìœ ì €ì™€ ì•„ì´í…œ ì‚¬ì´ì˜ í”¼ì²˜ì´ë‹¤ â†’ ì´ëŠ” ì£¼ë¬¸ì˜ ìµœëŒ€ ì£¼ë¬¸ ê¸°ê°„(ì¼)ì„ ì˜ë¯¸í•œë‹¤. index 0ì„ ë³´ë©´, ì´ëŠ” ìœ ì €ê°€ ì£¼ë¬¸ì„ 14ì¼ ì „ì— ìƒ€ê³  ì£¼ë¬¸ ìµœëŒ€ ê¸°ê°„ì´ 30ì¼ì¸ê±¸ ì•Œìˆ˜ ìˆë‹¤. ì´ í”¼ì²˜ëŠ” ìœ ì €ê°€ ê·¸ ì•„ì´í…œì„ ì‚¬ëŠ”ê²ƒì— ì‹«ì¦ì„ ëŠë¼ëŠ”ì§€ ì•„ë‹Œì§€ì— ëŒ€í•œ ê²ƒ ì´ë¯¸ ìš°ë¦¬ëŠ” ì•¼ì±„ë³´ë‹¤ ê³¼ì¼ì˜ ì¬ì£¼ë¬¸ìœ¨ì´ ë” ë§ì€ê±¸ ì•Œê³  ìˆë‹¤. ì–¼ë§ˆë‚˜ ë” ìì£¼ì¸ì§€ ì•Œì•„ë³´ì item_10to1_ratio í”¼ì²˜ë¥¼ ë§Œë“¦: ì£¼ë¬¸ëœ ê²½ìš°ì™€ ì£¼ë¬¸ë˜ì§€ ì•Šì€ ê²½ìš°ì˜ ì¬ì£¼ë¬¸ ë¹„ìœ¨ë¡œ ì •ì˜ ìœ ì € Aê°€ item Aë¥¼ order_number 1ê³¼ 4ë¥¼ ìƒ€ë‹¤. ìœ ì €BëŠ” item_Aë¥¼ order_number 1ê³¼ 3ì„ ìƒ€ë‹¤. item_10to1_ratioëŠ” 0.5 : ìœ ì € aê°€ ë¨¼ì € 1,4ë¥¼ ì‚¬ê³  ìœ ì € bê°€ 1,3ì„ ìƒ€ëŠ”ë° ì‚° 1,3 ì¤‘ì— ì¬ì£¼ë¬¸ ëœ order_numberê°€ 1ì´ì–´ì„œ 2ê°œ ì¤‘ì— ì¬ì£¼ë¬¸ìœ¨ 1ê°œ ê·¸ë˜ì„œ ratioëŠ” 0.5 Importance Findings for reorder useritem_sum_pos_cart(ì‚¬ìš©ì A, ì•„ì´í…œ B)ëŠ” ì‚¬ìš©ì Aì˜ ì¹´íŠ¸ì—ì„œ ì•„ì´í…œ Bê°€ ì†í•˜ëŠ” í‰ê·  ìœ„ì¹˜ì…ë‹ˆë‹¤. useritem_sum_pos_cart-mean(ì‚¬ìš©ì A)ì€ ëª¨ë“  í•­ëª©ì— ëŒ€í•œ ìœ„ ê¸°ëŠ¥ì˜ í‰ê· ì…ë‹ˆë‹¤. ë”°ë¼ì„œ ì´ featureëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì¹´íŠ¸ì—ì„œ ì•„ì´í…œì˜ í‰ê·  ìœ„ì¹˜ë¥¼ ìº¡ì²˜í•˜ë©°, í•œ ë²ˆì— ë§ì€ ì•„ì´í…œì„ êµ¬ë§¤í•˜ì§€ ì•ŠëŠ” ì‚¬ìš©ìëŠ” â€˜ì¬ì£¼ë¬¸ì´ ì•„ë‹˜â€™ì¼ ê°€ëŠ¥ì„±ì´ ë” ë†’ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. total_buyëŠ” total orderì˜ ìˆ«ì ë§Œì•½ ìœ ì €aê°€ item Aë¥¼ ê³¼ê±°ì— 3ë²ˆ ìƒ€ë‹¤ë©´, total_buyëŠ” 3ì´ ë¨ ê·¸ë˜ì„œ total_buy_maxëŠ” ìœ ì €ë‹¹ ì£¼ë¬¸ì˜ ìµœëŒ€ê°’ ìš°ë¦¬ëŠ” ì´ë¥¼ í†µí•´ ìœ ì €ê°€ ì¬ì£¼ë¬¸í• ì§€ ì•ˆí• ì§€ë¥¼ ì˜ˆì¸¡ t-1_is_None(User A)ëŠ” binary feature: ìœ ì €ì˜ ì „ ì£¼ë¬¸ì´ ì¬ì£¼ë¬¸ì¸ì§€ ì•„ë‹Œì§€ë¥¼ ì•Œë ¤ì¤Œ ë§Œì•½ ì´ ì „ì£¼ë¬¸ì´ â€˜ì¬ì£¼ë¬¸í•˜ì§€ ì•ŠìŒâ€™ì´ë¼ë©´, ë‹¤ìŒ ì£¼ë¬¸ì€ ì¬ì£¼ë¬¸í•˜ì§€ ì•Šì„ ê°€ëŠ¥ì„±ì´ 30%ê°€ ìˆëŠ”ê²ƒ. &#x2F; F1 maximization F1 ìµœëŒ€í™”ì— ê´€í•´ì„œëŠ” Faronì´ ì»¤ë„ì„ ë°œí‘œí•˜ê¸° ì „ê¹Œì§€ëŠ” ê·¸ ë…¼ë¬¸ì„ ì½ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì €ëŠ” F1 ìµœëŒ€í™” ë•ë¶„ì— ë†’ì€ ì ìˆ˜ë¥¼ ë°›ì•˜ìŠµë‹ˆë‹¤. ì„¤ëª…í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤. F1ì„ ìµœëŒ€í™”í•˜ê¸° ìœ„í•´ ì €ëŠ” ì˜ˆì¸¡ëœ í™•ë¥ ì— ë”°ë¼ y_trueë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ê·¸ë¦¬ê³  ë” ë†’ì€ í™•ë¥ ì—ì„œ F1ì„ í™•ì¸í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ {A: 0.3, B: 0.5, C: 0.4}ì™€ ê°™ì´ í•­ëª©ê³¼ í™•ë¥ ì´ ì •ë ¬ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•´ ë³´ê² ìŠµë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ y_trueë¥¼ ì—¬ëŸ¬ ë²ˆ ìƒì„±í•©ë‹ˆë‹¤. ì œ ê²½ìš°ì—ëŠ” 9999ë²ˆ ìƒì„±í–ˆìŠµë‹ˆë‹¤. ì´ì œ [[A,B],[B],[B,C],[C],[B],[None]â€¦..]ì™€ ê°™ì´ ë§ì€ y_trueê°€ ìƒê²¼ìŠµë‹ˆë‹¤. ìœ„ì—ì„œ ì–¸ê¸‰í–ˆë“¯ì´ ë‹¤ìŒìœ¼ë¡œ í•  ì¼ì€ [B], [B,C], [B,C,A]ì—ì„œ F1ì„ í™•ì¸í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ F1 í”¼í¬ ì•„ì›ƒì„ ì¶”ì •í•˜ê³  ê³„ì‚°ì„ ì¤‘ì§€í•˜ê³  ë‹¤ìŒ ìˆœì„œë¡œ ë„˜ì–´ê°ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ë°©ë²•ì—ì„œëŠ” [A],[A,B],[A,B,C],[B]â€¦ì™€ ê°™ì€ ëª¨ë“  íŒ¨í„´ì„ í™•ì¸í•  í•„ìš”ê°€ ì—†ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. â€œë” ë©€ë¦¬ ê°€ê¸° ìœ„í•œ íŒâ€ì´ë¼ëŠ” ì œ ì½”ë©˜íŠ¸ì—ì„œ ì´ ë°©ë²•ì„ ì•Œì•„ë‚¸ ë¶„ë„ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ ë°©ë²•ì€ ì‹œê°„ì´ ë§ì´ ê±¸ë¦¬ê³  ì‹œë“œì— ë”°ë¼ ë‹¬ë¼ì§‘ë‹ˆë‹¤. ê·¸ë˜ì„œ ì €ëŠ” ê²°êµ­ Faronì˜ ì»¤ë„ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ë‹¤í–‰íˆë„ Faronì˜ ì»¤ë„ì„ ì‚¬ìš©í•´ì„œ ê±°ì˜ ë™ì¼í•œ ê²°ê³¼ë¥¼ ì–»ì—ˆìŠµë‹ˆë‹¤. py_model&#x2F;pyx_get_best_items.pyxë¥¼ ì°¸ê³ í•˜ì„¸ìš”. ëª¨ë¸ í‰ê°€ ì§€í‘œì¸ F1 score: ë‹¨ì¼ ë©”íŠ¸ë¦­ì—ì„œ ì •í™•ë„ì™€ ë¦¬ì½œì„ ëª¨ë‘ ìº¡ì²˜í•˜ëŠ” ë°©ë²• ê·¸ë˜ì„œ ìš°ë¦¬ëŠ” ì¬ì£¼ë¬¸ í™•ë¥ ì„ ì´ì§„ í˜•íƒœë¡œ ë°”ê¿”ì•¼ í•¨. í•˜ì§€ë§Œ ì´ ë³€í™˜ì„ ìˆ˜í–‰í•˜ë ¤ë©´ ì„ê³„ê°’ì„ ì•Œì•„ì•¼ í•¨. ì²˜ìŒì—ëŠ” ê·¸ë¦¬ë“œ ê²€ìƒ‰ì„ ì‚¬ìš©í•˜ì—¬ 0.2ë¼ëŠ” ë³´í¸ì ì¸ ì„ê³„ê°’ì„ ì°¾ì•˜ì§€ë§Œ Kaggle í† ë¡  ê²Œì‹œíŒì—ì„œ ì£¼ë¬¸ì— ë”°ë¼ ì„ê³„ê°’ì´ ë‹¬ë¼ì•¼ í•œë‹¤ëŠ” ì˜ê²¬ì„ ë´„ ì™œê·¸ë˜ì•¼ í•˜ëŠ”ì§€ì— ëŒ€í•´ì„œ ë°‘ì˜ ì˜ˆì‹œë¥¼ ì‚´í´ ë³´ì ì²« ë²ˆì§¸ ì˜ˆì—ì„œ ì„ê³„ê°’ì€ 0.9ì—ì„œ 0.3 ì‚¬ì´ì…ë‹ˆë‹¤. ë‘ ë²ˆì§¸ ì˜ˆì—ì„œëŠ” ì„ê³„ê°’ì´ 0.2ë³´ë‹¤ ë‚®ìŠµë‹ˆë‹¤. ì•ì„œ ì„¤ëª…í–ˆë“¯ì´ ê° ì£¼ë¬¸ì—ëŠ” ê°ê°ì˜ ì„ê³„ê°’ì´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ìœ„ì˜ ê³„ì‚°ì„ ì‚¬ìš©í•˜ë©´ ëª¨ë“  í™•ë¥  íŒ¨í„´ì„ ë¨¼ì € ì¤€ë¹„í•´ì•¼ í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ë‹¤ë¥¸ ê³„ì‚°ì„ ìƒê°í•´ ë‚´ì•¼í–ˆìŠµë‹ˆë‹¤. ëª¨ë¸ì—ì„œ í’ˆëª© Aê°€ 0.9ì˜ í™•ë¥ ë¡œ ì¬ì£¼ë¬¸ë˜ê³  í’ˆëª© Bê°€ 0.3ì˜ í™•ë¥ ë¡œ ì¬ì£¼ë¬¸ë  ê²ƒìœ¼ë¡œ ì˜ˆì¸¡í•œë‹¤ê³  ê°€ì •í•´ ë³´ê² ìŠµë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ì´ í™•ë¥ ì„ ì‚¬ìš©í•˜ì—¬ 9,999ê°œì˜ ëŒ€ìƒ ë ˆì´ë¸”(Aì™€ Bê°€ ì¬ì£¼ë¬¸ë ì§€ ì—¬ë¶€)ì„ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì‹œë®¬ë ˆì´ì…˜ëœ ë ˆì´ë¸”ì€ ë‹¤ìŒê³¼ ê°™ì´ ë³´ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ í™•ë¥ ì´ ê°€ì¥ ë†’ì€ í•­ëª©ë¶€í„° ì‹œì‘í•˜ì—¬ F1 ì ìˆ˜ê°€ ì •ì ì— ë‹¬í–ˆë‹¤ê°€ ê°ì†Œí•  ë•Œê¹Œì§€ í•­ëª©(ì˜ˆ: [A], [A, B], [A, B, C] ë“±)ì„ ì¶”ê°€í•˜ì—¬ ê° ë ˆì´ë¸” ì„¸íŠ¸ì— ëŒ€í•œ ì˜ˆìƒ F1 ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. A, B, ABâ€¦ì™€ ê°™ì€ ëª¨ë“  íŒ¨í„´ì„ ê³„ì‚°í•  í•„ìš”ëŠ” ì—†ìŠµë‹ˆë‹¤. í•­ëª©Bë¥¼ ì„ íƒí•´ì•¼ í•œë‹¤ë©´ í•­ëª©Aë„ ì„ íƒí•´ì•¼ í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. â€˜ì¬ì£¼ë¬¸ ì—†ìŒâ€™(None)ì— ëŒ€í•´ ìƒê°í•˜ëŠ” í•œ ê°€ì§€ ë°©ë²•ì€ (1 - í•­ëª© A) * (1 - í•­ëª© B) * â€¦ì˜ í™•ë¥ ë¡œ ìƒê°í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ë˜ ë‹¤ë¥¸ ë°©ë²•ì€ Noneì„ íŠ¹ìˆ˜í•œ ê²½ìš°ë¡œ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. None ëª¨ë¸ì„ ì‚¬ìš©í•˜ê³  Noneì„ ë‹¤ë¥¸ í•­ëª©ìœ¼ë¡œ ì·¨ê¸‰í•˜ë©´ F1 ì ìˆ˜ë¥¼ 0.400ì—ì„œ 0.407ë¡œ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.","categories":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"Kaggle","slug":"Kaggle","permalink":"https://jmj3047.github.io/tags/Kaggle/"}],"keywords":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}]},{"title":"Unsupervised Learning, Recommenders, Reinforcement Learning","slug":"ML_Part3","date":"2023-04-03T15:00:00.000Z","updated":"2023-04-13T10:57:37.183Z","comments":true,"path":"2023/04/04/ML_Part3/","link":"","permalink":"https://jmj3047.github.io/2023/04/04/ML_Part3/","excerpt":"","text":"Course Lecture 3 in Machine Learning course Unsupervised Learning1. clustering Usage: Grouping similar news, DNA analysis, Astronomical data analysis k-means algorithm local minimum Occurs when optimization algorithm converges to a suboptimal solution K-means is sensitive to initial centroid placement Different initializations can lead to different local minima Multiple runs with random initializations can mitigate this issue elbow method Technique to determine optimal number of clusters (k) Plot within-cluster sum of squares (WCSS) against k Look for â€œelbowâ€ point in the plot Elbow indicates diminishing returns in WCSS reduction with increasing k Choose k at elbow point as optimal number of clusters Evaluate K-means based on how well it performs on that later purpose â†’ not depending on mathematics but business. 2. anomaly detection gaussian(normal) distribution anomaly detection algorithm Anomlay detection VS Supervised Learning Just train the system to decide if a new smartphone that you just manufactured has any scratches in it. If you just see scratch smartphones over and over and you want to check if your phones are scratched, then supervised learning works well. Whereas if you suspect that they are going to be brand new ways for something go wrong in the future, then anomaly detection will work well. Anomaly detection tries to find brand new positive examples that may be unlike anything youâ€™ve seen before. Where supervised learning looks at your positive examples and tires to decide if a future example is similar to the positive examples that youâ€™ve already seen. Recommender Systems With this framework for recommended systems one possible way to approach the problem is to look at the movies that users have not rated. And to try to predict how users would rate those movies because then we can try to recommend to users things that they are more likely to rate as five stars. 1. collaborative filtering Using per-item features Collaborative filtering In which of the following situations will a collaborative filtering system be the most appropriate learning algorithm (compared to linear or logistic regression)? You run an online bookstore and collect the ratings of many users. You want to use this to identify what books are â€œsimilarâ€ to each other (i.e., if a user likes a certain book, what are other books that they might also like?) â†’ You can find â€œsimilarâ€ books by learning feature values using collaborative filtering. Binary lables: favs, likes and clicks 2. Recommender systems implementation detail mean normalization It seems more reasonable to think Eve is likely to rate this movie 2.5 rather than think Eve will rate all movie zero stars just because she hasnâ€™t rated any movies yet. In fact the effect of this algorithm is it will cause the initial guesses for the new user Eve to be just equal to the mean of whatever other users have rated these five movies. It turns out that by normalizing the mean of the different movies ratings to be zero, the optimization algorithm for the recommended system will also run just a little bit faster. But it does make algorithm behave much better for users who have rated no movies or very small numbers of movies. And the predictions will become more reasonable. Thereâ€™s one other alternative that you could use which is to instead normalize the columns of this matrix to have zero mean. And that would be a reasonable thing to do too. But I think in this application, normalizing the rows so that you can give reasonable ratings for a new user seems more important than normalizing the columns. Limitations of Collaborative Filtering 3. Content-based filtering Collaborative filtering: Recommend items based on similar user ratings Algorithm uses existing ratings to make recommendations Content-based filtering: Recommend items based on user and item features Requires features of users and items Finds good matches based on these features Can potentially find better matches than collaborative filtering alone 4. Recommending from a large catalogue During the retrieval step, retrieving more items will tend to result in better performance. But the algorithm will end up being slower to analyze or to optimize the trade off between how many items to retrieve to retrieve 100 or 500 or 1000 items. 5. Principle Componant Analysis(PCA) the idea of PCA is to find one or more new axes, such as z so that when you measure your datas coordinates on the new axis, you end up still with very useful information about the car. PCA is a powerful algorithm for taking data with a lot of features, with a lot of dimensions or high-dimensional data, and reducing it to two or three features to two or three dimensional data so you can plot it and visualize it and better understand whatâ€™s in your data. Linear regression: One target variable Y Measures distance between fitted line and Y Distances measured in the direction of the y-axis PCA (Principal Component Analysis): Can have many features (e.g., X1, X2, â€¦ X50) All features treated equally Finds axis Z to retain maximum variance Projects data onto Z while preserving variance But when you have more than two features, which is most of the case, the difference between linear regression and PCA and what the algorithms do is very large. These algorithms are used for totally different purposes and give you very different answers. you should use linear regression if youâ€™re trying to predict the value of y, and you should use PCA if youâ€™re trying to reduce the number of features in your data set, say to visualize it. PCA (Principal Component Analysis) other applications: Data compression (less popular now): Reduce features for smaller storage or transmission costs Example: 50 features per car reduced to 10 principal components Speed up training of supervised learning models (less popular now): Reduce high-dimensional features to smaller set Used to make a difference for older learning algorithms (e.g., Support Vector Machines) Modern developments: Improved storage and networking capabilities Modern machine learning algorithms like deep learning handle high-dimensional data more effectively Less need for PCA in these applications Reinforcement 1. Reinforcement learning introduction the key idea is rather than you needing to tell the algorithm what is the right output y for every single input, all you have to do instead is specify a reward function that tells it when itâ€™s doing well and when itâ€™s doing poorly. And itâ€™s the job of the algorithm to automatically figure out how to choose good actions. the goal is given a board position to pick a good action using a policy Pi. This formalism of a reinforcement learning application actually has a name. Itâ€™s called a Markov decision process in a Markov decision process, the future depends only on where you are now, not on how you got here. 2. State-action value function In reinforcement learning, thereâ€™s a key equation called the Bellman equation that will help us to compute the state action value function.","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Coursera","slug":"Coursera","permalink":"https://jmj3047.github.io/tags/Coursera/"},{"name":"AI Studies","slug":"AI-Studies","permalink":"https://jmj3047.github.io/tags/AI-Studies/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}]},{"title":"Ecommerce Recommendation System","slug":"Ecommerce_Rec","date":"2023-04-02T15:00:00.000Z","updated":"2023-04-06T09:01:21.752Z","comments":true,"path":"2023/04/03/Ecommerce_Rec/","link":"","permalink":"https://jmj3047.github.io/2023/04/03/Ecommerce_Rec/","excerpt":"","text":"ê°œìš” kaggle instacart ë°ì´í„°ë¡œ ì¶”ì²œ ëª¨ë¸ë§ ì‹œìŠ¤í…œì„ ë§Œë“¤ê¸° ë¹…ì¿¼ë¦¬, Vertex AIë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë§Œë“¤ê³  ì˜ˆì¸¡í•˜ê¸° ì°¸ê³ í•œ ìœ íˆ¬ë¸Œì˜ ì¶”ì²œì‹œìŠ¤í…œì€ í‰ì  featureê°€ ìˆì§€ë§Œ ì´ ë°ì´í„°ì—ëŠ” ì¡´ì¬ í•˜ì§€ ì•ŠìŒ. ë”°ë¼ì„œ ì¬ì£¼ë¬¸ ì—¬ë¶€ì™€ ì£¼ë¬¸íšŒì°¨ë¥¼ í‰ì ìœ¼ë¡œ ê°€ì •í•˜ê³  ëª¨ë¸ì— ë„ì…í•¨ 200ë§Œ ìœ ì € ì¤‘ì— 10000ëª…ìœ¼ë¡œ ì œí•œí•˜ì˜€ê³ , ëª¨ë¸ í‰ê°€ì— ëŒ€í•œ ì§€í‘œëŠ” nDCGë¥¼ ì‚¬ìš©í•¨ ëª¨ë¸ì— ëŒ€í•œ ê°„ëµí•œ ì„¤ëª… ì‚¬ìš©í•œ ë°ì´í„°ì…‹ ë° cpu&#x2F;gpu ì„±ëŠ¥ Instacart ë°ì´í„°ì…‹ ì„¸ë¶€ íŠ¹ì„± í™•ì¸ : Instacart data dictionary version 1: vCPU 4ê°œ, 15GB RAM, GPU ì—†ìŒ â†’ í…ì„œí”Œë¡œìš°ê°€ ëŒì•„ê°€ì§€ ì•ŠìŒ ë¹„ìš©: ì›” 202 ë‹¬ëŸ¬, ì‹œê°„ë‹¹ 0.27 ë‹¬ëŸ¬ version2: vCPU 8ê°œ, 52GB RAM, GPU NVIDIA Tesla P4 1ê°œ ë¹„ìš©: ì›” 940 ë‹¬ëŸ¬, ì‹œê°„ë‹¹ 1.27 ë‹¬ëŸ¬ Data import (BQ â†’ Jupyter) ì´ë¯¸ ë¹…ì¿¼ë¦¬ì— csv íŒŒì¼ì„ ì ì¬í•¨ ì£¼í”¼í„° ì»¤ë„ ì•ˆì— â€˜%%â€™ ë¥¼ ì‚¬ìš©í•˜ë©´ ë¹…ì¿¼ë¦¬ ë¬¸ë²•ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ 123%%bigquery aislesSELECT * FROM `your-project-id.your-view.aisles`ORDER BY 1 12print(aisles.isna().sum())aisles 123%%bigquery departmentsSELECT * FROM `your-project-id.your-view.departments`ORDER BY 1 12print(departments.isna().sum())departments ì´ëŸ° í˜•ì‹ìœ¼ë¡œ order_product_prior, order_product_train, orders, products ë°ì´í„° í”„ë ˆì„ì„ ìƒì„± ë°ì´í„° ì „ì²˜ë¦¬: ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ë° ìƒ˜í”Œë§123# ê²°ì¸¡ì¹˜ ì²˜ë¦¬(&#x27;days_since_prior_order&#x27;:ì²« êµ¬ë§¤ì¸ê²½ìš° nanê°’ì´ ì…ë ¥ë˜ì–´ ìˆìŒ =&gt; 0ìœ¼ë¡œ ëŒ€ì²´)#orders[&#x27;days_since_prior_order&#x27;] = orders[&#x27;days_since_prior_order&#x27;].fillna(0)orders = orders.dropna() 12print(orders.shape)orders[&#x27;eval_set&#x27;].value_counts() 1orders[(orders[&#x27;user_id&#x27;]==10)] # =&gt; ì£¼ë¬¸ë°ì´í„° ë¶„ì„: ìœ ì €ë‹¹ priorì£¼ë¬¸ê¸°ë¡, trainì£¼ë¬¸ê¸°ë¡ì´ ìˆëŠ” ìœ ì €ë“¤ì´ ìˆìŒ. 1orders[orders[&#x27;user_id&#x27;].isin(orders[&#x27;user_id&#x27;][orders[&#x27;eval_set&#x27;]==&#x27;test&#x27;])] # test data -&gt; order_product ì •ë³´ê°€ ì—†ìŒ 1orders[&#x27;user_id&#x27;].value_counts() 1234orders_user_id_eval_set = []for i in orders[&#x27;user_id&#x27;][orders[&#x27;eval_set&#x27;]==&#x27;train&#x27;].unique(): i = int(i) orders_user_id_eval_set.append(i) 12345678import randomrandom.seed(2021)# 1ë§Œëª…ì˜ ìœ ì €ë§Œ ìƒ˜í”Œë§ (ì²«ì‹œë„)user = random.sample(orders_user_id_eval_set, 10000)#user = orders[&#x27;user_id&#x27;][orders[&#x27;eval_set&#x27;]==&#x27;train&#x27;].unique().tolist() #ìƒ˜í”Œë§ ì•ˆí•˜ê³  ì‹œë„ (ì‹¤íŒ¨)# ordersì—ì„œ test dataset ê´€ë ¨ ê¸°ë¡(priorí¬í•¨) ì œì™¸orders_2 = orders[orders[&#x27;user_id&#x27;].isin(user)] 1print(&quot;ìœ ì € ì¸ì›:&quot;,len(user), &quot;// ì£¼ë¬¸ê±´ìˆ˜ í•©:&quot;,len(orders_2)) 12# order_product_priorì—ì„œ test dataset ê´€ë ¨ ê¸°ë¡ ì œì™¸order_product_prior = order_product_prior[order_product_prior[&#x27;order_id&#x27;].isin(orders_2[&#x27;order_id&#x27;])] ë°ì´í„° ì¡°í•©123# ì œí’ˆ ì •ë³´ : product + aisles + department dataproduct_m = products.merge(aisles).merge(departments)product_m 1234import pandas as pd# product one-hot encoding data product_enc = pd.get_dummies(product_m, columns=[&#x27;aisle&#x27;], prefix=[None])product_encã„¹ 12order_product = pd.concat([order_product_prior,order_product_train]) order_product 12# ì£¼ë¬¸ í•­ëª© ì •ë³´ : product_m + order_productorder_detail = order_product.merge(product_m,how=&#x27;left&#x27;) #.sample(n=100000, random_state=2021) 1order_detail#.sort_values(by=order_detail[&#x27;order_id&#x27;]) 1data = orders_2.merge(order_detail)#, on =&#x27;order_id&#x27;) 1data 1data.isna().sum() ë°ì´í„° íƒìƒ‰ì  ë¶„ì„(EDA)ì£¼ë¬¸ í–‰íƒœ ë¶„ì„12345# ìš”ì¼ë³„ ì£¼ë¬¸ í˜„í™©data[&#x27;order_dow&#x27;].value_counts()data.hist(&#x27;order_dow&#x27;,grid=False, bins=7) 12345# ì‹œê°„ëŒ€ë³„ ì£¼ë¬¸ í˜„í™©data[&#x27;order_hour_of_day&#x27;].value_counts()data.hist(&#x27;order_hour_of_day&#x27;,grid=False, bins=24) 1234# ì£¼ë¬¸ íšŸìˆ˜ í˜„í™© data[&#x27;order_number&#x27;].value_counts()data.hist(&#x27;order_number&#x27;,grid=False, bins=24) 1234# ì¬ì£¼ë¬¸ ì—¬ë¶€ í˜„í™© data[&#x27;reordered&#x27;].value_counts()data.hist(&#x27;reordered&#x27;,grid=False) ìœ íˆ¬ë¸Œ ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ ì ìš©ë°ì´í„° ì „ì²˜ë¦¬ë°ì´í„° Encoding1234567data[&#x27;user_id&#x27;] = data[&#x27;user_id&#x27;].astype(int)data[&#x27;product_id&#x27;] = data[&#x27;product_id&#x27;].astype(int)data[&#x27;order_id&#x27;] = data[&#x27;order_id&#x27;].astype(int)data[&#x27;days_since_prior_order&#x27;] = data[&#x27;days_since_prior_order&#x27;].astype(int)data = data.set_index([&#x27;user_id&#x27;]).sort_index()data = data.reset_index() 12345678910111213141516171819# ìœ ì € ì¸ë±ìŠ¤ ì¸ì½”ë”©user_ids = data[&quot;user_id&quot;].unique().tolist()user2user_encoded = &#123;x: i for i, x in enumerate(user_ids)&#125;#userencoded2user = &#123;i: x for i, x in enumerate(user_ids)&#125;# ì£¼ë¬¸ ì¸ë±ìŠ¤ ì¸ì½”ë”©order_ids = data[&quot;order_id&quot;].unique().tolist()order2order_encoded = &#123;x: i for i, x in enumerate(order_ids)&#125;#order_encoded2order = &#123;i: x for i, x in enumerate(order_ids)&#125;# ìƒí’ˆ ì¸ë±ìŠ¤ ì¸ì½”ë”©product_ids = data[&quot;product_id&quot;].unique().tolist()product2product_encoded = &#123;x: i for i, x in enumerate(product_ids)&#125;#product_encoded2product = &#123;i: x for i, x in enumerate(product_ids)&#125;# ìƒí’ˆ ì´ë¦„ ì¸ì½”ë”©pd_name_ids = data[&quot;product_name&quot;].unique().tolist()pd_name2pd_name_encoded = &#123;x: i for i, x in enumerate(pd_name_ids)&#125;#pd_name_encoded2pd_name = &#123;i: x for i, x in enumerate(pd_name_ids)&#125; 12345678910111213141516171819202122232425# ìƒí’ˆ ëŒ€ë¶„ë¥˜ ì¸ë±ìŠ¤ ì¸ì½”ë”©department_ids = []for i in data[&quot;department_id&quot;].unique(): i = int(i) department_ids.append(i)department2department_encoded = &#123;x: i for i, x in enumerate(department_ids)&#125;#department_encoded2department = &#123;i: x for i, x in enumerate(department_ids)&#125;# ìƒí’ˆ ì†Œë¶„ë¥˜ ì¸ë±ìŠ¤ ì¸ì½”ë”©aisle_ids = []for i in data[&quot;aisle_id&quot;].unique(): i = int(i) aisle_ids.append(i)aisle2aisle_encoded = &#123;x: i for i, x in enumerate(aisle_ids)&#125;#aisle_encoded2aisle = &#123;i: x for i, x in enumerate(aisle_ids)&#125;# ìƒí’ˆ ëŒ€ë¶„ë¥˜ëª… ì¸ë±ìŠ¤ ì¸ì½”ë”©dept_name_ids = data[&quot;department&quot;].unique().tolist()dept_name2dept_name_encoded = &#123;x: i for i, x in enumerate(dept_name_ids)&#125;#dept_name_encoded2dept_name = &#123;i: x for i, x in enumerate(dept_name_ids)&#125;# ìƒí’ˆ ì†Œë¶„ë¥˜ëª… ì¸ë±ìŠ¤ ì¸ì½”ë”©aisle_name_ids = data[&quot;aisle&quot;].unique().tolist()aisle_name2aisle_name_encoded = &#123;x: i for i, x in enumerate(aisle_name_ids)&#125;#aisle_name_encoded2aisle_name = &#123;i: x for i, x in enumerate(aisle_name_ids)&#125; 12345678910# ì¸ì½”ë”©ìœ¼ë¡œ ë°”ê¾¸ê¸°data[&quot;user&quot;] = data[&quot;user_id&quot;].map(user2user_encoded)data[&quot;product&quot;] = data[&quot;product_id&quot;].map(product2product_encoded)data[&quot;order&quot;] = data[&quot;order_id&quot;].map(order2order_encoded)data[&quot;pd_name&quot;] = data[&quot;product_name&quot;].map(pd_name2pd_name_encoded)# data[&quot;department&quot;] = data[&quot;department_id&quot;].map(department2department_encoded)# data[&quot;aisle&quot;] = data[&quot;aisle&quot;].map(aisle2aisle_encoded)# data[&quot;dept_name&quot;] = data[&quot;department&quot;].map(dept_name2dept_name_encoded)# data[&quot;aisle_name&quot;] = data[&quot;aisle&quot;].map(aisle_name2aisle_name_encoded) User ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„° ì¡°ì •(feature engineering) êµ¬ë§¤ì ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„° í”„ë ˆì„ ì¬ìƒì„± feature engineering ì¶”ê°€ ê¸°ëŠ¥ 12345order_hist = data.groupby([&#x27;user&#x27;])[&#x27;order_id&#x27;].unique().apply(list).reset_index()product_hist = data.groupby([&#x27;user&#x27;])[&#x27;product_id&#x27;].apply(list).reset_index()order_dow_hist = data.groupby([&#x27;user&#x27;])[&#x27;order_dow&#x27;].apply(list).reset_index() # unique().ì ìš©í•´ë³´ê¸°order_hour_of_day_hist = data.groupby([&#x27;user&#x27;])[&#x27;order_hour_of_day&#x27;].apply(list).reset_index()days_since_prior_order_hist = data.groupby([&#x27;user&#x27;])[&#x27;days_since_prior_order&#x27;].apply(list).reset_index() 1data.groupby([&#x27;user&#x27;])[&#x27;order_dow&#x27;].unique().apply(list 1order_product_hist = data.groupby([&#x27;order&#x27;])[&#x27;product_id&#x27;].apply(list).reset_index() 1order_hist # ì‚¬ìš©ìì˜ ì£¼ë¬¸ëª©ë¡ 1product_hist # ì‚¬ìš©ìê°€ êµ¬ë§¤í•œ ìƒí’ˆ 1order_product_hist 1order_dow_hist 1order_hour_of_day_hist 1days_since_prior_order_hist 123# User dataset ìƒì„± (í•™ìŠµì— ì‚¬ìš©í•  ë°ì´í„°, prior order:[data[&#x27;eval_set&#x27;]==&#x27;prior&#x27;])user_data = data[[&#x27;user&#x27;,&#x27;user_id&#x27;]].merge(order_hist, how=&#x27;left&#x27;).merge(product_hist, how=&#x27;left&#x27;).merge(order_dow_hist, how=&#x27;left&#x27;).merge(order_hour_of_day_hist, how = &#x27;left&#x27;).merge(days_since_prior_order_hist,how=&#x27;left&#x27;) #eval_setuser_data 12user_data = user_data.drop_duplicates(&#x27;user&#x27;) # ì¤‘ë³µë°ì´í„° ì‚­ì œuser_data.shape (10000, 7) 1data_product_prior=data[&#x27;product&#x27;][data[&#x27;eval_set&#x27;]==&#x27;prior&#x27;] predict_label ìƒì„± ë° ë°ì´í„° ë¶„í• 123user_data[&#x27;predict_labels&#x27;] = user_data[&#x27;product_id&#x27;].apply(lambda x: int(random.uniform(0,data[&#x27;product_id&#x27;].max())))#user_data[&#x27;predict_labels&#x27;] = user_data[&#x27;product_id&#x27;].apply(lambda x: int(random.uniform(0,data[&#x27;product&#x27;].max())))# (random.uniform(0,data[&quot;product&quot;][data[&#x27;eval_set&#x27;]==&#x27;prior&#x27;].max())) train ë°ì´í„°ì˜ productì¤‘ í•˜ë‚˜ (=&gt; ì•Œë§ì€ ë°ì´í„°ê°€ ë“¤ì–´ê°€ëŠ”ì§€ ì½”ë“œ ê²€ì¦ í•„ìš”) 1user_data 1234train_data = user_data[(user_data.user&gt;=30) &amp; (user_data.user&lt;=39)]test_data = user_data[(user_data.user&gt;=40) &amp; (user_data.user&lt;=59)] í›„ë³´ëª¨ë¸(candidate generator model)1data[&quot;product_id&quot;].max() 49688 1data[&quot;product&quot;].max() 35406 12345678# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜EMBEDDING_DIMS = 16DENSE_UNITS = 64DROPOUT_PCT = 0.1ALPHA = 0.1NUM_CLASSES = data[&quot;product_id&quot;].max() + 2 LEARNING_RATE = 0.1 12import osos.environ[&#x27;TF_CPP_MIN_LOG_LEVEL&#x27;] = &#x27;2&#x27; 123456789101112131415161718192021222324252627282930313233343536# custom layersimport tensorflow as tfclass MaskedEmbeddingsAggregatorLayer(tf.keras.layers.Layer): def __init__(self, agg_mode=&#x27;sum&#x27;, **kwargs): super(MaskedEmbeddingsAggregatorLayer, self).__init__(**kwargs) if agg_mode not in [&#x27;sum&#x27;, &#x27;mean&#x27;]: raise NotImplementedError(&#x27;mode &#123;&#125; not implemented!&#x27;.format(agg_mode)) self.agg_mode = agg_mode @tf.function def call(self, inputs, mask=None): masked_embeddings = tf.ragged.boolean_mask(inputs, mask) if self.agg_mode == &#x27;sum&#x27;: aggregated = tf.reduce_sum(masked_embeddings, axis=1) elif self.agg_mode == &#x27;mean&#x27;: aggregated = tf.reduce_mean(masked_embeddings, axis=1) return aggregated def get_config(self): # this is used when loading a saved model that uses a custom layer return &#123;&#x27;agg_mode&#x27;: self.agg_mode&#125; class L2NormLayer(tf.keras.layers.Layer): def __init__(self, **kwargs): super(L2NormLayer, self).__init__(**kwargs) @tf.function def call(self, inputs, mask=None): if mask is not None: inputs = tf.ragged.boolean_mask(inputs, mask).to_tensor() return tf.math.l2_normalize(inputs, axis=-1) def compute_mask(self, inputs, mask): return mask 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889# modelingimport tensorflow as tfimport datetimeimport osinput_user = tf.keras.Input(shape=(None, ), name=&#x27;user&#x27;) input_product_hist = tf.keras.layers.Input(shape=(None,), name=&#x27;product_hist&#x27;)input_order_dow_hist = tf.keras.layers.Input(shape=(None,), name=&#x27;order_dow_hist&#x27;)input_order_hour_of_day_hist = tf.keras.Input(shape=(None, ), name=&#x27;order_hour_of_day_hist&#x27;)input_days_since_prior_order_hist = tf.keras.Input(shape=(None, ), name=&#x27;days_since_prior_order_hist&#x27;)# layer êµ¬ì„±features_embedding_layer = tf.keras.layers.Embedding(input_dim=NUM_CLASSES, output_dim=EMBEDDING_DIMS, mask_zero=True, trainable=True, name=&#x27;features_embeddings&#x27;)labels_embedding_layer = tf.keras.layers.Embedding(input_dim=NUM_CLASSES, output_dim=EMBEDDING_DIMS, mask_zero=True, trainable=True, name=&#x27;labels_embeddings&#x27;)avg_embeddings = MaskedEmbeddingsAggregatorLayer(agg_mode=&#x27;mean&#x27;, name=&#x27;aggregate_embeddings&#x27;)dense_1 = tf.keras.layers.Dense(units=DENSE_UNITS, name=&#x27;dense_1&#x27;)dense_2 = tf.keras.layers.Dense(units=DENSE_UNITS, name=&#x27;dense_2&#x27;)dense_3 = tf.keras.layers.Dense(units=DENSE_UNITS, name=&#x27;dense_3&#x27;)l2_norm_1 = L2NormLayer(name=&#x27;l2_norm_1&#x27;)dense_output = tf.keras.layers.Dense(NUM_CLASSES, activation=tf.nn.softmax, name=&#x27;dense_output&#x27;)# feature íˆ¬ì…features_embeddings = features_embedding_layer(input_user)l2_norm_features = l2_norm_1(features_embeddings)avg_features = avg_embeddings(l2_norm_features)labels_product_embeddings = labels_embedding_layer(input_product_hist)l2_norm_product = l2_norm_1(labels_product_embeddings)avg_product = avg_embeddings(l2_norm_product)labels_order_dow_embeddings = labels_embedding_layer(input_order_dow_hist)l2_norm_order_dow = l2_norm_1(labels_order_dow_embeddings)avg_order_dow = avg_embeddings(l2_norm_order_dow)labels_order_hour_embeddings = labels_embedding_layer(input_order_hour_of_day_hist)l2_norm_order_hour = l2_norm_1(labels_order_hour_embeddings)avg_order_hour = avg_embeddings(l2_norm_order_hour)labels_since_prior_embeddings = labels_embedding_layer(input_days_since_prior_order_hist)l2_norm_since_prior = l2_norm_1(labels_since_prior_embeddings)avg_since_prior = avg_embeddings(l2_norm_since_prior)print(avg_features)print(avg_order_dow)print(avg_order_hour)print(avg_since_prior)# ì„ë² ë”© ë²¡í„°ë“¤ ì—°ê²°concat_inputs = tf.keras.layers.Concatenate(axis=1)([avg_product, avg_order_dow, avg_order_hour, avg_since_prior ])# Dense Layersdense_1_features = dense_1(concat_inputs)dense_1_relu = tf.keras.layers.ReLU(name=&#x27;dense_1_relu&#x27;)(dense_1_features)dense_1_batch_norm = tf.keras.layers.BatchNormalization(name=&#x27;dense_1_batch_norm&#x27;)(dense_1_relu)dense_2_features = dense_2(dense_1_relu)dense_2_relu = tf.keras.layers.ReLU(name=&#x27;dense_2_relu&#x27;)(dense_2_features)dense_2_batch_norm = tf.keras.layers.BatchNormalization(name=&#x27;dense_2_batch_norm&#x27;)(dense_2_relu)dense_3_features = dense_3(dense_2_relu)dense_3_relu = tf.keras.layers.ReLU(name=&#x27;dense_3_relu&#x27;)(dense_3_features)dense_3_batch_norm = tf.keras.layers.BatchNormalization(name=&#x27;dense_3_batch_norm&#x27;)(dense_3_relu)outputs = dense_output(dense_3_batch_norm)#Optimizeroptimiser = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)#--- prep modelmodel = tf.keras.models.Model( inputs=[input_product_hist, input_order_dow_hist, input_order_hour_of_day_hist, input_days_since_prior_order_hist ], outputs=[outputs])logdir = os.path.join(&quot;logs&quot;, datetime.datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;))tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)model.compile(optimizer=optimiser, loss=&#x27;sparse_categorical_crossentropy&#x27;, metrics=[&#x27;acc&#x27;]) model.summary() 1train_data 12345678# í•™ìŠµ(training)history = model.fit([tf.keras.preprocessing.sequence.pad_sequences(train_data[&#x27;product_id&#x27;]), tf.keras.preprocessing.sequence.pad_sequences(train_data[&#x27;order_dow&#x27;]), tf.keras.preprocessing.sequence.pad_sequences(train_data[&#x27;order_hour_of_day&#x27;]), #+ 1e-10, dtype=float tf.keras.preprocessing.sequence.pad_sequences(train_data[&#x27;days_since_prior_order&#x27;]) ],train_data[&#x27;predict_labels&#x27;].values, #batch_size=16, steps_per_epoch=1, epochs=300) 12# ëª¨ë¸ ì €ì¥model.save(&quot;candidate_generation_v2.h5&quot;) 12345678# ëª¨ë¸ ì˜ˆì¸¡ê²°ê³¼ ì¶”ì¶œpred = model.predict([tf.keras.preprocessing.sequence.pad_sequences(test_data[&#x27;product_id&#x27;]), tf.keras.preprocessing.sequence.pad_sequences(test_data[&#x27;order_dow&#x27;]), tf.keras.preprocessing.sequence.pad_sequences(test_data[&#x27;order_hour_of_day&#x27;]), #+ 1e-10, dtype=float tf.keras.preprocessing.sequence.pad_sequences(test_data[&#x27;days_since_prior_order&#x27;]) ])pred 123456789# candidate generation: ###### ê° userë‹¹ top-7ê°œì˜ ì¶”ì²œ í›„ë³´ ë°ì´í„°(predict_label)ë¥¼ ë½‘ì•„ë‚¸ë‹¤.import numpy as npN = 7k = np.sort((-pred).argsort()[:,:N])print(k)k = k.flatten()k[k&gt;data[&quot;product&quot;].max()]=0k = np.unique(k) 1k ìˆœìœ„ ëª¨ë¸ 1,2ì•ˆ(ranking model 1,2)1ì•ˆ)ì¬ì£¼ë¬¸ì—¬ë¶€(reordered), 1:like, 0:dislike12345678# load candidate_generation model = tf.keras.models.load_model( &#x27;candidate_generation_v2.h5&#x27;, custom_objects=&#123; &#x27;L2NormLayer&#x27;:L2NormLayer, &#x27;MaskedEmbeddingsAggregatorLayer&#x27;:MaskedEmbeddingsAggregatorLayer &#125;) 123456789101112131415161718192021222324# ì•„ì´í…œì˜ ì†ì„±(ex.aisle)ì„ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜ aisle_cols = aisles[&#x27;aisle&#x27;].values.tolist()# type(aisle_cols)type(aisle_cols)aisle_colsaisles_encoded = &#123;x: i for i, x in enumerate(aisle_cols)&#125;# movies-&gt; product_enc# genres-&gt; aisles# aisles -&gt; [&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, ...]def get_aisles(products, aisles): def get_all_aisles(ai): active = [str(aisles_encoded[aisle]) for aisle, a in zip(aisles, ai) if a==1] if len(active) == 0: return &#x27;0&#x27; return &#x27;,&#x27;.join((active)) products[&#x27;all_aisles&#x27;] = [ get_all_aisles(ai) for ai in zip(*[products[aisle] for aisle in aisles]) # ë¬¸ì œì—†ìŒ. ]get_aisles(product_enc, aisle_cols) 1product_enc.head(1) 1data[&#x27;reordered&#x27;].value_counts() 12ratings = data[[&#x27;product_id&#x27;, &#x27;reordered&#x27;]]ratings 1234# normalizedef normalize_col(df,col_name): df[col_name] = (df[col_name] - df[col_name].min()) / (df[col_name].max() - df[col_name].min()) return df 12345678910111213141516171819202122232425262728293031#-- rating ë°ì´í„° í˜•íƒœë¡œ ì •ë¦¬í•˜ê¸° #-- ë ˆí¼ëŸ°ìŠ¤ ì½”ë“œì™€ ëŒ€ì¡°í•˜ì—¬ ì •ë¦¬. # ë ˆí¼ëŸ°ìŠ¤: https://github.com/HYEZ/Deep-Youtube-Recommendations/blob/master/neural_net.ipynb# dataframe: movies-&gt;product_enc# movie_id -&gt; product_id# title -&gt; product_name -&gt; pd_name# title2title_encoded -&gt;pd_name2pd_name_encoded# genre -&gt; aisleproduct_data = product_enc.set_index([&#x27;product_id&#x27;]).sort_index()# using candidate result &#x27;k&#x27;product_data = product_data.loc[k+1]product_data[&quot;pd_name_d&quot;] = product_data[&quot;product_name&quot;].map(pd_name2pd_name_encoded)print(product_data.head())# ë ˆí¼ëŸ°ìŠ¤ì™€ ëŒ€ì¡°í•˜ì—¬ feature ì •ë¦¬: # rating -&gt; reordered (1: like / 0: dislike) # unix_timestamp-&gt; order_hour_of_day / êµ³ì´ í•„ìš”ì—†ìœ¼ë©´ ì‚¬ìš©í•˜ì§€ ì•Šê¸°. ratings_cols = [&#x27;user_id&#x27;, &#x27;product_id&#x27;, &#x27;reordered&#x27;, &#x27;order_hour_of_day&#x27;]ratings = data[[&#x27;product_id&#x27;, &#x27;reordered&#x27;, &#x27;user_id&#x27;, &#x27;order_hour_of_day&#x27; ]]# ëª¨ë¸ì— ì‚¬ìš©í•  new_datanew_data = product_data.merge(ratings, on=&#x27;product_id&#x27;) # rating ì¶”ê°€print(new_data.columns)aisle_occurences = new_data[aisle_cols].sum().to_dict()aisles_encoded = &#123;x: i for i, x in enumerate(aisle_cols)&#125;get_aisles(product_data, aisle_cols) 1new_data.columns 123456789101112131415161718192021222324252627282930313233343536373839new_data = new_data[[&#x27;product_id&#x27;, &#x27;user_id&#x27;, &#x27;reordered&#x27;, &#x27;order_hour_of_day&#x27;, &#x27;all_aisles&#x27;, &#x27;pd_name_d&#x27;]]new_data[&#x27;product_type&#x27;] = np.where(new_data[&#x27;reordered&#x27;] ==1, &#x27;like&#x27;, &#x27;dislike&#x27;) # 1ì´ë©´ likeproduct_list = new_data.groupby([&#x27;user_id&#x27;,&#x27;product_type&#x27;])[&#x27;product_id&#x27;].apply(list).reset_index()aisle_list = new_data.groupby([&#x27;user_id&#x27;])[&#x27;all_aisles&#x27;].unique().apply(list).reset_index()aisle_list[&#x27;all_aisles&#x27;]=aisle_list[&#x27;all_aisles&#x27;].apply(lambda x: list(set(&#x27;,&#x27;.join(x))) ) # ì¤‘ë³µì œê±°aisle_list[&#x27;all_aisles&#x27;]=aisle_list[&#x27;all_aisles&#x27;].apply(lambda x:[ x for x in x if x.isdigit() ]) # normalizenew_data = normalize_col(new_data, &#x27;order_hour_of_day&#x27;)order_hour_of_day_list = new_data.groupby([&#x27;user_id&#x27;])[&#x27;order_hour_of_day&#x27;].unique().apply(list).reset_index()pd_name_list = new_data.groupby([&#x27;user_id&#x27;])[&#x27;pd_name_d&#x27;].apply(list).reset_index()print(product_list)dataset = product_list.pivot(index=&#x27;user_id&#x27;, columns=&#x27;product_type&#x27;, values=&#x27;product_id&#x27;).reset_index()dataset.fillna(new_data[&quot;product_id&quot;].max()+1, inplace=True)dataset[&#x27;like&#x27;] =dataset[&#x27;like&#x27;].apply(lambda x: x if type(x) is list else [])dataset[&#x27;dislike&#x27;] =dataset[&#x27;dislike&#x27;].apply(lambda x: x if type(x) is list else [])dataset = pd.merge(dataset, pd_name_list, how=&#x27;left&#x27;)dataset = pd.merge(dataset, aisle_list, how=&#x27;left&#x27;)dataset = pd.merge(dataset, order_hour_of_day_list, how=&#x27;left&#x27;)dataset[&#x27;predict_labels&#x27;] = dataset[&#x27;like&#x27;].apply(lambda x: int(random.uniform(1,new_data[&quot;product_id&quot;].max()))) dataset[&#x27;like&#x27;]=dataset[&#x27;like&#x27;].apply(lambda x: [new_data[&quot;product_id&quot;].max()+1] if x == [] else x)dataset[&#x27;dislike&#x27;]=dataset[&#x27;dislike&#x27;].apply(lambda x: [new_data[&quot;product_id&quot;].max()+1] if x == [] else x)# train_data=dataset[(dataset.user_id &gt;= 1)&amp;# (dataset.user_id &lt;= 5)]# test_data=dataset[(dataset.user_id &gt;= 6)&amp;# (dataset.user_id &lt;= 9)]train_data_r=dataset[(dataset.index &gt;= 0)&amp; (dataset.index &lt;= 9)]test_data_r=dataset[(dataset.index &gt;= 20)&amp; (dataset.index &lt;= 24)]print(dataset.index) 1train_data_r 1test_data_r 1234567# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜EMBEDDING_DIMS = 16DENSE_UNITS = 64DROPOUT_PCT = 0.1ALPHA = 0.0NUM_CLASSES=new_data[&quot;product_id&quot;].max() + 3LEARNING_RATE = 0.003 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586# ëª¨ë¸#---inputsimport tensorflow as tfimport datetimeimport osinput_name = tf.keras.Input(shape=(None, ), name=&#x27;product_name&#x27;)inp_item_liked = tf.keras.layers.Input(shape=(None,), name=&#x27;like&#x27;)inp_item_disliked = tf.keras.layers.Input(shape=(None,), name=&#x27;dislike&#x27;)input_aisle = tf.keras.Input(shape=(None, ), name=&#x27;aisle&#x27;)input_order_hour = tf.keras.Input(shape=(None, ), name=&#x27;order_hour&#x27;)#--- layersfeatures_embedding_layer = tf.keras.layers.Embedding(input_dim=NUM_CLASSES, output_dim=EMBEDDING_DIMS, mask_zero=True, trainable=True, name=&#x27;features_embeddings&#x27;)labels_embedding_layer = tf.keras.layers.Embedding(input_dim=NUM_CLASSES, output_dim=EMBEDDING_DIMS, mask_zero=True, trainable=True, name=&#x27;labels_embeddings&#x27;)avg_embeddings = MaskedEmbeddingsAggregatorLayer(agg_mode=&#x27;mean&#x27;, name=&#x27;aggregate_embeddings&#x27;)dense_1 = tf.keras.layers.Dense(units=DENSE_UNITS, name=&#x27;dense_1&#x27;)dense_2 = tf.keras.layers.Dense(units=DENSE_UNITS, name=&#x27;dense_2&#x27;)dense_3 = tf.keras.layers.Dense(units=DENSE_UNITS, name=&#x27;dense_3&#x27;)l2_norm_1 = L2NormLayer(name=&#x27;l2_norm_1&#x27;)dense_output = tf.keras.layers.Dense(NUM_CLASSES, activation=tf.nn.softmax, name=&#x27;dense_output&#x27;)#--- featuresfeatures_embeddings = features_embedding_layer(input_name)l2_norm_features = l2_norm_1(features_embeddings)avg_features = avg_embeddings(l2_norm_features)labels_liked_embeddings = labels_embedding_layer(inp_item_liked)l2_norm_liked = l2_norm_1(labels_liked_embeddings)avg_liked = avg_embeddings(l2_norm_liked)labels_disliked_embeddings = labels_embedding_layer(inp_item_disliked)l2_norm_disliked = l2_norm_1(labels_disliked_embeddings)avg_disliked = avg_embeddings(l2_norm_disliked)labels_aisle_embeddings = labels_embedding_layer(input_aisle)l2_norm_aisle = l2_norm_1(labels_aisle_embeddings)avg_aisle = avg_embeddings(l2_norm_aisle)labels_order_hour_embeddings = labels_embedding_layer(input_order_hour)l2_norm_order_hour = l2_norm_1(labels_order_hour_embeddings)avg_order_hour = avg_embeddings(l2_norm_order_hour)# ì„ë² ë”© ë²¡í„°ë“¤ ì—°ê²°concat_inputs = tf.keras.layers.Concatenate(axis=1)([avg_features, avg_liked, avg_disliked, avg_aisle, avg_order_hour ])# Dense Layersdense_1_features = dense_1(concat_inputs)dense_1_relu = tf.keras.layers.ReLU(name=&#x27;dense_1_relu&#x27;)(dense_1_features)dense_1_batch_norm = tf.keras.layers.BatchNormalization(name=&#x27;dense_1_batch_norm&#x27;)(dense_1_relu)dense_2_features = dense_2(dense_1_relu)dense_2_relu = tf.keras.layers.ReLU(name=&#x27;dense_2_relu&#x27;)(dense_2_features)# dense_2_batch_norm = tf.keras.layers.BatchNormalization(name=&#x27;dense_2_batch_norm&#x27;)(dense_2_relu)dense_3_features = dense_3(dense_2_relu)dense_3_relu = tf.keras.layers.ReLU(name=&#x27;dense_3_relu&#x27;)(dense_3_features)dense_3_batch_norm = tf.keras.layers.BatchNormalization(name=&#x27;dense_3_batch_norm&#x27;)(dense_3_relu)outputs = dense_output(dense_3_batch_norm)#Optimizeroptimiser = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)#--- prep modelmodel = tf.keras.models.Model( inputs=[input_name, inp_item_liked, inp_item_disliked, input_aisle, input_order_hour, ], outputs=[outputs])logdir = os.path.join(&quot;logs&quot;, datetime.datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;))tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)model.compile(optimizer=optimiser, loss=&#x27;sparse_categorical_crossentropy&#x27;, metrics=[&#x27;acc&#x27;])model.summary() Model: &quot;model_3&quot; 12#í•™ìŠµtrain_data_r.columns 1234567ratings1 = model.fit([tf.keras.preprocessing.sequence.pad_sequences(train_data_r[&#x27;pd_name_d&#x27;])+ 1e-10, tf.keras.preprocessing.sequence.pad_sequences(train_data_r[&#x27;like&#x27;])+ 1e-10, tf.keras.preprocessing.sequence.pad_sequences(train_data_r[&#x27;dislike&#x27;])+ 1e-10, tf.keras.preprocessing.sequence.pad_sequences(train_data_r[&#x27;all_aisles&#x27;])+ 1e-10, tf.keras.preprocessing.sequence.pad_sequences(train_data_r[&#x27;order_hour_of_day&#x27;], dtype=float) + 1e-10, ],train_data_r[&#x27;predict_labels&#x27;].values, steps_per_epoch=1, epochs=300) 123456789# prediction pred_1 = model.predict([tf.keras.preprocessing.sequence.pad_sequences(test_data_r[&#x27;pd_name_d&#x27;])+ 1e-10, tf.keras.preprocessing.sequence.pad_sequences(test_data_r[&#x27;like&#x27;])+ 1e-10, tf.keras.preprocessing.sequence.pad_sequences(test_data_r[&#x27;dislike&#x27;])+ 1e-10, tf.keras.preprocessing.sequence.pad_sequences(test_data_r[&#x27;all_aisles&#x27;])+ 1e-10, tf.keras.preprocessing.sequence.pad_sequences(test_data_r[&#x27;order_hour_of_day&#x27;], dtype=float) + 1e-10 ])pred_1 ranking ê²°ê³¼ 1234567891011121314# ranking###### ê° userë‹¹ top-7ê°œì˜ ì¶”ì²œ ë°ì´í„°ë¥¼ ë½‘ì•„ë‚¸ë‹¤.N = 7# k = np.sort((-pred_1).argsort()[:,:N]) # np.argsort(): probabilityì˜ ë§ˆì´ë„ˆìŠ¤ê°’ì„ ì‘ì€ ê°’ë¶€í„° ìˆœì„œëŒ€ë¡œ ë°ì´í„°ì˜ ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜. ==&gt; ì¦‰, ì–‘ì˜ê°’ìœ¼ë¡œëŠ” í° ê°’ë¶€í„° ë°˜í™˜í•œ ì…ˆ.# np.sort(): ì¸ë±ìŠ¤ ìˆœìœ¼ë¡œ ë‹¤ì‹œ ì •ë ¬. í™•ë¥ ì´ ë” ë†’ì€ ê²ƒë¶€í„° ë³´ê³ ì‹¶ìœ¼ë¯€ë¡œ ë ˆí¼ëŸ°ìŠ¤ì— ìˆëŠ” ì½”ë“œì´ì§€ë§Œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ. ranking_1 = (-pred_1).argsort()[:, :N]ranking_1[ranking_1&gt;new_data[&#x27;product_id&#x27;].max()]=0 print(ranking_1)ranking_1_probability = np.sort(pred_1[:, :N])print(ranking_1_probability) 2ì•ˆ) order_number: 18ë²ˆì§¸ ì´ìƒ like, 18ë²ˆì§¸ ë¯¸ë§Œ dislike12345678# load candidate_generation model = tf.keras.models.load_model( &#x27;candidate_generation_v2.h5&#x27;, custom_objects=&#123; &#x27;L2NormLayer&#x27;:L2NormLayer, &#x27;MaskedEmbeddingsAggregatorLayer&#x27;:MaskedEmbeddingsAggregatorLayer &#125;) 123456789101112131415161718192021222324# ì•„ì´í…œì˜ ë‹¤ë¥¸ ì†ì„±(ex. aisle)ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ê¸°ëŠ¥aisle_cols = aisles[&#x27;aisle&#x27;].values.tolist()# type(aisle_cols)type(aisle_cols)aisle_colsaisles_encoded = &#123;x: i for i, x in enumerate(aisle_cols)&#125;# movies-&gt; product_enc# genres-&gt; aisles# aisles -&gt; [&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, ...]def get_aisles(products, aisles): def get_all_aisles(ai): active = [str(aisles_encoded[aisle]) for aisle, a in zip(aisles, ai) if a==1] if len(active) == 0: return &#x27;0&#x27; return &#x27;,&#x27;.join((active)) products[&#x27;all_aisles&#x27;] = [ get_all_aisles(ai) for ai in zip(*[products[aisle] for aisle in aisles]) # ë¬¸ì œì—†ìŒ. ]get_aisles(product_enc, aisle_cols) 1product_enc.head(1) 1data.head() 1data[[&#x27;order_number&#x27;]].value_counts() 12# ì£¼ë¬¸ë°ì´í„°ì˜ í‰ê·  ì£¼ë¬¸íšŒì°¨ data[&#x27;order_number&#x27;].mean() 18.063830654172047 1data.shape (1570703, 19) 1234567# order_numberë¶„í¬ í™•ì¸ import matplotlib.pyplot as pltdata[&#x27;order_number&#x27;].value_counts()data.hist(&#x27;order_number&#x27;,grid=False, bins = 30)plt.minorticks_on()plt.tick_params(axis=&#x27;both&#x27;, which =&#x27;both&#x27;, direction=&#x27;in&#x27;, pad=8, top=True, right=True) 1data[&#x27;order_number&#x27;].min(), data[&#x27;order_number&#x27;].max(), data[&#x27;order_number&#x27;].mean() (2, 100, 18.063830654172047) 1234567891011121314151617# ratings_2 (order_number: 18ì´ìƒ like / 18ë¯¸ë§Œ dislike)# ë ˆí¼ëŸ°ìŠ¤: https://www.delftstack.com/ko/howto/python-pandas/how-to-create-dataframe-column-based-on-given-condition-in-pandas/# data[&#x27;ratings_2&#x27;] = data[&#x27;order_number&#x27;][(data[&#x27;order_number&#x27;]&gt;=18)]import numpy as np conditionlist = [ data[&#x27;order_number&#x27;].to_numpy() &gt;= 18, data[&#x27;order_number&#x27;].to_numpy()&lt;18]choicelist = [1, 0]data[&#x27;ratings_2&#x27;] = np.select(conditionlist, choicelist)print(data[&#x27;ratings_2&#x27;].value_counts())print(data.columns)# ë°ì´í„° data[&#x27;ratings_2&#x27;] = 1,0 ê°’ í™•ì¸# data[[&#x27;order_number&#x27;, &#x27;ratings_2&#x27;]][(data[&#x27;order_number&#x27;]&lt;18)].sort_values(by=&#x27;order_number&#x27;, ascending=False)data[[&#x27;order_number&#x27;, &#x27;ratings_2&#x27;]][(data[&#x27;order_number&#x27;]&gt;=18)].sort_values(by=&#x27;order_number&#x27;, ascending=False) 1234# normalizedef normalize_col(df,col_name): df[col_name] = (df[col_name] - df[col_name].min()) / (df[col_name].max() - df[col_name].min()) return df 12345678910111213141516171819202122232425262728293031#-- rating ë°ì´í„° í˜•íƒœë¡œ ì •ë¦¬í•˜ê¸° #-- ë ˆí¼ëŸ°ìŠ¤ ì½”ë“œì™€ ëŒ€ì¡°í•˜ì—¬ ì •ë¦¬. # ë ˆí¼ëŸ°ìŠ¤: https://github.com/HYEZ/Deep-Youtube-Recommendations/blob/master/neural_net.ipynb# dataframe: movies-&gt;product_enc# movie_id -&gt; product_id# title -&gt; product_name -&gt; pd_name# title2title_encoded -&gt;pd_name2pd_name_encoded# genre -&gt; aisle# 1ì•ˆì—ì„œ ì‚¬ìš©í•œ product_data ê·¸ëŒ€ë¡œ ì‚¬ìš©. ì•„ë˜ëŠ” ì—ëŸ¬ ë°œìƒì‹œ ëŒ€ì•ˆ ì½”ë“œ. # product_data_2 = product_enc.set_index([&#x27;product_id&#x27;]).sort_index()# using candidate result &#x27;k&#x27;# product_data_2 = product_data_2.loc[k+1]# product_data_2[&quot;pd_name_d&quot;] = product_data_2[&quot;product_name&quot;].map(pd_name2pd_name_encoded)# print(product_data.head())# ë ˆí¼ëŸ°ìŠ¤ì™€ ëŒ€ì¡°í•˜ì—¬ feature ì •ë¦¬: # rating -&gt; reordered (1: like / 0: dislike) # unix_timestamp-&gt; order_hour_of_day / êµ³ì´ í•„ìš”ì—†ìœ¼ë©´ ì‚¬ìš©í•˜ì§€ ì•Šê¸°. ratings_2_cols = [&#x27;user_id&#x27;, &#x27;product_id&#x27;, &#x27;ratings_2&#x27;, &#x27;order_hour_of_day&#x27;]ratings_2 = data[[&#x27;product_id&#x27;, &#x27;ratings_2&#x27;, &#x27;user_id&#x27;, &#x27;order_hour_of_day&#x27; ]]# ëª¨ë¸ì— ì‚¬ìš©í•  new_datanew_data_2 = product_data.merge(ratings_2, on=&#x27;product_id&#x27;) # rating ì¶”ê°€print(new_data.columns)aisle_occurences = new_data_2[aisle_cols].sum().to_dict()aisles_encoded = &#123;x: i for i, x in enumerate(aisle_cols)&#125;get_aisles(product_data, aisle_cols) 12print(product_data.columns)print(new_data_2.columns) 123456789101112131415161718192021222324252627282930313233343536373839new_data_2 = new_data_2[[&#x27;product_id&#x27;, &#x27;user_id&#x27;, &#x27;ratings_2&#x27;, &#x27;order_hour_of_day&#x27;, &#x27;all_aisles&#x27;, &#x27;pd_name_d&#x27;]]new_data_2[&#x27;product_type&#x27;] = np.where(new_data_2[&#x27;ratings_2&#x27;] ==1, &#x27;like&#x27;, &#x27;dislike&#x27;) # 1ì´ë©´ likeproduct_list_2 = new_data_2.groupby([&#x27;user_id&#x27;,&#x27;product_type&#x27;])[&#x27;product_id&#x27;].apply(list).reset_index()aisle_list_2 = new_data_2.groupby([&#x27;user_id&#x27;])[&#x27;all_aisles&#x27;].unique().apply(list).reset_index()aisle_list_2[&#x27;all_aisles&#x27;]=aisle_list_2[&#x27;all_aisles&#x27;].apply(lambda x: list(set(&#x27;,&#x27;.join(x))) ) # ì¤‘ë³µì œê±°aisle_list_2[&#x27;all_aisles&#x27;]=aisle_list_2[&#x27;all_aisles&#x27;].apply(lambda x:[ x for x in x if x.isdigit() ]) # normalizenew_data_2 = normalize_col(new_data_2, &#x27;order_hour_of_day&#x27;)order_hour_of_day_list_2 = new_data_2.groupby([&#x27;user_id&#x27;])[&#x27;order_hour_of_day&#x27;].unique().apply(list).reset_index()pd_name_list_2 = new_data_2.groupby([&#x27;user_id&#x27;])[&#x27;pd_name_d&#x27;].apply(list).reset_index()print(product_list_2)dataset_2 = product_list_2.pivot(index=&#x27;user_id&#x27;, columns=&#x27;product_type&#x27;, values=&#x27;product_id&#x27;).reset_index()dataset_2.fillna(new_data_2[&quot;product_id&quot;].max()+1, inplace=True)dataset_2[&#x27;like&#x27;] =dataset_2[&#x27;like&#x27;].apply(lambda x: x if type(x) is list else [])dataset_2[&#x27;dislike&#x27;] =dataset_2[&#x27;dislike&#x27;].apply(lambda x: x if type(x) is list else [])dataset_2 = pd.merge(dataset_2, pd_name_list_2, how=&#x27;left&#x27;)dataset_2 = pd.merge(dataset_2, aisle_list_2, how=&#x27;left&#x27;)dataset_2 = pd.merge(dataset_2, order_hour_of_day_list_2, how=&#x27;left&#x27;)dataset_2[&#x27;predict_labels&#x27;] = dataset_2[&#x27;like&#x27;].apply(lambda x: int(random.uniform(1,new_data_2[&quot;product_id&quot;].max()))) dataset_2[&#x27;like&#x27;]=dataset_2[&#x27;like&#x27;].apply(lambda x: [new_data_2[&quot;product_id&quot;].max()+1] if x == [] else x)dataset_2[&#x27;dislike&#x27;]=dataset_2[&#x27;dislike&#x27;].apply(lambda x: [new_data_2[&quot;product_id&quot;].max()+1] if x == [] else x)# train_data=dataset[(dataset.user_id &gt;= 1)&amp;# (dataset.user_id &lt;= 5)]# test_data=dataset[(dataset.user_id &gt;= 6)&amp;# (dataset.user_id &lt;= 9)]train_data_r_2=dataset_2[(dataset_2.index &gt;= 0)&amp; (dataset_2.index &lt;= 9)]test_data_r_2=dataset_2[(dataset_2.index &gt;= 20)&amp; (dataset_2.index &lt;= 24)]print(dataset_2.index) 1train_data_r_2 1test_data_r_2 1234567# í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì •ì˜EMBEDDING_DIMS = 16DENSE_UNITS = 64DROPOUT_PCT = 0.1ALPHA = 0.0NUM_CLASSES=new_data_2[&quot;product_id&quot;].max() + 3LEARNING_RATE = 0.003 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586# ëª¨ë¸#---inputsimport tensorflow as tfimport datetimeimport osinput_name = tf.keras.Input(shape=(None, ), name=&#x27;product_name&#x27;)inp_item_liked = tf.keras.layers.Input(shape=(None,), name=&#x27;like&#x27;)inp_item_disliked = tf.keras.layers.Input(shape=(None,), name=&#x27;dislike&#x27;)input_aisle = tf.keras.Input(shape=(None, ), name=&#x27;aisle&#x27;)input_order_hour = tf.keras.Input(shape=(None, ), name=&#x27;order_hour&#x27;)#--- layersfeatures_embedding_layer = tf.keras.layers.Embedding(input_dim=NUM_CLASSES, output_dim=EMBEDDING_DIMS, mask_zero=True, trainable=True, name=&#x27;features_embeddings&#x27;)labels_embedding_layer = tf.keras.layers.Embedding(input_dim=NUM_CLASSES, output_dim=EMBEDDING_DIMS, mask_zero=True, trainable=True, name=&#x27;labels_embeddings&#x27;)avg_embeddings = MaskedEmbeddingsAggregatorLayer(agg_mode=&#x27;mean&#x27;, name=&#x27;aggregate_embeddings&#x27;)dense_1 = tf.keras.layers.Dense(units=DENSE_UNITS, name=&#x27;dense_1&#x27;)dense_2 = tf.keras.layers.Dense(units=DENSE_UNITS, name=&#x27;dense_2&#x27;)dense_3 = tf.keras.layers.Dense(units=DENSE_UNITS, name=&#x27;dense_3&#x27;)l2_norm_1 = L2NormLayer(name=&#x27;l2_norm_1&#x27;)dense_output = tf.keras.layers.Dense(NUM_CLASSES, activation=tf.nn.softmax, name=&#x27;dense_output&#x27;)#--- featuresfeatures_embeddings = features_embedding_layer(input_name)l2_norm_features = l2_norm_1(features_embeddings)avg_features = avg_embeddings(l2_norm_features)labels_liked_embeddings = labels_embedding_layer(inp_item_liked)l2_norm_liked = l2_norm_1(labels_liked_embeddings)avg_liked = avg_embeddings(l2_norm_liked)labels_disliked_embeddings = labels_embedding_layer(inp_item_disliked)l2_norm_disliked = l2_norm_1(labels_disliked_embeddings)avg_disliked = avg_embeddings(l2_norm_disliked)labels_aisle_embeddings = labels_embedding_layer(input_aisle)l2_norm_aisle = l2_norm_1(labels_aisle_embeddings)avg_aisle = avg_embeddings(l2_norm_aisle)labels_order_hour_embeddings = labels_embedding_layer(input_order_hour)l2_norm_order_hour = l2_norm_1(labels_order_hour_embeddings)avg_order_hour = avg_embeddings(l2_norm_order_hour)# ì„ë² ë”© ë²¡í„°ë“¤ ì—°ê²°concat_inputs = tf.keras.layers.Concatenate(axis=1)([avg_features, avg_liked, avg_disliked, avg_aisle, avg_order_hour ])# Dense Layersdense_1_features = dense_1(concat_inputs)dense_1_relu = tf.keras.layers.ReLU(name=&#x27;dense_1_relu&#x27;)(dense_1_features)dense_1_batch_norm = tf.keras.layers.BatchNormalization(name=&#x27;dense_1_batch_norm&#x27;)(dense_1_relu)dense_2_features = dense_2(dense_1_relu)dense_2_relu = tf.keras.layers.ReLU(name=&#x27;dense_2_relu&#x27;)(dense_2_features)# dense_2_batch_norm = tf.keras.layers.BatchNormalization(name=&#x27;dense_2_batch_norm&#x27;)(dense_2_relu)dense_3_features = dense_3(dense_2_relu)dense_3_relu = tf.keras.layers.ReLU(name=&#x27;dense_3_relu&#x27;)(dense_3_features)dense_3_batch_norm = tf.keras.layers.BatchNormalization(name=&#x27;dense_3_batch_norm&#x27;)(dense_3_relu)outputs = dense_output(dense_3_batch_norm)#Optimizeroptimiser = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)#--- prep modelmodel = tf.keras.models.Model( inputs=[input_name, inp_item_liked, inp_item_disliked, input_aisle, input_order_hour, ], outputs=[outputs])logdir = os.path.join(&quot;logs&quot;, datetime.datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;))tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)model.compile(optimizer=optimiser, loss=&#x27;sparse_categorical_crossentropy&#x27;, metrics=[&#x27;acc&#x27;])model.summary() Model: &quot;model_4&quot; 12# í•™ìŠµtrain_data_r_2.columns 1234567ranking_2 = model.fit([tf.keras.preprocessing.sequence.pad_sequences(train_data_r_2[&#x27;pd_name_d&#x27;])+ 1e-10, tf.keras.preprocessing.sequence.pad_sequences(train_data_r_2[&#x27;like&#x27;])+ 1e-10, tf.keras.preprocessing.sequence.pad_sequences(train_data_r_2[&#x27;dislike&#x27;])+ 1e-10, tf.keras.preprocessing.sequence.pad_sequences(train_data_r_2[&#x27;all_aisles&#x27;])+ 1e-10, tf.keras.preprocessing.sequence.pad_sequences(train_data_r_2[&#x27;order_hour_of_day&#x27;], dtype=float) + 1e-10, ],train_data_r_2[&#x27;predict_labels&#x27;].values, steps_per_epoch=1, epochs=300) 12345678# prediction pred_2 = model.predict([tf.keras.preprocessing.sequence.pad_sequences(test_data_r_2[&#x27;pd_name_d&#x27;])+ 1e-10, tf.keras.preprocessing.sequence.pad_sequences(test_data_r_2[&#x27;like&#x27;])+ 1e-10, tf.keras.preprocessing.sequence.pad_sequences(test_data_r_2[&#x27;dislike&#x27;])+ 1e-10, tf.keras.preprocessing.sequence.pad_sequences(test_data_r_2[&#x27;all_aisles&#x27;])+ 1e-10, tf.keras.preprocessing.sequence.pad_sequences(test_data_r_2[&#x27;order_hour_of_day&#x27;], dtype=float) + 1e-10 ])pred_2 ranking ê²°ê³¼ 12345678910111213141516# ranking###### ê° userë‹¹ top-7ê°œì˜ ì¶”ì²œ ë°ì´í„°ë¥¼ ë½‘ì•„ë‚¸ë‹¤.N = 7# k = np.sort((-pred_1).argsort()[:,:N]) # np.argsort(): probabilityì˜ ë§ˆì´ë„ˆìŠ¤ê°’ì„ ì‘ì€ ê°’ë¶€í„° ìˆœì„œëŒ€ë¡œ ë°ì´í„°ì˜ ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜. ==&gt; ì¦‰, ì–‘ì˜ê°’ìœ¼ë¡œëŠ” í° ê°’ë¶€í„° ë°˜í™˜í•œ ì…ˆ.# np.sort(): ì¸ë±ìŠ¤ ìˆœìœ¼ë¡œ ë‹¤ì‹œ ì •ë ¬. í™•ë¥ ì´ ë” ë†’ì€ ê²ƒë¶€í„° ë³´ê³ ì‹¶ìœ¼ë¯€ë¡œ ë ˆí¼ëŸ°ìŠ¤ì— ìˆëŠ” ì½”ë“œì´ì§€ë§Œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ. ranking_2 = (-pred_2).argsort()[:, :N]ranking_2[ranking_2&gt;new_data_2[&#x27;product_id&#x27;].max()]=0 print(ranking_2)ranking_2_probability = np.sort(pred_2[:, :N])print(ranking_2_probability)# ì¶”ì¶œëœ ì¸ë±ìŠ¤ ê°’ë“¤ì€ userë³„ êµ¬ë§¤ëª©ë¡ë¼ë²¨(ì—¬ëŸ¬ê°œ ìƒí’ˆë“¤ì´ ìˆëŠ”). ìœ ì‚¬ë„ ë†’ì€ ë‹¤ë¥¸ ìœ ì €ë“¤ì˜ êµ¬ë§¤ëª©ë¡ì´ ì¶”ì²œëœ ì…ˆ. ìˆœìœ„ëª¨ë¸ 1,2ì•ˆ í‰ê³¼ ê²°ê³¼(ranking-eval(nDCG score))12# 1ì•ˆì˜ ìœ ì €ë³„ ì¶”ì²œ &#x27;ìƒí’ˆë¬¶ìŒ&#x27; ë¼ë²¨ë“¤. ranking_1 123456# -- ë¼ë²¨ì˜ like_idê°’ ë¹„êµìš© # pd_name_d# train_data_r[[&#x27;predict_labels&#x27;,&#x27;pd_name_d&#x27;]]train_df1 = train_data_r[[&#x27;like&#x27;, &#x27;predict_labels&#x27;]]train_df1 = train_df1.rename(columns = &#123;&#x27;like&#x27;:&#x27;like_id&#x27;, &#x27;predict_labels&#x27;: &#x27;label&#x27;&#125;)train_df1 12345678910# predict_labels: ìœ ì €ë³„ ë¼ë²¨ ì—­í• . # pd_name_d : user Aê°€ êµ¬ë§¤í•œ ìƒí’ˆëª… ì¸ë±ìŠ¤ ìˆ˜ì¹˜ ë¬¶ìŒ.# ë¹„ìŠ·í•œ ìœ ì €ì˜ ìƒí’ˆ ëª©ë¡ì„ ì¶”ì²œí•˜ëŠ” ê²ƒìœ¼ë¡œ ìƒê°. true_df1 = test_data_r[[&#x27;user_id&#x27;, &#x27;like&#x27;, &#x27;predict_labels&#x27;]]# pred_data_1true_df1 = true_df1.rename(columns=&#123;&#x27;like&#x27;: &#x27;true_like_id&#x27; ,&#x27;predict_labels&#x27;: &#x27;true_labels&#x27;&#125;)true_df1 = true_df1[[&#x27;user_id&#x27;, &#x27;true_like_id&#x27;]]true_df1 rank_1ì•ˆ ìµœì¢… ndcg ê°’(í•¨ìˆ˜ ì‚¬ìš©)1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# user ë‹¹ ì¶”ì²œëª©ë¡ ë°ì´í„° ì¶”ì¶œí•˜ëŠ” ê¸°ëŠ¥ def user_rec(n , rank, rec_df, train_df): # ì „ë‹¬ì¸ì(argument) ì„¤ëª… : ## n: ëª‡ë²ˆì§¸ ìœ ì €ì¸ì§€(the order of test-user starting from 0) ## rank: ë­í‚¹ ê²°ê³¼ (ranking result of ranking model (array type)) ## rec_df: ìœ ì €ë‹¹ ì¶”ì²œëœ ë¼ë²¨ê³¼ ë¼ë²¨ì— í•´ë‹¹í•˜ëŠ” ìƒí’ˆ ì•„ì´ë”” ë°ì´í„°í”„ë ˆì„ (recommended label and product_id for each user) ## train_df: ë­í‚¹ ëª¨ë¸í•™ìŠµì— ì‚¬ìš©ëœ í•™ìŠµë°ì´í„°ì—ì„œ í•„ìš”í•œ like, predict_labelsë§Œ ì¶”ì¶œí•œ ë°ì´í„°í”„ë ˆì„(train_data&#x27;s &#x27;like and predict_labels&#x27;features used for ranking_model training) for label in rank[n-1]: # user2ë²ˆì§¸(array index=1) DF ë§Œë“¤ê¸°. rec_id = train_df[&#x27;like_id&#x27;][(train_df[&#x27;label&#x27;]==label)].tolist() rec_df = rec_df.append(pd.DataFrame([[label, rec_id]], columns=[&#x27;label&#x27;, &#x27;rec_id&#x27;]), ignore_index=True) # rec_id = train_data[&#x27;product_id&#x27;][(train_data[&#x27;label&#x27;]==label)].values return rec_df# ---user1# user1_rec_data = pd.DataFrame(columns=[&#x27;label&#x27;, &#x27;rec_id&#x27;, &#x27;T/F&#x27;])user1_rec_data = pd.DataFrame(columns=[&#x27;label&#x27;, &#x27;rec_id&#x27;])rec_df1 = user_rec(1, ranking_1, user1_rec_data, train_df1)print(&#x27;user1_rec_data&#x27;)print(rec_df1)# ---user2# user2_rec_data = pd.DataFrame(columns=[&#x27;label&#x27;, &#x27;rec_id&#x27;, &#x27;T/F&#x27;])user2_rec_data = pd.DataFrame(columns=[&#x27;label&#x27;, &#x27;rec_id&#x27;])rec_df2 = user_rec(2, ranking_1, user2_rec_data, train_df1)print(&#x27;user2_rec_data&#x27;)print(rec_df2)# ---user3user3_rec_data = pd.DataFrame(columns=[&#x27;label&#x27;, &#x27;rec_id&#x27;])rec_df3 = user_rec(3, ranking_1, user3_rec_data, train_df1)print(&#x27;user3_rec_data&#x27;)print(rec_df3)# ---user4user4_rec_data = pd.DataFrame(columns=[&#x27;label&#x27;, &#x27;rec_id&#x27;])rec_df4 = user_rec(4, ranking_1, user4_rec_data, train_df1)print(&#x27;user4_rec_data&#x27;)print(rec_df4)# ---user5user5_rec_data = pd.DataFrame(columns=[&#x27;label&#x27;, &#x27;rec_id&#x27;])rec_df5 = user_rec(5, ranking_1, user5_rec_data, train_df1)print(&#x27;user5_rec_data&#x27;)print(rec_df5) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# ê° ìœ ì €ë³„ ì¶”ì²œëª©ë¡ì˜ ìƒí’ˆë“¤ ì¤‘ true-likeí•œ ê²ƒì´ ìˆëŠ”ì§€ T/F(trud:1/false:0)ìœ¼ë¡œ ë°ì´í„°í”„ë ˆì„ì— ë„£ì–´ì£¼ëŠ” ê¸°ëŠ¥.def true_like(n, rec_df, true_df): # ì „ë‹¬ì¸ì(argument)ì„¤ëª…: ## n: ëª‡ë²ˆì§¸ ìœ ì €ì¸ì§€ (the order of test-user starting from 0) ## rec_df: ìœ ì €ë‹¹ ì¶”ì²œëœ ë¼ë²¨ê³¼ ë¼ë²¨ì— í•´ë‹¹í•˜ëŠ” ìƒí’ˆ ì•„ì´ë”” ë°ì´í„°í”„ë ˆì„ (recommended label and product_id for each user) ## true_df: ë­í‚¹ëª¨ë¸ì˜ ì˜ˆì¸¡ì— ì‚¬ìš©ëœ test_dataì¤‘ì—ì„œ í•„ìš”í•œ &#x27;user_id, true_like_id&#x27;íŠ¹ì„±ë§Œ ì¶”ì¶œí•œ ë°ì´í„°í”„ë ˆì„(test_data&#x27;s &#x27;user_id and true_like_id&#x27;features used for ranking_model predicting) tf_list = [] rec_id = rec_df[&#x27;rec_id&#x27;].tolist() # rec_id = rec_df[&#x27;rec_id&#x27;].values # trueë°ì´í„°ì¸ test_dataì˜ í˜•íƒœìƒ, rowì— ìˆëŠ” ìœ ì € n-1ë²ˆì§¸ rowë¥¼ ê³ ì •í•´ì•¼ í•¨. true = true_df[&#x27;true_like_id&#x27;].tolist()[n-1] for rec in rec_id[:][:]: if len(rec) == 0: tf_list.append(0.0) elif len(rec)&gt;=1: tf = true[0] in rec[0] if tf == True: tf_list.append(1) else: tf_list.append(0) rec_df[&#x27;T/F&#x27;] = pd.DataFrame(tf_list) print(&quot;true_like_id:&quot;, true) return rec_df# ---user1ì˜ ì¶”ì²œëª©ë¡ì— ëŒ€í•œ T/F(true-like or false) ê²°ê³¼ rec_df1 = true_like(1, rec_df1, true_df1)print(rec_df1)# ---user2ì˜ ì¶”ì²œëª©ë¡ì— ëŒ€í•œ T/F(true-like or false) ê²°ê³¼ rec_df2 = true_like(2, rec_df2, true_df1)print(rec_df2)# ---user3ì˜ ì¶”ì²œëª©ë¡ì— ëŒ€í•œ T/F(true-like or false) ê²°ê³¼ rec_df3 = true_like(3, rec_df3, true_df1)print(rec_df3)# ---user4ì˜ ì¶”ì²œëª©ë¡ì— ëŒ€í•œ T/F(true-like or false) ê²°ê³¼ rec_df4 = true_like(4, rec_df4, true_df1)print(rec_df4)# ---user5ì˜ ì¶”ì²œëª©ë¡ì— ëŒ€í•œ T/F(true-like or false) ê²°ê³¼ rec_df5 = true_like(5, rec_df5, true_df1)print(rec_df5) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# ê° ìœ ì €ë³„ dcg, idcg, ndcg ê³„ì‚°í•˜ëŠ” ê¸°ëŠ¥import numpy as npdef get_ndcg(rec_df): # rec_df: ìœ ì €ë‹¹ ì¶”ì²œëœ &#x27;ë¼ë²¨&#x27;ê³¼ &#x27;ë¼ë²¨ì— í•´ë‹¹í•˜ëŠ” ìƒí’ˆ ì•„ì´ë””&#x27;, &#x27;T/F(true_likeì—¬ë¶€)&#x27;íŠ¹ì„±ì´ í¬í•¨ëœ ë°ì´í„°í”„ë ˆì„ (dataframe about recommended &#x27;label&#x27; and &#x27;product_id&#x27;, &#x27;T/F(true_like or not)&#x27; for each user) rec = rec_df[&#x27;rec_id&#x27;].tolist() t = rec_df[&#x27;T/F&#x27;].tolist() dcg = 0.0 #dcg ê³„ì‚° for i, j in enumerate(t): # i, j: T/Fì•ˆì˜ ì¸ë±ìŠ¤ì™€ ê°’ë“¤(1.0, 0.0..)ì„ ëˆë‹¤. if j == 1.0: dcg += (1.0/np.log2(i+1+1)) else: dcg += 0 #idcg ê³„ì‚° idcg = sum((1.0/np.log2(i+1+1) for i in range(0, len(t)+1))) #ndcg ê³„ì‚° ndcg = dcg / idcg # set ìœ¼ë¡œ ê²°ê³¼ ì‚°ì¶œí•  ê²½ìš°, # return dcg, idcg, ndcg #dataframeìœ¼ë¡œ ê²°ê³¼ ì‚°ì¶œí•  ê²½ìš°, ndcg_df = pd.DataFrame(columns=[&#x27;dcg&#x27;, &#x27;idcg&#x27;, &#x27;ndcg&#x27;]) ndcg_df = ndcg_df.append(pd.DataFrame([[dcg, idcg, ndcg]], columns=[&#x27;dcg&#x27;, &#x27;idcg&#x27;, &#x27;ndcg&#x27;]), ignore_index=True) return ndcg_df# --- user1ì˜ dcg, idcg, ndcg ndcg_df1 = get_ndcg(rec_df1)print(ndcg_df1)# --- user2ì˜ dcg, idcg, ndcg ndcg_df2 = get_ndcg(rec_df2)print(ndcg_df2)# --- user3ì˜ dcg, idcg, ndcg ndcg_df3 = get_ndcg(rec_df3)print(ndcg_df3)# --- user4ì˜ dcg, idcg, ndcg ndcg_df4 = get_ndcg(rec_df4)print(ndcg_df4)# --- user5ì˜ dcg, idcg, ndcg ndcg_df5 = get_ndcg(rec_df5)print(ndcg_df5) 123456789101112131415# userë³„ dcg, idcg, ndcg ê²°ê³¼ë¥¼ í•©ì³ì£¼ëŠ” í•¨ìˆ˜ ê¸°ëŠ¥. import pandas as pddef concat_result(df1, df2): df3 = pd.concat([df1, df2]) return df3result = concat_result(ndcg_df1, ndcg_df2)result = concat_result(result, ndcg_df3)result = concat_result(result, ndcg_df4)result = concat_result(result, ndcg_df5)result.index=[&#x27;test-user1&#x27;, &#x27;test-user2&#x27;, &#x27;test-user3&#x27;, &#x27;test-user4&#x27;, &#x27;test-user5&#x27;]print(&#x27;[ ranking_1ì•ˆì˜ ìœ ì €ë³„ dcg, idcg, ndcg ] \\n &#x27;, result)print(&#x27;-------------------\\n&#x27;)print(&#x27;[ ranking_1ì•ˆì˜ dcg, idcg, ndcgí‰ê·  ] \\n&#x27;, result.mean()) rank_2ì•ˆ(order_number ê¸°ì¤€ like&#x2F;dislike ì— ë”°ë¥¸ ìˆœìœ„ ëª©ë¡)ì˜ nDCG12# 1ì•ˆì˜ ìœ ì €ë³„ ì¶”ì²œ &#x27;ìƒí’ˆë¬¶ìŒ&#x27; ë¼ë²¨ë“¤. ranking_2 1234# pd_name_d# train_data_r[[&#x27;predict_labels&#x27;,&#x27;pd_name_d&#x27;]]train_df2 = train_data_r_2[[&#x27;like&#x27;, &#x27;predict_labels&#x27;]].rename(columns=&#123;&#x27;like&#x27;: &#x27;like_id&#x27;, &#x27;predict_labels&#x27;:&#x27;label&#x27;&#125;)train_df2 123456# predict_labels: ìœ ì €ë³„ ë¼ë²¨ ì—­í• . # pd_name_d : user Aê°€ êµ¬ë§¤í•œ ìƒí’ˆëª… ì¸ë±ìŠ¤ ìˆ˜ì¹˜ ë¬¶ìŒ.# ë¹„ìŠ·í•œ ìœ ì €ì˜ ìƒí’ˆ ëª©ë¡ì„ ì¶”ì²œí•˜ëŠ” ê²ƒìœ¼ë¡œ ìƒê°. true_df2 = test_data_r_2[[&#x27;user_id&#x27;, &#x27;like&#x27;]].rename(columns=&#123;&#x27;like&#x27;: &#x27;true_like_id&#x27;&#125;)true_df2 rank_2ì•ˆ ìµœì¢… ndcg ê°’1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# user ë‹¹ ì¶”ì²œëª©ë¡ ë°ì´í„° ì¶”ì¶œí•˜ëŠ” ê¸°ëŠ¥ def user_rec(n , rank, rec_df, train_df): # ì „ë‹¬ì¸ì(argument) ì„¤ëª… : ## n: ëª‡ë²ˆì§¸ ìœ ì €ì¸ì§€(the order of test-user starting from 0) ## rank: ë­í‚¹ ê²°ê³¼ (ranking result of ranking model (array type)) ## rec_df: ìœ ì €ë‹¹ ì¶”ì²œëœ ë¼ë²¨ê³¼ ë¼ë²¨ì— í•´ë‹¹í•˜ëŠ” ìƒí’ˆ ì•„ì´ë”” ë°ì´í„°í”„ë ˆì„ (recommended label and product_id for each user) ## train_df: ë­í‚¹ ëª¨ë¸í•™ìŠµì— ì‚¬ìš©ëœ í•™ìŠµë°ì´í„°ì—ì„œ í•„ìš”í•œ like, predict_labelsë§Œ ì¶”ì¶œí•œ ë°ì´í„°í”„ë ˆì„(train_data&#x27;s &#x27;like and predict_labels&#x27;features used for ranking_model training) for label in rank[n-1]: # user2ë²ˆì§¸(array index=1) DF ë§Œë“¤ê¸°. rec_id = train_df[&#x27;like_id&#x27;][(train_df[&#x27;label&#x27;]==label)].tolist() rec_df = rec_df.append(pd.DataFrame([[label, rec_id]], columns=[&#x27;label&#x27;, &#x27;rec_id&#x27;]), ignore_index=True) # rec_id = train_data[&#x27;product_id&#x27;][(train_data[&#x27;label&#x27;]==label)].values return rec_df# ---user1# user1_rec_data = pd.DataFrame(columns=[&#x27;label&#x27;, &#x27;rec_id&#x27;, &#x27;T/F&#x27;])user1_rec_data = pd.DataFrame(columns=[&#x27;label&#x27;, &#x27;rec_id&#x27;])rec_df1 = user_rec(1, ranking_2, user1_rec_data, train_df2)print(&#x27;user1_rec_data&#x27;)print(rec_df1)# ---user2# user2_rec_data = pd.DataFrame(columns=[&#x27;label&#x27;, &#x27;rec_id&#x27;, &#x27;T/F&#x27;])user2_rec_data = pd.DataFrame(columns=[&#x27;label&#x27;, &#x27;rec_id&#x27;])rec_df2 = user_rec(2, ranking_2, user2_rec_data, train_df2)print(&#x27;user2_rec_data&#x27;)print(rec_df2)# ---user3user3_rec_data = pd.DataFrame(columns=[&#x27;label&#x27;, &#x27;rec_id&#x27;])rec_df3 = user_rec(3, ranking_2, user3_rec_data, train_df2)print(&#x27;user3_rec_data&#x27;)print(rec_df3)# ---user4user4_rec_data = pd.DataFrame(columns=[&#x27;label&#x27;, &#x27;rec_id&#x27;])rec_df4 = user_rec(4, ranking_2, user4_rec_data, train_df2)print(&#x27;user4_rec_data&#x27;)print(rec_df4)# ---user5user5_rec_data = pd.DataFrame(columns=[&#x27;label&#x27;, &#x27;rec_id&#x27;])rec_df5 = user_rec(5, ranking_2, user5_rec_data, train_df2)print(&#x27;user5_rec_data&#x27;)print(rec_df5) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# ê° ìœ ì €ë³„ ì¶”ì²œëª©ë¡ì˜ ìƒí’ˆë“¤ ì¤‘ true-likeí•œ ê²ƒì´ ìˆëŠ”ì§€ T/F(trud:1/false:0)ìœ¼ë¡œ ë°ì´í„°í”„ë ˆì„ì— ë„£ì–´ì£¼ëŠ” ê¸°ëŠ¥.def true_like(n, rec_df, true_df): # ì „ë‹¬ì¸ì(argument)ì„¤ëª…: ## n: ëª‡ë²ˆì§¸ ìœ ì €ì¸ì§€ (the order of test-user starting from 0) ## rec_df: ìœ ì €ë‹¹ ì¶”ì²œëœ ë¼ë²¨ê³¼ ë¼ë²¨ì— í•´ë‹¹í•˜ëŠ” ìƒí’ˆ ì•„ì´ë”” ë°ì´í„°í”„ë ˆì„ (recommended label and product_id for each user) ## true_df: ë­í‚¹ëª¨ë¸ì˜ ì˜ˆì¸¡ì— ì‚¬ìš©ëœ test_dataì¤‘ì—ì„œ í•„ìš”í•œ &#x27;user_id, true_like_id&#x27;íŠ¹ì„±ë§Œ ì¶”ì¶œí•œ ë°ì´í„°í”„ë ˆì„(test_data&#x27;s &#x27;user_id and true_like_id&#x27;features used for ranking_model predicting) tf_list = [] rec_id = rec_df[&#x27;rec_id&#x27;].tolist() # rec_id = rec_df[&#x27;rec_id&#x27;].values # trueë°ì´í„°ì¸ test_dataì˜ í˜•íƒœìƒ, rowì— ìˆëŠ” ìœ ì € n-1ë²ˆì§¸ rowë¥¼ ê³ ì •í•´ì•¼ í•¨. true = true_df[&#x27;true_like_id&#x27;].tolist()[n-1] for rec in rec_id[:][:]: if len(rec) == 0: tf_list.append(0.0) elif len(rec)&gt;=1: tf = true[0] in rec[0] if tf == True: tf_list.append(1) else: tf_list.append(0) rec_df[&#x27;T/F&#x27;] = pd.DataFrame(tf_list) print(&quot;true_like_id:&quot;, true) return rec_df# ---user1ì˜ ì¶”ì²œëª©ë¡ì— ëŒ€í•œ T/F(true-like or false) ê²°ê³¼ rec_df1 = true_like(1, rec_df1, true_df2)print(rec_df1)# ---user2ì˜ ì¶”ì²œëª©ë¡ì— ëŒ€í•œ T/F(true-like or false) ê²°ê³¼ rec_df2 = true_like(2, rec_df2, true_df2)print(rec_df2)# ---user3ì˜ ì¶”ì²œëª©ë¡ì— ëŒ€í•œ T/F(true-like or false) ê²°ê³¼ rec_df3 = true_like(3, rec_df3, true_df2)print(rec_df3)# ---user4ì˜ ì¶”ì²œëª©ë¡ì— ëŒ€í•œ T/F(true-like or false) ê²°ê³¼ rec_df4 = true_like(4, rec_df4, true_df2)print(rec_df4)# ---user5ì˜ ì¶”ì²œëª©ë¡ì— ëŒ€í•œ T/F(true-like or false) ê²°ê³¼ rec_df5 = true_like(5, rec_df5, true_df2)print(rec_df5) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# ê° ìœ ì €ë³„ dcg, idcg, ndcg ê³„ì‚°í•˜ëŠ” ê¸°ëŠ¥import numpy as npdef get_ndcg(rec_df): # rec_df: ìœ ì €ë‹¹ ì¶”ì²œëœ &#x27;ë¼ë²¨&#x27;ê³¼ &#x27;ë¼ë²¨ì— í•´ë‹¹í•˜ëŠ” ìƒí’ˆ ì•„ì´ë””&#x27;, &#x27;T/F(true_likeì—¬ë¶€)&#x27;íŠ¹ì„±ì´ í¬í•¨ëœ ë°ì´í„°í”„ë ˆì„ (dataframe about recommended &#x27;label&#x27; and &#x27;product_id&#x27;, &#x27;T/F(true_like or not)&#x27; for each user) rec = rec_df[&#x27;rec_id&#x27;].tolist() t = rec_df[&#x27;T/F&#x27;].tolist() dcg = 0.0 #dcg ê³„ì‚° for i, j in enumerate(t): # i, j: T/Fì•ˆì˜ ì¸ë±ìŠ¤ì™€ ê°’ë“¤(1.0, 0.0..)ì„ ëˆë‹¤. if j == 1.0: dcg += (1.0/np.log2(i+1+1)) else: dcg += 0 #idcg ê³„ì‚° idcg = sum((1.0/np.log2(i+1+1) for i in range(0, len(t)+1))) #ndcg ê³„ì‚° ndcg = dcg / idcg # set ìœ¼ë¡œ ê²°ê³¼ ì‚°ì¶œí•  ê²½ìš°, # return dcg, idcg, ndcg #dataframeìœ¼ë¡œ ê²°ê³¼ ì‚°ì¶œí•  ê²½ìš°, ndcg_df = pd.DataFrame(columns=[&#x27;dcg&#x27;, &#x27;idcg&#x27;, &#x27;ndcg&#x27;]) ndcg_df = ndcg_df.append(pd.DataFrame([[dcg, idcg, ndcg]], columns=[&#x27;dcg&#x27;, &#x27;idcg&#x27;, &#x27;ndcg&#x27;]), ignore_index=True) return ndcg_df# --- user1ì˜ dcg, idcg, ndcg ndcg_df1 = get_ndcg(rec_df1)print(ndcg_df1)# --- user2ì˜ dcg, idcg, ndcg ndcg_df2 = get_ndcg(rec_df2)print(ndcg_df2)# --- user3ì˜ dcg, idcg, ndcg ndcg_df3 = get_ndcg(rec_df3)print(ndcg_df3)# --- user4ì˜ dcg, idcg, ndcg ndcg_df4 = get_ndcg(rec_df4)print(ndcg_df4)# --- user5ì˜ dcg, idcg, ndcg ndcg_df5 = get_ndcg(rec_df5)print(ndcg_df5) 12345678910111213141516# userë³„ dcg, idcg, ndcg ê²°ê³¼ë¥¼ í•©ì³ì£¼ëŠ” í•¨ìˆ˜ ê¸°ëŠ¥. import pandas as pddef concat_result(df1, df2): df3 = pd.concat([df1, df2]) return df3result = concat_result(ndcg_df1, ndcg_df2)result = concat_result(result, ndcg_df3)result = concat_result(result, ndcg_df4)result = concat_result(result, ndcg_df5)result.index=[&#x27;test-user1&#x27;, &#x27;test-user2&#x27;, &#x27;test-user3&#x27;, &#x27;test-user4&#x27;, &#x27;test-user5&#x27;]print(&#x27;[ ranking_2ì•ˆì˜ ìœ ì €ë³„ dcg, idcg, ndcg ] \\n &#x27;, result)print(&#x27;-------------------\\n&#x27;)print(&#x27;[ ranking_2ì•ˆì˜ dcg, idcg, ndcgí‰ê·  ] \\n&#x27;, result.mean()) ê²°ë¡  ë° í–¥í›„ ë³´ì™„í•  ì‚¬í•­(1) ranking_2ì•ˆì˜ ì „ì²´ í‰ê·  ê²°ê³¼ëŠ” nDCG ìŠ¤ì½”ì–´ì—ì„œ 0.1 ë†’ìŒ. ranking_1ì•ˆ ndcg í‰ê· : 0.28 ranking_2ì•ˆ ndcg í‰ê· : 0.38 (2) ìœ ì €ë³„ë¡œ nDCGìŠ¤ì½”ì–´ê°€ ranking_1ì•ˆì´ ë†’ì€ ê²½ìš°ê°€ ìˆê³ , ranking_2ì•ˆì´ ë†’ì€ ê²½ìš°ê°€ ìˆìœ¼ë¯€ë¡œ, ê°œì¸ë³„ ë§ì¶¤ ì œì•ˆì„ í•˜ë©´ ì¢‹ì„ ê²ƒ, íŠ¹íˆ 0ì´ ë‚˜ì˜¤ëŠ” ê²½ìš°ë„ ìˆì–´ì„œ ì´ ê²½ìš°ëŠ” ì‹ ê²½ ì¨ì„œ ì œì•ˆì„ í•´ì•¼ í•¨. user2: 0.37 (ranking1_ndcg) &lt; 0.57 (ranking2_ndcg) user1: 0.52 (ranking1_ndcg) &lt; 0.00 (ranking2_ndcg) 12345678910111213141516171819202122232425262728[ ranking_1ì•ˆì˜ ìœ ì €ë³„ dcg, idcg, ndcg ] dcg idcg ndcgtest-user1 2.061606 3.953465 0.521468test-user2 1.487137 3.953465 0.376160test-user3 1.487137 3.953465 0.376160test-user4 0.500000 3.953465 0.126471test-user5 0.000000 3.953465 0.000000-------------------[ ranking_1ì•ˆì˜ dcg, idcg, ndcgí‰ê·  ] dcg 1.107176idcg 3.953465ndcg 0.280052[ ranking_2ì•ˆì˜ ìœ ì €ë³„ dcg, idcg, ndcg ] dcg idcg ndcgtest-user1 0.000000 3.953465 0.000000test-user2 2.264010 3.953465 0.572665test-user3 1.500000 3.953465 0.379414test-user4 1.500000 3.953465 0.379414test-user5 2.351116 3.953465 0.594698-------------------[ ranking_2ì•ˆì˜ dcg, idcg, ndcgí‰ê·  ] dcg 1.523025idcg 3.953465ndcg 0.385238 (3) ranking 1,2 ì•ˆì˜ ì°¨ì´ì : (ranking_1ì•ˆ) reorder(ì¬ì£¼ë¬¸ ì—¬ë¶€)ë¡œ ratingì„ ë§¤ê¸´ ê°œë…ìœ¼ë¡œ, ì¬ì£¼ë¬¸ì„ í•œ ê²½ìš°(1)ë¼ë©´ like, ì¬ì£¼ë¬¸ì„ ì•ˆ í•œ ê²½ìš°(0)ë¼ë©´ dislikeë¡œ ìƒí’ˆì„ êµ¬ë¶„í•œ featureë¥¼ ë­í‚¹ëª¨ë¸ì— ì¶”ê°€í•˜ì—¬ ì¶”ì²œëª©ë¡ì„ ìƒì„±í•œ ë°©ë²•. (ranking_2ì•ˆ) order_number(ì£¼ë¬¸íšŒì°¨: ëª‡ë²ˆì§¸ êµ¬ë§¤ì¸ì§€)ë¡œ ratingì„ ë§¤ê¸´ ê°œë…ìœ¼ë¡œ, í‰ê·  ì£¼ë¬¸íšŒì°¨ì¸ 18 ì´ìƒì¸ ê²½ìš°(1) like, 18 ë¯¸ë§Œ(0)ì´ë©´ dislikeë¡œ ìƒí’ˆì„ êµ¬ë¶„í•œ featureë¥¼ ë­í‚¹ëª¨ë¸ì— ì¶”ê°€í•˜ì—¬ ì¶”ì²œëª©ë¡ì„ ìƒì„±í•œ ë°©ë²•. Reference https://jalynne-kim.medium.com/ì¶”ì²œëª¨ë¸-ì´ì»¤ë¨¸ìŠ¤-ì¶”ì²œëª¨ë¸ë§-ë”¥ëŸ¬ë‹ëª¨ë¸-í”„ë¡œì íŠ¸-íšŒê³ -d5017cb1335f","categories":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}],"tags":[{"name":"Recommendation System","slug":"Recommendation-System","permalink":"https://jmj3047.github.io/tags/Recommendation-System/"},{"name":"Deep Nueral Networks","slug":"Deep-Nueral-Networks","permalink":"https://jmj3047.github.io/tags/Deep-Nueral-Networks/"},{"name":"Vertex AI","slug":"Vertex-AI","permalink":"https://jmj3047.github.io/tags/Vertex-AI/"}],"keywords":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}]},{"title":"Deep Nueral Networks for YouTube Recommendations","slug":"DNN_Youtube_Rec","date":"2023-03-30T15:00:00.000Z","updated":"2023-04-05T08:46:31.915Z","comments":true,"path":"2023/03/31/DNN_Youtube_Rec/","link":"","permalink":"https://jmj3047.github.io/2023/03/31/DNN_Youtube_Rec/","excerpt":"","text":"Journal&#x2F;Conference: GoogleYear(published year): 2016Author: Paul Covington, Jay Adams, Emre SarginSubject: Recommendation, Youtube Deep Nueral Networks for YouTube Recommendations GPT ìš”ì•½ ë”¥ëŸ¬ë‹ì´ ì–´ë–»ê²Œ ìœ íŠœë¸Œ ì¶”ì²œì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ì–´? ë”¥ëŸ¬ë‹ì€ ìœ íŠœë¸Œ ì¶”ì²œ ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° í° ì—­í• ì„ í•©ë‹ˆë‹¤. ì´ì „ì—ëŠ” ì‘ì€ ë¬¸ì œì—ì„œ ì˜ ì‘ë™í•˜ëŠ” ë§ì€ ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ì´ ìœ íŠœë¸Œì™€ ê°™ì€ ëŒ€ê·œëª¨ ë¬¸ì œì—ì„œ ì‹¤íŒ¨í–ˆì—ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë”¥ëŸ¬ë‹ì„ ì‚¬ìš©í•˜ë©´ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ ê·¹ë³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ë…¼ë¬¸ì—ì„œëŠ” ë”¥ í›„ë³´ ìƒì„± ëª¨ë¸ê³¼ ë³„ë„ì˜ ë”¥ ë­í‚¹ ëª¨ë¸ì„ ì„¤ëª…í•˜ë©°, ì´ë“¤ì´ ì–´ë–»ê²Œ ìœ íŠœë¸Œ ì¶”ì²œ ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ëŠ”ì§€ ìì„¸íˆ ë‹¤ë£¨ê³  ìˆìŠµë‹ˆë‹¤. ì´ì „ì—ëŠ” ìœ íŠœë¸Œ ì¶”ì²œ ì‹œìŠ¤í…œì—ì„œ ì‚¬ìš©ë˜ëŠ” ì „í†µì ì¸ ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ì€ ì‚¬ìš©ìì˜ ì´ì „ ê¸°ë¡ê³¼ ê´€ë ¨ëœ ë¹„ìŠ·í•œ ë™ì˜ìƒì„ ì°¾ì•„ì„œ ì¶”ì²œí•˜ëŠ” ë°©ì‹ì´ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŸ¬í•œ ë°©ì‹ì€ ëŒ€ê·œëª¨ ë°ì´í„°ì—ì„œëŠ” ì œëŒ€ë¡œ ì‘ë™í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ë”¥ëŸ¬ë‹ì„ ì‚¬ìš©í•˜ì—¬ ìœ íŠœë¸Œ ì¶”ì²œ ì‹œìŠ¤í…œì„ ê°œì„ í•˜ì˜€ìŠµë‹ˆë‹¤. ë…¼ë¬¸ì—ì„œëŠ” ë”¥ í›„ë³´ ìƒì„± ëª¨ë¸ê³¼ ë³„ë„ì˜ ë”¥ ë­í‚¹ ëª¨ë¸ì„ ì†Œê°œí•©ë‹ˆë‹¤. ë”¥ í›„ë³´ ìƒì„± ëª¨ë¸ì€ ë§ì€ í›„ë³´ ë™ì˜ìƒ ì¤‘ì—ì„œ ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ ë™ì˜ìƒì„ ì„ íƒí•˜ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ë‹¤ì–‘í•œ ì‹ í˜¸ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµë©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ë™ì˜ìƒ ì œëª©, ì„¤ëª…, íƒœê·¸ ë° ì‹œì²­ ê¸°ë¡ ë“±ì´ ìˆìŠµë‹ˆë‹¤. ë”¥ ë­í‚¹ ëª¨ë¸ì€ í›„ë³´ ë™ì˜ìƒ ì¤‘ì—ì„œ ê°€ì¥ ê´€ì‹¬ ìˆëŠ” ë™ì˜ìƒì„ ì„ íƒí•˜ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ì˜ˆìƒ ì‹œì²­ ì‹œê°„ì„ ì˜ˆì¸¡í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ì „ì—ëŠ” í´ë¦­ í™•ë¥ ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ì—ˆì§€ë§Œ, ì´ ë…¼ë¬¸ì—ì„œëŠ” ì˜ˆìƒ ì‹œì²­ ì‹œê°„ì„ ì˜ˆì¸¡í•˜ì—¬ ë³´ë‹¤ ì •í™•í•œ ì¶”ì²œì„ ì œê³µí•©ë‹ˆë‹¤. ë”¥ëŸ¬ë‹ì„ ì‚¬ìš©í•˜ë©´ ì´ì „ì—ëŠ” í•´ê²°í•  ìˆ˜ ì—†ì—ˆë˜ ëŒ€ê·œëª¨ ë°ì´í„°ì—ì„œì˜ ì¶”ì²œ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ë”¥ëŸ¬ë‹ì€ ìœ íŠœë¸Œ ì¶”ì²œ ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. feature engineeringì„ ì–´ë–»ê²Œ í–ˆì–´? ì €í¬ëŠ” ë³´í†µ ë­í‚¹ ëª¨ë¸ì—ì„œ ìˆ˜ë°± ê°œì˜ íŠ¹ì§•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ íŠ¹ì§•ì€ ë²”ì£¼í˜•ê³¼ ì—°ì†í˜•ìœ¼ë¡œ ê±°ì˜ ë°˜ë°˜ì”© êµ¬ì„±ë©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë”¥ëŸ¬ë‹ì€ íŠ¹ì§• ê³µí•™ì„ ëŒ€ì²´í•  ìˆ˜ ìˆëŠ” ì ì¬ë ¥ì´ ìˆì§€ë§Œ, ì €í¬ê°€ ë‹¤ë£¨ëŠ” ë°ì´í„°ì˜ ë³¸ì§ˆ ë•Œë¬¸ì— ë°”ë¡œ ë”¥ëŸ¬ë‹ ëª¨ë¸ì— ì…ë ¥í•˜ê¸°ì—ëŠ” ì í•©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì €í¬ëŠ” ì—¬ì „íˆ ë§ì€ ì‹œê°„ì„ íŠ¹ì§• ê³µí•™ì— í• ì• í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ìš°ë¦¬ëŠ” ê° ë™ì˜ìƒê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ì–‘í•œ íŠ¹ì§•ì„ ì¶”ì¶œí•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ë™ì˜ìƒ ì œëª©, ì„¤ëª…, íƒœê·¸ ë° ì‹œì²­ ê¸°ë¡ ë“±ì´ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ íŠ¹ì§•ë“¤ì€ í›„ë³´ ìƒì„± ë° ë­í‚¹ ëª¨ë¸ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤. ë”°ë¼ì„œ ìš°ë¦¬ê°€ ì„ íƒí•˜ëŠ” íŠ¹ì§•ë“¤ì€ ì¶”ì²œ ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì— í° ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤. ë”¥ ë­í‚¹ ëª¨ë¸ê³¼ ë”¥ í›„ë³´ ëª¨ë¸ì˜ ì°¨ì´ì ì´ ë­ì•¼? ë”¥ í›„ë³´ ìƒì„± ëª¨ë¸ê³¼ ë”¥ ë­í‚¹ ëª¨ë¸ì€ ìœ íŠœë¸Œ ì¶”ì²œ ì‹œìŠ¤í…œì—ì„œ ì„œë¡œ ë‹¤ë¥¸ ì—­í• ì„ í•©ë‹ˆë‹¤. ë”¥ í›„ë³´ ìƒì„± ëª¨ë¸ì€ ë§ì€ í›„ë³´ ë™ì˜ìƒ ì¤‘ì—ì„œ ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ ë™ì˜ìƒì„ ì„ íƒí•˜ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ë‹¤ì–‘í•œ ì‹ í˜¸ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµë©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ë™ì˜ìƒ ì œëª©, ì„¤ëª…, íƒœê·¸ ë° ì‹œì²­ ê¸°ë¡ ë“±ì´ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì‹ í˜¸ë¥¼ ì‚¬ìš©í•˜ì—¬ í›„ë³´ ë™ì˜ìƒì„ ìƒì„±í•˜ê³ , ì´ë“¤ ì¤‘ì—ì„œ ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ ë™ì˜ìƒì„ ì„ íƒí•©ë‹ˆë‹¤. ë°˜ë©´ì— ë”¥ ë­í‚¹ ëª¨ë¸ì€ í›„ë³´ ë™ì˜ìƒ ì¤‘ì—ì„œ ê°€ì¥ ê´€ì‹¬ ìˆëŠ” ë™ì˜ìƒì„ ì„ íƒí•˜ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ì˜ˆìƒ ì‹œì²­ ì‹œê°„ì„ ì˜ˆì¸¡í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ì „ì—ëŠ” í´ë¦­ í™•ë¥ ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ì—ˆì§€ë§Œ, ì´ ë…¼ë¬¸ì—ì„œëŠ” ì˜ˆìƒ ì‹œì²­ ì‹œê°„ì„ ì˜ˆì¸¡í•˜ì—¬ ë³´ë‹¤ ì •í™•í•œ ì¶”ì²œì„ ì œê³µí•©ë‹ˆë‹¤. ë”°ë¼ì„œ ë”¥ í›„ë³´ ìƒì„± ëª¨ë¸ê³¼ ë”¥ ë­í‚¹ ëª¨ë¸ì€ ì„œë¡œ ë‹¤ë¥¸ ëª©ì ê³¼ ê¸°ëŠ¥ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ ì´ìš©í•´ì„œ ì–´ë–»ê²Œ ì‚¬ìš©ìì—ê²Œ ì¶”ì²œí•˜ëŠ”ê±°ì•¼? ìœ íŠœë¸Œ ì¶”ì²œ ì‹œìŠ¤í…œì€ í¬ê²Œ ë‘ ë‹¨ê³„ë¡œ ë‚˜ë‰©ë‹ˆë‹¤. ì²« ë²ˆì§¸ëŠ” ë”¥ í›„ë³´ ìƒì„± ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìˆ˜ë°±ë§Œ ê°œì˜ ë™ì˜ìƒ ì¤‘ì—ì„œ ìˆ˜ë°± ê°œì˜ í›„ë³´ ë™ì˜ìƒì„ ì„ íƒí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ë‘ ë²ˆì§¸ëŠ” ë”¥ ë­í‚¹ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì´ëŸ¬í•œ í›„ë³´ ë™ì˜ìƒ ì¤‘ì—ì„œ ê°€ì¥ ê´€ì‹¬ ìˆëŠ” ë™ì˜ìƒì„ ì„ íƒí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ë”°ë¼ì„œ ìœ ì €ì—ê²Œ ì¶”ì²œí•˜ê¸° ìœ„í•´ì„œëŠ”, ë¨¼ì € ìœ ì €ì˜ ì‹œì²­ ê¸°ë¡, ê²€ìƒ‰ ê¸°ë¡, ì¢‹ì•„ìš” ë° ì‹«ì–´ìš” ë“±ê³¼ ê°™ì€ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ì´ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë”¥ í›„ë³´ ìƒì„± ëª¨ë¸ì„ ì‹¤í–‰í•˜ì—¬ ìˆ˜ë°± ê°œì˜ í›„ë³´ ë™ì˜ìƒì„ ìƒì„±í•©ë‹ˆë‹¤. ê·¸ ë‹¤ìŒ, ë”¥ ë­í‚¹ ëª¨ë¸ì„ ì‹¤í–‰í•˜ì—¬ ì´ëŸ¬í•œ í›„ë³´ ë™ì˜ìƒ ì¤‘ì—ì„œ ê°€ì¥ ê´€ì‹¬ ìˆëŠ” ë™ì˜ìƒì„ ì„ íƒí•©ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ì˜ˆìƒ ì‹œì²­ ì‹œê°„ì„ ì˜ˆì¸¡í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ì˜ˆì¸¡ëœ ì‹œì²­ ì‹œê°„ì´ ê¸´ ë™ì˜ìƒì¼ìˆ˜ë¡, ìœ ì €ê°€ í•´ë‹¹ ë™ì˜ìƒì— ëŒ€í•´ ê´€ì‹¬ì´ ìˆì„ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ìœ íŠœë¸Œ ì¶”ì²œ ì‹œìŠ¤í…œì€ ë”¥ í›„ë³´ ìƒì„± ëª¨ë¸ê³¼ ë”¥ ë­í‚¹ ëª¨ë¸ì„ ê²°í•©í•˜ì—¬ ìœ ì €ì—ê²Œ ë§ì¶¤í˜• ì¶”ì²œì„ ì œê³µí•©ë‹ˆë‹¤. ì „í†µì ì¸ ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ë³´ë‹¤ ë” ì„±ëŠ¥ì´ ë‚˜ì€ì§€ ì–´ë–»ê²Œ ì•Œì•„? ì´ ë…¼ë¬¸ì—ì„œëŠ” ë”¥ í›„ë³´ ìƒì„± ëª¨ë¸ê³¼ ë”¥ ë­í‚¹ ëª¨ë¸ì„ ê¸°ì¡´ì˜ ì „í†µì ì¸ ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ê³¼ ë¹„êµí•˜ì—¬ ì„±ëŠ¥ì„ í‰ê°€í•˜ì˜€ìŠµë‹ˆë‹¤. ê¸°ì¡´ì˜ ì „í†µì ì¸ ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ì€ ì£¼ë¡œ í˜‘ì—… í•„í„°ë§, ì½˜í…ì¸  ê¸°ë°˜ í•„í„°ë§ ë“±ì´ ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì•Œê³ ë¦¬ì¦˜ë“¤ì€ ìœ ì €ì™€ ì•„ì´í…œ ê°„ì˜ ìƒí˜¸ì‘ìš© ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¶”ì²œì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ë…¼ë¬¸ì—ì„œëŠ” A&#x2F;B í…ŒìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ë”¥ í›„ë³´ ìƒì„± ëª¨ë¸ê³¼ ë”¥ ë­í‚¹ ëª¨ë¸ì„ ê¸°ì¡´ì˜ ì „í†µì ì¸ ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ê³¼ ë¹„êµí•˜ì˜€ìŠµë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´, ì¼ë¶€ ìœ ì €ì—ê²ŒëŠ” ê¸°ì¡´ì˜ ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ë™ì˜ìƒì„ ì¶”ì²œí•˜ê³ , ë‹¤ë¥¸ ì¼ë¶€ ìœ ì €ì—ê²ŒëŠ” ë”¥ í›„ë³´ ìƒì„± ëª¨ë¸ê³¼ ë”¥ ë­í‚¹ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë™ì˜ìƒì„ ì¶”ì²œí•˜ì˜€ìŠµë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ, ë”¥ í›„ë³´ ìƒì„± ëª¨ë¸ê³¼ ë”¥ ë­í‚¹ ëª¨ë¸ì´ ê¸°ì¡´ì˜ ì „í†µì ì¸ ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ë³´ë‹¤ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” ë”¥ëŸ¬ë‹ì´ ë³µì¡í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë° ìˆì–´ì„œ ë§¤ìš° ìœ ìš©í•˜ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì£¼ë©°, ìœ íŠœë¸Œ ì¶”ì²œ ì‹œìŠ¤í…œì—ì„œ ë”¥ëŸ¬ë‹ì´ ì–´ë–»ê²Œ í™œìš©ë  ìˆ˜ ìˆëŠ”ì§€ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ì˜ ëª¨ë¸ êµ¬ì¡°ì— ëŒ€í•´ì„œ input layer, hidden layer, outputì„ í¬í•¨í•´ì„œ ìì„¸í•˜ê²Œ ì•Œë ¤ì¤˜ ì´ ë…¼ë¬¸ì—ì„œëŠ” ë”¥ í›„ë³´ ìƒì„± ëª¨ë¸ê³¼ ë”¥ ë­í‚¹ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìœ íŠœë¸Œ ì¶”ì²œ ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. ê° ëª¨ë¸ì˜ êµ¬ì¡°ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. ë”¥ í›„ë³´ ìƒì„± ëª¨ë¸ Input layer: ìœ ì €ì˜ ì‹œì²­ ê¸°ë¡, ê²€ìƒ‰ ê¸°ë¡, ì¢‹ì•„ìš” ë° ì‹«ì–´ìš” ë“±ê³¼ ê°™ì€ ì •ë³´ë¥¼ ì…ë ¥ ë°›ìŠµë‹ˆë‹¤. Hidden layers: ì´ëŸ¬í•œ ì…ë ¥ ì •ë³´ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì—¬ëŸ¬ ê°œì˜ ì€ë‹‰ì¸µì´ ìˆìŠµë‹ˆë‹¤. ì´ ì€ë‹‰ì¸µë“¤ì€ ê°ê° ë‹¤ì–‘í•œ ì‹ í˜¸ë¥¼ ì‚¬ìš©í•˜ì—¬ í›„ë³´ ë™ì˜ìƒì„ ìƒì„±í•©ë‹ˆë‹¤. Output layer: í›„ë³´ ë™ì˜ìƒ ì¤‘ì—ì„œ ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ ë™ì˜ìƒì„ ì„ íƒí•©ë‹ˆë‹¤. ë”¥ ë­í‚¹ ëª¨ë¸ Input layer: í›„ë³´ ë™ì˜ìƒì˜ ì •ë³´ë¥¼ ì…ë ¥ ë°›ìŠµë‹ˆë‹¤. Hidden layers: ì´ëŸ¬í•œ ì…ë ¥ ì •ë³´ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì—¬ëŸ¬ ê°œì˜ ì€ë‹‰ì¸µì´ ìˆìŠµë‹ˆë‹¤. ì´ ì€ë‹‰ì¸µë“¤ì€ ì˜ˆìƒ ì‹œì²­ ì‹œê°„ì„ ì˜ˆì¸¡í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. Output layer: ì˜ˆìƒ ì‹œì²­ ì‹œê°„ì´ ê°€ì¥ ê¸´ ë™ì˜ìƒì„ ì„ íƒí•©ë‹ˆë‹¤. ë‘ ëª¨ë¸ ëª¨ë‘ ë‹¤ì–‘í•œ ì‹ í˜¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ë³µì¡í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì—¬ëŸ¬ ê°œì˜ ì€ë‹‰ì¸µì„ ê°€ì§€ê³  ìˆìœ¼ë©°, ë”¥ëŸ¬ë‹ì˜ íŠ¹ì§•ì¸ ì—­ì „íŒŒ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµë©ë‹ˆë‹¤. ì´ëŸ¬í•œ ëª¨ë¸ êµ¬ì¡°ëŠ” ìœ íŠœë¸Œ ì¶”ì²œ ì‹œìŠ¤í…œì—ì„œ ë§¤ìš° íš¨ê³¼ì ì´ë©°, ëŒ€ê·œëª¨ ë°ì´í„°ì—ì„œ ë³µì¡í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë° ìˆì–´ì„œ ë”¥ëŸ¬ë‹ì´ ë§¤ìš° ìœ ìš©í•˜ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì „ì²´ ë…¼ë¬¸ ë‚´ìš© ê°œìš”ë‘ ì œëª© í¬í•¨í•´ì„œ ìš”ì•½í•´ì¤˜ ì´ ë…¼ë¬¸ì˜ ì œëª©ì€ â€œDeep Neural Networks for YouTube Recommendationsâ€ì…ë‹ˆë‹¤. ì´ ë…¼ë¬¸ì—ì„œëŠ” ìœ íŠœë¸Œ ì¶”ì²œ ì‹œìŠ¤í…œì— ëŒ€í•œ ë”¥ëŸ¬ë‹ ê¸°ë°˜ì˜ ì ‘ê·¼ ë°©ì‹ì„ ì œì•ˆí•˜ê³ , ì´ë¥¼ í†µí•´ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. ìœ íŠœë¸ŒëŠ” ì„¸ê³„ì—ì„œ ê°€ì¥ í° ë¹„ë””ì˜¤ ê³µìœ  í”Œë«í¼ ì¤‘ í•˜ë‚˜ì´ë©°, ì¶”ì²œ ì‹œìŠ¤í…œì€ ìˆ˜ì–µ ëª…ì˜ ì‚¬ìš©ìê°€ ë§¤ì¼ ì‚¬ìš©í•˜ëŠ” í•µì‹¬ ê¸°ëŠ¥ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. ì´ ë…¼ë¬¸ì—ì„œëŠ” ìœ íŠœë¸Œ ì¶”ì²œ ì‹œìŠ¤í…œì´ ì–´ë–»ê²Œ êµ¬ì„±ë˜ì–´ ìˆëŠ”ì§€ì— ëŒ€í•œ ê°œìš”ë¥¼ ì œê³µí•˜ë©°, ë”¥ í›„ë³´ ìƒì„± ëª¨ë¸ê³¼ ë”¥ ë­í‚¹ ëª¨ë¸ì´ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ì— ëŒ€í•´ ìì„¸íˆ ì„¤ëª…í•©ë‹ˆë‹¤. ë”¥ í›„ë³´ ìƒì„± ëª¨ë¸ì€ ìœ ì €ì˜ ì‹œì²­ ê¸°ë¡, ê²€ìƒ‰ ê¸°ë¡, ì¢‹ì•„ìš” ë° ì‹«ì–´ìš” ë“±ê³¼ ê°™ì€ ì •ë³´ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ì„œ ìˆ˜ë°±ë§Œ ê°œì˜ ë™ì˜ìƒ ì¤‘ì—ì„œ ìˆ˜ë°± ê°œì˜ í›„ë³´ ë™ì˜ìƒì„ ìƒì„±í•©ë‹ˆë‹¤. ë”¥ ë­í‚¹ ëª¨ë¸ì€ ì´ëŸ¬í•œ í›„ë³´ ë™ì˜ìƒ ì¤‘ì—ì„œ ê°€ì¥ ê´€ì‹¬ ìˆëŠ” ë™ì˜ìƒì„ ì„ íƒí•˜ëŠ”ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ì˜ˆìƒ ì‹œì²­ ì‹œê°„ì„ ì˜ˆì¸¡í•˜ëŠ” ë° ì‚¬ìš©ë˜ë©°, ì˜ˆì¸¡ëœ ì‹œì²­ ì‹œê°„ì´ ê¸´ ë™ì˜ìƒì¼ìˆ˜ë¡ ìœ ì €ê°€ í•´ë‹¹ ë™ì˜ìƒì— ëŒ€í•´ ê´€ì‹¬ì´ ìˆì„ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤. ë…¼ë¬¸ì—ì„œëŠ” A&#x2F;B í…ŒìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ë”¥ í›„ë³´ ìƒì„± ëª¨ë¸ê³¼ ë”¥ ë­í‚¹ ëª¨ë¸ì´ ê¸°ì¡´ì˜ ì „í†µì ì¸ ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ë³´ë‹¤ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤ëŠ” ê²ƒì„ ì‹¤í—˜ì ìœ¼ë¡œ ì…ì¦í•˜ì˜€ìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” ë”¥ëŸ¬ë‹ì´ ë³µì¡í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë° ìˆì–´ì„œ ë§¤ìš° ìœ ìš©í•˜ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì£¼ë©°, ìœ íŠœë¸Œ ì¶”ì²œ ì‹œìŠ¤í…œì—ì„œ ë”¥ëŸ¬ë‹ì´ ì–´ë–»ê²Œ í™œìš©ë  ìˆ˜ ìˆëŠ”ì§€ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. 4.ranking ë¶€ë¶„ì„ featureê³¼ modelingë¶€ë¶„ì„ í¬í•¨í•´ì„œ ìš”ì•½í•´ì¤˜ Ranking is a crucial step in the YouTube recommendation system, which uses impression data to specialize and calibrate candidate predictions for the particular user interface. During ranking, many more features describing the video and the userâ€™s relationship to the video are accessed, because only a few hundred videos are being scored rather than the millions scored in candidate generation. The ranking model is separate from the candidate generation model and uses deep neural networks to predict expected watch time for each candidate video. The model has several hidden layers that process input information and use backpropagation algorithm to learn from large-scale data. The output layer selects the video with the highest predicted watch time as the recommended video. ìˆœìœ„ëŠ” ë…¸ì¶œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ì— ëŒ€í•œ í›„ë³´ ì˜ˆì¸¡ì„ ì „ë¬¸í™”í•˜ê³  ë³´ì •í•˜ëŠ” YouTube ì¶”ì²œ ì‹œìŠ¤í…œì—ì„œ ì¤‘ìš”í•œ ë‹¨ê³„ì…ë‹ˆë‹¤. ìˆœìœ„ë¥¼ ë§¤ê¸°ëŠ” ë™ì•ˆì—ëŠ” í›„ë³´ ìƒì„±ì—ì„œ ì ìˆ˜ë¥¼ ë§¤ê¸°ëŠ” ìˆ˜ë°±ë§Œ ê°œì˜ ë™ì˜ìƒì´ ì•„ë‹Œ ìˆ˜ë°± ê°œì˜ ë™ì˜ìƒë§Œ ì ìˆ˜ë¥¼ ë§¤ê¸°ê¸° ë•Œë¬¸ì— ë™ì˜ìƒê³¼ ì‚¬ìš©ìì™€ ë™ì˜ìƒê³¼ì˜ ê´€ê³„ë¥¼ ì„¤ëª…í•˜ëŠ” ë” ë§ì€ ê¸°ëŠ¥ì— ì•¡ì„¸ìŠ¤í•©ë‹ˆë‹¤. ìˆœìœ„ ëª¨ë¸ì€ í›„ë³´ ìƒì„± ëª¨ë¸ê³¼ëŠ” ë³„ê°œì˜ ëª¨ë¸ì´ë©° ì‹¬ì¸µ ì‹ ê²½ë§ì„ ì‚¬ìš©í•˜ì—¬ ê° í›„ë³´ ë™ì˜ìƒì˜ ì˜ˆìƒ ì‹œì²­ ì‹œê°„ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤. ì´ ëª¨ë¸ì—ëŠ” ì…ë ¥ ì •ë³´ë¥¼ ì²˜ë¦¬í•˜ê³  ëŒ€ê·œëª¨ ë°ì´í„°ì—ì„œ í•™ìŠµí•˜ê¸° ìœ„í•´ ì—­ì „íŒŒ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ëŠ” ì—¬ëŸ¬ ê°œì˜ ìˆ¨ê²¨ì§„ ë ˆì´ì–´ê°€ ìˆìŠµë‹ˆë‹¤. ì¶œë ¥ ë ˆì´ì–´ëŠ” ì˜ˆìƒ ì‹œì²­ ì‹œê°„ì´ ê°€ì¥ ë†’ì€ ë™ì˜ìƒì„ ì¶”ì²œ ë™ì˜ìƒìœ¼ë¡œ ì„ íƒí•©ë‹ˆë‹¤. ê·¸ë˜ì„œ ê²°ë¡ ì´ ë­ì•¼? The paper proposes a deep neural network architecture for recommending YouTube videos, which is split into two distinct problems: candidate generation and ranking. The deep candidate generation model generates hundreds of relevant videos for the user based on their history and context, while the deep ranking model selects the most relevant video from these candidates using impression data and many more features. The paper shows that the proposed approach outperforms traditional recommendation algorithms in A&#x2F;B testing. ì´ ë…¼ë¬¸ì—ì„œëŠ” í›„ë³´ ìƒì„± ë° ìˆœìœ„ ì§€ì •ì´ë¼ëŠ” ë‘ ê°€ì§€ ë¬¸ì œë¡œ ë‚˜ë‰˜ì–´ì ¸ ìˆëŠ” YouTube ë™ì˜ìƒ ì¶”ì²œì„ ìœ„í•œ ì‹¬ì¸µ ì‹ ê²½ë§ ì•„í‚¤í…ì²˜ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ë”¥ í›„ë³´ ìƒì„± ëª¨ë¸ì€ ì‚¬ìš©ìì˜ ê¸°ë¡ê³¼ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìˆ˜ë°± ê°œì˜ ê´€ë ¨ ë™ì˜ìƒì„ ìƒì„±í•˜ê³ , ë”¥ ë­í‚¹ ëª¨ë¸ì€ ë…¸ì¶œ ë°ì´í„°ì™€ ë” ë§ì€ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì—¬ ì´ëŸ¬í•œ í›„ë³´ ì¤‘ì—ì„œ ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ ë™ì˜ìƒì„ ì„ íƒí•©ë‹ˆë‹¤. ì´ ë…¼ë¬¸ì€ ì œì•ˆëœ ì ‘ê·¼ ë°©ì‹ì´ A&#x2F;B í…ŒìŠ¤íŠ¸ì—ì„œ ê¸°ì¡´ ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. Summary ë”¥ëŸ¬ë‹ì„ ì‚¬ìš©í•˜ì—¬ ì„±ëŠ¥ í–¥ìƒì„ í•œ Youtubeì˜ ì¶”ì²œ ì‹œìŠ¤í…œì„ ì†Œê°œí•œë‹¤. deep candidate generation modelê³¼ deep ranking modelì„ ì„¤ëª… YouTube represents one of the largest scale and most sophisticated industrial recommendation systems in existence. In this paper, we describe the system at a high level and focus on the dramatic performance improvements brought by deep learning. The paper is split according to the classic two-stage information retrieval dichotomy: first, we detail a deep candidate generation model and then describe a separate deep ranking model. We also provide practical lessons and insights derived from designing, iterating and maintaining a massive recommendation system with enormous userfacing impact. YouTubeëŠ” í˜„ì¡´í•˜ëŠ” ê°€ì¥ í° ê·œëª¨ì™€ ì •êµí•œ ì‚°ì—… ì¶”ì²œ ì‹œìŠ¤í…œ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. ì´ paperì—ì„œëŠ” ì´ ì‹œìŠ¤í…œì„ ë†’ì€ ìˆ˜ì¤€ì—ì„œ ì„¤ëª…í•˜ê³  ë”¥ëŸ¬ë‹ì´ ê°€ì ¸ì˜¨ ê·¹ì ì¸ ì„±ëŠ¥ í–¥ìƒì— ì´ˆì ì„ ë§ì¶¥ë‹ˆë‹¤. ì´ paperëŠ” ê¸°ì¡´ì˜ 2ë‹¨ê³„ ì •ë³´ ê²€ìƒ‰ ì´ë¶„ë²•ì— ë”°ë¼ ë¨¼ì € deep candidate generation modelì„ ìì„¸íˆ ì„¤ëª…í•œ ë‹¤ìŒ ë³„ë„ì˜ deep ranking modelì„ ì„¤ëª…í•©ë‹ˆë‹¤. ë˜í•œ ì‚¬ìš©ìì—ê²Œ ë§‰ëŒ€í•œ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ëŒ€ê·œëª¨ ì¶”ì²œ ì‹œìŠ¤í…œì„ ì„¤ê³„, ë°˜ë³µ ë° ìœ ì§€ ê´€ë¦¬í•˜ë©´ì„œ ì–»ì€ ì‹¤ì§ˆì ì¸ êµí›ˆê³¼ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤. INTRODUCTIONYouTube is the worldâ€™s largest platform for creating, sharing and discovering video content. YouTube recommendations are responsible for helping more than a billion users discover personalized content from an ever-growing corpus of videos. In this paper we will focus on the immense impact deep learning has recently had on the YouTube video recommendations system. Figure 1 illustrates the recommendations on the YouTube mobile app home. Recommending YouTube videos is extremely challenging from three major perspectives: YouTubeëŠ” ë™ì˜ìƒ ì½˜í…ì¸ ë¥¼ ì œì‘, ê³µìœ  ë° ê²€ìƒ‰í•  ìˆ˜ ìˆëŠ” ì„¸ê³„ ìµœëŒ€ì˜ í”Œë«í¼ì…ë‹ˆë‹¤. YouTube ì¶”ì²œì€ 10ì–µ ëª… ì´ìƒì˜ ì‚¬ìš©ìê°€ ê³„ì† ì¦ê°€í•˜ëŠ” ë™ì˜ìƒ ëª¨ìŒì—ì„œ ê°œì¸í™”ëœ ì½˜í…ì¸ ë¥¼ ë°œê²¬í•  ìˆ˜ ìˆë„ë¡ ì§€ì›í•©ë‹ˆë‹¤. ì´ paperì—ì„œëŠ” ìµœê·¼ ë”¥ëŸ¬ë‹ì´ YouTube ë™ì˜ìƒ ì¶”ì²œ ì‹œìŠ¤í…œì— ë¯¸ì¹œ ë§‰ëŒ€í•œ ì˜í–¥ì— ì´ˆì ì„ ë§ì¶œ ê²ƒì…ë‹ˆë‹¤. ê·¸ë¦¼ 1ì€ YouTube ëª¨ë°”ì¼ ì•± í™ˆì˜ ì¶”ì²œ ì‹œìŠ¤í…œì„ ë³´ì—¬ì¤ë‹ˆë‹¤. YouTube ë™ì˜ìƒ ì¶”ì²œì€ í¬ê²Œ ì„¸ ê°€ì§€ ê´€ì ì—ì„œ ë§¤ìš° ê¹Œë‹¤ë¡œìš´ ì‘ì—…ì…ë‹ˆë‹¤: Scale: Many existing recommendation algorithms proven to work well on small problems fail to operate on our scale. Highly specialized distributed learning algorithms and efficient serving systems are essential for handling YouTubeâ€™s massive user base and corpus. ê·œëª¨: ì‘ì€ ë¬¸ì œì—ì„œëŠ” ì˜ ì‘ë™í•˜ëŠ” ê²ƒìœ¼ë¡œ ì…ì¦ëœ ê¸°ì¡´ì˜ ë§ì€ ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ì€ YouTubeì˜ ê·œëª¨ì—ì„œëŠ” ì‘ë™í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. YouTubeì˜ ë°©ëŒ€í•œ ì‚¬ìš©ì ê¸°ë°˜ê³¼ ë§ë­‰ì¹˜ë¥¼ ì²˜ë¦¬í•˜ë ¤ë©´ ê³ ë„ë¡œ ì „ë¬¸í™”ëœ ë¶„ì‚° í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ê³¼ íš¨ìœ¨ì ì¸ ì„œë¹™ ì‹œìŠ¤í…œì´ í•„ìˆ˜ì ì…ë‹ˆë‹¤. Freshness: YouTube has a very dynamic corpus with many hours of video are uploaded per second. The recommendation system should be responsive enough to model newly uploaded content as well as the latest actions taken by the user. Balancing new content with well established videos can be understood from an exploration&#x2F;exploitation perspective.(ê°•í™”í•™ìŠµ) ì‹ ì„ ë„: YouTubeëŠ” ì´ˆë‹¹ ìˆ˜ ì‹œê°„ ë¶„ëŸ‰ì˜ ë™ì˜ìƒì´ ì—…ë¡œë“œë˜ëŠ” ë§¤ìš° ì—­ë™ì ì¸ ì½”í¼ìŠ¤ë¥¼ ë³´ìœ í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì¶”ì²œ ì‹œìŠ¤í…œì€ ìƒˆë¡œ ì—…ë¡œë“œëœ ì½˜í…ì¸ ì™€ ì‚¬ìš©ìê°€ ìµœê·¼ì— ìˆ˜í–‰í•œ ì‘ì—…ì„ ëª¨ë¸ë§í•  ìˆ˜ ìˆì„ ë§Œí¼ ë°˜ì‘ì„±ì´ ë›°ì–´ë‚˜ì•¼ í•©ë‹ˆë‹¤. ìƒˆë¡œìš´ ì½˜í…ì¸ ì™€ ì˜ ì•Œë ¤ì§„ ë™ì˜ìƒì˜ ê· í˜•ì„ ë§ì¶”ëŠ” ê²ƒì€ íƒìƒ‰&#x2F;í™œìš© ê´€ì ì—ì„œ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Noise: Historical user behavior on YouTube is inherently difficult to predict due to sparsity and a variety of unobservable external factors. We rarely obtain the ground truth of user satisfaction and instead model noisy implicit feedback signals. Furthermore, metadata associated with content is poorly structured without a well de ned ontology. Our algorithms need to be robust to these particular characteristics of our training data. ë…¸ì´ì¦ˆ: YouTubeì˜ ê³¼ê±° ì‚¬ìš©ì í–‰ë™ì€ í¬ì†Œì„±ê³¼ ê´€ì°°í•  ìˆ˜ ì—†ëŠ” ë‹¤ì–‘í•œ ì™¸ë¶€ ìš”ì¸ìœ¼ë¡œ ì¸í•´ ë³¸ì§ˆì ìœ¼ë¡œ ì˜ˆì¸¡í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤. ì‚¬ìš©ì ë§Œì¡±ë„ì— ëŒ€í•œ ì‹¤ì¸¡ ë°ì´í„°ë¥¼ ê±°ì˜ í™•ë³´í•˜ì§€ ëª»í•˜ê³  ëŒ€ì‹  ë…¸ì´ì¦ˆê°€ ë§ì€ ì•”ì‹œì  í”¼ë“œë°± ì‹ í˜¸ë¥¼ ëª¨ë¸ë§í•©ë‹ˆë‹¤. ë˜í•œ ì½˜í…ì¸ ì™€ ê´€ë ¨ëœ ë©”íƒ€ë°ì´í„°ëŠ” ì˜ ì •ì˜ëœ ì˜¨í†¨ë¡œì§€ ì—†ì´ëŠ” ì œëŒ€ë¡œ êµ¬ì¡°í™”ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. í•™ìŠµ ë°ì´í„°ì˜ ì´ëŸ¬í•œ íŠ¹ì • íŠ¹ì„±ì— ë§ê²Œ ì•Œê³ ë¦¬ì¦˜ì„ ê°•ë ¥í•˜ê²Œ ì„¤ê³„í•´ì•¼ í•©ë‹ˆë‹¤. In conjugation with other product areas across Google, YouTube has undergone a fundamental paradigm shift towards using deep learning as a general-purpose solution for nearly all learning problems. Our system is built on Google Brain [4] which was recently open sourced as TensorFlow [1]. TensorFlow provides a flexible framework for experimenting with various deep neural network architectures using large scale distributed training. Our models learn approximately one billion parameters and are trained on hundreds of billions of examples. YouTubeëŠ” Googleì˜ ë‹¤ë¥¸ ì œí’ˆ ì˜ì—­ê³¼ í•¨ê»˜ ê±°ì˜ ëª¨ë“  í•™ìŠµ ë¬¸ì œì— ëŒ€í•œ ë²”ìš© ì†”ë£¨ì…˜ìœ¼ë¡œ ë”¥ ëŸ¬ë‹ì„ ì‚¬ìš©í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ê·¼ë³¸ì ì¸ íŒ¨ëŸ¬ë‹¤ì„ì˜ ë³€í™”ë¥¼ ê²ªì—ˆìŠµë‹ˆë‹¤. YouTubeì˜ ì‹œìŠ¤í…œì€ ìµœê·¼ TensorFlow[1]ë¡œ ì˜¤í”ˆ ì†ŒìŠ¤í™”ëœ Google Brain[4]ì„ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì¶•ë˜ì—ˆìŠµë‹ˆë‹¤. í…ì„œí”Œë¡œëŠ” ëŒ€ê·œëª¨ ë¶„ì‚° í•™ìŠµì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘í•œ ì‹¬ì¸µ ì‹ ê²½ë§ ì•„í‚¤í…ì²˜ë¥¼ ì‹¤í—˜í•  ìˆ˜ ìˆëŠ” ìœ ì—°í•œ í”„ë ˆì„ì›Œí¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ìœ ë‹ˆí‹°ì˜ ëª¨ë¸ì€ ì•½ 10ì–µ ê°œì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ í•™ìŠµí•˜ê³  ìˆ˜ì²œì–µ ê°œì˜ ì˜ˆì œë¥¼ í†µí•´ í›ˆë ¨ë©ë‹ˆë‹¤. In contrast to vast amount of research in matrix factorization methods [19], there is relatively little work using deep neural networks for recommendation systems. Neural networks are used for recommending news in [17], citations in [8] and review ratings in [20]. Collaborative filtering is formulated as a deep neural network in [22] and autoencoders in [18]. Elkahky et al. used deep learning for cross domain user modeling [5]. In a content-based setting, Burges et al. used deep neural networks for music recommendation [21]. í–‰ë ¬ ì¸ìˆ˜ë¶„í•´ ë°©ë²•[19]ì— ëŒ€í•œ ë°©ëŒ€í•œ ì–‘ì˜ ì—°êµ¬ì™€ ë‹¬ë¦¬, ì¶”ì²œ ì‹œìŠ¤í…œì— ì‹¬ì¸µ ì‹ ê²½ë§ì„ ì‚¬ìš©í•œ ì—°êµ¬ëŠ” ìƒëŒ€ì ìœ¼ë¡œ ì ìŠµë‹ˆë‹¤. ì‹ ê²½ë§ì€ [17]ì—ì„œ ë‰´ìŠ¤ ì¶”ì²œ, [8]ì—ì„œ ì¸ìš©, [20]ì—ì„œ ë¦¬ë·° í‰ê°€ì— ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤. í˜‘ì—… í•„í„°ë§ì€ [22]ì—ì„œ ì‹¬ì¸µ ì‹ ê²½ë§ìœ¼ë¡œ, [18]ì—ì„œ ìë™ ì¸ì½”ë”ë¡œ ê³µì‹í™”ë˜ì—ˆìŠµë‹ˆë‹¤. Elkahky ë“±ì€ í¬ë¡œìŠ¤ ë„ë©”ì¸ ì‚¬ìš©ì ëª¨ë¸ë§ì— ë”¥ëŸ¬ë‹ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤[5]. ì½˜í…ì¸  ê¸°ë°˜ í™˜ê²½ì—ì„œ Burges ë“±ì€ ìŒì•… ì¶”ì²œì— ì‹¬ì¸µ ì‹ ê²½ë§ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤[21]. The paper is organized as follows: A brief system overview is presented in Section 2. Section 3 describes the candidate generation model in more detail, including how it is trained and used to serve recommendations. Experimental results will show how the model benefits from deep layers of hidden units and additional heterogeneous signals. Section 4 details the ranking model, including how classic logistic regression is modified to train a model predicting expected watch time (rather than click probability). Experimental results will show that hidden layer depth is helpful as well in this situation. Finally, Section 5 presents our conclusions and lessons learned. paperëŠ” ë‹¤ìŒê³¼ ê°™ì´ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤: ì„¹ì…˜ 2ì—ì„œëŠ” ê°„ëµí•œ ì‹œìŠ¤í…œ ê°œìš”ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. ì„¹ì…˜ 3ì—ì„œëŠ” í›„ë³´ ìƒì„± ëª¨ë¸ì´ ì–´ë–»ê²Œ í•™ìŠµë˜ê³  ì¶”ì²œì„ ì œê³µí•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ”ì§€ ë“± í›„ë³´ ìƒì„± ëª¨ë¸ì— ëŒ€í•´ ìì„¸íˆ ì„¤ëª…í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼ë¥¼ í†µí•´ ëª¨ë¸ì´ ì‹¬ì¸µ ê³„ì¸µì˜ ìˆ¨ê²¨ì§„ ìœ ë‹›ê³¼ ì¶”ê°€ì ì¸ ì´ì§ˆì ì¸ ì‹ í˜¸ë¥¼ í†µí•´ ì–´ë–¤ ì´ì ì„ ì–»ì„ ìˆ˜ ìˆëŠ”ì§€ ë³´ì—¬ì¤ë‹ˆë‹¤. ì„¹ì…˜ 4ì—ì„œëŠ” í´ë¦­ í™•ë¥ ì´ ì•„ë‹Œ ì˜ˆìƒ ì‹œì²­ ì‹œê°„ì„ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì„ í›ˆë ¨í•˜ê¸° ìœ„í•´ ê³ ì „ì ì¸ ë¡œì§€ìŠ¤í‹± íšŒê·€ë¥¼ ìˆ˜ì •í•˜ëŠ” ë°©ë²•ì„ í¬í•¨í•˜ì—¬ ìˆœìœ„ ëª¨ë¸ì„ ìì„¸íˆ ì„¤ëª…í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼ë¥¼ í†µí•´ ìˆ¨ê²¨ì§„ ë ˆì´ì–´ ê¹Šì´ê°€ ì´ëŸ¬í•œ ìƒí™©ì—ì„œë„ ìœ ìš©í•˜ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ ì„¹ì…˜ 5ì—ì„œëŠ” ê²°ë¡ ê³¼ êµí›ˆì„ ì œì‹œí•©ë‹ˆë‹¤. SYSTEM OVERVIEWThe overall structure of our recommendation system is illustrated in Figure 2. The system is comprised of two neural networks: one for candidate generation and one for ranking. The candidate generation network takes events from the userâ€™s YouTube activity history as input and retrieves a small subset (hundreds) of videos from a large corpus. These candidates are intended to be generally relevant to the user with high precision. The candidate generation network only provides broad personalization via collaborative filtering. The similarity between users is expressed in terms of coarse features such as IDs of video watches, search query tokensand demographics. ì¶”ì²œ ì‹œìŠ¤í…œì˜ ì „ì²´ êµ¬ì¡°ëŠ” ê·¸ë¦¼ 2ì— ë‚˜ì™€ ìˆìŠµë‹ˆë‹¤. ì´ ì‹œìŠ¤í…œì€ í›„ë³´ ìƒì„±ìš© ë„¤íŠ¸ì›Œí¬ì™€ ë­í‚¹ìš© ë„¤íŠ¸ì›Œí¬ì˜ ë‘ ê°€ì§€ ì‹ ê²½ë§ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. í›„ë³´ ìƒì„± ë„¤íŠ¸ì›Œí¬ëŠ” ì‚¬ìš©ìì˜ YouTube í™œë™ ê¸°ë¡ì—ì„œ ì´ë²¤íŠ¸ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ëŒ€ê·œëª¨ ë§ë­‰ì¹˜ì—ì„œ ì‘ì€ í•˜ìœ„ ì§‘í•©(ìˆ˜ë°± ê°œ)ì˜ ë™ì˜ìƒì„ ê²€ìƒ‰í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ í›„ë³´ ë™ì˜ìƒì€ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ìì™€ ê´€ë ¨ì„±ì´ ë†’ê³  ì •ë°€ë„ê°€ ë†’ì•„ì•¼ í•©ë‹ˆë‹¤. í›„ë³´ ìƒì„± ë„¤íŠ¸ì›Œí¬ë§Œ í˜‘ì—… í•„í„°ë§ì„ í†µí•´ ê´‘ë²”ìœ„í•œ ê°œì¸í™”ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì‚¬ìš©ì ê°„ì˜ ìœ ì‚¬ì„±ì€ ë™ì˜ìƒ ì‹œì²­ì˜ ID, ê²€ìƒ‰ ì¿¼ë¦¬ í† í° ë° ì¸êµ¬ í†µê³„ì™€ ê°™ì€ ê±°ì¹œ ê¸°ëŠ¥ìœ¼ë¡œ í‘œí˜„ë©ë‹ˆë‹¤. Presenting a few â€œbestâ€ recommendations in a list requires a fine-level representation to distinguish relative importance among candidates with high recall. The ranking network accomplishes this task by assigning a score to each video according to a desired objective function using a rich set of features describing the video and user. The highest scoring videos are presented to the user, ranked by their score. ëª©ë¡ì— ëª‡ ê°€ì§€ â€˜ìµœê³ ì˜â€™ ì¶”ì²œì„ ì œì‹œí•˜ë ¤ë©´ recallë¥ ì´ ë†’ì€ í›„ë³´ë“¤ ê°„ì˜ ìƒëŒ€ì  ì¤‘ìš”ì„±ì„ êµ¬ë¶„í•  ìˆ˜ ìˆëŠ” ì„¸ë°€í•œ ìˆ˜ì¤€ì˜ í‘œí˜„ì´ í•„ìš”í•©ë‹ˆë‹¤. ë­í‚¹ ë„¤íŠ¸ì›Œí¬ëŠ” ë™ì˜ìƒê³¼ ì‚¬ìš©ìë¥¼ ì„¤ëª…í•˜ëŠ” í’ë¶€í•œ ê¸°ëŠ¥ ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì›í•˜ëŠ” ëª©ì  í•¨ìˆ˜ì— ë”°ë¼ ê° ë™ì˜ìƒì— ì ìˆ˜ë¥¼ í• ë‹¹í•¨ìœ¼ë¡œì¨ ì´ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ê°€ì¥ ë†’ì€ ì ìˆ˜ë¥¼ ë°›ì€ ë™ì˜ìƒì´ ì ìˆ˜ì— ë”°ë¼ ìˆœìœ„ì— ë”°ë¼ ì‚¬ìš©ìì—ê²Œ í‘œì‹œë©ë‹ˆë‹¤. The two-stage approach to recommendation allows us to make recommendations from a very large corpus (millions) of videos while still being certain that the small number of videos appearing on the device are personalized and engaging for the user. Furthermore, this design enables blending candidates generated by other sources, such as those described in an earlier work [3]. ì¶”ì²œì— ëŒ€í•œ 2ë‹¨ê³„ ì ‘ê·¼ ë°©ì‹ì„ í†µí•´ ìˆ˜ë°±ë§Œ ê°œì— ë‹¬í•˜ëŠ” ëŒ€ê·œëª¨ ë™ì˜ìƒ ì½”í¼ìŠ¤ì—ì„œ ì¶”ì²œì„ ìƒì„±í•˜ëŠ” ë™ì‹œì— ë””ë°”ì´ìŠ¤ì— í‘œì‹œë˜ëŠ” ì†Œìˆ˜ì˜ ë™ì˜ìƒì´ ì‚¬ìš©ìì—ê²Œ ë§ì¶¤í™”ë˜ê³  í¥ë¯¸ë¥¼ ëŒ ìˆ˜ ìˆë„ë¡ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ ì´ ì„¤ê³„ëŠ” ì´ì „ ì—°êµ¬[3]ì—ì„œ ì„¤ëª…í•œ ê²ƒê³¼ ê°™ì´ ë‹¤ë¥¸ ì†ŒìŠ¤ì—ì„œ ìƒì„±ëœ í›„ë³´ë¥¼ ë¸”ë Œë”©í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤. During development, we make extensive use of o ine metrics (precision, recall, ranking loss, etc.) to guide iterative improvements to our system. However for the final determination of the effectiveness of an algorithm or model, we rely on A&#x2F;B testing via live experiments. In a live experiment, we can measure subtle changes in click-through rate, watch time, and many other metrics that measure user engagement. This is important because live A&#x2F;B results arenot always correlated with offline experiments. ê°œë°œ ê³¼ì •ì—ì„œ ì •í™•ë„, ë¦¬ì½œ, ìˆœìœ„ ì†ì‹¤ ë“± ë‹¤ì–‘í•œ ì§€í‘œë¥¼ ê´‘ë²”ìœ„í•˜ê²Œ í™œìš©í•˜ì—¬ ì‹œìŠ¤í…œì˜ ë°˜ë³µì ì¸ ê°œì„ ì„ ìœ ë„í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì•Œê³ ë¦¬ì¦˜ì´ë‚˜ ëª¨ë¸ì˜ íš¨ê³¼ë¥¼ ìµœì¢…ì ìœ¼ë¡œ ê²°ì •í•˜ê¸° ìœ„í•´ì„œëŠ” ë¼ì´ë¸Œ ì‹¤í—˜ì„ í†µí•œ A&#x2F;B í…ŒìŠ¤íŠ¸ì— ì˜ì¡´í•©ë‹ˆë‹¤. ì‹¤ì‹œê°„ ì‹¤í—˜ì—ì„œëŠ” í´ë¦­ë¥ , ì‹œì²­ ì‹œê°„ ë° ì‚¬ìš©ì ì°¸ì—¬ë¥¼ ì¸¡ì •í•˜ëŠ” ê¸°íƒ€ ì—¬ëŸ¬ ì§€í‘œì˜ ë¯¸ë¬˜í•œ ë³€í™”ë¥¼ ì¸¡ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‹¤ì‹œê°„ A&#x2F;B ê²°ê³¼ê°€ ì˜¤í”„ë¼ì¸ ì‹¤í—˜ê³¼ í•­ìƒ ìƒê´€ê´€ê³„ê°€ ìˆëŠ” ê²ƒì€ ì•„ë‹ˆê¸° ë•Œë¬¸ì— ì´ ì ì´ ì¤‘ìš”í•©ë‹ˆë‹¤. CANDIDATE GENERATIONDuring candidate generation, the enormous YouTube corpus is winnowed down to hundreds of videos that may be relevant to the user. The predecessor to the recommender described here was a matrix factorization approach trained under rank loss [23]. Early iterations of our neural network model mimicked this factorization behavior with shallow networks that only embedded the userâ€™s previous watches. From this perspective, our approach can be viewed as a nonlineargeneralization of factorization techniques. í›„ë³´ë¥¼ ìƒì„±í•˜ëŠ” ë™ì•ˆ ë°©ëŒ€í•œ YouTube ë§ë­‰ì¹˜ê°€ ì‚¬ìš©ìì™€ ê´€ë ¨ì´ ìˆì„ ìˆ˜ ìˆëŠ” ìˆ˜ë°± ê°œì˜ ë™ì˜ìƒìœ¼ë¡œ ì¢í˜€ì§‘ë‹ˆë‹¤. ì—¬ê¸°ì— ì„¤ëª…ëœ ì¶”ì²œì˜ ì „ì‹ ì€ ìˆœìœ„ ì†ì‹¤ í•˜ì—ì„œ í›ˆë ¨ëœ í–‰ë ¬ ì¸ìˆ˜ë¶„í•´ ì ‘ê·¼ ë°©ì‹ì´ì—ˆìŠµë‹ˆë‹¤[23]. ì‹ ê²½ë§ ëª¨ë¸ì˜ ì´ˆê¸° ë°˜ë³µì€ ì‚¬ìš©ìì˜ ì´ì „ ì‹œì²­ë§Œ í¬í•¨í•˜ëŠ” ì–•ì€ ë„¤íŠ¸ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ëŸ¬í•œ ì¸ìˆ˜ ë¶„í•´ ë™ì‘ì„ ëª¨ë°©í–ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê´€ì ì—ì„œ ë³¼ ë•Œ, ìš°ë¦¬ì˜ ì ‘ê·¼ ë°©ì‹ì€ ì¸ìˆ˜ë¶„í•´ ê¸°ë²•ì„ ë¹„ì„ í˜•ì ìœ¼ë¡œ ì¼ë°˜í™”ë¼ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Recommendation as ClassificationWe pose recommendation as extreme multiclass classification where the prediction problem becomes accurately classifying a specific video watch wt at time t among millions of videos i (classes) from a corpus V based on a user U and context C, ì¶”ì²œì„ ê·¹ë‹¨ì ì¸ ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ë¡œ ì„¤ì •í•˜ë©´, ì˜ˆì¸¡ ë¬¸ì œëŠ” ì‚¬ìš©ì Uì™€ ì»¨í…ìŠ¤íŠ¸ Cë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë§ë­‰ì¹˜ Vì˜ ìˆ˜ë°±ë§Œ ê°œì˜ ë™ì˜ìƒ i(í´ë˜ìŠ¤) ì¤‘ì—ì„œ íŠ¹ì • ë™ì˜ìƒ ì‹œì²­ íšŸìˆ˜ të¥¼ ì •í™•í•˜ê²Œ ë¶„ë¥˜í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤, where u 2 Rn represents a high-dimensional \\embeddingâ€ of the user, context pair and the vj 2 RN represent embeddings of each candidate video. In this setting, an embedding is simply a mapping of sparse entities (individual videos, users etc.) into a dense vector in RN. The task of the deep neural network is to learn user embeddings u as a function of the userâ€™s history and context that are useful for discriminating among videos with a softmax classifier. ì—¬ê¸°ì„œ u 2 Rnì€ ì‚¬ìš©ì, ì»¨í…ìŠ¤íŠ¸ pair ë° vj 2 RNì˜ ê³ ì°¨ì› â€˜ì„ë² ë”©â€™ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì´ ì„¤ì •ì—ì„œ ì„ë² ë”©ì€ ë‹¨ìˆœíˆ ìŠ¤íŒŒìŠ¤ ì—”í‹°í‹°(ê°œë³„ ë™ì˜ìƒ, ì‚¬ìš©ì ë“±)ë¥¼ RNì˜ ë°€ë„ê°€ ë†’ì€ ë²¡í„°ì— ë§¤í•‘í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì‹¬ì¸µ ì‹ ê²½ë§ì˜ ì„ë¬´ëŠ” ì†Œí”„íŠ¸ë§¥ìŠ¤ ë¶„ë¥˜ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë™ì˜ìƒì„ êµ¬ë¶„í•˜ëŠ” ë° ìœ ìš©í•œ ì‚¬ìš©ì íˆìŠ¤í† ë¦¬ ë° ì»¨í…ìŠ¤íŠ¸ì˜ í•¨ìˆ˜ë¡œì„œ ì‚¬ìš©ì ì„ë² ë”©ì„ í•™ìŠµí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. Although explicit feedback mechanisms exist on YouTube (thumbs up&#x2F;down, in-product surveys, etc.) we use the implicit feedback [16] of watches to train the model, where a user completing a video is a positive example. This choice is based on the orders of magnitude more implicit user history available, allowing us to produce recommendations deep in the tail where explicit feedback is extremely sparse. YouTubeì—ëŠ” ëª…ì‹œì ì¸ í”¼ë“œë°± ë©”ì»¤ë‹ˆì¦˜(ì¢‹ì•„ìš”&#x2F;ì‹«ì–´ìš”, ì œí’ˆ ë‚´ ì„¤ë¬¸ì¡°ì‚¬ ë“±)ì´ ì¡´ì¬í•˜ì§€ë§Œ, ì €í¬ëŠ” ì‚¬ìš©ìê°€ ë™ì˜ìƒì„ ì™„ë£Œí•˜ëŠ” ê²ƒì´ ê¸ì •ì ì¸ ì˜ˆì¸ ì‹œì²­ì˜ ì•”ë¬µì  í”¼ë“œë°±[16]ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í›ˆë ¨í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì„ íƒì€ ì‚¬ìš© ê°€ëŠ¥í•œ ì•”ì‹œì  ì‚¬ìš©ì ê¸°ë¡ì´ í›¨ì”¬ ë” ë§ê¸° ë•Œë¬¸ì— ëª…ì‹œì  í”¼ë“œë°±ì´ ê·¹íˆ ë“œë¬¸ ê¼¬ë¦¬ ë¶€ë¶„ì— ëŒ€í•œ ì¶”ì²œì„ ìƒì„±í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì„ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤. Efficient Extreme MulticlassTo efficiently train such a model with millions of classes, we rely on a technique to sample negative classes from the background distribution (â€candidate samplingâ€) and then correct for this sampling via importance weighting [10]. For each example the cross-entropy loss is minimized for the true label and the sampled negative classes. In practice several thousand negatives are sampled, corresponding to more than 100 times speedup over traditional softmax. A popular alternative approach is hierarchical softmax [15], but we werenâ€™t able to achieve comparable accuracy. In hierarchical softmax, traversing each node in the tree involves discriminating between sets of classes that are often unrelated, making the classification problem much more difficult and degrading performance. ìˆ˜ë°±ë§Œ ê°œì˜ í´ë˜ìŠ¤ë¡œ ì´ëŸ¬í•œ ëª¨ë¸ì„ íš¨ìœ¨ì ìœ¼ë¡œ í›ˆë ¨í•˜ê¸° ìœ„í•´ ë°°ê²½ ë¶„í¬ì—ì„œ ìŒìˆ˜ í´ë˜ìŠ¤ë¥¼ ìƒ˜í”Œë§í•˜ëŠ” ê¸°ë²•(â€˜í›„ë³´ ìƒ˜í”Œë§â€™)ì„ ì‚¬ìš©í•˜ê³  ì´ ìƒ˜í”Œë§ì— ì¤‘ìš”ë„ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ì—¬ ì¤‘ìš”ë„ ê°€ì¤‘ì¹˜[10]ë¥¼ í†µí•´ ì´ ìƒ˜í”Œë§ì„ ë³´ì •í•©ë‹ˆë‹¤. ê° ì˜ˆì œì—ì„œ êµì°¨ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ì€ ì‹¤ì œ ë ˆì´ë¸”ê³¼ ìƒ˜í”Œë§ëœ ìŒìˆ˜ í´ë˜ìŠ¤ì— ëŒ€í•´ ìµœì†Œí™”ë©ë‹ˆë‹¤. ì‹¤ì œë¡œëŠ” ìˆ˜ì²œ ê°œì˜ ê°œì˜ ë„¤ê±°í‹°ë¸Œê°€ ìƒ˜í”Œë§ë˜ë©°, ì´ëŠ” ê¸°ì¡´ ì†Œí”„íŠ¸ë§¥ìŠ¤ë³´ë‹¤ 100ë°° ì´ìƒì˜ ì†ë„ í–¥ìƒì— í•´ë‹¹í•©ë‹ˆë‹¤. ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ëŒ€ì•ˆì  ì ‘ê·¼ ë°©ì‹ì€ ê³„ì¸µì  ì†Œí”„íŠ¸ë§¥ìŠ¤[15]ì´ì§€ë§Œ, ë¹„ìŠ·í•œ ì •í™•ë„ë¥¼ ë‹¬ì„±í•  ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤. ê³„ì¸µì  ì†Œí”„íŠ¸ë§¥ìŠ¤ì—ì„œëŠ” íŠ¸ë¦¬ì˜ ê° ë…¸ë“œë¥¼ íƒìƒ‰í•  ë•Œ ì¢…ì¢… ì„œë¡œ ê´€ë ¨ì´ ì—†ëŠ” í´ë˜ìŠ¤ ì§‘í•©ì„ êµ¬ë³„í•´ì•¼ í•˜ë¯€ë¡œ ë¶„ë¥˜ ë¬¸ì œê°€ í›¨ì”¬ ë” ì–´ë ¤ì›Œì§€ê³  ì„±ëŠ¥ì´ ì €í•˜ë©ë‹ˆë‹¤. At serving time we need to compute the most likely N classes (videos) in order to choose the top N to present to the user. Scoring millions of items under a strict serving latency of tens of milliseconds requires an approximate scoring scheme sublinear in the number of classes. Previous systems at YouTube relied on hashing [24] and the classifier described here uses a similar approach. Since calibrated likelihoods from the softmax output layer are not needed at serving time, the scoring problem reduces to a nearest neighbor search in the dot product space for which general purpose libraries can be used [12]. We found that A&#x2F;B results were not particularly sensitive to the choice of nearest neighbor search algorithm. ì„œë¹™ ì‹œê°„ì— ì‚¬ìš©ìì—ê²Œ í‘œì‹œí•  ìƒìœ„ Nê°œë¥¼ ì„ íƒí•˜ë ¤ë©´ ê°€ì¥ ê°€ëŠ¥ì„±ì´ ë†’ì€ Nê°œì˜ í´ë˜ìŠ¤(ë™ì˜ìƒ)ë¥¼ ê³„ì‚°í•´ì•¼ í•©ë‹ˆë‹¤. ìˆ˜ì‹­ ë°€ë¦¬ì´ˆì˜ ì—„ê²©í•œ ì„œë¹™ ì§€ì—° ì‹œê°„ ì•„ë˜ì—ì„œ ìˆ˜ë°±ë§Œ ê°œì˜ í•­ëª©ì— ì ìˆ˜ë¥¼ ë§¤ê¸°ë ¤ë©´ í´ë˜ìŠ¤ ìˆ˜ì— ë”°ë¼ ëŒ€ëµì ì¸ ì ìˆ˜ ì²´ê³„ê°€ í•„ìš”í•©ë‹ˆë‹¤. YouTubeì˜ ì´ì „ ì‹œìŠ¤í…œì€ í•´ì‹±ì— ì˜ì¡´í–ˆìœ¼ë©°[24], ì—¬ê¸°ì— ì„¤ëª…ëœ ë¶„ë¥˜ê¸°ëŠ” ìœ ì‚¬í•œ ì ‘ê·¼ ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì†Œí”„íŠ¸ë§¥ìŠ¤ ì¶œë ¥ ë ˆì´ì–´ì˜ ë³´ì •ëœ ê°€ëŠ¥ì„±ì€ ì„œë¹™ ì‹œì ì— í•„ìš”í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì ìˆ˜ ë¬¸ì œëŠ” ë²”ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë„íŠ¸ í”„ë¡œë•íŠ¸ ê³µê°„ì—ì„œ ê°€ì¥ ê°€ê¹Œìš´ ì´ì›ƒ ê²€ìƒ‰ìœ¼ë¡œ ì¶•ì†Œë©ë‹ˆë‹¤[12]. A&#x2F;B ê²°ê³¼ëŠ” ìµœì¸ì ‘ ì´ì›ƒ ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ì˜ ì„ íƒì— íŠ¹ë³„íˆ ë¯¼ê°í•˜ì§€ ì•Šë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. Model ArchitectureInspired by continuous bag of words language models [14], we learn high dimensional embeddings for each video in a fixed vocabulary and feed these embeddings into a feedforward neural network. A userâ€™s watch history is represented by a variable-length sequence of sparse video IDs which is mapped to a dense vector representation via the embeddings. The network requires fixed-sized dense inputs and simply averaging the embeddings performed best among several strategies (sum, component-wise max, etc.). Importantly, the embeddings are learned jointly with all other model parameters through normal gradient descent back-propagation updates. Features are concatenated into a wide first layer, followed by several layers of fully connected Rectified Linear Units (ReLU) [6]. Figure 3 shows the general network architecture with additional non-video watch features described below. ì—°ì† ë‹¨ì–´ ê°€ë°© ì–¸ì–´ ëª¨ë¸[14]ì—ì„œ ì˜ê°ì„ ë°›ì•„ ê° ë™ì˜ìƒì— ëŒ€í•œ ê³ ì°¨ì› ì„ë² ë”©ì„ ê³ ì • ì–´íœ˜ë¡œ í•™ìŠµí•˜ê³  ì´ëŸ¬í•œ ì„ë² ë”©ì„ í”¼ë“œí¬ì›Œë“œ ì‹ ê²½ë§ì— ê³µê¸‰í•©ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì‹œì²­ ê¸°ë¡ì€ ì„ë² ë”©ì„ í†µí•´ ê³ ë°€ë„ ë²¡í„° í‘œí˜„ì— ë§¤í•‘ë˜ëŠ” ê°€ë³€ ê¸¸ì´ì˜ ìŠ¤íŒŒìŠ¤ ë¹„ë””ì˜¤ ID ì‹œí€€ìŠ¤ë¡œ í‘œí˜„ë©ë‹ˆë‹¤. ë„¤íŠ¸ì›Œí¬ì—ëŠ” ê³ ì • í¬ê¸°ì˜ ê³ ë°€ë„ ì…ë ¥ì´ í•„ìš”í•˜ë©° ì—¬ëŸ¬ ì „ëµ(í•©ê³„, êµ¬ì„± ìš”ì†Œë³„ ìµœëŒ€ê°’ ë“±) ì¤‘ì—ì„œ ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ ì„ë² ë”©ì˜ í‰ê· ì„ êµ¬í•©ë‹ˆë‹¤. ì¤‘ìš”í•œ ê²ƒì€ ì„ë² ë”©ì´ ì¼ë°˜ ê²½ì‚¬ í•˜ê°• ì—­ì „íŒŒ ì—…ë°ì´íŠ¸ë¥¼ í†µí•´ ë‹¤ë¥¸ ëª¨ë“  ëª¨ë¸ íŒŒë¼ë¯¸í„°ì™€ í•¨ê»˜ í•™ìŠµëœë‹¤ëŠ” ì ì…ë‹ˆë‹¤. íŠ¹ì§•ì€ ë„“ì€ ì²« ë²ˆì§¸ ë ˆì´ì–´ë¡œ ì—°ê²°ë˜ê³ , ê·¸ ë‹¤ìŒì—ëŠ” ì™„ì „íˆ ì—°ê²°ëœ ì—¬ëŸ¬ ë ˆì´ì–´ì˜ ì •ë¥˜ ì„ í˜• ë‹¨ìœ„(ReLU)[6]ë¡œ ì—°ê²°ë©ë‹ˆë‹¤. ê·¸ë¦¼ 3ì€ ì•„ë˜ì— ì„¤ëª…ëœ non-video ê°ì‹œ ê¸°ëŠ¥ì´ ì¶”ê°€ëœ ì¼ë°˜ì ì¸ ë„¤íŠ¸ì›Œí¬ ì•„í‚¤í…ì²˜ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. Heterogeneous SignalsA key advantage of using deep neural networks as a generalization of matrix factorization is that arbitrary continuous and categorical features can be easily added to the model. Search history is treated similarly to watch history - each query is tokenized into unigrams and bigrams and each token is embedded. Once averaged, the userâ€™s tokenized, embedded queries represent a summarized dense search history. Demographic features are important for providing priors so that the recommendations behave reasonably for new users. The userâ€™s geographic region and device are embedded and concatenated. Simple binary and continuous features such as the userâ€™s gender, logged-in state and age are input directly into the network as real values normalized to [0,1]. í–‰ë ¬ ì¸ìˆ˜ë¶„í•´ì˜ ì¼ë°˜í™”ë¡œ ì‹¬ì¸µ ì‹ ê²½ë§ì„ ì‚¬ìš©í•  ë•Œì˜ ì£¼ìš” ì¥ì ì€ ì„ì˜ì˜ ì—°ì† ë° ë²”ì£¼í˜• íŠ¹ì§•ì„ ëª¨ë¸ì— ì‰½ê²Œ ì¶”ê°€í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ê²€ìƒ‰ ê¸°ë¡ì€ ì‹œì²­ ê¸°ë¡ê³¼ ìœ ì‚¬í•˜ê²Œ ì·¨ê¸‰ë˜ë©°, ê° ì¿¼ë¦¬ëŠ” ìœ ë‹ˆê·¸ë¨ê³¼ ë¹…ê·¸ë¨ìœ¼ë¡œ í† í°í™”ë˜ê³  ê° í† í°ì€ ì„ë² ë“œë©ë‹ˆë‹¤. í‰ê· ì„ ë‚´ë©´ ì‚¬ìš©ìì˜ í† í°í™”ëœ ì„ë² ë””ë“œ ì¿¼ë¦¬ëŠ” ì¡°ë°€í•˜ê²Œ ìš”ì•½ëœ ê²€ìƒ‰ ê¸°ë¡ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì¸êµ¬í†µê³„í•™ì  íŠ¹ì§•ì€ ì¶”ì²œì´ ì‹ ê·œ ì‚¬ìš©ìì—ê²Œ í•©ë¦¬ì ìœ¼ë¡œ ì‘ë™í•˜ë„ë¡ ìš°ì„ ìˆœìœ„ë¥¼ ì œê³µí•˜ëŠ” ë° ì¤‘ìš”í•©ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§€ë¦¬ì  ì§€ì—­ê³¼ ë””ë°”ì´ìŠ¤ê°€ ì„ë² ë“œë˜ì–´ ì—°ê²°ë©ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì„±ë³„, ë¡œê·¸ì¸ ìƒíƒœ, ë‚˜ì´ì™€ ê°™ì€ ê°„ë‹¨í•œ ì´ì§„ ë° ì—°ì†í˜• íŠ¹ì§•ì€ [0,1]ë¡œ ì •ê·œí™”ëœ ì‹¤ì œ ê°’ìœ¼ë¡œ ë„¤íŠ¸ì›Œí¬ì— ì§ì ‘ ì…ë ¥ë©ë‹ˆë‹¤. â€œExample Ageâ€ FeatureMany hours worth of videos are uploaded each second to YouTube. Recommending this recently uploaded (â€freshâ€) content is extremely important for YouTube as a product. We consistently observe that users prefer fresh content, though not at the expense of relevance. In addition to the first-order effect of simply recommending new videos that users want to watch, there is a critical secondary phenomenon of bootstrapping and propagating viral content [11]. YouTubeì—ëŠ” ë§¤ì´ˆ ëª‡ ì‹œê°„ ë¶„ëŸ‰ì˜ ë™ì˜ìƒì´ ì—…ë¡œë“œë©ë‹ˆë‹¤. ìµœê·¼ì— ì—…ë¡œë“œëœ(â€˜ìƒˆë¡œìš´â€™) ì½˜í…ì¸ ë¥¼ ì¶”ì²œí•˜ëŠ” ê²ƒì€ ì œí’ˆìœ¼ë¡œì„œ YouTubeì— ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤. ì‚¬ìš©ìë“¤ì€ ê´€ë ¨ì„±ì„ í¬ìƒí•˜ë”ë¼ë„ ìƒˆë¡œìš´ ì½˜í…ì¸ ë¥¼ ì„ í˜¸í•œë‹¤ëŠ” ì‚¬ì‹¤ì„ ì§€ì†ì ìœ¼ë¡œ ê´€ì°°í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë‹¨ìˆœíˆ ì‚¬ìš©ìê°€ ë³´ê³  ì‹¶ì–´í•˜ëŠ” ìƒˆë¡œìš´ ë™ì˜ìƒì„ ì¶”ì²œí•˜ëŠ” 1ì°¨ íš¨ê³¼ ì™¸ì—ë„, ë°”ì´ëŸ´ ì½˜í…ì¸ ë¥¼ ë¶€íŠ¸ìŠ¤íŠ¸ë©í•˜ê³  ì „íŒŒí•˜ëŠ” ì¤‘ìš”í•œ 2ì°¨ í˜„ìƒì´ ìˆìŠµë‹ˆë‹¤[11]. Machine learning systems often exhibit an implicit bias towards the past because they are trained to predict future behavior from historical examples. The distribution of video popularity is highly non-stationary but the multinomial distribution over the corpus produced by our recommender will reflect the average watch likelihood in the training window of several weeks. To correct for this, we feed the age of the training example as a feature during training. At serving time, this feature is set to zero (or slightly negative) to reflect that the model is making predictions at the very end of the training window. ë¨¸ì‹  ëŸ¬ë‹ ì‹œìŠ¤í…œì€ ê³¼ê±°ì˜ ì‚¬ë¡€ë¥¼ í†µí•´ ë¯¸ë˜ì˜ í–‰ë™ì„ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµë˜ê¸° ë•Œë¬¸ì— ê³¼ê±°ì— ëŒ€í•œ ì•”ë¬µì ì¸ í¸í–¥ì„±ì„ ë³´ì´ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. ë™ì˜ìƒ ì¸ê¸°ë„ ë¶„í¬ëŠ” ë§¤ìš° ë¹„ê³ ì •ì ì´ì§€ë§Œ, ì¶”ì²œìê°€ ìƒì„±í•œ ë§ë­‰ì¹˜ì— ëŒ€í•œ ë‹¤í•­ì‹ ë¶„í¬ëŠ” ëª‡ ì£¼ ë™ì•ˆì˜ í›ˆë ¨ ê¸°ê°„ ë™ì•ˆì˜ í‰ê·  ì‹œì²­ ê°€ëŠ¥ì„±ì„ ë°˜ì˜í•©ë‹ˆë‹¤. ì´ë¥¼ ë³´ì •í•˜ê¸° ìœ„í•´ í›ˆë ¨ ì¤‘ì— í›ˆë ¨ ì˜ˆì œì˜ ì—°ë ¹ì„ í”¼ì²˜ë¡œ ì œê³µí•©ë‹ˆë‹¤. ì„œë¹™ ì‹œê°„ì—ëŠ” ì´ í”¼ì²˜ê°€ 0(ë˜ëŠ” ì•½ê°„ ìŒìˆ˜)ìœ¼ë¡œ ì„¤ì •ë˜ì–´ ëª¨ë¸ì´ í›ˆë ¨ ê¸°ê°„ì˜ ë§¨ ë§ˆì§€ë§‰ì— ì˜ˆì¸¡ì„ í•˜ê³  ìˆìŒì„ ë°˜ì˜í•©ë‹ˆë‹¤. Figure 4 demonstrates the efficacy of this approach on an arbitrarily chosen video [26]. Label and Context SelectionIt is important to emphasize that recommendation often involves solving a surrogate problem and transferring the result to a particular context. A classic example is the assumption that accurately predicting ratings leads to effective movie recommendations [2]. We have found that the choice of this surrogate learning problem has an outsized importance on performance in A&#x2F;B testing but is very difficult to measure with offline experiments. ì¶”ì²œì—ëŠ” ì¢…ì¢… ëŒ€ì²´ ëª¨ë¸ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³  ê·¸ ê²°ê³¼ë¥¼ íŠ¹ì • ì»¨í…ìŠ¤íŠ¸ë¡œ ì˜®ê¸°ëŠ” ì‘ì—…ì´ í¬í•¨ëœë‹¤ëŠ” ì ì„ ê°•ì¡°í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ëŒ€í‘œì ì¸ ì˜ˆë¡œ í‰ì ì„ ì •í™•í•˜ê²Œ ì˜ˆì¸¡í•˜ë©´ íš¨ê³¼ì ì¸ ì˜í™” ì¶”ì²œì´ ê°€ëŠ¥í•˜ë‹¤ëŠ” ê°€ì •ì„ ë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤[2]. ìš°ë¦¬ëŠ” ì´ ëŒ€ë¦¬ í•™ìŠµ ë¬¸ì œì˜ ì„ íƒì´ A&#x2F;B í…ŒìŠ¤íŠ¸ì˜ ì„±ëŠ¥ì— ë§¤ìš° ì¤‘ìš”í•˜ì§€ë§Œ ì˜¤í”„ë¼ì¸ ì‹¤í—˜ìœ¼ë¡œ ì¸¡ì •í•˜ê¸°ëŠ” ë§¤ìš° ì–´ë µë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. Training examples are generated from all YouTube watches (even those embedded on other sites) rather than just watches on the recommendations we produce. Otherwise, it would be very difficult for new content to surface and the recommender would be overly biased towards exploitation. If users are discovering videos through means other than our recommendations, we want to be able to quickly propagate this discovery to others via collaborative filtering. Another key insight that improved live metrics was to generate a fixed number of training examples per user, effectively weighting our users equally in the loss function. This prevented a small cohort of highly active users from dominating the loss. Training examplesëŠ” ì¶”ì²œ ë™ì˜ìƒë¿ë§Œ ì•„ë‹ˆë¼ ëª¨ë“  YouTube ë™ì˜ìƒ(ë‹¤ë¥¸ ì‚¬ì´íŠ¸ì— ì„ë² ë“œëœ ë™ì˜ìƒë„ í¬í•¨)ì—ì„œ ìƒì„±ë©ë‹ˆë‹¤. ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ìƒˆë¡œìš´ ì½˜í…ì¸ ê°€ ë…¸ì¶œë˜ê¸°ê°€ ë§¤ìš° ì–´ë µê³  ì¶”ì²œì´ ì§€ë‚˜ì¹˜ê²Œ ì•…ìš©ì— í¸í–¥ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‚¬ìš©ìê°€ ì¶”ì²œ ì´ì™¸ì˜ ê²½ë¡œë¥¼ í†µí•´ ë™ì˜ìƒì„ ë°œê²¬í•˜ëŠ” ê²½ìš°, ê³µë™ í•„í„°ë§ì„ í†µí•´ ì´ëŸ¬í•œ ë°œê²¬ì„ ë‹¤ë¥¸ ì‚¬ìš©ìì—ê²Œ ì‹ ì†í•˜ê²Œ ì „íŒŒí•  ìˆ˜ ìˆê¸°ë¥¼ ë°”ëë‹ˆë‹¤. ì‹¤ì‹œê°„ ì§€í‘œë¥¼ ê°œì„ í•œ ë˜ ë‹¤ë¥¸ ì£¼ìš” ì¸ì‚¬ì´íŠ¸ëŠ” ì‚¬ìš©ìë‹¹ ê³ ì •ëœ ì‚¬ìš©ìë‹¹ í›ˆë ¨ ì˜ˆì œ ìˆ˜ë¥¼ ê³ ì •í•˜ì—¬ ì†ì‹¤ í•¨ìˆ˜ì—ì„œ ëª¨ë“  ì‚¬ìš©ìì—ê²Œ ë™ì¼í•œ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ í™œë™ì„±ì´ ë†’ì€ ì†Œìˆ˜ì˜ ì‚¬ìš©ì ì§‘ë‹¨ì´ ì†ì‹¤ì„ ì§€ë°°í•˜ëŠ” ê²ƒì„ ë°©ì§€í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. Somewhat counter-intuitively, great care must be taken to withhold information from the classifier in order to prevent the model from exploiting the structure of the site and overfitting the surrogate problem. Consider as an example a case in which the user has just issued a search query for â€œtaylor swiftâ€. Since our problem is posed as predicting the next watched video, a classifier given this information will predict that the most likely videos to be watched are those which appear on the corresponding search results page for â€œtaylor swiftâ€. Unsurpisingly, reproducing the userâ€™s last search page as homepage recommendations performs very poorly. By discarding sequence information and representing search queries with an unordered bag of tokens, the classifier is no longer directly aware of the origin of the label. ë‹¤ì†Œ ì§ê´€ì ì´ì§€ ì•Šì„ ìˆ˜ ìˆì§€ë§Œ, ëª¨ë¸ì´ ì‚¬ì´íŠ¸ì˜ êµ¬ì¡°ë¥¼ ì•…ìš©í•˜ì—¬ ëŒ€ë¦¬ ë¬¸ì œì— ê³¼ë„í•˜ê²Œ ì í•©í•˜ì§€ ì•Šë„ë¡ ë¶„ë¥˜ê¸°ì—ì„œ ì •ë³´ë¥¼ ë³´ë¥˜í•˜ëŠ” ë° ì„¸ì‹¬í•œ ì£¼ì˜ë¥¼ ê¸°ìš¸ì—¬ì•¼ í•©ë‹ˆë‹¤. ì‚¬ìš©ìê°€ ë°©ê¸ˆ â€˜í…Œì¼ëŸ¬ ìŠ¤ìœ„í”„íŠ¸â€™ì— ëŒ€í•œ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ì‹¤í–‰í•œ ê²½ìš°ë¥¼ ì˜ˆë¡œ ë“¤ì–´ ë³´ê² ìŠµë‹ˆë‹¤. ìš°ë¦¬ì˜ ë¬¸ì œëŠ” ë‹¤ìŒì— ì‹œì²­í•  ë™ì˜ìƒì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ë¯€ë¡œ, ì´ ì •ë³´ê°€ ì£¼ì–´ì§„ ë¶„ë¥˜ê¸°ëŠ” â€˜í…Œì¼ëŸ¬ ìŠ¤ìœ„í”„íŠ¸â€™ì— ëŒ€í•œ í•´ë‹¹ ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ì— í‘œì‹œë˜ëŠ” ë™ì˜ìƒì´ ì‹œì²­ë  ê°€ëŠ¥ì„±ì´ ê°€ì¥ ë†’ì€ ë™ì˜ìƒì´ë¼ê³  ì˜ˆì¸¡í•  ê²ƒì…ë‹ˆë‹¤. ë‹¹ì—°íˆ ì‚¬ìš©ìê°€ ë§ˆì§€ë§‰ìœ¼ë¡œ ê²€ìƒ‰í•œ í˜ì´ì§€ë¥¼ í™ˆí˜ì´ì§€ ì¶”ì²œ í˜ì´ì§€ë¡œ ì¬ìƒì‚°í•˜ëŠ” ê²ƒì€ ë§¤ìš° ì €ì¡°í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. ì‹œí€€ìŠ¤ ì •ë³´ë¥¼ ë²„ë¦¬ê³  ì •ë ¬ë˜ì§€ ì•Šì€ í† í° ë°±ìœ¼ë¡œ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ í‘œí˜„í•˜ë©´ ë¶„ë¥˜ê¸°ëŠ” ë” ì´ìƒ ë ˆì´ë¸”ì˜ ì¶œì²˜ë¥¼ ì§ì ‘ ì¸ì‹í•˜ì§€ ëª»í•©ë‹ˆë‹¤. Natural consumption patterns of videos typically lead to very asymmetric co-watch probabilities. Episodic series are usually watched sequentially and users often discover artists in a genre beginning with the most broadly popular before focusing on smaller niches. We therefore found much better performance predicting the userâ€™s next watch, rather than predicting a randomly held-out watch (Figure 5). Many collaborative filtering systems implicitly choose the labels and context by holding out a random item and predicting it from other items in the userâ€™s history (5a). This leaks future information and ignores any asymmetric consumption patterns. In contrast, we â€œrollbackâ€ a userâ€™s history by choosing a random watch and only input actions the user took before the held-out label watch (5b). ë™ì˜ìƒì˜ ìì—°ìŠ¤ëŸ¬ìš´ ì†Œë¹„ íŒ¨í„´ì€ ì¼ë°˜ì ìœ¼ë¡œ ë§¤ìš° ë¹„ëŒ€ì¹­ì ì¸ ê³µë™ ì‹œì²­ í™•ë¥ ë¡œ ì´ì–´ì§‘ë‹ˆë‹¤. ì—í”¼ì†Œë“œ ì‹œë¦¬ì¦ˆëŠ” ì¼ë°˜ì ìœ¼ë¡œ ìˆœì°¨ì ìœ¼ë¡œ ì‹œì²­ë˜ë©°, ì‚¬ìš©ìëŠ” ê°€ì¥ ë„ë¦¬ ì•Œë ¤ì§„ ì¥ë¥´ë¶€í„° ì‹œì‘í•˜ì—¬ ì‘ì€ í‹ˆìƒˆ ì¥ë¥´ì— ì§‘ì¤‘í•˜ë©´ì„œ í•´ë‹¹ ì¥ë¥´ì˜ ì•„í‹°ìŠ¤íŠ¸ë¥¼ ë°œê²¬í•˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ë¬´ì‘ìœ„ë¡œ ë³´ë¥˜ëœ ì‹œì²­ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒë³´ë‹¤ ì‚¬ìš©ìì˜ ë‹¤ìŒ ì‹œì²­ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ í›¨ì”¬ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤(ê·¸ë¦¼ 5). ë§ì€ í˜‘ì—… í•„í„°ë§ ì‹œìŠ¤í…œì€ ì•”ë¬µì ìœ¼ë¡œ ë ˆì´ë¸”ê³¼ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì„ íƒí•©ë‹ˆë‹¤. ë ˆì´ë¸”ê³¼ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì•”ì‹œì ìœ¼ë¡œ ì„ íƒí•©ë‹ˆë‹¤(ê·¸ë¦¼ 5a). ì´ëŠ” ë¯¸ë˜ ì •ë³´ë¥¼ ìœ ì¶œí•˜ê³  ë¹„ëŒ€ì¹­ì ì¸ ì†Œë¹„ íŒ¨í„´ì„ ë¬´ì‹œí•©ë‹ˆë‹¤. ì´ì™€ëŠ” ëŒ€ì¡°ì ìœ¼ë¡œ, ë¬´ì‘ìœ„ ì‹œê³„ë¥¼ ì„ íƒí•˜ì—¬ ì‚¬ìš©ìì˜ ê¸°ë¡ì„ â€˜ë¡¤ë°±â€™í•˜ê³  ì‚¬ìš©ìê°€ ë³´ë¥˜ëœ ë ˆì´ë¸” ì‹œê³„ ì´ì „ì— ìˆ˜í–‰í•œ ì‘ì—…ë§Œ ì…ë ¥í•©ë‹ˆë‹¤(5b). Experiments with Features and DepthAdding features and depth significantly improves precision on holdout data as shown in Figure 6. In these experiments, a vocabulary of 1M videos and 1M search tokens were embedded with 256 floats each in a maximum bag size of 50 recent watches and 50 recent searches. The softmax layer outputs a multinomial distribution over the same 1M video classes with a dimension of 256 (which can be thought of as a separate output video embedding). These models were trained until convergence over all YouTube users, corresponding to several epochs over the data. Network structure followed a common â€œtowerâ€ pattern in which the bottom of the network is widest and each successive hidden layer halves the number of units (similar to Figure 3). The depth zero network is effectively a linear factorization scheme which performed very similarly to the predecessor system. Width and depth were added until the incremental benefit diminished and convergence became difficult: ê·¸ë¦¼ 6ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ íŠ¹ì§•ê³¼ ê¹Šì´ë¥¼ ì¶”ê°€í•˜ë©´ í™€ë“œì•„ì›ƒ ë°ì´í„°ì˜ ì •ë°€ë„ê°€ í¬ê²Œ í–¥ìƒë©ë‹ˆë‹¤. ì´ ì‹¤í—˜ì—ì„œëŠ” 1ë°±ë§Œ ê°œì˜ ë™ì˜ìƒê³¼ 1ë°±ë§Œ ê°œì˜ ê²€ìƒ‰ í† í°ìœ¼ë¡œ êµ¬ì„±ëœ ì–´íœ˜ë¥¼ ìµœê·¼ ì‹œì²­ 50ê°œì™€ ìµœê·¼ ê²€ìƒ‰ 50ê°œì˜ ìµœëŒ€ ê°€ë°© í¬ê¸°ì— ê°ê° 256ê°œì˜ í”Œë¡œíŠ¸ë¡œ ì„ë² ë“œí–ˆìŠµë‹ˆë‹¤. ì†Œí”„íŠ¸ë§¥ìŠ¤ ë ˆì´ì–´ëŠ” ë™ì¼í•œ 1M ë™ì˜ìƒ í´ë˜ìŠ¤ì— ëŒ€í•´ 256 ì°¨ì›ìœ¼ë¡œ ë‹¤í•­ì‹ ë¶„í¬ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤(ë³„ë„ì˜ ì¶œë ¥ ë™ì˜ìƒ ì„ë² ë”©ìœ¼ë¡œ ìƒê°í•  ìˆ˜ ìˆìŒ). ì´ëŸ¬í•œ ëª¨ë¸ì€ ë°ì´í„°ì˜ ì—¬ëŸ¬ ì‹œëŒ€ì— í•´ë‹¹í•˜ëŠ” ëª¨ë“  YouTube ì‚¬ìš©ìì— ëŒ€í•´ ìˆ˜ë ´í•  ë•Œê¹Œì§€ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤. ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°ëŠ” ë„¤íŠ¸ì›Œí¬ì˜ ë§¨ ì•„ë˜ê°€ ê°€ì¥ ë„“ê³  ì—°ì†ë˜ëŠ” ê° ìˆ¨ê²¨ì§„ ë ˆì´ì–´ê°€ ìœ ë‹› ìˆ˜ë¥¼ ì ˆë°˜ìœ¼ë¡œ ì¤„ì´ëŠ” ì¼ë°˜ì ì¸ â€œíƒ€ì›Œâ€ íŒ¨í„´ì„ ë”°ëìŠµë‹ˆë‹¤(ê·¸ë¦¼ 3ê³¼ ìœ ì‚¬). ê¹Šì´ ì œë¡œ ë„¤íŠ¸ì›Œí¬ëŠ” ì‚¬ì‹¤ìƒ ì„ í˜• ì¸ìˆ˜ë¶„í•´ ë°©ì‹ì´ë©°, ì´ì „ ì‹œìŠ¤í…œê³¼ ë§¤ìš° ìœ ì‚¬í•˜ê²Œ ì‘ë™í–ˆìŠµë‹ˆë‹¤. ì ì§„ì  ì´ë“ì´ ê°ì†Œí•˜ê³  ìˆ˜ë ´ì´ ì–´ë ¤ì›Œì§ˆ ë•Œê¹Œì§€ í­ê³¼ ê¹Šì´ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤: Depth 0: A linear layer simply transforms the concatenationlayer to match the softmax dimension of 256 Depth 1: 256 ReLU Depth 2: 512 ReLU â†’ 256 ReLU Depth 3: 1024 ReLU â†’ 512 ReLU â†’ 256 ReLU Depth 4: 2048 ReLU â†’ 1024 ReLU â†’ 512 ReLU â†’ 256 ReLU RankingThe primary role of ranking is to use impression data to specialize and calibrate candidate predictions for the particular user interface. For example, a user may watch a given video with high probability generally but is unlikely to click on the specific homepage impression due to the choice of thumbnail image. During ranking, we have access to many more features describing the video and the userâ€™s relationship to the video because only a few hundred videos are being scored rather than the millions scored in candidate generation. Ranking is also crucial for ensembling different candidate sources whose scores are not directly comparable. ë­í‚¹ì˜ ì£¼ìš” ì—­í• ì€ ë…¸ì¶œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ì— ëŒ€í•œ í›„ë³´ ì˜ˆì¸¡ì„ ì „ë¬¸í™”í•˜ê³  ë³´ì •í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì‚¬ìš©ìê°€ íŠ¹ì • ë™ì˜ìƒì„ ì¼ë°˜ì ìœ¼ë¡œ ë†’ì€ í™•ë¥ ë¡œ ì‹œì²­í•˜ì§€ë§Œ ì¸ë„¤ì¼ ì´ë¯¸ì§€ ì„ íƒìœ¼ë¡œ ì¸í•´ íŠ¹ì • í™ˆí˜ì´ì§€ ë…¸ì¶œì„ í´ë¦­í•  ê°€ëŠ¥ì„±ì€ ë‚®ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìˆœìœ„ë¥¼ ë§¤ê¸°ëŠ” ë™ì•ˆì—ëŠ” í›„ë³´ ìƒì„±ì—ì„œ ìˆ˜ë°±ë§Œ ê°œì˜ ë™ì˜ìƒì´ ì ìˆ˜í™”ë˜ëŠ” ëŒ€ì‹  ìˆ˜ë°± ê°œì˜ ë™ì˜ìƒë§Œ ì ìˆ˜í™”ë˜ê¸° ë•Œë¬¸ì— ë™ì˜ìƒê³¼ ì‚¬ìš©ìì™€ì˜ ê´€ê³„ë¥¼ ì„¤ëª…í•˜ëŠ” ë” ë§ì€ ê¸°ëŠ¥ì— ì•¡ì„¸ìŠ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìˆœìœ„ëŠ” ì ìˆ˜ê°€ ì§ì ‘ì ìœ¼ë¡œ ë¹„êµí•  ìˆ˜ ì—†ëŠ” ë‹¤ì–‘í•œ í›„ë³´ ì†ŒìŠ¤ë¥¼ ì¡°í•©í•˜ëŠ” ë°ì—ë„ ì¤‘ìš”í•©ë‹ˆë‹¤. We use a deep neural network with similar architecture as candidate generation to assign an independent score to each video impression using logistic regression (Figure 7). The list of videos is then sorted by this score and returned to the user. Our final ranking objective is constantly being tuned based on live A&#x2F;B testing results but is generally a simple function of expected watch time per impression. Ranking by click-through rate often promotes deceptive videos that the user does not complete (\\clickbaitâ€) whereas watch time better captures engagement [13, 25]. í›„ë³´ ìƒì„±ê³¼ ìœ ì‚¬í•œ ì•„í‚¤í…ì²˜ë¥¼ ê°€ì§„ ì‹¬ì¸µ ì‹ ê²½ë§ì„ ì‚¬ìš©í•˜ì—¬ ë¡œì§€ìŠ¤í‹± íšŒê·€ë¥¼ í†µí•´ ê° ë™ì˜ìƒ ë…¸ì¶œì— ë…ë¦½ì ì¸ ì ìˆ˜ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤(ê·¸ë¦¼ 7). ê·¸ëŸ° ë‹¤ìŒ ë™ì˜ìƒ ëª©ë¡ì€ ì´ ì ìˆ˜ì— ë”°ë¼ ì •ë ¬ë˜ì–´ ì‚¬ìš©ìì—ê²Œ ë°˜í™˜ë©ë‹ˆë‹¤. ìµœì¢… ìˆœìœ„ ëª©í‘œëŠ” ì‹¤ì‹œê°„ A&#x2F;B í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§€ì†ì ìœ¼ë¡œ ì¡°ì •ë˜ê³  ìˆì§€ë§Œ, ì¼ë°˜ì ìœ¼ë¡œ ë…¸ì¶œë‹¹ ì˜ˆìƒ ì‹œì²­ ì‹œê°„ì´ë¼ëŠ” ê°„ë‹¨í•œ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. í´ë¦­ë¥ ë¡œ ìˆœìœ„ë¥¼ ë§¤ê¸°ë©´ ì‚¬ìš©ìê°€ ì™„ë£Œí•˜ì§€ ì•ŠëŠ” ê¸°ë§Œì ì¸ ë™ì˜ìƒ(â€í´ë¦­ë² ì´íŠ¸â€)ì„ í™ë³´í•˜ëŠ” ê²½ìš°ê°€ ë§ì€ ë°˜ë©´, ì‹œì²­ ì‹œê°„ì€ ì°¸ì—¬ë„ë¥¼ ë” ì˜ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤[13, 25]. Feature RepresentationOur features are segregated with the traditional taxonomy of categorical and continuous&#x2F;ordinal features. The categorical features we use vary widely in their cardinality - some are binary (e.g. whether the user is logged-in) while others have millions of possible values (e.g. the userâ€™s last search query). Features are further split according to whether they contribute only a single value (â€univalentâ€) or a set of values (â€multivalentâ€). An example of a univalent categorical feature is the video ID of the impression being scored, while a corresponding multivalent feature might be a bag of the last N video IDs the user has watched. We also classify features according to whether they describe properties of the item (â€impressionâ€) or properties of the user&#x2F;context (â€queryâ€). Query features are computed once per request while impression features are computed for each item scored. ìš°ë¦¬ì˜ featureëŠ” ë²”ì£¼í˜• ê¸°ëŠ¥ê³¼ ì—°ì†í˜•&#x2F;ì„œìˆ˜í˜• ê¸°ëŠ¥ì´ë¼ëŠ” ì „í†µì ì¸ ë¶„ë¥˜ ì²´ê³„ë¡œ êµ¬ë¶„ë©ë‹ˆë‹¤. ìš°ë¦¬ê°€ ì‚¬ìš©í•˜ëŠ” ë²”ì£¼í˜• í”¼ì²˜ëŠ” ì¹´ë””ë„ë¦¬í‹°ê°€ ë§¤ìš° ë‹¤ì–‘í•©ë‹ˆë‹¤. ì¼ë¶€ í”¼ì²˜ëŠ” ì´ì§„(ì˜ˆ: ì‚¬ìš©ìì˜ ë¡œê·¸ì¸ ì—¬ë¶€)ì¸ ë°˜ë©´, ë‹¤ë¥¸ í”¼ì²˜ëŠ” ìˆ˜ë°±ë§Œ ê°œì˜ ê°€ëŠ¥í•œ ê°’(ì˜ˆ: ì‚¬ìš©ìì˜ ë§ˆì§€ë§‰ ê²€ìƒ‰ ì¿¼ë¦¬)ì„ ê°€ì§‘ë‹ˆë‹¤. ê¸°ëŠ¥ì€ ë‹¨ì¼ ê°’ë§Œ ì œê³µí•˜ëŠ”ì§€(â€˜1í•­â€™) ë˜ëŠ” ì—¬ëŸ¬ ê°’ì˜ ì§‘í•©ì„ ì œê³µí•˜ëŠ”ì§€(â€˜2í•­â€™)ì— ë”°ë¼ ë” ì„¸ë¶„í™”ë©ë‹ˆë‹¤. ë‹¨í•­ ë²”ì£¼í˜• íŠ¹ì§•ì˜ ì˜ˆë¡œëŠ” ì ìˆ˜ê°€ ë§¤ê²¨ì§„ ë…¸ì¶œì˜ ë™ì˜ìƒ IDë¥¼ ë“¤ ìˆ˜ ìˆìœ¼ë©°, í•´ë‹¹ ë‹¤í•­ íŠ¹ì§•ìœ¼ë¡œëŠ” ì‚¬ìš©ìê°€ ë§ˆì§€ë§‰ìœ¼ë¡œ ì‹œì²­í•œ Nê°œì˜ ë™ì˜ìƒ IDë¥¼ ë¬¶ì€ ê²ƒì„ ë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ í•­ëª©ì˜ ì†ì„±(â€˜ë…¸ì¶œâ€™)ì„ ì„¤ëª…í•˜ëŠ”ì§€, ì•„ë‹ˆë©´ ì‚¬ìš©ì&#x2F;ì»¨í…ìŠ¤íŠ¸ì˜ ì†ì„±(â€˜ì¿¼ë¦¬â€™)ì„ ì„¤ëª…í•˜ëŠ”ì§€ì— ë”°ë¼ í”¼ì²˜ë¥¼ ë¶„ë¥˜í•©ë‹ˆë‹¤. ì¿¼ë¦¬ í”¼ì²˜ëŠ” ìš”ì²­ë‹¹ í•œ ë²ˆ ê³„ì‚°ë˜ë©°, ë…¸ì¶œ í”¼ì²˜ëŠ” ì ìˆ˜ê°€ ë§¤ê²¨ì§„ ê° í•­ëª©ì— ëŒ€í•´ ê³„ì‚°ë©ë‹ˆë‹¤. Feature EngineeringWe typically use hundreds of features in our ranking models, roughly split evenly between categorical and continuous. Despite the promise of deep learning to alleviate the burden of engineering features by hand, the nature of our raw data does not easily lend itself to be input directly into feedforward neural networks. We still expend considerable engineering resources transforming user and video data into useful features. The main challenge is in representing a temporal sequence of user actions and how these actions relate to the video impression being scored. ì¼ë°˜ì ìœ¼ë¡œ ìˆœìœ„ ëª¨ë¸ì—ëŠ” ìˆ˜ë°± ê°œì˜ í”¼ì²˜ê°€ ì‚¬ìš©ë˜ë©°, ëŒ€ëµ ë²”ì£¼í˜•ê³¼ ì—°ì†í˜•ìœ¼ë¡œ ê³ ë¥´ê²Œ ë‚˜ë‰©ë‹ˆë‹¤. ìˆ˜ì‘ì—…ìœ¼ë¡œ ê¸°ëŠ¥ì„ ì—”ì§€ë‹ˆì–´ë§í•˜ëŠ” ë¶€ë‹´ì„ ëœì–´ì£¼ëŠ” ë”¥ëŸ¬ë‹ì˜ ì¥ì ì—ë„ ë¶ˆêµ¬í•˜ê³ , ì›ì‹œ ë°ì´í„°ì˜ íŠ¹ì„±ìƒ í”¼ë“œí¬ì›Œë“œ ì‹ ê²½ë§ì— ì§ì ‘ ì…ë ¥í•˜ëŠ” ê²ƒì€ ì‰½ì§€ ì•ŠìŠµë‹ˆë‹¤. ì—¬ì „íˆ ì‚¬ìš©ì ë° ë¹„ë””ì˜¤ ë°ì´í„°ë¥¼ ìœ ìš©í•œ ê¸°ëŠ¥ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë° ìƒë‹¹í•œ ì—”ì§€ë‹ˆì–´ë§ ë¦¬ì†ŒìŠ¤ë¥¼ íˆ¬ì…í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì£¼ìš” ê³¼ì œëŠ” ì‚¬ìš©ì í–‰ë™ì˜ ì‹œê°„ì  ìˆœì„œì™€ ì´ëŸ¬í•œ í–‰ë™ì´ ì ìˆ˜í™”ë˜ëŠ” ë¹„ë””ì˜¤ ë…¸ì¶œê³¼ ì–´ë–»ê²Œ ì—°ê´€ë˜ëŠ”ì§€ë¥¼ í‘œí˜„í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. We observe that the most important signals are those that describe a userâ€™s previous interaction with the item itself and other similar items, matching othersâ€™ experience in ranking ads [7]. As an example, consider the userâ€™s past history with the channel that uploaded the video being scored - how many videos has the user watched from this channel? When was the last time the user watched a video on this topic? These continuous features describing past user actions on relateditems are particularly powerful because they generalize well across disparate items. We have also found it crucial to propagate information from candidate generation into ranking in the form of features, e.g. which sources nominated this video candidate? What scores did they assign? Features describing the frequency of past video impressions are also critical for introducing \\churnâ€ in recommendations (successive requests do not return identical lists). If a user was recently recommended a video but did not watch it then the model will naturally demote this impression on the next page load. Serving up-to-the-second impression and watch history is an engineering feat onto itself outside the scope of this paper, but is vital for producing responsiverecommendations. Embedding Categorical FeaturesSimilar to candidate generation, we use embeddings to map sparse categorical features to dense representations suitable for neural networks. Each unique ID space (\\vocabularyâ€) has a separate learned embedding with dimension that increases approximately proportional to the logarithm of the number of unique values. These vocabularies are simple look-up tables built by passing over the data once before training. Very large cardinality ID spaces (e.g. video IDs or search query terms) are truncated by including only the top N after sorting based on their frequency in clicked impressions. Out-of-vocabulary values are simply mapped to the zero embedding. As in candidate generation, multivalent categorical feature embeddings are averaged before being fed in to the network. Importantly, categorical features in the same ID space also share underlying emeddings. For example, there exists a single global embedding of video IDs that many distinct featuresuse (video ID of the impression, last video ID watched by the user, video ID that \\seededâ€ the recommendation, etc.). Despite the shared embedding, each feature is fed separately into the network so that the layers above can learn specialized representations per feature. Sharing embeddings is important for improving generalization, speeding up training and reducing memory requirements. The overwhelming majority of model parameters are in these high-cardinality embedding spaces - for example, one million IDs embedded in a 32 dimensional space have 7 times more parameters than fully connected layers 2048 units wide. TBC.. Link : https://research.google/pubs/pub45530/","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Recommendation System","slug":"Paper/Recommendation-System","permalink":"https://jmj3047.github.io/categories/Paper/Recommendation-System/"}],"tags":[{"name":"Recommendation System","slug":"Recommendation-System","permalink":"https://jmj3047.github.io/tags/Recommendation-System/"},{"name":"Deep Nueral Networks","slug":"Deep-Nueral-Networks","permalink":"https://jmj3047.github.io/tags/Deep-Nueral-Networks/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Recommendation System","slug":"Paper/Recommendation-System","permalink":"https://jmj3047.github.io/categories/Paper/Recommendation-System/"}]},{"title":"Advanced Learning Algorithms","slug":"ML_Part2","date":"2023-03-29T15:00:00.000Z","updated":"2023-04-28T10:44:28.837Z","comments":true,"path":"2023/03/30/ML_Part2/","link":"","permalink":"https://jmj3047.github.io/2023/03/30/ML_Part2/","excerpt":"","text":"Course Lecture 2 in Machine Learning Course Neural Networks matrix multiplication: it is a binary operation that takes a pair of matrices and produces another matrix. It is defined as the product of an mÃ—n matrix and an nÃ—p matrix, resulting in an mÃ—p matrix. The product is calculated by taking the dot product of the rows of the first matrix with the columns of the second matrix. The entry in the i-th row and j-th column of the resulting matrix is obtained by multiplying each element of the i-th row of the first matrix by the corresponding element of the j-th column of the second matrix and then summing the products. Nueral Network training ML advice One nice thing about transfer learning as well is maybe you donâ€™t need to be the one to carry out supervised pre-training. For a lot of neural networks, there will already be researchers they have already trained a neural network on a large image and will have posted a trained neural networks on the Internet, freely licensed for anyone to download and use. What that means is rather than carrying out the first step yourself, you can just download the neural network that someone else may have spent weeks training and then replace the output layer with your own output layer and carry out either Option 1 or Option 2 to fine tune a neural network that someone else has already carried out supervised pre-training on, and just do a little bit of fine tuning to quickly be able to get a neural network that performs well on your task. Downloading a pre-trained model that someone else has trained and provided for free is one of those techniques where by building on each otherâ€™s work on machine learning community we can all get much better results. By the generosity of other researchers that have pre-trained and posted their neural networks online. One restriction of pre-training though, is that the image type x has to be the same for the pre-training and fine-tuning steps. If your goal is to build a speech recognition system to process audio, then a neural network pre-trained on images probably wonâ€™t do much good on audio. Instead, you want a neural network pre-trained on audio data, there you then fine tune on your own audio dataset and the same for other types of applications. You can pre-train a neural network on text data and If your application has a save feature input x of text data, then you can fine tune that neural network on your own data. To summarize, these are the two steps for transfer learning. Step 1 is download neural network with parameters that have been pre-trained on a large dataset with the same input type as your application. Decision Tree entropy function measure of the impurity of a set of data. starts from zero, goes up to one, and then comes back down to zero as a function of the fraction of positive examples in your sample. similar with Gini crtiteria if thereâ€™s a node with a lot of examples in it with high entropy that seems worse than if there was a node with just a few examples in it with high entropy. Because entropy, as a measure of impurity, is worse if you have a very large and impure dataset compared to just a few examples and a branch of the tree that is very impure. information gain it measures the reduction in entropy that you get in your tree resulting from making a split. recursive algorithm the way you build a decision tree at the root is by building other smaller decision trees in the left and right sub-branches recursion refers to writing code that calls itself.","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Coursera","slug":"Coursera","permalink":"https://jmj3047.github.io/tags/Coursera/"},{"name":"AI Studies","slug":"AI-Studies","permalink":"https://jmj3047.github.io/tags/AI-Studies/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}]},{"title":"Supervised Machine Learning, Regression and Classification","slug":"ML_Part1","date":"2023-03-25T15:00:00.000Z","updated":"2023-04-18T03:38:12.136Z","comments":true,"path":"2023/03/26/ML_Part1/","link":"","permalink":"https://jmj3047.github.io/2023/03/26/ML_Part1/","excerpt":"","text":"Course Lecture 1 in Machine Learning Course Regression with multiple input variables1. vectorization Many CPUs have â€œvectorâ€ or â€œSIMDâ€ instruction sets which apply the same operation simultaneously to two, four, or more pieces of data. Modern x86 chips have the SSE instructions, many PPC chips have the â€œAltivecâ€ instructions, and even some ARM chips have a vector instruction set, called NEON. â€œVectorizationâ€ (simplified) is the process of rewriting a loop so that instead of processing a single element of an array N times, it processes (say) 4 elements of the array simultaneously N&#x2F;4 times. I chose 4 because itâ€™s what modern hardware is most likely to directly support for 32-bit floats or ints. The difference between vectorization and loop unrolling: Consider the following very simple loop that adds the elements of two arrays and stores the results to a third array. 12for (int i=0; i&lt;16; ++i) C[i] = A[i] + B[i]; Unrolling this loop would transform it into something like this: 123456for (int i=0; i&lt;16; i+=4) &#123; C[i] = A[i] + B[i]; C[i+1] = A[i+1] + B[i+1]; C[i+2] = A[i+2] + B[i+2]; C[i+3] = A[i+3] + B[i+3];&#125; Vectorizing it, on the other hand, produces something like this: 12for (int i=0; i&lt;16; i+=4) addFourThingsAtOnceAndStoreResult(&amp;C[i], &amp;A[i], &amp;B[i]); Where â€œaddFourThingsAtOnceAndStoreResultâ€ is a placeholder for whatever intrinsic(s) your compiler uses to specify vector instructions. 2. feature scaling Feature scaling is a method used to normalize the range of independent variables or features of data. It is generally performed during the data preprocessing step. It is important because it can help machine learning algorithms work properly and converge faster. There are several methods for feature scaling, including rescaling (min-max normalization), mean normalization, and standardization (Z-score normalization) Rescaling (min-max normalization): This method rescales the range of features to a new range, such as [0, 1] or [-1, 1]. The general formula for a min-max of [0, 1] is given as: x&#39; = (x - min) / (max - min) where x is an original value and xâ€™ is the normalized value. Mean normalization: This method involves subtracting the mean of a feature from each data point and then dividing by the range (max - min) or standard deviation of that feature. Standardization (Z-score normalization): This method involves subtracting the mean of a feature from each data point and then dividing by the standard deviation of that feature. This results in each feature having a mean of 0 and a standard deviation of 1. 3. feature engineering(audio data) Feature engineering in audio data involves using domain knowledge to extract relevant features from raw audio data. This can include techniques such as converting audio files into spectrograms, which produce a high-dimensional space of data that can be further reduced by applying a convolutional neural network model. Spectrogram generation: This involves converting audio files into spectrograms, which produce a high-dimensional space of data that can be further reduced by applying a convolutional neural network model. Time series analysis: This involves analyzing audio data as a set of time series and extracting relevant features from it. Sound engineering: This involves using domain knowledge from the field of sound engineering to extract relevant features from audio data. 4. polynomial regression(ë‹¤í•­ íšŒê·€) Polynomial regression is a form of regression analysis in which the relationship between the independent variable x and the dependent variable y is modeled as an nth degree polynomial in x. Polynomial regression fits a nonlinear relationship between the value of x and the corresponding conditional mean of y, denoted E(y|x). Although polynomial regression fits a nonlinear model to the data, as a statistical estimation problem it is linear, in the sense that the regression function E(y|x) is linear in the unknown parameters that are estimated from the data. example Suppose we have a dataset with the following x and y values: x = [1, 2, 3, 4, 5] y = [2, 6, 12, 20, 30] If we plot these points on a graph, we can see that the relationship between x and y is not linear. However, we can use polynomial regression to fit a curve to these points. First, we need to decide on the degree of the polynomial. In this case, letâ€™s use a second-degree polynomial (a quadratic equation). This means our model will have the form: y = a * x^2 + b * x + c, where a, b, and c are coefficients that we need to estimate from the data. We can use the polyfit function from the numpy library to estimate these coefficients: 123456import numpy as npx = np.array([1, 2, 3, 4, 5])y = np.array([2, 6, 12, 20, 30])coefficients = np.polyfit(x, y, deg=2) This will return an array of coefficients [a, b, c]. In this case, the coefficients are [1.0, -1.7763568394002505e-15, 1.0]. Now that we have the coefficients of our polynomial model, we can use it to make predictions for new x values. For example: 12x_new = 6y_new = coefficients[0] * x_new**2 + coefficients[1] * x_new + coefficients[2] This will give us a predicted y value of **42**for x=6 Classification Classification and Logistic Regression Cost function Gradient descent Overfitting Reference https://stackoverflow.com/questions/1422149/what-is-vectorization https://en.wikipedia.org/wiki/Feature_scaling https://scikit-learn.org/stable/auto_examples&#x2F;preprocessing&#x2F;plot_scaling_importance.html https://towardsdatascience.com/what-is-feature-scaling-why-is-it-important-in-machine-learning-2854ae877048 https://bkshin.tistory.com/entry/ë¨¸ì‹ ëŸ¬ë‹-8-Feature-Scaling-Feature-Selection https://link.springer.com/chapter/10.1007/978-1-4842-8925-9_9 https://en.wikipedia.org/wiki/Polynomial_regression","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Coursera","slug":"Coursera","permalink":"https://jmj3047.github.io/tags/Coursera/"},{"name":"AI Studies","slug":"AI-Studies","permalink":"https://jmj3047.github.io/tags/AI-Studies/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}]},{"title":"BQMLë¡œ ê²Œì„ì‚¬ ì£¼ê°€ ì˜ˆì¸¡í•˜ê¸°","slug":"BQML_Stock_Predict","date":"2023-03-19T15:00:00.000Z","updated":"2023-03-27T10:45:16.078Z","comments":true,"path":"2023/03/20/BQML_Stock_Predict/","link":"","permalink":"https://jmj3047.github.io/2023/03/20/BQML_Stock_Predict/","excerpt":"","text":"ê°œìš” ë°ì´í„°ë¡œ BQMLì„ í†µí•´ì„œ ì£¼ê°€ ì˜ˆì¸¡ì„ í•´ë³´ì ë„¥ìŠ¨, ì»´íˆ¬ìŠ¤, ë„·ë§ˆë¸”, nc ì†Œí”„íŠ¸ì˜ ì£¼ê°€ë¥¼ ì˜ˆì¸¡ 5ë…„ë™ì•ˆ ë°ì´í„°ë¡œ í•™ìŠµí•˜ê³  ë„¥ìŠ¨ ê²Œì„ì‚¬ì˜ 2023-03ì›”ë‹¬ì˜ ì£¼ê°€ ì˜ˆì¸¡ì„ ì‹œí–‰í•¨ ë°ì´í„° íë¦„: API í¬ë¡¤ë§ â†’ ë¹…ì¿¼ë¦¬, ë¹…ì¿¼ë¦¬ML â†’ Looker Studio ëª©ì  5ë…„ì¹˜ ì£¼ê°€ ë°ì´í„°ë¥¼ í™œìš©íŒ ë„¥ìŠ¨ ê²Œì„ì¦ˆì˜ ì£¼ê°€ ë“±ë½ ì˜ˆì¸¡ê³¼ ê²½ìŸ 3ì‚¬ì™€ì˜ ë¹„êµ BQML ë§Œìœ¼ë¡œ ê´œì°®ì€ ê²°ê³¼ê°€ ë‚˜ì˜¬ìˆ˜ ìˆëŠ”ì§€ í™•ì¸ ëª¨ë¸ë“¤ ë¹„êµë¥¼ í•´ë³´ë©° ì£¼ì œì— ì–´ë–¤ ëª¨ë¸ì´ ì í•©í• ì§€ í™•ì¸ ë°ì´í„° ë°ì´í„° í¬ë¡¤ë§ â†’ ë¹…ì¿¼ë¦¬ ì ì¬ í•˜ëŠ” ë°©ë²• ëŒ€ìƒ íšŒì‚¬ ì •ì˜ 2018ë…„ ì´ì „ì— ìƒì¥í•œ ê²Œì„ íšŒì‚¬ ë§¤ì¶œ 1~5ìœ„ ìˆœìœ„ ì¤‘ ìœ„ ì¡°ê±´ì„ ì¶©ì¡±í•˜ëŠ” ê²Œì„ì‚¬ëŠ” ë„¥ìŠ¨, ë„·ë§ˆë¸”, ncsoft ì˜€ìœ¼ë©°, ê·¸ ì™¸ ìˆœìœ„ì˜ ìœ„ ì¡°ê±´ì„ ì¶©ì¡±í•˜ëŠ” ì»´íˆ¬ìŠ¤ê¹Œì§€ í¬í•¨í•˜ì—¬ ì´ 4ê°œ ê²Œì„ì‚¬ ê²Œì„ì‚¬ ì„ íƒ ì´ìœ : ê²½ìŸì‚¬ê°€ ë§ê³ , ë§¤ì¶œ ê¸°ì¤€ìœ¼ë¡œ ìˆœìœ„ê¶Œ íšŒì‚¬ë“¤ì´ ìƒì¥ì„ ë§ì´ í•¨. í”¼ì²˜ ì„¤ëª… 5ë…„ì¹˜ ë°ì´í„°ë¥¼ ê°€ì ¸ ì˜¬ìˆ˜ ìˆëŠ” íŒŒì´ì¬ apiì¤‘ ê°€ì¥ ë§ì€ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ìˆ˜ ìˆëŠ” marcap dataë¥¼ ì‚¬ìš©í•˜ì˜€ìŒ Adj_Close(ìˆ˜ì •ì¢…ê°€) í”¼ì²˜ëŠ” yfinanceì—ì„œë§Œ ìˆì—ˆê³ , ëª¨ë“  ë‚ ì§œì— ëŒ€í•´ì„œ ìˆëŠ”ê±´ ì•„ë‹ˆì§€ë§Œ, ì´ë¥¼ í†µí•´ì„œ ì•¡ë©´ë¶„í•  ëœ ì£¼ê°€ë¥¼ ëª¨ë¸ì— ì œëŒ€ë¡œ ë°˜ì˜í• ìˆ˜ ìˆì–´ì„œ ì¶”ê°€ í•˜ê²Œ ë˜ì—ˆìŒ marcapë°ì´í„° ìˆ˜ì§‘ ë‹¹ì‹œ 2ì›” ë§ê¹Œì§€ë§Œ ìˆ˜ì§‘ì„ í•˜ì˜€ê³  ì´ë¡œ ì¸í•´ 3ì›” ì˜ˆì¸¡ ë°ì´í„°ë¥¼ finance data readerë¡œ ì§„í–‰í•¨ ml_label: í›ˆë ¨ì„ ìˆ˜ì›”í•˜ê²Œ í•˜ê¸° ìœ„í•´ ì„ì˜ë¡œ ë§Œë“¦, volumeì„ ê¸°ì¤€ìœ¼ë¡œ training:evaluation:prediction &#x3D; 8:1:1ë¡œ ë‚˜ëˆ„ê³  prediction ë°ì´í„°ëŠ” 3ì›” ë°ì´í„° ê¹Œì§€ í¬í•¨ë˜ì–´ ìˆìŒ. ë°ì´í„° ê°œìˆ˜ marcap + yfinance ì˜ ë°ì´í„°ë¥¼ 8:1:1ë¡œ ë‚˜ëˆ„ê³  ì˜ˆì¸¡ ë°ì´í„°ì— finance data readerì—ì„œ 2023-03 ê¸°ê°„ë™ì•ˆì˜ ë°ì´í„°ë¥¼ í•©ì³ì„œ í•™ìŠµì„ ì§„í–‰ ì „ì¼ëŒ€ë¹„ ë“±ë½ë¥ ë¡œ ì˜ˆì¸¡ì„ ì§„í–‰í•˜ì˜€ê³  ì •í™•í•œ ìˆ˜ì¹˜ê°€ ì•„ë‹Œ ì–‘ìˆ˜ë©´ ì˜¤ë¦„, ìŒìˆ˜ë©´ ë‚´ë¦¼ìœ¼ë¡œ í™˜ì‚°í•˜ì—¬ ì˜ˆì¸¡ì˜ ì •í™•ë„ë¥¼ íŒë³„ ë°ì´í„° ê¸°ê°„ marcap: 2018-01-02 ~ 2023-02-28 yfinance: 2018-01-02 ~ 2023-02-28 finance data reader: 2023-03-02 ~ 2023-03-21 ëª¨ë¸ ì„ íƒ ë° ë¹„êµ Big Query MLì—ì„œ ì¶”ì²œí•˜ëŠ” ì£¼ì‹ ë°ì´í„° ì˜ˆì¸¡ ëª¨ë¸ë¡œëŠ” Linear Regression, Boosted Trees Regressor, Auto ML Table Regressor, DNN Regressor, Wide &amp; Deep Regressor ì´ ìˆìŒ ì´ ë°ì´í„°ê°€ ë§Œê°œë¥¼ ë„˜ì§€ ì•Šì„ì •ë„ë¡œ ì ê³ (5104í–‰), ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ì§€ ì•ŠëŠ” Linear Regression, Boosted Trees Regression ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡ ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ Linear Regressionì´ ëª¨ë¸ í•™ìŠµ ì†ë„ëŠ” ì ê²Œ ê±¸ë¦¬ë‚˜ ì˜¤ë¥˜ë‚˜ ê³¼ì í•© ë©´ì—ì„œ Boosted Trees Regressionì˜ ê²°ê³¼ê°€ í›¨ì”¬ ë‚˜ìŒ ì‹¤ì œ ë“±ë½ì´ í¬ê²Œ ì˜¤ë¥´ê±°ë‚˜ ë‚´ë ¤ê°ˆë•Œì˜ ê°’ì„ Linear Regression ëª¨ë¸ ë³´ë‹¤ Boosted Trees Regression ëª¨ë¸ì´ ë” ì˜ ì˜ˆì¸¡í•¨ Model Evaluation ê°’ ì§€í‘œ ì„¤ëª… ì˜ˆì¸¡ ml_ labelì´ predictionìœ¼ë¡œ ì±…ì •ëœ ë°ì´í„°ë¥¼ ì˜ˆì¸¡í•œ ê²°ê³¼ì™€ ì‹¤ì œ Chages Ratio ê°’ì„ ë¹„êµ ì •í™•í•œ ìˆ˜ì¹˜ë¥¼ ë¹„êµí•˜ëŠ”ê²ƒì´ ì•„ë‹Œ ë“±ë½ìœ¼ë¡œ ë¹„êµ -&gt; ë¹„ìœ¨ì´ ì–‘ìˆ˜ë¼ë©´ ê°’ì´ ì˜¤ë¥¸ê²ƒ, ìŒìˆ˜ë¼ë©´ ë‚´ë¦°ê²ƒìœ¼ë¡œ ê°„ì£¼ feature importance ì‹œê°í™” ê²°ë¡  ì• ì´ˆì— ë°ì´í„°ë¶€í„° ì˜ëª» ì„¤ê³„ê°€ ëœ ì‹¤í—˜ ì „ë‚  ë°ì´í„°ë¥¼ ê°–ê³  ë‹¤ìŒë‚ ì„ ì˜ˆì¸¡ í•´ì•¼ í•˜ëŠ”ë° ë‹¹ì¼ ë°ì´í„°ë¡œ ë‹¹ì¼ì„ ì˜ˆì¸¡í•´ ë²„ë¦¼ â†’ ì•ˆì •í™•í•œê²Œ ë” ì´ìƒí•œê²ƒ Reference BigQuery MLì„ ì‚¬ìš©í•˜ì—¬ í­ê·„ ì²´ì¤‘ ì˜ˆì¸¡ Mean_Absolute_Error MAE,MSE,RMSE R2_Explained_Variance_Score","categories":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"Big Query","slug":"Big-Query","permalink":"https://jmj3047.github.io/tags/Big-Query/"},{"name":"BigQueryML","slug":"BigQueryML","permalink":"https://jmj3047.github.io/tags/BigQueryML/"},{"name":"Looker Studio","slug":"Looker-Studio","permalink":"https://jmj3047.github.io/tags/Looker-Studio/"}],"keywords":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}]},{"title":"Datastream,Â Dataflow,Â BigQuery ML,Â Looker Studioë¥¼ ì‚¬ìš©í•˜ì—¬ ìˆ˜ìš” ì˜ˆì¸¡ ë¹Œë“œ ë° ì‹œê°í™”","slug":"Dataflow_BQML_Looker","date":"2023-03-12T15:00:00.000Z","updated":"2023-03-19T07:02:20.896Z","comments":true,"path":"2023/03/13/Dataflow_BQML_Looker/","link":"","permalink":"https://jmj3047.github.io/2023/03/13/Dataflow_BQML_Looker/","excerpt":"","text":"ê°œìš” ì´ ë…¸íŠ¸ë¶ì˜ ì˜ˆì œë¡œ ì‹¤ìŠµ ì§„í–‰ ë°ì´í„° íë¦„: oracle â†’ pub&#x2F;sub â†’ datastream â†’ dataflow â†’ bigquery â†’ looker ì „ì²´ì ì¸ process compute engineì—ì„œ vm ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì €ì¥ì¥ì†Œ Cloud Storage Bucket ìƒì„± ê°ì²´ ë³€ê²½ì‚¬í•­ì— ëŒ€í•œ ì•Œë¦¼ì„ Pub&#x2F;subìœ¼ë¡œ ì „ì†¡í•˜ë„ë¡ bucket êµ¬ì„± Data streamì„ ë§Œë“¤ì–´ google cloud storageì— ì˜¤ë¼í´ ë°ì´í„° ë³µì œ DataFlowë¡œ ë³µì œëœ ë°ì´í„°ë¥¼ jsoníŒŒì¼ë¡œ ë¹…ì¿¼ë¦¬ì— ì ì¬ ë¹…ì¿¼ë¦¬ë¡œ ë°ì´í„° ë¶„ì„ ë° ML ë£¨ì»¤ë¡œ ì‹œê°í™” ë³¸ í¬ìŠ¤íŠ¸ëŠ” ìœ„ ì „ì²´ í”„ë¡œì„¸ìŠ¤ì—ì„œ 5~6ë²ˆì— í•´ë‹¹í•˜ëŠ” ë‚´ìš©(ì´ì „ ë‚´ìš© í™•ì¸) DataFlowë¡œ ë³µì œëœ ë°ì´í„°ë¥¼ jsoníŒŒì¼ë¡œ ë³€í™˜í•˜ì—¬ Bigquery ì ì¬ í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë³´ì BQMLì˜ ARIMA_PLUS ëŒ€í•´ì„œ ì•Œì•„ë³´ì ë£¨ì»¤ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  ë£¨ì»¤ ìŠ¤íŠœë””ì˜¤ë¡œ ì‹œê°í™” ì‚¬ë¡€ì—ì„œ í™œìš©ëœ DataFlow Dataflow Datastream to BigQuery ìŠ¤íŠ¸ë¦¬ë° í…œí”Œë¦¿ì— ë°°í¬í•˜ì—¬ Datastreamì—ì„œ ìº¡ì²˜í•œ ë³€ê²½ì‚¬í•­ì„ BigQueryì— ë³µì œ UDFë¥¼ ë§Œë“¤ê³  ì‚¬ìš©í•˜ì—¬ ì´ í…œí”Œë¦¿ì˜ ê¸°ëŠ¥ì„ í™•ì¥ ìˆ˜ì‹  ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” UDF ë§Œë“¤ê¸° UDFë¥¼ ë§Œë“¤ì–´ backfillëœ ë°ì´í„°ì™€ ëª¨ë“  ìƒˆ ìˆ˜ì‹  ë°ì´í„°ì—ì„œ ë‹¤ìŒ ì‘ì—…ì„ ìˆ˜í–‰ backfill: ë°ì´í„° íŒŒì´í”„ë¼ì¸ì„ ìš´ìš©í• ë•Œ ì´ë¯¸ ì§€ë‚œ ë‚ ì§œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¬ì²˜ë¦¬í•˜ëŠ” ì‘ì—…, ë©”ìš°ëŠ” ì‘ì—…, ë²„ê·¸ê°€ ìˆê±°ë‚˜ ì–´ë–¤ ì´ìœ ë¡œ ë¡œì§ì´ ë³€ê²½ëì„ë•Œ ì „ì²´ ë°ì´í„°ë¥¼ ìƒˆë¡œ ë§ì•„ì£¼ì–´ì•¼ í• ë•Œ ì»¬ëŸ¼ ë“±ì˜ ë©”íƒ€ ë°ì´í„°ê°€ ë³€ê²½ë˜ì—ˆì„ ë•Œ ì´ë¥¼ ë°˜ì˜í•˜ê¸° ìœ„í•œ append ì„±ì˜ ì‘ì—…ì´ í•„ìš”í• ë•Œ ê³ ê° ê²°ì œ ìˆ˜ë‹¨ê³¼ ê°™ì€ ë¯¼ê°í•œ ì •ë³´ë¥¼ ìˆ˜ì • ë°ì´í„° ê³„ë³´ì™€ ê²€ìƒ‰ì„ ìœ„í•´ Oracle ì†ŒìŠ¤ í…Œì´ë¸”ì„ BigQueryì— ì¶”ê°€ ì´ ë¡œì§ì€ Datastreamì—ì„œ ìƒì„±ëœ JSON íŒŒì¼ì„ ì…ë ¥ ë§¤ê°œë³€ìˆ˜ë¡œ ì‚¬ìš©í•˜ëŠ” ìë°”ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ì—ì„œ ìº¡ì²˜ë¨ Cloud Shell ì„¸ì…˜ì—ì„œ ë‹¤ìŒ ì½”ë“œë¥¼ ë³µì‚¬í•˜ì—¬ retail_transform.js íŒŒì¼ì— ì €ì¥ Oracleì—ì„œ ì¶”ì¶œí•œ json íŒŒì¼ì„ ì•”í˜¸í™” í•´ì„œ ìƒˆë¡œìš´ json íŒŒì¼ì„ ìƒì„±í•¨. 123456789101112function process(inJson) &#123; var obj = JSON.parse(inJson), includePubsubMessage = obj.data &amp;&amp; obj.attributes, data = includePubsubMessage ? obj.data : obj; data.PAYMENT_METHOD = data.PAYMENT_METHOD.split(&#x27;:&#x27;)[0].concat(&quot;XXX&quot;); data.ORACLE_SOURCE = data._metadata_schema.concat(&#x27;.&#x27;, data._metadata_table); return JSON.stringify(obj);&#125; retail_transform.js fileì„ ì €ì¥í•  Cloud Storage ë²„í‚·ì„ ë§Œë“  í›„ ìë°”ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ì„ ìƒˆë¡œ ë§Œë“  ë²„í‚·ì— ì—…ë¡œë“œí•©ë‹ˆë‹¤. ìœ„ì—ì„œ ìƒì„±ëœ json íŒŒì¼ì„ ìƒˆë¡œìš´ ë²„ì¼“ì— ì €ì¥í•¨ 1234gsutil mb gs://js-$&#123;BUCKET_NAME&#125;gsutil cp retail_transform.js \\gs://js-$&#123;BUCKET_NAME&#125;/utils/retail_transform.js Dataflow ì‘ì—… ë§Œë“¤ê¸° Cloud Shellì—ì„œ ë°ë“œ ë ˆí„° í(DLQ) ë²„í‚·ì„ ë§Œë“ ë‹¤: ì´ ë²„í‚·ì€ DataFlowì—ì„œ ì‚¬ìš©ë¨ dead-letter-queue: í•˜ë‚˜ ì´ìƒì˜ Source Queueê°€ ì„±ê³µì ìœ¼ë¡œ ì»¨ìŠ˜ë˜ì§€ ëª»í•œ ë©”ì„¸ì§€ë“¤ì„ ì¬ì „ì†¡í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•˜ëŠ” ë³„ë„ì˜ í. DLQì— ìŒ“ì¸ ë©”ì„¸ì§€ë“¤ì„ ë³´ë©´ ì™œ ì´ ë©”ì„¸ì§€ë“¤ì´ ì»¨ìŠˆë¨¸ì— ì˜í•´ ì²˜ë¦¬ë˜ì§€ ëª»í–ˆëŠ”ì§€ë¥¼ ì•Œ ìˆ˜ ìˆë‹¤. 1gsutil mb gs://dlq-$&#123;BUCKET_NAME&#125; Dataflow ì‹¤í–‰ì— í•„ìš”í•œ ì„œë¹„ìŠ¤ ê³„ì •ì„ ë§Œë“¤ê³  ê³„ì •ì„ Dataflow Worker, Dataflow Admin, Pub/Sub Admin, BigQuery Data Editor, BigQuery Job User, Datastream Admin ì—­í• ì— í• ë‹¹ 1234567891011121314151617181920212223242526272829gcloud iam service-accounts create df-tutorialgcloud projects add-iam-policy-binding $&#123;PROJECT_ID&#125; \\--member=&quot;serviceAccount:df-tutorial@$&#123;PROJECT_ID&#125;.iam.gserviceaccount.com&quot; \\--role=&quot;roles/dataflow.admin&quot;gcloud projects add-iam-policy-binding $&#123;PROJECT_ID&#125; \\--member=&quot;serviceAccount:df-tutorial@$&#123;PROJECT_ID&#125;.iam.gserviceaccount.com&quot; \\--role=&quot;roles/dataflow.worker&quot;gcloud projects add-iam-policy-binding $&#123;PROJECT_ID&#125; \\--member=&quot;serviceAccount:df-tutorial@$&#123;PROJECT_ID&#125;.iam.gserviceaccount.com&quot; \\--role=&quot;roles/pubsub.admin&quot;gcloud projects add-iam-policy-binding $&#123;PROJECT_ID&#125; \\--member=&quot;serviceAccount:df-tutorial@$&#123;PROJECT_ID&#125;.iam.gserviceaccount.com&quot; \\--role=&quot;roles/bigquery.dataEditor&quot;gcloud projects add-iam-policy-binding $&#123;PROJECT_ID&#125; \\--member=&quot;serviceAccount:df-tutorial@$&#123;PROJECT_ID&#125;.iam.gserviceaccount.com&quot; \\--role=&quot;roles/bigquery.jobUser&quot;gcloud projects add-iam-policy-binding $&#123;PROJECT_ID&#125; \\--member=&quot;serviceAccount:df-tutorial@$&#123;PROJECT_ID&#125;.iam.gserviceaccount.com&quot; \\--role=&quot;roles/datastream.admin&quot;gcloud projects add-iam-policy-binding $&#123;PROJECT_ID&#125; \\--member=&quot;serviceAccount:df-tutorial@$&#123;PROJECT_ID&#125;.iam.gserviceaccount.com&quot; \\--role=&quot;roles/storage.admin&quot; ìë™ í™•ì¥ì´ ì‚¬ìš© ì„¤ì •ë˜ë©´ Dataflow VMì´ TCP í¬íŠ¸ 12345 ë° 12346ì—ì„œ ë„¤íŠ¸ì›Œí¬ íŠ¸ë˜í”½ê³¼ í†µì‹ í•˜ê³  ì „ì†¡ ë° ìˆ˜ì‹ í•  ìˆ˜ ìˆë„ë¡ ë°©í™”ë²½ ì´ê·¸ë ˆìŠ¤ ê·œì¹™ì„ ë§Œë“ ë‹¤: VMë¼ë¦¬ í†µì‹ ì´ ê°€ëŠ¥í•˜ê²Œë” í•˜ëŠ” ì‘ì—… 12345678gcloud compute firewall-rules create fw-allow-inter-dataflow-comm \\--action=allow \\--direction=ingress \\--network=GCP_NETWORK_NAME \\--target-tags=dataflow \\--source-tags=dataflow \\--priority=0 \\--rules tcp:12345-12346 Dataflow ì‘ì—…ì„ ë§Œë“¤ê³  ì‹¤í–‰ â†’ Dataflow ì½˜ì†”ì„ í™•ì¸í•˜ì—¬ ìƒˆ ìŠ¤íŠ¸ë¦¬ë° ì‘ì—…ì´ ì‹œì‘ë˜ì—ˆëŠ”ì§€ í™•ì¸ 123456789101112131415export REGION=us-central1gcloud dataflow flex-template run orders-cdc-template --region $&#123;REGION&#125; \\--template-file-gcs-location &quot;gs://dataflow-templates/latest/flex/Cloud_Datastream_to_BigQuery&quot; \\--service-account-email &quot;df-tutorial@$&#123;PROJECT_ID&#125;.iam.gserviceaccount.com&quot; \\--parameters \\inputFilePattern=&quot;gs://$&#123;BUCKET_NAME&#125;/&quot;,\\gcsPubSubSubscription=&quot;projects/$&#123;PROJECT_ID&#125;/subscriptions/oracle_retail_sub&quot;,\\inputFileFormat=&quot;json&quot;,\\outputStagingDatasetTemplate=&quot;retail&quot;,\\outputDatasetTemplate=&quot;retail&quot;,\\deadLetterQueueDirectory=&quot;gs://dlq-$&#123;BUCKET_NAME&#125;&quot;,\\autoscalingAlgorithm=&quot;THROUGHPUT_BASED&quot;,\\mergeFrequencyMinutes=1,\\javascriptTextTransformGcsPath=&quot;gs://js-$&#123;BUCKET_NAME&#125;/utils/retail_transform.js&quot;,\\javascriptTextTransformFunctionName=&quot;process&quot; Cloud Shellì—ì„œ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì—¬ Datastream ìŠ¤íŠ¸ë¦¼ì„ ì‹œì‘ 12gcloud datastream streams update oracle-cdc \\--location=us-central1 --state=RUNNING --update-mask=state DataStream ìŠ¤íŠ¸ë¦¼ ìƒíƒœë¥¼ í™•ì¸ 12gcloud datastream streams list \\--location=us-central1 ìƒíƒœê°€ Runningìœ¼ë¡œ í‘œì‹œë˜ëŠ”ì§€ í™•ì¸. ìƒˆ ìƒíƒœê°’ì´ ë°˜ì˜ë˜ê¸°ê¹Œì§€ ëª‡ ì´ˆ ì •ë„ ê±¸ë¦´ ìˆ˜ ìˆìŒ. Datastream ì½˜ì†”ì„ í™•ì¸í•˜ì—¬ ORDERS í…Œì´ë¸” ë°±í•„ ì§„í–‰ ìƒí™©ì„ í™•ì¸ ì´ taskëŠ” ì´ˆê¸° ë¡œë“œ ì´ë¯€ë¡œ Datastreamì€ ORDERS ê°ì²´ì—ì„œ ì½ìŒ. ìŠ¤íŠ¸ë¦¼ ìƒì„± ì¤‘ì— ì§€ì •í•œ Cloud Storage ë²„í‚·ì— ìˆëŠ” JSON íŒŒì¼ì— ëª¨ë“  ë ˆì½”ë“œë¥¼ ì“´ë‹¤. ë°±í•„ íƒœìŠ¤í¬ê°€ ì™„ë£Œë˜ëŠ”ë° ì•½ 10ë¶„ ì •ë„ ê±¸ë¦¼. BigQueryì—ì„œ ë°ì´í„° ë¶„ì„ ë°ì´í„° ì„¸íŠ¸ì—ì„œ ë‹¤ìŒ ìƒˆ í…Œì´ë¸” ë‘ ê°œê°€ Dataflow ì‘ì—…ìœ¼ë¡œ ìƒì„± ORDERS: ì´ ì¶œë ¥ í…Œì´ë¸”ì€ Oracle í…Œì´ë¸” ë³µì œë³¸ì´ë©° Dataflow í…œí”Œë¦¿ì˜ ì¼ë¶€ë¡œ ë°ì´í„°ì— ì ìš©ëœ ë³€í™˜ì„ í¬í•¨ ORDERS_log: ì´ ìŠ¤í…Œì´ì§• í…Œì´ë¸”ì€ Oracle ì†ŒìŠ¤ì˜ ëª¨ë“  ë³€ê²½ì‚¬í•­ì„ ê¸°ë¡. í…Œì´ë¸”ì€ íŒŒí‹°ì…˜ìœ¼ë¡œ ë‚˜ëˆ ì§€ê³  ë³€ê²½ì‚¬í•­ì´ ì—…ë°ì´íŠ¸, ì‚½ì… ë˜ëŠ” ì‚­ì œì¸ì§€ ì—¬ë¶€ì™€ ê°™ì€ ì¼ë¶€ ë©”íƒ€ë°ì´í„° ë³€ê²½ ì •ë³´ì™€ í•¨ê»˜ ì—…ë°ì´íŠ¸ëœ ë ˆì½”ë“œë¥¼ ì €ì¥ BigQuery MLì—ì„œ ìˆ˜ìš” ì˜ˆì¸¡ ëª¨ë¸ ë¹Œë“œ BigQuery MLì€ ARIMA_PLUS ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ìˆ˜ìš” ì˜ˆì¸¡ ëª¨ë¸ì„ ë¹Œë“œí•˜ê³  ë°°í¬í•˜ëŠ” ë° ì‚¬ìš©ë  ìˆ˜ ìˆìŒ. ì´ ì„¹ì…˜ì—ì„œëŠ” BigQuery MLì„ ì‚¬ìš©í•˜ì—¬ ë§¤ì¥ ë‚´ ì œí’ˆ ìˆ˜ìš”ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì„ ë¹Œë“œ. í•™ìŠµ ë°ì´í„° ì¤€ë¹„ ë°±í•„í•œ ë°ì´í„°ì˜ ìƒ˜í”Œì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµ ì´ ê²½ìš° 1ë…„ ë™ì•ˆì˜ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. í•™ìŠµ ë°ì´í„°ì—ì„œëŠ” ë‹¤ìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì œí’ˆ ì´ë¦„(product_name) íŒë§¤ëœ ê° ì œí’ˆì˜ ë‹¨ìœ„ ìˆ˜ëŸ‰(total_sold) ì‹œê°„ë‹¹ íŒë§¤ëœ ì œí’ˆ ìˆ˜(hourly_timestamp) BigQueryì—ì„œ ë‹¤ìŒ SQLì„ ì‹¤í–‰í•˜ì—¬ í•™ìŠµ ë°ì´í„°ë¥¼ ë§Œë“¤ê³  training_dataë¼ëŠ” ìƒˆ í…Œì´ë¸”ì— ì €ì¥ 1234567891011CREATE OR REPLACE TABLE `retail.training_data`AS SELECT TIMESTAMP_TRUNC(time_of_sale, HOUR) as hourly_timestamp, product_name, SUM(quantity) AS total_sold FROM `retail.ORDERS` GROUP BY hourly_timestamp, product_name HAVING hourly_timestamp BETWEEN TIMESTAMP_TRUNC(&#x27;2021-11-22&#x27;, HOUR) ANDTIMESTAMP_TRUNC(&#x27;2021-11-28&#x27;, HOUR)ORDER BY hourly_timestamp ì˜ˆì¸¡ ìˆ˜ìš” BigQueryì—ì„œ ë‹¤ìŒ SQLì„ ì‹¤í–‰í•˜ì—¬ ARIMA_PLUS ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ëŠ” ì‹œê³„ì—´ ëª¨ë¸ì„ ìƒì„± 12345678910111213CREATE OR REPLACE MODEL `retail.arima_plus_model` OPTIONS( MODEL_TYPE=&#x27;ARIMA_PLUS&#x27;, TIME_SERIES_TIMESTAMP_COL=&#x27;hourly_timestamp&#x27;, TIME_SERIES_DATA_COL=&#x27;total_sold&#x27;, TIME_SERIES_ID_COL=&#x27;product_name&#x27; ) ASSELECT hourly_timestamp, product_name, total_soldFROM `retail.training_data` ML.FORECASTí•¨ìˆ˜ëŠ” nì‹œê°„ ë²”ìœ„ì— ê±¸ì³ ì˜ˆìƒë˜ëŠ” ìˆ˜ìš”ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë° ì‚¬ìš©ë¨ ë‹¤ìŒ SQLì„ ì‹¤í–‰í•˜ì—¬ í–¥í›„ 30ì¼ ë™ì•ˆì˜ ìœ ê¸°ë† ë°”ë‚˜ë‚˜ ìˆ˜ìš”ë¥¼ ì˜ˆì¸¡ 1SELECT * FROM ML.FORECAST(MODEL `retail.arima_plus_model`, STRUCT(720 AS horizon)) í•™ìŠµ ë°ì´í„°ëŠ” ì‹œê°„ ë‹¨ìœ„ì´ë¯€ë¡œ ë²”ìœ„ ê°’ì€ ì˜ˆì¸¡ ì‹œ ë™ì¼í•œ ì‹œê°„ ë‹¨ìœ„(ì‹œê°„)ë¥¼ ì‚¬ìš©. 720ì‹œê°„ ë²”ìœ„ ê°’ì€ ë‹¤ìŒ 30ì¼ ë™ì•ˆì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë°˜í™˜ ì´ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” ì†ŒëŸ‰ì˜ ìƒ˜í”Œ ë°ì´í„° ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ëª¨ë¸ì˜ ì •í™•ì„±ì— ëŒ€í•œ ìì„¸í•œ ì¡°ì‚¬ëŠ” ì´ íŠœí† ë¦¬ì–¼ì—ì„œ ë‹¤ë£¨ì§€ ì•ŠìŒ. ì‹œê°í™”í•˜ê¸° BigQueryì—ì„œ ë‹¤ìŒ SQL ì¿¼ë¦¬ë¥¼ ì‹¤í–‰í•˜ì—¬ ìœ ê¸°ë† ë°”ë‚˜ë‚˜ì˜ ì‹¤ì œ íŒë§¤ëŸ‰ê³¼ ì˜ˆìƒ íŒë§¤ëŸ‰ì„ í†µí•©í•˜ëŠ” ë·°ë¥¼ ìƒì„± 12345678910111213141516171819202122232425262728CREATE OR REPLACE VIEW `retail.orders_forecast` AS (SELECTtimestamp,product_name,SUM(forecast_value) AS forecast,SUM(actual_value) AS actualfrom(SELECT TIMESTAMP_TRUNC(TIME_OF_SALE, HOUR) AS timestamp, product_name, SUM(QUANTITY) as actual_value, NULL AS forecast_value FROM `retail.ORDERS` GROUP BY timestamp, product_nameUNION ALLSELECT forecast_timestamp AS timestamp, product_name, NULL AS actual_value, forecast_value, FROM ML.FORECAST(MODEL `retail.arima_plus_model`, STRUCT(720 AS horizon)) ORDER BY timestamp)GROUP BY timestamp, product_nameORDER BY timestamp) ì´ ë·°ë¥¼ ì‚¬ìš©í•˜ë©´ ì‹¤ì œ ë°ì´í„°ì™€ ì˜ˆì¸¡ ë°ì´í„°ë¥¼ íƒìƒ‰í•  ë•Œ Lookerì—ì„œ ê´€ë ¨ ë°ì´í„°ë¥¼ ì¿¼ë¦¬í•  ìˆ˜ ìˆìŒ ë‹¤ìŒ SQLì„ ì‹¤í–‰í•˜ì—¬ ë·°ë¥¼ ê²€ì¦ 1234SELECT * FROM `retail.orders_forecast`WHERE PRODUCT_NAME=&#x27;Bag of Organic Bananas&#x27;AND TIMESTAMP_TRUNC(timestamp, HOUR) BETWEEN TIMESTAMP_TRUNC(&#x27;2021-11-28&#x27;, HOUR) AND TIMESTAMP_TRUNC(&#x27;2021-11-30&#x27;, HOUR)LIMIT 100; ì´ì „ í¬ìŠ¤íŒ…ì˜ â€˜Big Queryë¥¼ ì‚¬ìš©í•˜ì—¬ SQLë¡œ ë°ì´í„°ë¥¼ ì •ì œí•œ í›„ Looker Studioì— SQLë¡œ ì°¨íŠ¸ ë§Œë“¤ê¸°â€™ ì²˜ëŸ¼ sqlì¿¼ë¦¬ë¡œ ë°ì´í„° ì…‹ì„ ìƒì„± 12SELECT * FROM `retail.orders_forecast`WHERE actual IS NOT NULL ê²°ê³¼ í˜ì´ì§€ ë“œë¡­ë‹¤ìš´ ëª©ë¡ì˜ ë‚ ì§œ ë¶€ë¶„ì€ í•¨ìˆ˜ë¥¼ ê±¸ì–´ dateí˜•íƒœë¡œ ì¡°íšŒí• ìˆ˜ ìˆê²Œ í•˜ì˜€ìŒ. Reference https://cloud.google.com/architecture/build-visualize-demand-forecast-prediction-datastream-dataflow-bigqueryml-looker?hl=ko#create-a-dataflow-job https://cloud.google.com/dataflow?hl=ko https://cloud.google.com/dataflow/pricing?hl=ko#shuffle-pricing-details https:&#x2F;&#x2F;velog.io&#x2F;@usaindream&#x2F;Dead-Letter-QueueDLQ https://wookiist.dev/175","categories":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}],"tags":[{"name":"Big Query","slug":"Big-Query","permalink":"https://jmj3047.github.io/tags/Big-Query/"},{"name":"BigQueryML","slug":"BigQueryML","permalink":"https://jmj3047.github.io/tags/BigQueryML/"},{"name":"Looker Studio","slug":"Looker-Studio","permalink":"https://jmj3047.github.io/tags/Looker-Studio/"},{"name":"DataFlow","slug":"DataFlow","permalink":"https://jmj3047.github.io/tags/DataFlow/"}],"keywords":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}]},{"title":"K-Means Clustering(2)","slug":"K-Means_Clustering_2","date":"2023-03-10T15:00:00.000Z","updated":"2023-03-13T11:06:45.805Z","comments":true,"path":"2023/03/11/K-Means_Clustering_2/","link":"","permalink":"https://jmj3047.github.io/2023/03/11/K-Means_Clustering_2/","excerpt":"","text":"ê°œìš” ì´ì „ í¬ìŠ¤íŒ…ì— ì´ì–´ Big Query MLì—ì„œ K-Means Clusteringì—ì„œ kê°’ì„ ê²°ì •í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë³´ì ì´ í¬ìŠ¤íŒ… í•˜ë‹¨ë¶€ì— ëŒ€í•œ ì„¤ëª… Elbow Method ì‚¬ìš©í•˜ê³ ì í•˜ëŠ” í´ëŸ¬ìŠ¤í„° ë²”ìœ„ë¥¼ ì§€ì •í•œë‹¤. ê° í´ëŸ¬ìŠ¤í„°ë¥¼ WCSSë°©ë²•ìœ¼ë¡œ ê³„ì‚°ì„ í•©ë‹ˆë‹¤. WCSSê°’ê³¼ í´ëŸ¬ìŠ¤í„° K ê°¯ìˆ˜ì— ëŒ€í•œ ì»¤ë¸Œì„ ì„ ê·¸ë¦½ë‹ˆë‹¤. ë¾°ì¡±í•˜ê²Œ êµ¬ë¶€ëŸ¬ì§„ ë¶€ë¶„ì´ë‚˜ íŠ¹ì • ì§€ì ì´ íŒ”ì²˜ëŸ¼ êµ½ì–´ì§€ëŠ” ë¶€ë¶„ì„ Kë¡œ ì§€ì •í•©ë‹ˆë‹¤. **Within Cluster Sum of Squares(**WCSS) í´ëŸ¬ìŠ¤í„° ë‚´ ì œê³±í•©(WCSS) í´ëŸ¬ìŠ¤í„° ë‚´ ëª¨ë“  í¬ì¸íŠ¸ì—ì„œ í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ê¹Œì§€ì˜ ì œê³± í‰ê·  ê±°ë¦¬ë¥¼ ì¸¡ì • WCSSë¥¼ ê³„ì‚°í•˜ë ¤ë©´ ë¨¼ì € ì§€ì •ëœ í¬ì¸íŠ¸ì™€ í•´ë‹¹ í¬ì¸íŠ¸ê°€ í• ë‹¹ëœ ì¤‘ì‹¬ ì‚¬ì´ì˜ ìœ í´ë¦¬ë“œ ê±°ë¦¬(ì•„ë˜ ê·¸ë¦¼ ì°¸ì¡°)ë¥¼ ì°¾ëŠ”ë‹¤ í´ëŸ¬ìŠ¤í„°ì˜ ëª¨ë“  í¬ì¸íŠ¸ì— ëŒ€í•´ ì´ í”„ë¡œì„¸ìŠ¤ë¥¼ ë°˜ë³µí•œ ë‹¤ìŒ í´ëŸ¬ìŠ¤í„°ì˜ ê°’ì„ í•©ì‚°í•˜ê³  í¬ì¸íŠ¸ ìˆ˜ë¡œ ë‚˜ëˆˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ ëª¨ë“  í´ëŸ¬ìŠ¤í„°ì˜ í‰ê· ì„ ê³„ì‚°í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ í‰ê·  WCSSê°€ ê³„ì‚°ë©ë‹ˆë‹¤. Davies Bouldin Index(DBI) n &#x3D; cluster ê°œìˆ˜ $c_x$ &#x3D; cluster $x$ì˜ ì¤‘ì‹¬ì  $\\sigma_x$ &#x3D; cluster $x$ë‚´ì˜ ëª¨ë“  ë°ì´í„° ì˜¤ë¸Œì íŠ¸ë¡œ ë¶€í„° ì¤‘ì‹¬ì  $c_x$ê¹Œì§€ ê±°ë¦¬ì˜ í‰ê· ê°’ $d(c_i,c_j)$ &#x3D; ì¤‘ì‹¬ì  $c_i$ì™€ ì¤‘ì‹¬ì  $c_j$ê°„ì˜ ê±°ë¦¬ ë†’ì€ í´ëŸ¬ìŠ¤í„° ë‚´ ìœ ì‚¬ë„ë¥¼ ê°€ì§€ê³  ë‚®ì€ í´ëŸ¬ìŠ¤í„°ê°„ ìœ ì‚¬ë„ë¥¼ ê°€ì§€ëŠ” í´ëŸ¬ìŠ¤í„°ë¥¼ ìƒì„±í•˜ëŠ” í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜ì€ ë‚®ì€ DBIê°’ì„ ê°–ê²Œ ë¨. ì´ ì§€í‘œê°€ ë‚®ì€ í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜ì´ ì¢‹ì€ í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ í‰ê°€ ë¨ ê°„ë‹¨í•œ ì˜ˆì œë¥¼ í†µí•´ ì´í•´í•´ ë³´ì ë‹¤ìŒê³¼ ê°™ì´ ì  4ê°œê°€ ì£¼ì–´ì§€ê³  ì (1,1)ê³¼ ì  (1,3)ì„ ì¤‘ì‹¬ìœ¼ë¡œ í• ë•Œ â†’ ê° ê·¸ë£¹ì˜ ì¤‘ì‹¬ì€ (1,3)ê³¼ (3,3)ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŒ. ë°˜ë©´ ì  (1,1)ê³¼ ì (1,5)ê°€ ì¤‘ì‹¬ì´ ë˜ì—ˆì„ ë•Œ â†’ ê° ê·¸ë£¹ì˜ ì¤‘ì‹¬ì€ (1,2)ì™€ (5,2)ë¡œ ë‚˜íƒ€ë‚¨ ìœ„ì—ì„œ êµ¬í•œ ë‘ ê°€ì§€ ê²½ìš°ë“¤ì— ëŒ€í•´ ê°ê° DBIê°’ì„ êµ¬í•œë‹¤ë©´ ê·¸ë£¹ì˜ ì¤‘ì‹¬ì´ (1,3)ê³¼ (3,3)ì¸ ê²½ìš°: 2 ê·¸ë£¹ì˜ ì¤‘ì‹¬ì´ (1,2)ì™€ (5,2)ì¸ ê²½ìš°: 0.5 ì´ ê²°ê³¼ë¥¼ ë³´ì•˜ì„ë•Œ ê°’ì´ ì‘ì€ í›„ìì˜ ê²½ìš°ê°€ clusterë¥¼ ìì„¸í•˜ êµ¬ë¶„í–ˆë‹¤ê³  ë§í•  ìˆ˜ ìˆìŒ. Reference https://odsc.medium.com/unsupervised-learning-evaluating-clusters-bd47eed175ce https://nicola-ml.tistory.com/66 https://ko.wikipedia.org/wiki/K-í‰ê· _ì•Œê³ ë¦¬ì¦˜ https://elecs.tistory.com/303","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"BigQueryML","slug":"BigQueryML","permalink":"https://jmj3047.github.io/tags/BigQueryML/"},{"name":"Clustering","slug":"Clustering","permalink":"https://jmj3047.github.io/tags/Clustering/"},{"name":"K-Means Clustering","slug":"K-Means-Clustering","permalink":"https://jmj3047.github.io/tags/K-Means-Clustering/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}]},{"title":"BQMLì„ ì´ìš©í•œ ê³ ê° ë¶„ë¥˜","slug":"BQML_Classification","date":"2023-03-09T15:00:00.000Z","updated":"2023-03-19T07:02:05.304Z","comments":true,"path":"2023/03/10/BQML_Classification/","link":"","permalink":"https://jmj3047.github.io/2023/03/10/BQML_Classification/","excerpt":"","text":"ê°œìš” K means clusteringì„ ë¹…ì¿¼ë¦¬ ML(BQML)ì„ ì‚¬ìš©í•˜ì—¬ ê³ ê°ì„ ì„¸ë¶„í™” í•˜ê¸° GA360ì˜ ë°ì´í„°ë¥¼ ë¹…ì¿¼ë¦¬ì— ì ì¬í•´ MLí•™ìŠµí•˜ê¸° íŒŒì´ì¬ì„ ì‚¬ìš©í•˜ì—¬ ë¹…ì¿¼ë¦¬ì™€ ì—°ë™í•˜ê³  ê´€ë ¨ ê·¸ë˜í”„ ì‹œê°í™”í•˜ê¸° ëª©í‘œ êµ¬ê¸€ ë¸Œëœë“œ ìƒí’ˆì„ íŒë§¤í•˜ëŠ” ì‹¤ì œ ì´ì»¤ë¨¸ìŠ¤ ìŠ¤í† ì–´ì¸ êµ¬ê¸€ ë¨¸ì²œë‹¤ì´ìŠ¤ ìŠ¤í† ì–´ì˜ ë‚œë…í™”ëœ GA360 12ê°œì›”(2016ë…„ 8ì›”~2017ë…„ 8ì›”)ì˜ ë°ì´í„°ë¥¼ ê°€ì§€ê³  ê³ ê°ì„ ë¶„ë¥˜ í™˜ê²½ êµ¬ì¶• GCPì™€ ì£¼í”¼í„° ë…¸íŠ¸ë¶ì„ ì—°ê²° configíŒŒì¼ì´ í•„ìš”(ì´ì „í¬ìŠ¤íŠ¸ ì„¤ëª…) PIP install packages and dependencies 123456!pip install google-cloud-bigquery!pip install google-cloud-bigquery-storage!pip install pandas-gbq# Reservation package needed to setup flex slots for flat-rate pricing!pip install google-cloud-bigquery-reservation 123456789101112131415161718192021222324252627282930Requirement already satisfied: google-cloud-bigquery in /opt/homebrew/lib/python3.10/site-packages (3.6.0)Requirement already satisfied: google-cloud-core&lt;3.0.0dev,&gt;=1.6.0 in /opt/homebrew/lib/python3.10/site-packages (from google-cloud-bigquery) (2.3.2)Requirement already satisfied: requests&lt;3.0.0dev,&gt;=2.21.0 in /opt/homebrew/lib/python3.10/site-packages (from google-cloud-bigquery) (2.28.2)Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,&lt;5.0.0dev,&gt;=3.19.5 in /opt/homebrew/lib/python3.10/site-packages (from google-cloud-bigquery) (4.22.1)Requirement already satisfied: grpcio&lt;2.0dev,&gt;=1.47.0 in /opt/homebrew/lib/python3.10/site-packages (from google-cloud-bigquery) (1.51.3)Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,&lt;3.0.0dev,&gt;=1.31.5 in /opt/homebrew/lib/python3.10/site-packages (from google-cloud-bigquery) (2.11.0)Requirement already satisfied: google-resumable-media&lt;3.0dev,&gt;=0.6.0 in /opt/homebrew/lib/python3.10/site-packages (from google-cloud-bigquery) (2.4.1)Requirement already satisfied: packaging&gt;=20.0.0 in /opt/homebrew/lib/python3.10/site-packages (from google-cloud-bigquery) (23.0)Requirement already satisfied: python-dateutil&lt;3.0dev,&gt;=2.7.2 in /opt/homebrew/lib/python3.10/site-packages (from google-cloud-bigquery) (2.8.2)Requirement already satisfied: proto-plus&lt;2.0.0dev,&gt;=1.15.0 in /opt/homebrew/lib/python3.10/site-packages (from google-cloud-bigquery) (1.22.2)Requirement already satisfied: google-auth&lt;3.0dev,&gt;=2.14.1 in /opt/homebrew/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,&lt;3.0.0dev,&gt;=1.31.5-&gt;google-cloud-bigquery) (2.16.1)Requirement already satisfied: googleapis-common-protos&lt;2.0dev,&gt;=1.56.2 in /opt/homebrew/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,&lt;3.0.0dev,&gt;=1.31.5-&gt;google-cloud-bigquery) (1.58.0)Requirement already satisfied: grpcio-status&lt;2.0dev,&gt;=1.33.2 in /opt/homebrew/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,&lt;3.0.0dev,&gt;=1.31.5-&gt;google-cloud-bigquery) (1.51.3)Requirement already satisfied: google-crc32c&lt;2.0dev,&gt;=1.0 in /opt/homebrew/lib/python3.10/site-packages (from google-resumable-media&lt;3.0dev,&gt;=0.6.0-&gt;google-cloud-bigquery) (1.5.0)Requirement already satisfied: six&gt;=1.5 in /opt/homebrew/lib/python3.10/site-packages (from python-dateutil&lt;3.0dev,&gt;=2.7.2-&gt;google-cloud-bigquery) (1.16.0)Requirement already satisfied: idna&lt;4,&gt;=2.5 in /opt/homebrew/lib/python3.10/site-packages (from requests&lt;3.0.0dev,&gt;=2.21.0-&gt;google-cloud-bigquery) (3.4)Requirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /opt/homebrew/lib/python3.10/site-packages (from requests&lt;3.0.0dev,&gt;=2.21.0-&gt;google-cloud-bigquery) (3.0.1)Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /opt/homebrew/lib/python3.10/site-packages (from requests&lt;3.0.0dev,&gt;=2.21.0-&gt;google-cloud-bigquery) (1.26.14)Requirement already satisfied: certifi&gt;=2017.4.17 in /opt/homebrew/lib/python3.10/site-packages (from requests&lt;3.0.0dev,&gt;=2.21.0-&gt;google-cloud-bigquery) (2022.12.7)Requirement already satisfied: pyasn1-modules&gt;=0.2.1 in /opt/homebrew/lib/python3.10/site-packages (from google-auth&lt;3.0dev,&gt;=2.14.1-&gt;google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,&lt;3.0.0dev,&gt;=1.31.5-&gt;google-cloud-bigquery) (0.2.8)Requirement already satisfied: rsa&lt;5,&gt;=3.1.4 in /opt/homebrew/lib/python3.10/site-packages (from google-auth&lt;3.0dev,&gt;=2.14.1-&gt;google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,&lt;3.0.0dev,&gt;=1.31.5-&gt;google-cloud-bigquery) (4.9)Requirement already satisfied: cachetools&lt;6.0,&gt;=2.0.0 in /opt/homebrew/lib/python3.10/site-packages (from google-auth&lt;3.0dev,&gt;=2.14.1-&gt;google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,&lt;3.0.0dev,&gt;=1.31.5-&gt;google-cloud-bigquery) (5.3.0)Requirement already satisfied: pyasn1&lt;0.5.0,&gt;=0.4.6 in /opt/homebrew/lib/python3.10/site-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;3.0dev,&gt;=2.14.1-&gt;google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,&lt;3.0.0dev,&gt;=1.31.5-&gt;google-cloud-bigquery) (0.4.8)Requirement already satisfied: google-cloud-bigquery-storage in /opt/homebrew/lib/python3.10/site-packages (2.19.0)Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,&lt;5.0.0dev,&gt;=3.19.5 in /opt/homebrew/lib/python3.10/site-packages (from google-cloud-bigquery-storage) (4.22.1)...Requirement already satisfied: certifi&gt;=2017.4.17 in /opt/homebrew/lib/python3.10/site-packages (from requests&lt;3.0.0dev,&gt;=2.18.0-&gt;google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,&lt;3.0.0dev,&gt;=1.34.0-&gt;google-cloud-bigquery-reservation) (2022.12.7)Requirement already satisfied: pyasn1&lt;0.5.0,&gt;=0.4.6 in /opt/homebrew/lib/python3.10/site-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;3.0dev,&gt;=2.14.1-&gt;google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,&lt;3.0.0dev,&gt;=1.34.0-&gt;google-cloud-bigquery-reservation) (0.4.8)Installing collected packages: google-cloud-bigquery-reservationSuccessfully installed google-cloud-bigquery-reservation-1.10.0 ì„¤ì¹˜ í›„ ì»¤ë„ ë‹¤ì‹œ ì‹œì‘ 1234# Automatically restart kernel after installsimport IPythonapp = IPython.Application.instance()app.kernel.do_shutdown(True) Project IDì™€ ì¸ì¦ 12345678PROJECT_ID = &quot;your-project-id&quot; REGION = &#x27;US&#x27;DATA_SET_ID = &#x27;bqml_kmeans&#x27; # Ensure you first create a data set in BigQuery# If you have not built the Data Set, the following command will build it for you!bq mk --location=$REGION --dataset $PROJECT_ID:$DATA_SET_ID !gcloud config set project $PROJECT_ID Import libraries and define constants 1234567891011121314151617181920import globfrom google.cloud import bigqueryfrom google.oauth2 import service_accountfrom google.cloud import bigqueryimport numpy as npimport pandas as pdimport pandas_gbqimport matplotlib.pyplot as plt# ì„œë¹„ìŠ¤ ê³„ì • í‚¤ JSON íŒŒì¼ ê²½ë¡œkey_path = glob.glob(&quot;./config/*.json&quot;)[0]# Credentials ê°ì²´ ìƒì„±credentials = service_account.Credentials.from_service_account_file(key_path)project_id=&quot;your-project-id&quot;# GCP í´ë¼ì´ì–¸íŠ¸ ê°ì²´ ìƒì„±pd.set_option(&#x27;display.float_format&#x27;, lambda x: &#x27;%.3f&#x27; % x) # used to display float formatclient = bigquery.Client(credentials = credentials, project = credentials.project_id) ë°ì´í„°ëª¨ë¸ì„ êµ¬ì¶•í•˜ê¸° ì „ì— ì¼ë°˜ì ìœ¼ë¡œ ëª¨ë¸ë§ì„ ìœ„í•´ ì˜ë¯¸ ìˆëŠ” ë°©ì‹ìœ¼ë¡œ ë°ì´í„° ì§‘í•©ì„ ì •ë¦¬, íƒìƒ‰ ë° ì§‘ê³„í•˜ëŠ” ë° ìƒë‹¹í•œ ì‹œê°„ì„ íˆ¬ìí•´ì•¼ í•¨. ì´ í¬ìŠ¤íŠ¸ ëª©ì ìƒ ì´ ë‹¨ê³„ëŠ” BigQuery MLì—ì„œ k-í‰ê· ì„ ì‚¬ìš©í•œ í´ëŸ¬ìŠ¤í„°ë§ì„ ìš°ì„ ì ìœ¼ë¡œ ë³´ì—¬ì£¼ê¸° ìœ„í•´ í‘œì‹œí•˜ì§€ ì•ŠìŒ. GA360, GA4ì°¨ì´ GA360 GAì˜ ìœ ë£Œ ë²„ì „ìœ¼ë¡œ ë¬´ë£Œ ë²„ì „ê³¼ ê°€ì¥ í° ì°¨ì´ì ì€ â€˜ë°ì´í„° ì†Œìœ ê¶Œâ€™ ë°ì´í„° ì†Œìœ ê¶Œì´ êµ¬ê¸€ì¸ ë¬´ë£Œ ë²„ì „ì— ë¹„í•´ GA360ì€ ë°ì´í„° ì†Œìœ ê¶Œì´ ì‚¬ìš©ì ë•Œë¬¸ì— ë°ì´í„° ìƒ˜í”Œë§ì´ ì—†ê³  ë¹…ì¿¼ë¦¬ë¥¼ í†µí•´ Raw Dataë¥¼ ì´ìš©í•  ìˆ˜ ìˆìŒ. ë‹¤ë§Œ ìœ ë£Œì¸ë§Œí¼ ì—°ê°„ 1.5ì–µì˜ ì‚¬ìš©ë£Œë¥¼ ì§€ë¶ˆí•´ì•¼ í•¨. GA4 2019ë…„ì— ìƒˆë¡œ ìƒê¸´ êµ¬ê¸€ ì• ë„ë¦¬í‹±ìŠ¤ë¡œ WEBê³¼ APPì„ ì‹¬ë¦¬ìŠ¤í•˜ê²Œ ë³´ê¸° ìœ„í•œ GA 360ì²˜ëŸ¼ ìœ ë£Œ ë²„ì „ì„ ì“°ì§€ ì•Šì•„ë„ ë¹…ì¿¼ë¦¬ë¡œ ë°ì´í„°ë¥¼ ë³´ë‚´ì£¼ê¸° ë•Œë¬¸ì— Raw Dataë¥¼ ì¿¼ë¦¬ ë¹„ìš©ë§Œ ë‚´ê³  ì‚¬ìš©í• ìˆ˜ ìˆìŒ. GA4 UIì—ì„  ì†ë„ê°€ ë§¤ìš° ë¹ ë¥´ì§€ë§Œ GAUIì— ë¹„í•´ GA4 UIì—ì„œëŠ” ë§ì€ ê²ƒìœ¼ë¥´ ë³´ì˜‚ì¥ ì•Šê³  ë°ì´í„° ë¶„ì„ì„ ìœ„í•´ ì œëŒ€ë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ë¹…ì¿¼ë¦¬ì— ìµìˆ™ì• í– í•¨. í•©ì„± ë°ì´í„° êµ¬ì¶• ìµœì¢… ëª©í‘œëŠ” ì˜¨ë¼ì¸(GA360) ë° ì˜¤í”„ë¼ì¸(CRM) ë°ì´í„°ë¥¼ ëª¨ë‘ ì‚¬ìš©í•˜ëŠ” ê²ƒ. ìì²´ CRM ë°ì´í„°ë¥¼ ì‚¬ìš©í•  ìˆ˜ë„ ìˆì§€ë§Œ, ì´ ê²½ìš°ì—ëŠ” ë³´ì—¬ì¤„ CRM ë°ì´í„°ê°€ ì—†ìœ¼ë¯€ë¡œ ëŒ€ì‹  í•©ì„± ë°ì´í„°ë¥¼ ìƒì„±: ì˜ˆìƒ ê°€êµ¬ ì†Œë“(House Hold income, hhi)ê³¼ ì„±ë³„ ì´ë¥¼ ìœ„í•´ ì „ì²´ ë°©ë¬¸ì IDë¥¼ í•´ì‹œí•˜ê³  í•´ì‹œì˜ ë§ˆì§€ë§‰ ìˆ«ìë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°„ë‹¨í•œ ê·œì¹™ì„ êµ¬ì¶•í•©ë‹ˆë‹¤. hash: ì„ì˜ì˜ ê¸¸ì´ë¥¼ ê°–ëŠ” ì„ì˜ì˜ ë°ì´í„°ë¥¼ ê³ ì •ëœ ê¸¸ì´ì˜ ë°ì´í„°ë¡œ ë§¤í•‘í•˜ëŠ” ê²ƒ ìì²´ ë°ì´í„°ë¡œ ì´ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹¤í–‰í•˜ë©´ ì—¬ëŸ¬ ì°¨ì›ìœ¼ë¡œ CRM ë°ì´í„°ë¥¼ ì¡°ì¸í•  ìˆ˜ ìˆìŒ. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960# We start with GA360 data, and will eventually build synthetic CRM as an example. # This block is the first step, just working with GA360ga360_only_view = &#x27;GA360_View&#x27;shared_dataset_ref = client.dataset(DATA_SET_ID)ga360_view_ref = shared_dataset_ref.table(ga360_only_view)ga360_view = bigquery.Table(ga360_view_ref)ga360_query = &#x27;&#x27;&#x27;SELECT fullVisitorID, ABS(farm_fingerprint(fullVisitorID)) AS Hashed_fullVisitorID, # This will be used to generate random data. MAX(device.operatingSystem) AS OS, # We can aggregate this because an OS is tied to a fullVisitorID. SUM (CASE WHEN REGEXP_EXTRACT (v2ProductCategory, r&#x27;^(?:(?:.*?)Home/)(.*?)/&#x27;) = &#x27;Apparel&#x27; THEN 1 ELSE 0 END) AS Apparel, SUM (CASE WHEN REGEXP_EXTRACT (v2ProductCategory, r&#x27;^(?:(?:.*?)Home/)(.*?)/&#x27;) = &#x27;Office&#x27; THEN 1 ELSE 0 END) AS Office, SUM (CASE WHEN REGEXP_EXTRACT (v2ProductCategory, r&#x27;^(?:(?:.*?)Home/)(.*?)/&#x27;) = &#x27;Electronics&#x27; THEN 1 ELSE 0 END) AS Electronics, SUM (CASE WHEN REGEXP_EXTRACT (v2ProductCategory, r&#x27;^(?:(?:.*?)Home/)(.*?)/&#x27;) = &#x27;Limited Supply&#x27; THEN 1 ELSE 0 END) AS LimitedSupply, SUM (CASE WHEN REGEXP_EXTRACT (v2ProductCategory, r&#x27;^(?:(?:.*?)Home/)(.*?)/&#x27;) = &#x27;Accessories&#x27; THEN 1 ELSE 0 END) AS Accessories, SUM (CASE WHEN REGEXP_EXTRACT (v2ProductCategory, r&#x27;^(?:(?:.*?)Home/)(.*?)/&#x27;) = &#x27;Shop by Brand&#x27; THEN 1 ELSE 0 END) AS ShopByBrand, SUM (CASE WHEN REGEXP_EXTRACT (v2ProductCategory, r&#x27;^(?:(?:.*?)Home/)(.*?)/&#x27;) = &#x27;Bags&#x27; THEN 1 ELSE 0 END) AS Bags, ROUND (SUM (productPrice/1000000),2) AS productPrice_USDFROM `bigquery-public-data.google_analytics_sample.ga_sessions_*`, UNNEST(hits) AS hits, UNNEST(hits.product) AS hits_productWHERE _TABLE_SUFFIX BETWEEN &#x27;20160801&#x27; AND &#x27;20160831&#x27; AND geoNetwork.country = &#x27;United States&#x27; AND type = &#x27;EVENT&#x27;GROUP BY 1, 2&#x27;&#x27;&#x27;ga360_view.view_query = ga360_query.format(PROJECT_ID)ga360_view = client.create_table(ga360_view) # API requestprint(f&quot;Successfully created view at &#123;ga360_view.full_table_id&#125;&quot;) ë°ì´í„° í™•ì¸ 1234567891011121314# Show a sample of GA360 dataga360_query_df = f&#x27;&#x27;&#x27;SELECT * FROM &#123;ga360_view.full_table_id.replace(&quot;:&quot;, &quot;.&quot;)&#125; LIMIT 5&#x27;&#x27;&#x27;job_config = bigquery.QueryJobConfig()# Start the queryquery_job = client.query(ga360_query_df, job_config=job_config) #API Requestdf_ga360 = query_job.result()df_ga360 = df_ga360.to_dataframe()df_ga360 CRM data ì¶”ì¶œí•˜ì—¬ í•©ì„±ë°ì´í„° ë§Œë“¤ê¸° 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# Create synthetic CRM data in SQLCRM_only_view = &#x27;CRM_View&#x27;shared_dataset_ref = client.dataset(DATA_SET_ID)CRM_view_ref = shared_dataset_ref.table(CRM_only_view)CRM_view = bigquery.Table(CRM_view_ref)# Query below works by hashing the fullVisitorID, which creates a random distribution. # We use modulo to artificially split gender and hhi distribution.CRM_query = &#x27;&#x27;&#x27;SELECT fullVisitorID,IF (MOD(Hashed_fullVisitorID,2) = 0, &quot;M&quot;, &quot;F&quot;) AS gender, CASE WHEN MOD(Hashed_fullVisitorID,10) = 0 THEN 55000 WHEN MOD(Hashed_fullVisitorID,10) &lt; 3 THEN 65000 WHEN MOD(Hashed_fullVisitorID,10) &lt; 7 THEN 75000 WHEN MOD(Hashed_fullVisitorID,10) &lt; 9 THEN 85000 WHEN MOD(Hashed_fullVisitorID,10) = 9 THEN 95000 ELSE Hashed_fullVisitorIDEND AS hhiFROM ( SELECT fullVisitorID, ABS(farm_fingerprint(fullVisitorID)) AS Hashed_fullVisitorID, FROM `bigquery-public-data.google_analytics_sample.ga_sessions_*`, UNNEST(hits) AS hits, UNNEST(hits.product) AS hits_product WHERE _TABLE_SUFFIX BETWEEN &#x27;20160801&#x27; AND &#x27;20160831&#x27; AND geoNetwork.country = &#x27;United States&#x27; AND type = &#x27;EVENT&#x27; GROUP BY 1, 2)&#x27;&#x27;&#x27;CRM_view.view_query = CRM_query.format(PROJECT_ID)CRM_view = client.create_table(CRM_view) # API requestprint(f&quot;Successfully created view at &#123;CRM_view.full_table_id&#125;&quot;) ë°ì´í„° í™•ì¸ 1234567891011121314# See an output of the synthetic CRM dataCRM_query_df = f&#x27;&#x27;&#x27;SELECT * FROM &#123;CRM_view.full_table_id.replace(&quot;:&quot;, &quot;.&quot;)&#125; LIMIT 5&#x27;&#x27;&#x27;job_config = bigquery.QueryJobConfig()# Start the queryquery_job = client.query(CRM_query_df, job_config=job_config) #API Requestdf_CRM = query_job.result()df_CRM = df_CRM.to_dataframe()df_CRM í´ëŸ¬ìŠ¤í„°ë§ì„ ìœ„í•œ í•™ìŠµ ë°ì´í„°ë¡œ ì‚¬ìš©í•  ìµœì¢…ë·° ì‘ì„±1234567891011121314151617181920# Build a final view, which joins GA360 data with CRM datafinal_data_view = &#x27;Final_View&#x27;shared_dataset_ref = client.dataset(DATA_SET_ID)final_view_ref = shared_dataset_ref.table(final_data_view)final_view = bigquery.Table(final_view_ref)final_data_query = f&#x27;&#x27;&#x27;SELECT g.*, c.* EXCEPT(fullVisitorId)FROM &#123;ga360_view.full_table_id.replace(&quot;:&quot;, &quot;.&quot;)&#125; gJOIN &#123;CRM_view.full_table_id.replace(&quot;:&quot;, &quot;.&quot;)&#125; cON g.fullVisitorId = c.fullVisitorId&#x27;&#x27;&#x27;final_view.view_query = final_data_query.format(PROJECT_ID)final_view = client.create_table(final_view) # API requestprint(f&quot;Successfully created view at &#123;final_view.full_table_id&#125;&quot;) ë°ì´í„° ì‹œê°í™” 1234567891011121314# Show final data used prior to modelingsql_demo = f&#x27;&#x27;&#x27;SELECT * FROM &#123;final_view.full_table_id.replace(&quot;:&quot;, &quot;.&quot;)&#125; LIMIT 5&#x27;&#x27;&#x27;job_config = bigquery.QueryJobConfig()# Start the queryquery_job = client.query(sql_demo, job_config=job_config) #API Requestdf_demo = query_job.result()df_demo = df_demo.to_dataframe()df_demo K-Means Modelì´ˆê¸° ëª¨ë¸ ë§Œë“¤ê¸° ì´ˆê¸° k-means modelì„ êµ¬ì¶• ì•„ì§ ìµœì ì˜ k ë˜ëŠ” ë‹¤ë¥¸ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ì—ëŠ” ì´ˆì ì„ ë§ì¶”ì§€ ì•Šê² ìŠµë‹ˆë‹¤. ëª‡ ê°€ì§€ ì¶”ê°€ ì‚¬í•­ í´ëŸ¬ìŠ¤í„°ë§ì„ ìœ„í•œ í”¼ì²˜ë¡œ fullVisitorIDê°€ í•„ìš”í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— í•´ë‹¹ ìˆ˜ì¤€ì—ì„œ ê·¸ë£¹í™”ë˜ì–´ ìˆë”ë¼ë„ fullVisitorIDë¥¼ ì…ë ¥ì—ì„œ ì œê±°. ì „ì²´ ë°©ë¬¸ì IDë¥¼ í”¼ì²˜ë¡œ ì‚¬ìš©í•´ì„œëŠ” ì•ˆë¨. ë²”ì£¼í˜• í”¼ì²˜ì™€ ìˆ«ì í”¼ì²˜ê°€ ëª¨ë‘ ì¡´ì¬ ìˆ«ì í”¼ì²˜ë¥¼ ì •ê·œí™”í•  í•„ìš”ê°€ ì—†ëŠ”ë°, ì´ëŠ” BigQuery MLì´ ìë™ìœ¼ë¡œ ìˆ˜í–‰í•˜ê¸° ë•Œë¬¸ 12345678910111213141516171819202122def makeModel (n_Clusters, Model_Name): sql =f&#x27;&#x27;&#x27; CREATE OR REPLACE MODEL `&#123;PROJECT_ID&#125;.&#123;DATA_SET_ID&#125;.&#123;Model_Name&#125;` OPTIONS(model_type=&#x27;kmeans&#x27;, kmeans_init_method = &#x27;KMEANS++&#x27;, num_clusters=&#123;n_Clusters&#125;) AS SELECT * except(fullVisitorID, Hashed_fullVisitorID) FROM `&#123;final_view.full_table_id.replace(&quot;:&quot;, &quot;.&quot;)&#125;` &#x27;&#x27;&#x27; job_config = bigquery.QueryJobConfig() client.query(sql, job_config=job_config) # Make an API request.# Let&#x27;s start with a simple test to ensure everything works. # After running makeModel(), allow a few minutes for training to complete.model_test_name = &quot;test&quot;makeModel(3, model_test_name)# After training is completed, you can either check in the UI, or you can interact with it using list_models(). for model in client.list_models(DATA_SET_ID): print(model) ë” ë‚˜ì€ ëª¨ë¸ì„ ë§Œë“¤ê¸° ìœ„í•œ ì‘ì—… ì˜¬ë°”ë¥¸ k ê°’ì„ ê²°ì •í•˜ëŠ” ê²ƒì€ ì „ì ìœ¼ë¡œ ì‚¬ìš© ì‚¬ë¡€ì— ë”°ë¼ ë‹¬ë¼ì§.ex) ì†ìœ¼ë¡œ ì“´ ìˆ«ìë¥¼ ì‚¬ì „ ì²˜ë¦¬ â†’ k &#x3D; 10 ë¹„ì¦ˆë‹ˆìŠ¤ ì´í•´ê´€ê³„ìê°€ ì„¸ ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ë§ˆì¼€íŒ… ìº í˜ì¸ë§Œ ì œê³µí•˜ê³ ì í•˜ê³  ì„¸ ê°œì˜ ê³ ê° í´ëŸ¬ìŠ¤í„°ë¥¼ ì‹ë³„í•´ì•¼ í•˜ëŠ” ê²½ìš° â†’ k&#x3D;3 ê·¸ëŸ¬ë‚˜ í˜„ì—…ì—ì„œ ìœ„ì˜ ì˜ˆì‹œì²˜ëŸ¼ ë”± ë–¨ì–´ì§€ëŠ” ì‚¬ìš© ì‚¬ë¡€ëŠ” ê±°ì˜ ì—†ê¸° ë•Œë¬¸ì— ë³´í†µ kì˜ ë²”ìœ„ë¥¼ ì§€ì •í•˜ê³  ê·¸ ì•ˆì—ì„œ ê²°ê³¼ê°’ì´ ì¢‹ì€ kë¥¼ ì„ íƒí•˜ê¸°ë„ í•¨. k ê°’ì„ ê²°ì •í•˜ê¸° ìœ„í•œ ìˆ˜ë‹¨ìœ¼ë¡œ ì—˜ë³´ìš° ë°©ë²•ì„ ìˆ˜í–‰í•œí›„ ë°ì´ë¹„ìŠ¤-ë³¼ë”˜ ì ìˆ˜ë¡œ í‰ê°€í•¨. DBIê°€ ì‘ì„ìˆ˜ë¡ clusterë¥¼ ìì„¸íˆ êµ¬ë¶„í–ˆë‹¤ê³  ë§í• ìˆ˜ ìˆìŒ(ê´€ë ¨ í¬ìŠ¤íŠ¸) ì•„ë˜ì—ì„œëŠ” ì—˜ë³´ ë°©ë²•ì„ ëª¨ë‘ ìˆ˜í–‰í•˜ê³  ë°ì´ë¹„ìŠ¤-ë³¼ë”˜ ì ìˆ˜ë¥¼ ì–»ê¸° ìœ„í•œ ëª‡ ê°€ì§€ ëª¨ë¸ì„ ìƒì„±í•¨. low_k, high_k: í•˜ì´í¼ íŒŒë¼ë¯¸í„°, ë‘ ê°’ ì‚¬ì´ì˜ ëª¨ë¸ì„ ìƒì„±. 12345678910111213# Define upper and lower bound for k, then build individual models for each. # After running this loop, look at the UI to see several model objects that exist. low_k = 3high_k = 15model_prefix_name = &#x27;kmeans_clusters_&#x27;lst = list(range (low_k, high_k+1)) #build list to iterate through k valuesfor k in lst: model_name = model_prefix_name + str(k) makeModel(k, model_name) print(f&quot;Model started: &#123;model_name&#125;&quot;) 123456# list all current modelsmodels = client.list_models(DATA_SET_ID) # Make an API request.print(&quot;Listing current models:&quot;)for model in models: full_model_id = f&quot;&#123;model.dataset_id&#125;.&#123;model.model_id&#125;&quot; print(full_model_id) 12345# Remove our sample model from BigQuery, so we only have remaining models from our previous loopmodel_id = DATA_SET_ID+&quot;.&quot;+model_test_nameclient.delete_model(model_id) # Make an API request.print(f&quot;Deleted model &#x27;&#123;model_id&#125;&#x27;&quot;) 123456789101112131415161718192021# This will create a dataframe with each model name, the Davies Bouldin Index, and Loss. # It will be used for the elbow method and to help determine optimal Kdf = pd.DataFrame(columns=[&#x27;davies_bouldin_index&#x27;, &#x27;mean_squared_distance&#x27;])models = client.list_models(DATA_SET_ID) # Make an API request.for model in models: full_model_id = f&quot;&#123;model.dataset_id&#125;.&#123;model.model_id&#125;&quot; sql =f&#x27;&#x27;&#x27; SELECT davies_bouldin_index, mean_squared_distance FROM ML.EVALUATE(MODEL `&#123;full_model_id&#125;`) &#x27;&#x27;&#x27; job_config = bigquery.QueryJobConfig() # Start the query, passing in the extra configuration. query_job = client.query(sql, job_config=job_config) # Make an API request. df_temp = query_job.to_dataframe() # Wait for the job to complete. df_temp[&#x27;model_name&#x27;] = model.model_id df = pd.concat([df, df_temp], axis=0) ì•„ë˜ ì½”ë“œëŠ” ì›ë˜ ì´ ë…¸íŠ¸ë¶ì—ì„œ ë§Œë“  ëª…ëª… ê·œì¹™ì„ ì‚¬ìš©í–ˆìœ¼ë©°, ë‘ ë²ˆì§¸ ë°‘ì¤„ ë’¤ì— k ê°’ì´ ìˆë‹¤ê³  ê°€ì •. model_prefix_name ë³€ìˆ˜ë¥¼ ë³€ê²½í•œ ê²½ìš°, ì´ ì½”ë“œê°€ ê¹¨ì§ˆ ìˆ˜ ìˆìŒ. 123456# This will modify the dataframe above, produce a new field with &#x27;n_clusters&#x27;, and will sort for graphingdf[&#x27;n_clusters&#x27;] = df[&#x27;model_name&#x27;].str.split(&#x27;_&#x27;).map(lambda x: x[2])df[&#x27;n_clusters&#x27;] = df[&#x27;n_clusters&#x27;].apply(pd.to_numeric)df = df.sort_values(by=&#x27;n_clusters&#x27;, ascending=True)df 1df.plot.line(x=&#x27;n_clusters&#x27;, y=[&#x27;davies_bouldin_index&#x27;, &#x27;mean_squared_distance&#x27;]) ì°¸ê³  - ì´ ë…¸íŠ¸ë¶ì„ ì‹¤í–‰í•˜ë©´ ë¬´ì‘ìœ„ í´ëŸ¬ìŠ¤í„° ì´ˆê¸°í™”ë¡œ ì¸í•´ ë‹¤ë¥¸ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŒ. ë„ë‹¬ ë²”ìœ„ ì‹¤í–‰ì— ëŒ€í•´ ì¼ê´€ë˜ê²Œ ë™ì¼í•œ í´ëŸ¬ìŠ¤í„°ë¥¼ ë°˜í™˜í•˜ë ¤ë©´ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„ íƒì„ í†µí•´ ì´ˆê¸°í™”ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì„ íƒê°€ëŠ¥. k ì„ íƒí•˜ê¸°: ìµœì ì˜ k ê°’ì„ ê²°ì •í•  ë•Œ ì™„ë²½í•œ ì ‘ê·¼ ë°©ì‹ì´ë‚˜ í”„ë¡œì„¸ìŠ¤ëŠ” ì •í•´ì ¸ ìˆì§€ ì•ŠìŒ. ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ì´ë‚˜ ìš”êµ¬ ì‚¬í•­ì— ë”°ë¼ ê²°ì •ë˜ëŠ” ê²½ìš°ê°€ ë§ìŒ. ì´ ì˜ˆì—ì„œëŠ” ê°„ë‹¨í•œ ìš”êµ¬ ì‚¬í•­ì´ ì—†ìœ¼ë¯€ë¡œ ë‹¤ìŒê³¼ ê°™ì€ ê³ ë ¤ ì‚¬í•­ì„ ë”°ë¥¼ ìˆ˜ë„ ìˆìŒ: í•­ìƒ ê·¸ëŸ° ê²ƒì€ ì•„ë‹ˆì§€ë§Œ, ì¦ë¶„ í´ëŸ¬ìŠ¤í„°ê°€ ì†ì‹¤ì„ í¬ê²Œ ì¤„ì´ì§€ ëª»í•˜ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ â€˜ì—˜ë³´ ë°©ë²•â€™ì´ ìˆëŠ” ê²½ìš°ê°€ ìˆìŒ. ì´ íŠ¹ì • ì˜ˆì—ì„œëŠ”, ê·¸ë¦¬ê³  ì¢…ì¢… ë°œê²¬í•  ìˆ˜ ìˆë“¯ì´, ì•ˆíƒ€ê¹ê²Œë„ ìì—°ìŠ¤ëŸ¬ìš´ â€˜ì—˜ë³´â€™ê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ í”„ë¡œì„¸ìŠ¤ë¥¼ ê³„ì† ì§„í–‰í•´ì•¼ í•¨. ë‹¤ìŒìœ¼ë¡œ ë°ì´ë¹„ìŠ¤-ë³¼ë”˜ê³¼ kë¥¼ ì°¨íŠ¸ë¡œ í‘œ. ì´ ì ìˆ˜ëŠ” ê° í´ëŸ¬ìŠ¤í„°ê°€ ì–¼ë§ˆë‚˜ â€˜ë‹¤ë¥¸ì§€â€™ë¥¼ ì•Œë ¤ì£¼ë©°, ìµœì  ì ìˆ˜ëŠ” 0. í´ëŸ¬ìŠ¤í„°ê°€ 5ê°œì¸ ê²½ìš° ì ìˆ˜ëŠ” ì•½ 1.4ì´ë©°, kê°€ 9ë¥¼ ì´ˆê³¼í•˜ëŠ” ê²½ìš°ì—ë§Œ ë” ë‚˜ì€ ê°’ì„ ë³¼ ìˆ˜ ìˆìŒ. ë§ˆì§€ë§‰ìœ¼ë¡œ ê° ëª¨ë¸ì˜ ì°¨ì´ë¥¼ í•´ì„í•˜ê¸° ì‹œì‘. ë‹¤ì–‘í•œ ëª¨ë¸ì— ëŒ€í•œ í‰ê°€ ëª¨ë“ˆì„ ê²€í† í•˜ì—¬ ê¸°ëŠ¥ì˜ ë¶„í¬ë¥¼ ì´í•´í•  ìˆ˜ ìˆìŒ. ë°ì´í„°ë¥¼ í†µí•´ ì„±ë³„, ê°€êµ¬ ì†Œë“, ì‡¼í•‘ ìŠµê´€ì— ë”°ë¥¸ íŒ¨í„´ì„ ì°¾ì„ ìˆ˜ ìˆìŒ. ìµœì¢… í´ëŸ¬ìŠ¤í„° ë¶„ì„ ëª¨ë¸ì˜ íŠ¹ì„±ì„ ì´í•´í•˜ëŠ” ë°ëŠ” ë‘ ê°€ì§€ ì˜µì…˜ì´ ìˆìŠµë‹ˆë‹¤. BigQuery UIë¥¼ ì‚´í´ë³´ê±°ë‚˜ ëª¨ë¸ ê°œì²´ì™€ í”„ë¡œê·¸ë˜ë° ë°©ì‹ ì•„ë˜ì—ì„œ í›„ìì˜ ì˜µì…˜ì— ëŒ€í•œ ê°„ë‹¨í•œ ì˜ˆì œë¥¼ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 123456789101112131415161718192021model_to_use = &#x27;kmeans_clusters_5&#x27; # User can edit thisfinal_model = DATA_SET_ID+&#x27;.&#x27;+model_to_usesql_get_attributes = f&#x27;&#x27;&#x27;SELECT centroid_id, feature, categorical_valueFROM ML.CENTROIDS(MODEL &#123;final_model&#125;)WHERE feature IN (&#x27;OS&#x27;,&#x27;gender&#x27;)&#x27;&#x27;&#x27;job_config = bigquery.QueryJobConfig()# Start the queryquery_job = client.query(sql_get_attributes, job_config=job_config) #API Requestdf_attributes = query_job.result()df_attributes = df_attributes.to_dataframe()df_attributes.head() 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556# get numerical information about clusterssql_get_numerical_attributes = f&#x27;&#x27;&#x27;WITH T AS (SELECT centroid_id, ARRAY_AGG(STRUCT(feature AS name, ROUND(numerical_value,1) AS value) ORDER BY centroid_id) AS clusterFROM ML.CENTROIDS(MODEL &#123;final_model&#125;)GROUP BY centroid_id),Users AS(SELECT centroid_id, COUNT(*) AS Total_UsersFROM(SELECT * EXCEPT(nearest_centroids_distance)FROM ML.PREDICT(MODEL &#123;final_model&#125;, ( SELECT * FROM &#123;final_view.full_table_id.replace(&quot;:&quot;, &quot;.&quot;)&#125; )))GROUP BY centroid_id)SELECT centroid_id, Total_Users, (SELECT value from unnest(cluster) WHERE name = &#x27;Apparel&#x27;) AS Apparel, (SELECT value from unnest(cluster) WHERE name = &#x27;Office&#x27;) AS Office, (SELECT value from unnest(cluster) WHERE name = &#x27;Electronics&#x27;) AS Electronics, (SELECT value from unnest(cluster) WHERE name = &#x27;LimitedSupply&#x27;) AS LimitedSupply, (SELECT value from unnest(cluster) WHERE name = &#x27;Accessories&#x27;) AS Accessories, (SELECT value from unnest(cluster) WHERE name = &#x27;ShopByBrand&#x27;) AS ShopByBrand, (SELECT value from unnest(cluster) WHERE name = &#x27;Bags&#x27;) AS Bags, (SELECT value from unnest(cluster) WHERE name = &#x27;productPrice_USD&#x27;) AS productPrice_USD, (SELECT value from unnest(cluster) WHERE name = &#x27;hhi&#x27;) AS hhiFROM T LEFT JOIN Users USING(centroid_id)ORDER BY centroid_id ASC&#x27;&#x27;&#x27;job_config = bigquery.QueryJobConfig()# Start the queryquery_job = client.query(sql_get_numerical_attributes, job_config=job_config) #API Requestdf_numerical_attributes = query_job.result()df_numerical_attributes = df_numerical_attributes.to_dataframe()df_numerical_attributes.head() ìœ„ì˜ ê²°ê³¼ë¥¼ ë¶„ì„í•´ë³´ë©´ 1ë²ˆ í´ëŸ¬ìŠ¤í„°ëŠ” ìœ ì €ìˆ˜ê°€ ë‘ë²ˆì§¸ë¡œ ë§ê³ , ì œì¼ ìœ ì €ìˆ˜ê°€ ë§ì€ í´ëŸ¬ìŠ¤í„° 2ë²ˆë³´ë‹¤ êµ¬ë§¤ìœ¨ì´ ë†’ì€ê±¸ ì•Œìˆ˜ ìˆìŒ 2ë²ˆ í´ëŸ¬ìŠ¤í„°ëŠ” ê°€ì¥ ì¸êµ¬ê°€ ë§ì§€ë§Œ êµ¬ë§¤ íšŸìˆ˜ê°€ ì ê³  í‰ê·  ì§€ì¶œì•¡ì´ ì ìŒ. ë¸Œëœë“œ ì¶©ì„±ë„ê°€ ë†’ë‹¤ê¸° ë³´ë‹¤ëŠ” ì¼íšŒì„± êµ¬ë§¤ì 3ë²ˆ í´ëŸ¬ìŠ¤í„°ëŠ” ì˜ë¥˜ì— ê´€ì‹¬ì´ ë§ê³  í‰ê·  êµ¬ë§¤ ê°€ê²©ì´ ì œì¼ ë†’ìŒ. ë¸Œëœë“œë³„ë¡œ ì†Œë¹„í•˜ì§„ ì•Šì§€ë§Œ ê°€ì¹˜ê°€ ê°€ì¥ ë†’ì€ ê³ ê° 4ë²ˆ í´ëŸ¬ìŠ¤í„°ëŠ” ë¸Œëœë“œ ë³„ë¡œ ì†Œë¹„ë¥¼ ë§ì´ í•˜ëŠ” ê³ ê°ë“¤ì´ ëª°ë ¤ ìˆìŒ. 5ë²ˆ í´ëŸ¬ìŠ¤í„°ëŠ” ì‚¬ë¬´ìš©í’ˆì— ê°€ì¥ ëˆì„ ë§ì´ ì‚¬ìš©í•˜ëŠ” ê³ ê° Export to GA360 ëª¨ë¸ì„ ì™„ì„±í•œ í›„ì—ëŠ” ì´ë¥¼ ì¶”ë¡ ì— ì‚¬ìš© ì•„ë˜ ì½”ë“œëŠ” ì‚¬ìš©ìë¥¼ ì ìˆ˜í™”í•˜ê±°ë‚˜ í´ëŸ¬ìŠ¤í„°ì— í• ë‹¹í•˜ëŠ” ë°©ë²•ì„ ê°„ëµí•˜ê²Œ ì„¤ëª… ì´ ì½”ë“œì—ëŠ” CENTROID_IDë¼ëŠ” ë ˆì´ë¸”ì´ ë¶™ìŠµë‹ˆë‹¤. ì´ ì½”ë“œ ìì²´ë„ ë„ì›€ì´ ë˜ì§€ë§Œ, ì´ ì ìˆ˜ë¥¼ ë‹¤ì‹œ GA360ìœ¼ë¡œ ìˆ˜ì§‘í•˜ëŠ” í”„ë¡œì„¸ìŠ¤ë¥¼ ê¶Œì¥ BigQuery í…Œì´ë¸”ì—ì„œ Google ì• ë„ë¦¬í‹±ìŠ¤ 360ìœ¼ë¡œ BigQuery ML ì˜ˆì¸¡ì„ ë‚´ë³´ë‚´ëŠ” ê°€ì¥ ì‰¬ìš´ ë°©ë²•ì€ MoDeM(ë§ˆì¼€íŒ…ì„ ìœ„í•œ ëª¨ë¸ ë°°í¬) ì°¸ì¡° êµ¬í˜„ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒ MoDeMì€ Google ê´‘ê³ , ë””ìŠ¤í”Œë ˆì´ ë° ë™ì˜ìƒ 360, ê²€ìƒ‰ ê´‘ê³  360ì—ì„œ ìµœì¢…ì ìœ¼ë¡œ í™œì„±í™”í•  ìˆ˜ ìˆë„ë¡ ë°ì´í„°ë¥¼ Google ì• ë„ë¦¬í‹±ìŠ¤ì— ë¡œë“œí•˜ëŠ” ë° ë„ì›€ì´ ë¨ 1234567891011121314151617181920sql_score = f&#x27;&#x27;&#x27;SELECT * EXCEPT(nearest_centroids_distance)FROM ML.PREDICT(MODEL &#123;final_model&#125;, ( SELECT * FROM &#123;final_view.full_table_id.replace(&quot;:&quot;, &quot;.&quot;)&#125; -- LIMIT 1 ))&#x27;&#x27;&#x27;job_config = bigquery.QueryJobConfig()# Start the queryquery_job = client.query(sql_score, job_config=job_config) #API Requestdf_score = query_job.result()df_score = df_score.to_dataframe()df_score Reference http://googleanalytics360.com/board/view.php?bo_table&#x3D;googleanalytics&amp;wr_id&#x3D;34 https://dev-kani.tistory.com/2","categories":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}],"tags":[{"name":"Big Query","slug":"Big-Query","permalink":"https://jmj3047.github.io/tags/Big-Query/"},{"name":"BigQueryML","slug":"BigQueryML","permalink":"https://jmj3047.github.io/tags/BigQueryML/"}],"keywords":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}]},{"title":"Mac VScode GCP ì¸ì¦ ê´€ë ¨ ì˜¤ë¥˜","slug":"Mac_GCP_Error","date":"2023-03-08T15:00:00.000Z","updated":"2023-03-19T07:03:36.636Z","comments":true,"path":"2023/03/09/Mac_GCP_Error/","link":"","permalink":"https://jmj3047.github.io/2023/03/09/Mac_GCP_Error/","excerpt":"","text":"gcpë‚´ì— ìˆëŠ” ì˜ˆì œë“¤ì„ ì‹¤í–‰ ì‹œí‚¬ë•Œë©´ ì£¼í”¼í„° ë…¸íŠ¸ë¶ìœ¼ë¡œ gcpë¥¼ ì‚¬ìš©í• ë•Œ ì‚¬ìš©ìë¥¼ ì¸ì¦í•´ì•¼ í•˜ëŠ” ì´ìŠˆê°€ ìƒê¹€ 1Error google.auth.exceptions.DefaultCredentialsError: Could not automatically determine credentials. Please set GOOGLE_APPLICATION_CREDENTIALS or explicitly create credentials and re-run the application. macì—ì„œ ë°©ë²•ì„ ì°¾ë‹¤ê°€ ì˜¤ë¥˜ë¥¼ í•´ê²°í•¨ ì´ ë…¸íŠ¸ë¶ì˜ ì•„ë˜ ì½”ë“œ(!gcloud~)ë¥¼ ì‹¤í–‰í•˜ë‹¤ê°€ ì˜¤ë¥˜ê°€ ë‚¨ ì´ ê²Œì‹œë¬¼ì—ì„œ â€˜GCPì— ë°ì´í„° ì„¸íŠ¸ ë§Œë“¤ê³  ì„œë¹„ìŠ¤ ê³„ì • ìƒì„±í•˜ê¸°â€™ í•­ëª©ì˜ ì„œë¹„ìŠ¤ ê³„ì •ì„ ìƒì„±í•˜ê³  jsoníŒŒì¼ì„ ë‹¤ìš´ ë°›ì•„ ì£¼ì—ˆë‹¤. ê·¸ë¦¬ê³  ì½”ë“œë¥¼ ìˆ˜ì •í•˜ì—¬ ì´ë ‡ê²Œ ì‘ì„±í–ˆë”ë‹ˆ ì£¼í”¼í„° ë…¸íŠ¸ë¶ì—ì„œë„ bqëª…ë ¹ì–´ë‚˜ gcloudëª…ë ¹ì–´ê°€ ì˜ ëŒì•„ê°”ë‹¤..! ë¬¸ì œëŠ” m1 ë•Œë¬¸ì¸ê±° ê°™ì€ë° ì´ê±° ë•Œë¬¸ì— vscodeê¹Œì§€ ì§€ìš°ê³  ë‹¤ì‹œ ê¹”ì•˜ë‹¤. ì•„ë§ˆ êµ¬ê¸€ì´ ì‚¬ìš©ì ì¸ì¦í•˜ëŠ” ì„œë¹„ìŠ¤ë¥¼ macOSì—ì„œëŠ” ì œê³µí•˜ì§€ ì•Šì•„ì„œ ê·¸ëŸ°ê±° ê°™ì€ë°.. ë¬¼ë¦¬ì ìœ¼ë¡œ json íŒŒì¼ì„ ì €ì¥í•˜ì§€ ì•Šê³ ë„ credential ì‚¬ìš©í•´ì„œ ë°”ë¡œ ì—°ê²° í• ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì°¾ì•˜ë‹¤ë©´ ì¶”í›„ì— í¬ìŠ¤íŒ… í•˜ê² ë‹¤.","categories":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}],"tags":[{"name":"GCP","slug":"GCP","permalink":"https://jmj3047.github.io/tags/GCP/"},{"name":"Auth","slug":"Auth","permalink":"https://jmj3047.github.io/tags/Auth/"},{"name":"Error","slug":"Error","permalink":"https://jmj3047.github.io/tags/Error/"}],"keywords":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}]},{"title":"BQMLì„ ì´ìš©í•œ ê²Œì„ìœ ì € ê²½í–¥ ëª¨ë¸ë§","slug":"Game_Modeling","date":"2023-03-07T15:00:00.000Z","updated":"2023-03-19T07:02:35.325Z","comments":true,"path":"2023/03/08/Game_Modeling/","link":"","permalink":"https://jmj3047.github.io/2023/03/08/Game_Modeling/","excerpt":"","text":"ê°œìš” ë¹…ì¿¼ë¦¬ MLì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘í•œ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ ëŒë¦¬ê¸° GA4ì™€ ë¹…ì¿¼ë¦¬ ì—°ë™ ì‹œ ì¶”ì¶œë˜ëŠ” ë°ì´í„°ë“¤ì„ ì •ì œí•´ì„œ ë¨¸ì‹ ëŸ¬ë‹ í›ˆë ¨ë°ì´í„°ë¡œ ë§Œë“¤ê¸° ê° ëª¨ë¸ì˜ í‰ê°€, íŒŒë¼ë¯¸í„°ë“¤ì„ ì•Œì•„ë³´ê³  ì¡°ì •í•´ë³´ê¸° ëª©í‘œ ì•± ì„¤ì¹˜ í›„ ì²« 24ì‹œê°„ ë™ì•ˆì˜ ì‚¬ìš©ì í™œë™ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” â€œFlood It!â€ ë°ì´í„° ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘í•œ ë¶„ë¥˜ ëª¨ë¸ì„ ì‹œë„í•˜ì—¬ ì´íƒˆ ì„±í–¥(1) ë˜ëŠ” ì´íƒˆí•˜ì§€ ì•Šì„ ì„±í–¥(0)ì„ ì˜ˆì¸¡ ë¹„ìš© BigQuery ê°€ê²© ë¶„ì„ ê°€ê²© ì±…ì •: SQL ì¿¼ë¦¬, ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜, ìŠ¤í¬ë¦½íŠ¸, í…Œì´ë¸”ì„ ìŠ¤ìº”í•˜ëŠ” DML(ë°ì´í„° ì¡°ì‘ ì–¸ì–´) ë° DDL(ë°ì´í„° ì •ì˜ ì–¸ì–´)ë¬¸ì„ í¬í•¨í•œ ì¿¼ë¦¬ë¥¼ ì²˜ë¦¬í•  ë•Œ ë°œìƒí•˜ëŠ” ë¹„ìš© ì£¼ë¬¸ë¶„ì„í˜• ê°€ê²©ì±…ì • (ì£¼ë¬¸í˜•)ì¿¼ë¦¬: 5$ per TB, ë§¤ì›” 1TBê¹Œì§€ëŠ” ë¬´ë£Œ ì •ì•¡ì œ ì›”ê°„ ì •ì•¡ì œ: ìŠ¬ë¡¯ 100ê°œë‹¹ 2000$ ì—°ê°„ ì •ì•¡ì œ: ìŠ¬ë¡¯ 100ê°œë‹¹ 1700$(ë‹¬ë§ˆë‹¤) ìŠ¤í† ë¦¬ì§€ ê°€ê²© ì±…ì •: BigQueryì— ë¡œë“œí•œ ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ” ë° ë“œëŠ” ë¹„ìš© BigQueyrML ê°€ê²© ë¬´ë£Œì‚¬ìš©ëŸ‰ í•œë„ ìŠ¤í† ë¦¬ì§€: ë§¤ì›” 10GB ì¿¼ë¦¬(ë¶„ì„): ë§¤ì›” ì²˜ë¦¬ë˜ëŠ” ì¿¼ë¦¬ ë°ì´í„°ì¤‘ ìµœì´ˆ 1TBëŠ” ë¬´ë£Œ BigQuery Storage Write API: ë§¤ì›” ì²˜ìŒ 2TBëŠ” ë¬´ë£Œ BigQuery ML CREATE MODELì¿¼ë¦¬: ë§¤ì›” 10GBê¹Œì§€ëŠ” CREATE MODEL ë¬¸ì´ í¬í•¨ëœ ì¿¼ë¦¬ ë°ì´í„°ê°€ ë¬´ë£Œë¡œ ì²˜ë¦¬ ì£¼ë¬¸í˜• ê°€ê²© ë°ì´í„° í•™ìŠµ ë°ì´í„°ë¡œ ì •ì œ í•˜ê¸° ì•±ìœ¼ë¡œ ë³µê·€í•  ê°€ëŠ¥ì„±ì´ ì—†ëŠ” ì‚¬ìš©ìë¥¼ í•„í„°ë§í•©ë‹ˆë‹¤. ì‚¬ìš©ì ì¸êµ¬í†µê³„ ë°ì´í„°ì— ëŒ€í•œ íŠ¹ì„±ì„ ë§Œë“­ë‹ˆë‹¤. ì‚¬ìš©ì í–‰ë™ ë°ì´í„°ì— ëŒ€í•œ íŠ¹ì„±ì„ ë§Œë“­ë‹ˆë‹¤. ì¸êµ¬í†µê³„ ë°ì´í„° ë° í–‰ë™ ë°ì´í„°ë¥¼ ê²°í•©í•˜ë©´ ë” íš¨ê³¼ì ì¸ ì˜ˆì¸¡ ëª¨ë¸ì„ ë§Œë“œëŠ” ë° ë„ì›€ì´ ëœë‹¤. ì²˜ë¦¬ í›„ í•™ìŠµ ë°ì´í„°ì˜ ê° í–‰ì€ user_pseudo_id ì—´ë¡œ ì‹ë³„ëœ ìˆœ ì‚¬ìš©ìì˜ ë°ì´í„°ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. ì „ì²´ ë°ì´í„° ì¡°íšŒâ†’ GA4ì—ì„œ ë„˜ì–´ì˜¨ ë°ì´í„° í˜•ì‹(ìŠ¤í‚¤ë§ˆ ë° ê° ì—´ì— ëŒ€í•œ ì„¸ë¶€ ì •ë³´) 12345SELECT *FROM `firebase-public-project.analytics_153293282.events_*` TABLESAMPLE SYSTEM (1 PERCENT) ì´ 15000ëª…ì˜ ìœ ì €ì™€ 5.7ë§Œê°œì˜ ì´ë²¤íŠ¸ê°€ ìˆëŠ”ê±¸ ë³¼ìˆ˜ ìˆìŒ 12345SELECT COUNT(DISTINCT user_pseudo_id) as count_distinct_users, COUNT(event_timestamp) as count_eventsFROM `firebase-public-project.analytics_153293282.events_*` í•™ìŠµ ë°ì´í„°ë¡œ ì •ì œí•˜ê¸° User ID: ê³ ìœ í•œ ì‚¬ìš©ì ID User demographic data: ì¸êµ¬ í†µê³„ í•™ì  ë°ì´í„° User behavioral data: í–‰ë™ ë°ì´í„° Churned: ì˜ˆì¸¡í•˜ê³ ì í•˜ëŠ” ì‹¤ì œ ë¼ë²¨(1&#x3D;ì´íƒˆ, 0&#x3D;ê·€í™˜) STEP 1: ê° ìœ ì €ì— ëŒ€í•œ ë¼ë²¨ ì‹ë³„ ì‚¬ìš©ì ì´íƒˆì„ ì •ì˜í•˜ëŠ” ë°©ë²•ì—ëŠ” ì—¬ëŸ¬ ê°€ì§€ê°€ ìˆì§€ë§Œ, ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” ì‚¬ìš©ìê°€ ì•±ì„ ì²˜ìŒ ì‚¬ìš©í•œ í›„ 24ì‹œê°„ì´ ì§€ë‚˜ë„ ë‹¤ì‹œ ì•±ì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì‚¬ìš©ìë¥¼ 1ì¼ ì´íƒˆë¡œ ì˜ˆì¸¡ â†’ ì‚¬ìš©ìê°€ ì•±ì— ì²˜ìŒ ì°¸ì—¬í•œ í›„ 24ì‹œê°„ì´ ì§€ë‚œ í›„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•©ë‹ˆë‹¤: ì‚¬ìš©ìê°€ ê·¸ ì´í›„ ì´ë²¤íŠ¸ ë°ì´í„°ë¥¼ í‘œì‹œí•˜ì§€ ì•ŠëŠ” ê²½ìš°, í•´ë‹¹ ì‚¬ìš©ìëŠ” íƒˆí‡´í•œ ê²ƒìœ¼ë¡œ ê°„ì£¼ë©ë‹ˆë‹¤. ì‚¬ìš©ìê°€ ê·¸ ì´í›„ ì´ë²¤íŠ¸ ë°ì´í„° í¬ì¸íŠ¸ê°€ í•˜ë‚˜ ì´ìƒ ìˆìœ¼ë©´, í•´ë‹¹ ì‚¬ìš©ìëŠ” ê·€í™˜í•œ ê²ƒìœ¼ë¡œ ê°„ì£¼ë©ë‹ˆë‹¤. ì•±ì—ì„œ ë‹¨ ëª‡ ë¶„ë§Œ ì‚¬ìš©í•œ í›„ ë‹¤ì‹œ ëŒì•„ì˜¬ ê°€ëŠ¥ì„±ì´ ë‚®ì€ ì‚¬ìš©ìë¥¼ ì œê±°í•˜ê³ ì í•  ìˆ˜ë„ ìˆëŠ”ë°, ì´ë¥¼ â€œbouncingâ€ì´ë¼ê³ ë„ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì•±ì„ ìµœì†Œ 10ë¶„ ì´ìƒ ì‚¬ìš©í•œ ì‚¬ìš©ì(ì´íƒˆí•˜ì§€ ì•Šì€ ì‚¬ìš©ì)ë¥¼ ëŒ€ìƒìœ¼ë¡œë§Œ ëª¨ë¸ì„ êµ¬ì¶•í•˜ê³ ì í•œë‹¤ê³  ê°€ì •í•´ ë³´ê² ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì´ ë…¸íŠ¸ë¶ì˜ ì´íƒˆí•œ ì‚¬ìš©ìì— ëŒ€í•œ ì—…ë°ì´íŠ¸ëœ ì •ì˜ â€œì•±ì—ì„œ 10ë¶„ ì´ìƒ ì‹œê°„ì„ ë³´ëƒˆì§€ë§Œ ì•±ì„ ì²˜ìŒ ì‚¬ìš©í•œ í›„ 24ì‹œê°„ì´ ì§€ë‚œ í›„ ë‹¤ì‹œëŠ” ì•±ì„ ì‚¬ìš©í•˜ì§€ ì•Šì€ ëª¨ë“  ì‚¬ìš©ìâ€ 1234567891011121314151617181920212223242526272829303132333435363738394041424344CREATE OR REPLACE VIEW `bqmlga4.returningusers` AS ( WITH firstlasttouch AS ( SELECT user_pseudo_id, MIN(event_timestamp) AS user_first_engagement, MAX(event_timestamp) AS user_last_engagement FROM `firebase-public-project.analytics_153293282.events_*` WHERE event_name=&quot;user_engagement&quot; GROUP BY user_pseudo_id ) SELECT user_pseudo_id, user_first_engagement, user_last_engagement, EXTRACT(MONTH from TIMESTAMP_MICROS(user_first_engagement)) as month, EXTRACT(DAYOFYEAR from TIMESTAMP_MICROS(user_first_engagement)) as julianday, EXTRACT(DAYOFWEEK from TIMESTAMP_MICROS(user_first_engagement)) as dayofweek, #add 24 hr to user&#x27;s first touch (user_first_engagement + 86400000000) AS ts_24hr_after_first_engagement,#churned = 1 if last_touch within 24 hr of app installation, else 0IF (user_last_engagement &lt; (user_first_engagement + 86400000000), 1, 0 ) AS churned,#bounced = 1 if last_touch within 10 min, else 0IF (user_last_engagement &lt;= (user_first_engagement + 600000000), 1, 0 ) AS bounced, FROM firstlasttouch GROUP BY 1,2,3 );SELECT * FROM `bqmlga4.returningusers` Churned ì—´ì˜ ê²½ìš° ì²« 24ì‹œê°„ì´ ì§€ë‚œ í›„ì— ì•¡ì…˜ì„ ìˆ˜í–‰í•˜ë©´ Churned &#x3D;0ì´ ë˜ê³  ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ë§ˆì§€ë§‰ ì•¡ì…˜ì´ ì²« 24ì‹œê°„ ì´ë‚´ì—ë§Œ ì´ë£¨ì–´ì§„ ê²½ìš° Churned&#x3D;1ì´ ëœë‹¤. Bounced ì—´ì˜ ê²½ìš° ì‚¬ìš©ìì˜ ë§ˆì§€ë§‰ ë™ì‘ì´ ì•±ì„ ì²˜ìŒ í„°ì¹˜í•œ í›„ 10ë¶„ ì´ë‚´ì¸ ê²½ìš° bounced &#x3D; 1, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ bounced&#x3D;0 ì´ 15000ëª…ì˜ ì‚¬ìš©ì ì¤‘ ëª‡ëª…ì´ ì´íƒˆí–ˆë‹¤ê°€ ë‹¤ì‹œ ëŒì•„ì™”ëŠ”ì§€ í™•ì¸ 12345678SELECT bounced, churned, COUNT(churned) as count_usersFROM bqmlga4.returningusersGROUP BY 1,2ORDER BY bounced 15000ëª…ì˜ ì‚¬ìš©ìë¥¼ ê¸°ì¤€ìœ¼ë¡œ 5,557ëª…(41%)ì˜ ì‚¬ìš©ìê°€ ì•±ì„ ì²˜ìŒ ì‚¬ìš©í•œ í›„ 10ë¶„ ì´ë‚´ì— ì´íƒˆí–ˆì§€ë§Œ ë‚˜ë¨¸ì§€ 8,031ëª…ì˜ ì‚¬ìš©ì ì¤‘ 1,883(23%)ì´ 24ì‹œê°„ í›„ì— ì´íƒˆí•œ ê²ƒì„ í™•ì¸ í•™ìŠµ ë°ì´í„°ì˜ ê²½ìš° bounce&#x3D;0ì¸ ë°ì´í„°ë§Œ ì‚¬ìš©í•¨ â†’ ì´ë¯¸ ê·€í™˜í•œ ìœ ì €ì— ëŒ€í•´ì„œëŠ” í•  í•„ìš”ê°€ ì—†ê¸° ë•Œë¬¸ 12345SELECT COUNTIF(churned=1)/COUNT(churned) as churn_rateFROM bqmlga4.returningusersWHERE bounced = 0 STEP 2: ê° ì‚¬ìš©ìì— ëŒ€í•œ ì¸êµ¬ í†µê³„ ë°ì´í„° ì¶”ì¶œ ì•±ì •ë³´, ê¸°ê¸°, ì´ì»¤ë¨¸ìŠ¤, ì´ë²¤íŠ¸ íŒŒë¼ë¯¸í„°, ì§€ì—­ ë“± ì‚¬ìš©ìì— ëŒ€í•œ ì¸êµ¬í†µê³„í•™ì  ë°ì´í„° ì¶”ì¶œ ìì²´ ë°ì´í„° ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ê³  ìˆê³  ì¡°ì¸ ê°€ëŠ¥í•œ first-party dataê°€ ìˆëŠ” ê²½ìš° ì´ ì„¹ì…˜ì€ GA4ì—ì„œ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ê° ì‚¬ìš©ìì— ëŒ€í•œ ì¶”ê°€ ì†ì„±ì„ ì¶”ê°€í•  ìˆ˜ ìˆëŠ” ì¢‹ì€ ê¸°íšŒ ì¸êµ¬í†µê³„ëŠ” ë³€ê²½ê°€ëŠ¥ì„±ì´ ìˆìŒ. ê°„ë‹¨í•˜ê²Œ ì„¤ëª…í•˜ê¸° ìœ„í•´ ì‚¬ìš©ìê°€ ì•±ì— ì²˜ìŒì ‘ì† í–ˆì„ë•Œ êµ¬ê¸€ ì• ë„ë¦¬í‹±ìŠ¤ 4ê°€ ì œê³µí•˜ëŠ” ì¸êµ¬í†µê³„í•™ì  ì •ë³´ë¥¼ MIN(event_timestamp)ë¡œ í‘œì‹œëœëŒ€ë¡œë§Œ ì‚¬ìš© 123456789101112131415161718192021CREATE OR REPLACE VIEW bqmlga4.user_demographics AS ( WITH first_values AS ( SELECT user_pseudo_id, geo.country as country, device.operating_system as operating_system, device.language as language, ROW_NUMBER() OVER (PARTITION BY user_pseudo_id ORDER BY event_timestamp DESC) AS row_num FROM `firebase-public-project.analytics_153293282.events_*` WHERE event_name=&quot;user_engagement&quot; ) SELECT * EXCEPT (row_num) FROM first_values WHERE row_num = 1 );SELECT *FROM bqmlga4.user_demographics STEP 3: ê° ì‚¬ìš©ìì— ëŒ€í•œ í–‰ë™ ë°ì´í„° ì¶”ì¶œ ì²« 24ì‹œê°„ ë™ì•ˆì˜ ì‚¬ìš©ì í™œë™ì„ ê¸°ë°˜ìœ¼ë¡œ í•´ë‹¹ ì‚¬ìš©ìì˜ ì´íƒˆ ë˜ëŠ” ì¬ë°©ë¬¸ ì—¬ë¶€ë¥¼ ì˜ˆì¸¡ â†’ ì²« 24ì‹œê°„ í–‰ë™ ë°ì´í„°ë§Œ í•™ìŠµ ì‹œì¼œì•¼ í•¨ â€˜user_first_engagementâ€™ì—ì„œ ì²« ì¸ê²Œì´ì§€ë¨¼íŠ¸ì˜ ì›” ë˜ëŠ” ì¼ê³¼ ê°™ì€ ì¶”ê°€ ì‹œê°„ ê´€ë ¨ ê¸°ëŠ¥ì„ ì¶”ì¶œí•  ìˆ˜ë„ ìˆìŒ ìš°ì„  event_nameì„ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„° ì„¸íŠ¸ì— ì¡´ì¬í•˜ëŠ” ëª¨ë“  ê³ ìœ  ì´ë²¤íŠ¸ë¥¼ íƒìƒ‰ â†’ ì´ 37ê°œì˜ ì´ë²¤íŠ¸ 12345678SELECT event_name, COUNT(event_name) as event_countFROM `firebase-public-project.analytics_153293282.events_*`GROUP BY 1ORDER BY event_count DESC ì—¬ê¸°ì„œ user_engagement, level_start_quickplay, level_end_quickplay, level_complete_quickplay, level_reset_quickplay, post_score, spend_virtual_currency, ad_reward, challenge_a_friend, completed_5_levels, use_extra_steps â†’ ì´ í”¼ì²˜ë“¤ë§Œ ê°€ì§€ê³  ê°ê° ìœ ì €ê°€ ëª‡ë²ˆì´ë‚˜ ì´ ì´ë²¤íŠ¸ë¥¼ ë°œìƒì‹œì¼°ëŠ”ì§€ í™•ì¸ ìì²´ ë°ì´í„° ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ì§‘ê³„ ë° ì¶”ì¶œí•  ìˆ˜ ìˆëŠ” ì´ë²¤íŠ¸ ìœ í˜•ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ. ì•±ì´ GAì— ë§¤ìš° ë‹¤ë¥¸ event_namesë¥¼ ì „ì†¡í• ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì‹œë‚˜ë¦¬ì˜¤ ê°€ì¥ ì í•©í•œ ì´ë²¤íŠ¸ë¥¼ ì‚¬ìš©í•´ì•¼ í•¨ 1234567891011121314151617181920212223242526272829303132333435363738CREATE OR REPLACE VIEW bqmlga4.user_aggregate_behavior AS (WITH events_first24hr AS ( #select user data only from first 24 hr of using the app SELECT e.* FROM `firebase-public-project.analytics_153293282.events_*` e JOIN bqmlga4.returningusers r ON e.user_pseudo_id = r.user_pseudo_id WHERE e.event_timestamp &lt;= r.ts_24hr_after_first_engagement )SELECT user_pseudo_id, SUM(IF(event_name = &#x27;user_engagement&#x27;, 1, 0)) AS cnt_user_engagement, SUM(IF(event_name = &#x27;level_start_quickplay&#x27;, 1, 0)) AS cnt_level_start_quickplay, SUM(IF(event_name = &#x27;level_end_quickplay&#x27;, 1, 0)) AS cnt_level_end_quickplay, SUM(IF(event_name = &#x27;level_complete_quickplay&#x27;, 1, 0)) AS cnt_level_complete_quickplay, SUM(IF(event_name = &#x27;level_reset_quickplay&#x27;, 1, 0)) AS cnt_level_reset_quickplay, SUM(IF(event_name = &#x27;post_score&#x27;, 1, 0)) AS cnt_post_score, SUM(IF(event_name = &#x27;spend_virtual_currency&#x27;, 1, 0)) AS cnt_spend_virtual_currency, SUM(IF(event_name = &#x27;ad_reward&#x27;, 1, 0)) AS cnt_ad_reward, SUM(IF(event_name = &#x27;challenge_a_friend&#x27;, 1, 0)) AS cnt_challenge_a_friend, SUM(IF(event_name = &#x27;completed_5_levels&#x27;, 1, 0)) AS cnt_completed_5_levels, SUM(IF(event_name = &#x27;use_extra_steps&#x27;, 1, 0)) AS cnt_use_extra_steps,FROM events_first24hrGROUP BY 1 );SELECT *FROM bqmlga4.user_aggregate_behavior ì´ ë‹¨ê³„ëŠ” ë™ì‘ ìˆ˜í–‰ ë¹ˆë„ ì™¸ì—ë„ ì‚¬ìš©ìê°€ ì‚¬ìš©í•œ ê²Œì„ ë‚´ í™”íì˜ ì´ì•¡ì´ë‚˜ ì•±ê³¼ ë” ê´€ë ¨ì´ ìˆì„ìˆ˜ ìˆëŠ” íŠ¹ì • ì•±ë³„ ë§ˆì¼ìŠ¤í†¤(ì˜ˆ: íŠ¹ì • ì„ê³„ê°’ì˜ ê²½í—˜ì¹˜ íšë“ ë˜ëŠ” 5íšŒ ì´ìƒ ë ˆë²¨ì—…)ì— ë„ë‹¬í–ˆëŠ”ì§€ ì—¬ë¶€ì™€ ê°™ì€ ë‹¤ë¥¸ í–‰ë™ íŠ¹ì§•ì„ í¬í•¨í• ìˆ˜ ìˆë‹¤ëŠ” ì ì— ìœ ì˜. STEP 4: ì„¸ ë°ì´í„° ê²°í•©í•˜ì—¬ í•™ìŠµë°ì´í„° êµ¬ì¶• ìµœì¢… í•™ìŠµ ë°ì´í„° ë² ì´ìŠ¤ êµ¬ì¶•: ì—¬ê¸°ì–´ì„¸ bounce&#x3D;0ì„ ì§€ì •í•˜ì—¬ ì•± ì‚¬ìš©í›„ ì²˜ìŒ 10ë¶„ ì´ë‚´ì— bounceí•˜ì§€ ì•Šì€ ì‚¬ìš©ìë¡œë§Œ í•™ìŠµ ë°ì´í„°ë¥¼ ì œí• í•  ìˆ˜ë„ ìˆìŒ. 12345678910111213141516171819202122232425262728293031323334353637CREATE OR REPLACE VIEW bqmlga4.train AS ( SELECT dem.*, IFNULL(beh.cnt_user_engagement, 0) AS cnt_user_engagement, IFNULL(beh.cnt_level_start_quickplay, 0) AS cnt_level_start_quickplay, IFNULL(beh.cnt_level_end_quickplay, 0) AS cnt_level_end_quickplay, IFNULL(beh.cnt_level_complete_quickplay, 0) AS cnt_level_complete_quickplay, IFNULL(beh.cnt_level_reset_quickplay, 0) AS cnt_level_reset_quickplay, IFNULL(beh.cnt_post_score, 0) AS cnt_post_score, IFNULL(beh.cnt_spend_virtual_currency, 0) AS cnt_spend_virtual_currency, IFNULL(beh.cnt_ad_reward, 0) AS cnt_ad_reward, IFNULL(beh.cnt_challenge_a_friend, 0) AS cnt_challenge_a_friend, IFNULL(beh.cnt_completed_5_levels, 0) AS cnt_completed_5_levels, IFNULL(beh.cnt_use_extra_steps, 0) AS cnt_use_extra_steps, ret.user_first_engagement, ret.month, ret.julianday, ret.dayofweek, ret.churned FROM bqmlga4.returningusers ret LEFT OUTER JOIN bqmlga4.user_demographics dem ON ret.user_pseudo_id = dem.user_pseudo_id LEFT OUTER JOIN bqmlga4.user_aggregate_behavior beh ON ret.user_pseudo_id = beh.user_pseudo_id WHERE ret.bounced = 0 );SELECT *FROM bqmlga4.train í•™ìŠµë°ì´í„°ë¡œ ë¹…ì¿¼ë¦¬ ML í•™ìŠµí•˜ê¸° ì´ì§„ë¶„ë¥˜ ì‘ì—…ì´ë¯€ë¡œ ê°„ë‹¨í•˜ê²Œ logistic regressionìœ¼ë¡œ ì‹œì‘í• ìˆ˜ ìˆì§€ë§Œ ë‹¤ë¥¸ ëª¨ë¸ë„ ì‚¬ìš© ê°€ëŠ¥í•˜ë‹¤. M o d e l Advantage Disadvantage Logistic Regression LOGISTIC_REG ë‹¤ë¥¸ íƒ€ì…ì— ë¹„í•´ í•™ìŠµ ì‹œê°„ì´ ë¹ ë¦„ ëª¨ë¸ ì„±ëŠ¥ì´ ì¡°ê¸ˆ ë–¨ì–´ì§ XGBoost BOOSTED_TREE_CLASSIFIER ë†’ì€ ëª¨ë¸ ìˆ˜í–‰ ëŠ¥ë ¥ feature importanceë¥¼ ê²€ì‚¬í• ìˆ˜ ìˆë‹¤, LOGISTIC_REG ì— ë¹„í•´ í•™ìŠµ ì‹œê°„ì´ ëŠë¦¼ Deep Neural Networks DNN_CLASSIFIER ë†’ì€ ëª¨ë¸ ìˆ˜í–‰ ëŠ¥ë ¥ LOGISTIC_REG ì— ë¹„í•´ í•™ìŠµ ì‹œê°„ì´ ëŠë¦¼ AutoML AUTOML_CLASSIFIER ë§¤ìš° ë†’ì€ ëª¨ë¸ ìˆ˜í–‰ ëŠ¥ë ¥ ì ì–´ë„ ëª‡ì‹œê°„ ì •ë„ í›ˆë ¨ì‹œê°„ì´ ê±¸ë¦¬ê³  ëª¨ë¸ì´ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ ì„¤ëª…í•˜ê¸° ì‰½ì§€ ì•ŠìŒ í›ˆë ¨, í…ŒìŠ¤íŠ¸ ì…‹ìœ¼ë¡œ ë¶„í• í•˜ì§€ ì•Šì•„ë„ CREATE MODELë¬¸ì„ ì‹¤í–‰í•˜ë©´ BigQuery MLì´ ìë™ì ìœ¼ë¡œ í•™ìŠµí•˜ê¸° ë•Œë¬¸ì— í•™ìŠµ í›„ ë°”ë¡œ ëª¨ë¸ì„ í‰ê°€ í• ìˆ˜ ìˆìŒ í•˜ì´í¼ íŒŒë¼ë¯¸í„° íŠœë‹: ê° ëª¨ë¸ì— ëŒ€í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ íŠœë‹í• ìˆ˜ ë„ ìˆìŒ(link) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354CREATE OR REPLACE MODEL bqmlga4.churn_logregOPTIONS( MODEL_TYPE=&quot;LOGISTIC_REG&quot;, INPUT_LABEL_COLS=[&quot;churned&quot;]) ASSELECT *FROM bqmlga4.train---CREATE OR REPLACE MODEL bqmlga4.churn_xgbOPTIONS( MODEL_TYPE=&quot;BOOSTED_TREE_CLASSIFIER&quot;, INPUT_LABEL_COLS=[&quot;churned&quot;]) ASSELECT * EXCEPT(user_pseudo_id)FROM bqmlga4.train---CREATE OR REPLACE MODEL bqmlga4.churn_dnnOPTIONS( MODEL_TYPE=&quot;DNN_CLASSIFIER&quot;, INPUT_LABEL_COLS=[&quot;churned&quot;]) ASSELECT * EXCEPT(user_pseudo_id)FROM bqmlga4.train---CREATE OR REPLACE MODEL bqmlga4.churn_automlOPTIONS( MODEL_TYPE=&quot;AUTOML_CLASSIFIER&quot;, INPUT_LABEL_COLS=[&quot;churned&quot;], BUDGET_HOURS=1.0) ASSELECT * EXCEPT(user_pseudo_id)FROM bqmlga4.train AutoML AutoML Tablesë¥¼ ì‚¬ìš©í•˜ë©´ êµ¬ì¡°í™”ëœ ë°ì´í„°ì— ëŒ€í•œ ìµœì‹  ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ ì†ë„ì™€ ê·œëª¨ë¥¼ ëŒ€í­ í–¥ìƒ ì‹œì¼œ ìë™ìœ¼ë¡œ êµ¬ì¶•í•  ìˆ˜ ìˆìŒ. AutoML TablesëŠ” êµ¬ê¸€ì˜ ëª¨ë¸ ì§‘ë‹¨ì—ì„œ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¥¼ ìë™ìœ¼ë¡œ ê²€ìƒ‰í•˜ì—¬ ê°„ë‹¨í•œ ë°ì´í„° ì§‘í•©ì„ ìœ„í•œ ì„ í˜•&#x2F;ë¡œì§€ìŠ¤í‹± íœ˜ê·€ëª¨ë¸ë¶€í„° ë” í¬ê³  ë³µì¡í•œ ë°ì´í„° ì§‘í•©ì„ ìœ„í•œ ê³ ê¸‰ ì‹¬ì¸µ, ì•™ìƒë¸” ë° ì•„í‚¤í…ì³ ê²€ìƒ‰ ë°©ë²•ì— ì´ë¥´ê¸°ê¹Œì§€ ì‚¬ìš©ìì˜ ìš”êµ¬ì— ê°€ì¥ ì í•©í•œ ëª¨ë¸ì„ ì°¾ì•„ì¤Œ. BUDGET_HOURS ë§¤ê°œë³€ìˆ˜ëŠ” AutoML í…Œì´ë¸” í•™ìŠµì„ ìœ„í•œ ê²ƒìœ¼ë¡œ ì‹œê°„ ë‹¨ìœ„ë¡œ ì§€ì •ë¨. ê¸°ë³¸ê°’ì€ 1.0ì´ë©° 1.0ì—ì„œ 72.0ì‚¬ì´ì—¬ì•¼ í•¨. ëª¨ë¸ í‰ê°€Logistic Regression1234SELECT *FROM ML.EVALUATE(MODEL bqmlga4.churn_logreg) XGBoost1234SELECT *FROM ML.EVALUATE(MODEL bqmlga4.churn_xgb) DNN1234SELECT *FROM ML.EVALUATE(MODEL bqmlga4.churn_dnn) AutoML1234SELECT *FROM ML.EVALUATE(MODEL bqmlga4.churn_automl) ëª¨ë¸ ì˜ˆì¸¡ ì´íƒˆ ì„±í–¥ì— ëŒ€í•œ ì˜ˆì¸¡ì„ í•  ìˆ˜ ìˆìŒ 12345SELECT *FROM ML.PREDICT(MODEL bqmlga4.churn_logreg, (SELECT * FROM bqmlga4.train)) #can be replaced with a test dataset ì„±í–¥ ëª¨ë¸ë§ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ì¶œë ¥ì€ í–‰ë™ì´ ë°œìƒí•  í™•ë¥ . ë°‘ì˜ ì½”ë“œëŠ” ì‚¬ìš©ìê°€ 24ì‹œê°„ í›„ì— ì¬ë°©ë¬¸í•  í™•ë¥ ì„ ë°˜í™˜ â†’ í™•ë¥ ì´ ë†’ê³  1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì‚¬ìš©ìê°€ ì´íƒˆí•  ê°€ëŠ¥ì„±ì´ ë†’ê³ , 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì‚¬ìš©ìê°€ ì¬ë°©ë¬¸í•  ê°€ëŠ¥ì„±ì´ ë†’ìŒ 123456789SELECT user_pseudo_id, churned, predicted_churned, predicted_churned_probs[OFFSET(0)].prob as probability_churned FROM ML.PREDICT(MODEL bqmlga4.churn_logreg, (SELECT * FROM bqmlga4.train)) #can be replaced with a proper test dataset ë¹…ì¿¼ë¦¬ ë°–ìœ¼ë¡œ ì˜ˆì¸¡ ê²°ê³¼ export Bigquery Storage APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ Pandas ë°ì´í„° í”„ë ˆì„ìœ¼ë¡œ ë‚´ë³´ë‚¼ìˆ˜ ìˆìŒ(ë¬¸ì„œ ë° ì½”ë“œ ìƒ˜í”Œ) ë‹¤ë¥¸ BigQuery í´ë¼ì´ì–¸íŠ¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í• ìˆ˜ë„ ìˆìŒ. ë³„ë„ì˜ ì„œë¹„ìŠ¤ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì˜ˆì¸¡ í…Œì´ë¸”ì„ Google í´ë¼ìš°ë“œ ìŠ¤í† ë¦¬ì§€(GCS)ë¡œ ì§ì ‘ ë³´ë‚¼ìˆ˜ë„ ìˆìŒ â†’ ê°€ì¥ ì‰¬ìš´ ë°©ë²•ì€ SQLì„ ì‚¬ìš©í•˜ì—¬ GCSë¡œ ì§ì ‘ ë³´ë‚´ëŠ” ê²ƒ 123456789101112EXPORT DATA OPTIONS (uri=&quot;gs://mybucket/myfile/churnpredictions.csv&quot;, format=CSV) AS SELECT user_pseudo_id, churned, predicted_churned, predicted_churned_probs[OFFSET(0)].prob as probability_churnedFROM ML.PREDICT(MODEL bqmlga4.churn_logreg, (SELECT * FROM bqmlga4.train)) #can be replaced with a proper test dataset Reference https://cloud.google.com/architecture/propensity-modeling-gaming?hl=ko https://github.com/GoogleCloudPlatform/analytics-componentized-patterns/tree/master/gaming/propensity-model/bqml","categories":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"Big Query","slug":"Big-Query","permalink":"https://jmj3047.github.io/tags/Big-Query/"},{"name":"BigQueryML","slug":"BigQueryML","permalink":"https://jmj3047.github.io/tags/BigQueryML/"},{"name":"ML Process","slug":"ML-Process","permalink":"https://jmj3047.github.io/tags/ML-Process/"}],"keywords":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}]},{"title":"ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ í‰ê°€ì‹œ ë‚˜ì˜¤ëŠ” ìš©ì–´ë“¤ ì •ë¦¬","slug":"Logisitic_Eval_words","date":"2023-03-05T15:00:00.000Z","updated":"2023-03-19T07:03:43.987Z","comments":true,"path":"2023/03/06/Logisitic_Eval_words/","link":"","permalink":"https://jmj3047.github.io/2023/03/06/Logisitic_Eval_words/","excerpt":"","text":"ê°œìš” ì´ì „ í¬ìŠ¤íŠ¸ì˜ 5ë‹¨ê³„ì—ì„œ ML ëª¨ë¸ì„ í‰ê°€í• ë•Œ ë‚˜ì™”ë˜ ì§€í‘œë“¤ì— ëŒ€í•œ ì†Œê°œ ë°”ì´ë„ˆë¦¬ ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ì„ ì‚¬ìš©í–ˆì„ ë•Œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ì§€í‘œë“¤ì„ ì†Œê°œ ì•Œì•„ì•¼ í•  ê°œë…True&#x2F;False &amp; Positive&#x2F;Negative ì„ê³„ê°’(Threshold) ë¡œì§€ìŠ¤í‹± íšŒê·€ ê°’ì„ ì´ì§„ ì¹´í…Œê³ ë¦¬ì— ë§¤í•‘í•˜ë ¤ë©´ ë¶„ë¥˜ ì„ê³„ê°’(ê²°ì • ì„ê³„ê°’)ì„ ì •ì˜ í•´ì•¼ í•¨ ì¼ë°˜ì ìœ¼ë¡œ 0.5ë¡œ ì¡ì§€ë§Œ, ë°ì´í„°ë‚˜ ìƒí™©ì— ë”°ë¼ì„œ ë‹¬ë¼ì§ˆìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¡°ì •í•´ì•¼ í•˜ëŠ” ê°’. ì •ë°€ë„(Precision) &#x3D; $\\frac{TP}{TP+FP}$ ê±°ì§“ ì–‘ì„±(FP)ì´ ìƒì„±ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ì˜ ì •ë°€ë„ëŠ” 1.0 ì¬í˜„ìœ¨(Recall) &#x3D; $\\frac{TP}{TP+FN}$ ê±°ì§“ ìŒì„±(FN)ì„ ìƒì„±í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ì˜ ì¬í˜„ìœ¨ì€ 1.0 ì‹¤ì œ ì–‘ì„± ì¤‘ ì •í™•íˆ ì–‘ì„±ì´ë¼ê³  ì‹ë³„ëœ ì‚¬ë¡€ì˜ ë¹„ìœ¨ ì •í™•ë„(Accuracy) &#x3D; $\\frac{ì˜¬ë°”ë¥¸ ì˜ˆì¸¡ê°’}{ì „ì²´ ì˜ˆì¸¡ ê°’}$ ì´ì§„ë¶„ë¥˜ì—ì„œ ì •í™•ë„ &#x3D; $\\frac{TP+TN}{TP+TN+FP+FN}$ ë¹„ ê³µì‹ì ìœ¼ë¡œ ëª¨ë¸ì´ ì˜¬ë°”ë¥´ê²Œ ì˜ˆì¸¡í•œ ë¹„ìœ¨ í´ë˜ìŠ¤ê°„ ë°ì´í„° ì°¨ì´ê°€ ìˆë‹¤ë©´ ì •í™•í•˜ì§€ ì•Šì„ ê°€ëŠ¥ì„±ì´ ë†’ìŒ ROC curve ëª¨ë“  ë¶„ë¥˜ ì„ê³„ê°’ì—ì„œ ë¶„ë¥˜ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ëŠ” ê·¸ë˜í”„ ì°¸ ì–‘ì„±ë¥ (TPR)ê³¼ ê±°ì§“ì–‘ì„±ë¥ (FPR)ì„ ë§¤ê°œë³€ìˆ˜ë¡œ í‘œì‹œí•œë‹¤. ë¶„ë¥˜ ì„ê³„ê°’ì„ ë‚®ì¶”ë©´ ë” ë§ì€ í•­ëª©ì´ ì–‘ìˆ˜ë¡œ ë¶„ë¥˜ ë˜ë¯€ë¡œ ê±°ì§“ì–‘ì„±ê³¼ ì°¸ ì–‘ì„±ì´ ëª¨ë‘ ì¦ê°€í•œë‹¤. ROC ê³¡ì„ ì˜ í¬ì¸íŠ¸ë¥¼ ê³„ì‚¬ë‚˜ê¸° ìœ„í•´ ë¶„ë¥˜ ì…ê³—ê°’ì´ ë‹¤ë¥¸ ì—¬ëŸ¬ì°¨ë¡€ì˜ ë¡œì§€ìŠ¤í‹± íšŒê·€ëª¨ë¸ì„ í‰ê°€í–ˆì§€ë§Œ ì´ëŠ” ë¹„íš¨ìœ¨ì  â†’ ì´ëŸ¬í•œ ì •ë³´ì œê³µì„ í• ìˆ˜ ìˆëŠ” íš¨ìœ¨ì ì¸ ì •ë ¬ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ì¸ AUCë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ  AUC: ROC ê³¡ì„  ì•„ë˜ì˜ ì˜ì—­ ROC ê³¡ì„ ì˜ ì ë¶„ê°’ 0ë¶€í„° 1ì‚¬ì˜ ê°’ ì ˆëŒ“ê°’ì´ ì•„ë‹Œ ì˜ˆì¸¡ì˜ ìˆœìœ„ë¥¼ ì¸¡ì •í•˜ë¯€ë¡œ ê·œëª¨ ë¶ˆë³€ ì„ íƒí•œ ë¶„ìœ  ì„ê³—ê°’ê³¼ ê´€ê³„ ì—†ì´ ëª¨ë¸ì˜ ì˜ˆì¸¡ í’ˆì§ˆì„ ì¸¡ì •í•˜ê¸°ì— ë¶„ë¥˜ ê¸°ì¤€ ë¶ˆë³€ ì£¼ì˜ ì‚¬í•­ í™•ì¥ ë¶ˆë³€ì´ í•­ìƒ ë°”ëŒì§í•œê²ƒì€ ì•„ë‹˜ â†’ ì˜ ë³´ì •ëœ í™•ë¥  ì¶œë ¥ì´ í•„ìš”í•œ ê²½ìš°ê°€ ìˆëŠ”ë° AUCëŠ” ì´ë¥¼ ì•Œë ¤ì£¼ì§€ ì•ŠìŒ ë¶„ë¥˜ ì„ê³„ê°’ ë¶ˆë³€ì´ í•­ìƒ ë°”ëŒì§í•œ ê²ƒì€ ì•„ë‹˜ â†’ ê±°ì§“ìŒì„± ëŒ€ë¹„ ê±°ì§“ì–‘ì„±ì˜ ë¹„ìš©ì´ í° ê²½ìš° í•œ ìê¸° ìœ í˜•ì˜ ë¶„ë¥˜ ì˜¤ë¥˜ë¥¼ ìµœì†Œí™” í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•¨. ì˜ˆë¥¼ ë“¤ì–´ ìŠ¤íŒ¸ ê°ì§€ë¥¼ ì‹¤í–‰í• ë•Œ ê±°ì§“ ì–‘ì„±ì„ ìµœì†Œí™”í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•  ìˆ˜ ìˆìŒ.(ì´ë ‡ê²Œ í•˜ë©´ ê±°ì§“ ìŒì„±ì´ í¬ê²Œ ì¦ê°€í•˜ë”ë¼ë„ ìƒê´€ ì—†ìŒ) AUCëŠ” ì´ ìœ í˜•ì˜ ìµœì í™”ì— ìœ ìš©í•œ ì¸¡ì •í•­ëª©ì´ ì•„ë‹˜. GCP ê³µì‹ ì„¤ëª… Aggregate Metrics For binary classification models, all metrics reflect the values computed when threshold is set to 0.5. ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ì˜ ê²½ìš° ëª¨ë“  ì§€í‘œëŠ” ì„ê³„ê°’ì„ 0.5ë¡œ ì„¤ì •í–ˆì„ ë•Œ ê³„ì‚°ëœ ê°’ì„ ë°˜ì˜í•©ë‹ˆë‹¤. Threshold : For binary classification models, this is the positive class threshold. ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ì˜ ê²½ìš° ì´ëŠ” ì–‘ì„± í´ë˜ìŠ¤ì˜ ì„ê³„ê°’ì„ ëœ»í•©ë‹ˆë‹¤. Precision : Fraction of predicted positives that were actual positives. ì˜ˆì¸¡ëœ ì–‘ì„± ë°˜ì‘ ì¤‘ ì‹¤ì œ ì–‘ì„± ë°˜ì‘ìœ¼ë¡œ íŒëª…ëœ ë¹„ìœ¨ì…ë‹ˆë‹¤. â†’ í•™ìŠµ ëª¨ë¸ì´ íŒë‹¨í•œ ì–‘ì„± ë¹„ìœ¨ Recall : Fraction of actual positives that were predicted positives. ì˜ˆì¸¡ëœ ì–‘ì„± ë°˜ì‘ ì¤‘ ì‹¤ì œ ì–‘ì„± ë°˜ì‘ì˜ ë¹„ìœ¨ì…ë‹ˆë‹¤. â†’ ì‹¤ì œ ì–‘ì„±ì´ë¼ê³  ë‚˜ì˜¨ ë¹„ìœ¨ Accuracy : Fraction of predictions given the correct label. ì˜¬ë°”ë¥¸ ë ˆì´ë¸”ì´ ì£¼ì–´ì§„ ì˜ˆì¸¡ì˜ ë¹„ìœ¨ì…ë‹ˆë‹¤. â†’ precisionê³¼ recall ì„ ë¹„êµí–ˆì„ ë•Œ í•™ìŠµ ëª¨ë¸ì´ ë§ì¶˜ ë¹„ìœ¨ F1 score : Harmonic mean of precision and recall. ì •í™•ë„ ë° íšŒìˆ˜ìœ¨ì˜ ì¡°í™” í‰ê· ì…ë‹ˆë‹¤. Precision ê³¼ Recallì€ ì„œë¡œ Trade-off ê´€ê³„ë¥¼ ê°€ì§€ë©´ì„œ ì ‘ê·¼í•˜ëŠ” ë°©ì‹ë„ Precisionì€ ëª¨ë¸ì˜ ì˜ˆì¸¡, Recallì€ ì •ë‹µ ë°ì´í„° ê¸°ì¤€ì´ë¯€ë¡œ ì„œë¡œ ìƒì´ í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ë‘ ì§€í‘œ ëª¨ë‘ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í™•ì¸í•˜ëŠ” ë° ì¤‘ìš”í•˜ë¯€ë¡œ ë‘˜ ë‹¤ ì‚¬ìš©ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ë‘ ì§€í‘œë¥¼ í‰ê· ê°’ìœ¼ë¡œ í†µí•´ í•˜ë‚˜ì˜ ê°’ìœ¼ë¡œ ë‚˜íƒ€ë‚´ëŠ” ë°©ë²•ìœ¼ë¡œ F1 scoreë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ê·¹ë‹¨ì ìœ¼ë¡œ Precisionê³¼ Recall ì¤‘ì— í•œìª½ì´ 1ì— ê°€ê¹ê³  í•œìª½ì´ 0ì— ê°€ê¹Œìš´ ê²½ìš° ì‚°ìˆ  í‰ê· ê³¼ ê°™ì´ 0.5ê°€ ì•„ë‹ˆë¼ 0ì— ê°€ê¹ë„ë¡ ë§Œë“¤ì–´ ì¤ë‹ˆë‹¤. ë”°ë¼ì„œ F1 scoreë¥¼ ë†’ì´ë ¤ë©´ Precision, Recallì´ ê· ì¼í•œ ê°’ì´ í•„ìš” í•˜ê¸° ë•Œë¬¸ì— ë‘ ì§€í‘œ ì„±ëŠ¥ì„ ëª¨ë‘ ë†’ì¼ ìˆ˜ ìˆë„ë¡ í•´ì•¼ í•©ë‹ˆë‹¤. $F_1 &#x3D; 2*\\frac{precision X recall}{precision + recall}$ Log loss : A measure for model performance between 0 (perfect) and 1. The greater the log-loss, the greater the predicted probability diverges from the actual label. 0(ì™„ë²½)ì—ì„œ 1 ì‚¬ì´ì˜ ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •ê°’ì…ë‹ˆë‹¤. ë¡œê·¸ ì†ì‹¤ì´ í´ìˆ˜ë¡ ì˜ˆì¸¡ í™•ë¥ ì´ ì‹¤ì œ ë ˆì´ë¸”ê³¼ ì°¨ì´ê°€ ì»¤ì§‘ë‹ˆë‹¤. ROC AUC : Area under the receiver operating characteristic curve. Score threshold Positive class threshold: Predictions above the threshold are classified as positive. ì„ê³„ê°’ì„ ì´ˆê³¼í•˜ëŠ” ì˜ˆì¸¡ì€ ì–‘ìˆ˜ë¡œ ë¶„ë¥˜ë©ë‹ˆë‹¤. Positive class &gt;50K Negative class &lt;&#x3D;50K Precision : Fraction of predicted positives that were actual positives. ì˜ˆì¸¡ëœ ì–‘ì„± ë°˜ì‘ ì¤‘ ì‹¤ì œ ì–‘ì„± ë°˜ì‘ìœ¼ë¡œ íŒëª…ëœ ë¹„ìœ¨ì…ë‹ˆë‹¤. Recall: Fraction of actual positives that were predicted positives. ì˜ˆì¸¡ëœ ì–‘ì„± ë°˜ì‘ ì¤‘ ì‹¤ì œ ì–‘ì„± ë°˜ì‘ì˜ ë¹„ìœ¨ì…ë‹ˆë‹¤. Accuracy : Fraction of actual positives that were predicted positives. ì˜¬ë°”ë¥¸ ë ˆì´ë¸”ì´ ì£¼ì–´ì§„ ì˜ˆì¸¡ì˜ ë¹„ìœ¨ì…ë‹ˆë‹¤. F1 score : Harmonic mean of precision and recall. ì •ë°€ë„ ë° íšŒìˆ˜ìœ¨ì˜ ì¡°í™” í‰ê· ì…ë‹ˆë‹¤. Precision-recall by threshold: Shows how your model performs on the top-scored label along the full range of confidence threshold values. A higher confidence threshold produces fewer false positives, which increases precision. A lower confidence threshold produces fewer false negatives, which increases recall. ì „ì²´ ì‹ ë¢°ë„ ì„ê³„ê°’ ë²”ìœ„ì—ì„œ ê°€ì¥ ë†’ì€ ì ìˆ˜ë¥¼ ë°›ì€ ë ˆì´ë¸”ì— ëŒ€í•œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‘œì‹œí•©ë‹ˆë‹¤. ì‹ ë¢°ë„ ì„ê³„ê°’ì´ ë†’ì„ìˆ˜ë¡ false positive ê°’ì´ ì¤„ì–´ë“¤ì–´ ì •ë°€ë„ê°€ ë†’ì•„ì§‘ë‹ˆë‹¤. ì‹ ë¢°ë„ ì„ê³„ê°’ì´ ë‚®ì„ìˆ˜ë¡ false negative ê°’ì´ ì¤„ì–´ë“¤ì–´ íšŒìˆ˜ìœ¨ì´ ë†’ì•„ì§‘ë‹ˆë‹¤. Precision-recall curve: Shows the trade-off between precision and recall at different confidence thresholds. A lower threshold results in higher recall but typically lower precision, while a higher threshold results in lower recall but typically with higher precision. ì„œë¡œ ë‹¤ë¥¸ ì‹ ë¢° ì„ê³„ê°’ì—ì„œ ì •ë°€ë„ì™€ ë¦¬ì½œ ê°„ì˜ ê· í˜•ì„ í‘œì‹œí•©ë‹ˆë‹¤. ì„ê³„ê°’ì´ ë‚®ì„ìˆ˜ë¡ íšŒìˆ˜ìœ¨ì€ ë†’ì§€ë§Œ ì¼ë°˜ì ìœ¼ë¡œ ì •ë°€ë„ëŠ” ë‚®ìœ¼ë©° ì„ê³„ê°’ì´ ë†’ì„ìˆ˜ë¡ íšŒìˆ˜ìœ¨ì€ ë‚®ì•„ì§€ì§€ë§Œ ì¼ë°˜ì ìœ¼ë¡œ ì •ë°€ë„ê°€ ë†’ìŠµë‹ˆë‹¤. ROC curve: The receiver operating characteristic (ROC) curve shows the trade-off between true positive rate and false positive rate. A lower threshold results in a higher true positive rate (and a higher false positive rate), while a higher threshold results in a lower true positive rate (and a lower false positive rate) receiver operating characteristic(ROC) ê³¡ì„ ì€ true positiveì˜ ë¹„ìœ¨ê³¼ false positiveì˜ ë¹„ìœ¨ ì‚¬ì´ì˜ ê· í˜•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì„ê³„ê°’ì´ ë‚®ì„ìˆ˜ë¡ true positiveì˜ ë¹„ìœ¨(ë° false positiveì˜ ë¹„ìœ¨)ì´ ë†’ì•„ì§€ê³ , ì„ê³„ê°’ì´ ë†’ì„ìˆ˜ë¡ true positiveì˜ ë¹„ìœ¨(ë° false positiveì˜ ë¹„ìœ¨)ì´ ë‚®ì•„ì§‘ë‹ˆë‹¤ Confusion matrix This table shows how often the model classified each label correctly (in blue), and which labels were most often confused for that label (in gray). ì´ í‘œëŠ” ëª¨ë¸ì´ ê° ë ˆì´ë¸”ì„ ì˜¬ë°”ë¥´ê²Œ ë¶„ë¥˜í•œ ë¹ˆë„(íŒŒë€ìƒ‰)ì™€ í•´ë‹¹ ë ˆì´ë¸”ê³¼ ê°€ì¥ ìì£¼ í˜¼ë™ë˜ëŠ” ë ˆì´ë¸”(íšŒìƒ‰)ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. Reference https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall?hl=ko https://developers.google.com/machine-learning/crash-course/classification/true-false-positive-negative?hl=ko https://gaussian37.github.io/ml-concept-ml-evaluation/#f1-score-1","categories":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"Big Query","slug":"Big-Query","permalink":"https://jmj3047.github.io/tags/Big-Query/"}],"keywords":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}]},{"title":"Pythonìœ¼ë¡œ kaggle ë°ì´í„° GCPì— ì ì¬","slug":"Kaggle_GCP","date":"2023-03-02T15:00:00.000Z","updated":"2023-03-19T07:03:08.028Z","comments":true,"path":"2023/03/03/Kaggle_GCP/","link":"","permalink":"https://jmj3047.github.io/2023/03/03/Kaggle_GCP/","excerpt":"","text":"ìš”ì•½ Kaggle ë°ì´í„° ë‹¤ìš´ë¡œë“œ GCPì— ë°ì´í„° ì„¸íŠ¸ ë§Œë“¤ê³  ì„œë¹„ìŠ¤ ê³„ì • ìƒì„±í•˜ê¸° Python-BigQuery ì—°ê²° í›„ ë°ì´í„° ì¡°íšŒ ë°ì´í„° ì ì¬ í•˜ê¸° Kaggle ë°ì´í„° ë‹¤ìš´ë¡œë“œ kaggleì„ ì„¤ì¹˜í•œë‹¤ 1!pip install kaggle kaggleì˜ keyë¥¼ ë°›ì•„ì˜¨ë‹¤ 123!mkdir ~/.kaggle!echo &#x27;&#123;&quot;username&quot;:&quot;your_id&quot;,&quot;key&quot;:&quot;your_key&quot;&#125;&#x27; &gt; ~/.kaggle/kaggle.json!chmod 600 ~/.kaggle/kaggle.json kaggel TOKEN ë°›ì•„ì˜¤ê¸° kaggleì— ì ‘ì†í•œë‹¤ìŒ í”„ë¡œí•„ì„ ì„ íƒí•˜ê³  Accountë¥¼ ëˆ„ë¥¸ë‹¤ í™”ë©´ì„ ë‚´ë¦¬ë©´ API íƒ­ì„ ì°¾ì„ìˆ˜ ìˆë‹¤. Create New API Tokenì„ ëˆ„ë¥´ê³  ë‹¤ìš´ kaggle.json íŒŒì¼ì„ ë°›ì•„ì¤ë‹ˆë‹¤. í•œë²ˆ ë°œê¸‰ ë°›ìœ¼ë©´ ì´ì „ì˜ ê²ƒì€ ì•Œë ¤ì£¼ì§€ ì•Šê¸° ë•Œë¬¸ì— ìŠì–´ë²„ë ¸ë‹¤ë©´ Expire API TOKEN ìœ¼ë¡œ ëª¨ë‘ ì§€ê³  ìƒˆë¡œìš´ í† í°ì„ ë°›ëŠ”ë‹¤. kaggleì—ì„œ ì›í•˜ëŠ” ë°ì´í„°ë¥¼ ë‹¤ìš´ ë°›ì•„ì¤€ë‹¤ â†’ competitionsì—ì„œ ì›í•˜ëŠ” competitionì„ ì„ íƒí•˜ê³  data íƒ­ìœ¼ë¡œ ê°€ë©´ ë‹¤ìš´ ë°›ì„ ìˆ˜ ìˆëŠ” apiê°€ ìˆë‹¤. GCPì— ë°ì´í„° ì„¸íŠ¸ ë§Œë“¤ê³  ì„œë¹„ìŠ¤ ê³„ì • ìƒì„±í•˜ê¸° GCPì— í”„ë¡œì íŠ¸ë¥¼ ë§Œë“¤ê³  ìƒˆë¡œìš´ ë°ì´í„° ì„¸íŠ¸ë¥¼ ë§Œë“¤ì–´ ì¤€ë‹¤ ì¤‘ê°„ì— Default table expiration ì„ í´ë¦­í•˜ë©´ í…Œì´ë¸”ì´ ë©°ì¹  í›„ì— ë§Œë£Œë˜ëŠ”ì§€ë„ ì„¤ì • ê°€ëŠ¥í•˜ë‹¤ ì„œë¹„ìŠ¤ ê³„ì • ìƒì„± GCP ì¢Œì¸¡ ìƒë‹¨ â€˜íƒìƒ‰ ë©”ë‰´â€™ í´ë¦­í›„ â€˜IAM ë° ê´€ë¦¬ìâ€™ì˜ â€˜ì„œë¹„ìŠ¤ ê³„ì •â€™ìœ¼ë¡œ ì´ë™ â€˜+ ì„œë¹„ìŠ¤ ê³„ì • ë§Œë“¤ê¸°â€™ í´ë¦­ í›„ ì„œë¹„ìŠ¤ ê³„ì • ë§Œë“¤ê¸° ì§„í–‰ ì„œë¹„ìŠ¤ ê³„ì • í‚¤ ìƒì„± í›„ JSON íŒŒì¼ ì¶”ì¶œ Emailì— ìƒì„±ëœ ì„œë¹„ìŠ¤ ê³„ì • í´ë¦­ â†’ í˜ì´ì§€ ìƒë‹¨ì— KEYS â†’ ADD KEY â†’ Create new key config í´ë” ìƒì„± í›„ í•´ë‹¹ ê²½ë¡œì— json íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì„œë¹„ìŠ¤ ê³„ì •ì— ë¹…ì¿¼ë¦¬ ê´€ë ¨ ì—­í•  ì¶”ê°€ GCP ì¢Œì¸¡ ìƒë‹¨ â€˜íƒìƒ‰ ë©”ë‰´â€™ í´ë¦­í›„ â€˜IAM ë° ê´€ë¦¬ìâ€™ì˜ â€˜IAMâ€™ìœ¼ë¡œ ì´ë™ â€˜ì¶”ê°€â€™ í´ë¦­ - ìƒì„±ëœ ì„œë¹„ìŠ¤ ê³„ì • ì´ë©”ì¼ ì¶”ê°€ ë° â€˜BigQuery ê´€ë¦¬ìâ€™ ì—­í•  ì„ íƒ í›„ ì €ì¥ Python-BigQuery ì—°ê²° í›„ ë°ì´í„° ì¡°íšŒ êµ¬ê¸€ í´ë¼ìš°ë“œ ë¹…ì¿¼ë¦¬ í´ë¼ì´ì–¸íŠ¸ ì„¤ì¹˜ 1!pip install google-cloud-bigquery ì„œë¹„ìŠ¤ ê³„ì • í‚¤ ì„¤ì • â†’ ë¹…ì¿¼ë¦¬ í´ë¼ì´ì–¸íŠ¸ ì •ì˜ â†’ ë°ì´í„° ì¡°íšŒ ì¿¼ë¦¬ ì‹¤í–‰ ë°ì´í„° ì ì¬ í•˜ê¸° ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¨ë‹¤ 1234567import pandas as pdBASE_DIR = &quot;./&quot;train = pd.read_csv(BASE_DIR + &#x27;train.csv&#x27;)test = pd.read_csv(BASE_DIR + &#x27;test.csv&#x27;)train.shape, test.shape 1((3000888, 6), (28512, 5)) ë°ì´í„°ë¥¼ ì ì¬ í•œë‹¤ 12345678910from google.oauth2 import service_accountimport pandas_gbqcd = service_account.Credentials.from_service_account_file(&quot;./config/my-project-230227-c3691227da1d.json&quot;)project_id = &#x27;my-project-230227&#x27;train_table = &#x27;kaggle_data.train&#x27;test_table = &#x27;kaggle_data.test&#x27;train.to_gbq(train_table,project_id,if_exists=&#x27;replace&#x27;,credentials=cd) test.to_gbq(test_table,project_id,if_exists=&#x27;replace&#x27;,credentials=cd) print(&#x27;migration complete&#x27;) GCPì— ë“¤ì–´ê°€ì„œ í™•ì¸í•´ ë³¸ë‹¤ ì´ë ‡ê²Œ í…Œì´ë¸”ì„ gcpì— ì ì¬í•˜ë©´ ë°ì´í„°ë“¤ì„ ë‹¤ì–‘í•œ ë°©ë©´ìœ¼ë¡œ ì‚¬ìš©ì´ ê°€ëŠ¥í•˜ë‹¤: Looker Studioë¥¼ ì´ìš©í•œ ì‹œê°í™” Reference https://dschloe.github.io/gcp/bigquery/01_settings&#x2F;python_bigquery&#x2F; https:&#x2F;&#x2F;velog.io&#x2F;@skyepodium&#x2F;Kaggle-API-ì‚¬ìš©ë²• https://wooiljeong.github.io/python/python-bigquery/","categories":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}],"tags":[{"name":"Big Query","slug":"Big-Query","permalink":"https://jmj3047.github.io/tags/Big-Query/"},{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/tags/Python/"},{"name":"kaggle","slug":"kaggle","permalink":"https://jmj3047.github.io/tags/kaggle/"}],"keywords":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}]},{"title":"GCP - Looker Studio ì—°ê²°í•˜ì—¬ ëŒ€ì‹œë³´ë“œ ì‘ì„±","slug":"GCP_LookerStudio","date":"2023-03-01T15:00:00.000Z","updated":"2023-03-19T07:02:38.982Z","comments":true,"path":"2023/03/02/GCP_LookerStudio/","link":"","permalink":"https://jmj3047.github.io/2023/03/02/GCP_LookerStudio/","excerpt":"","text":"ê°œìš” GCP - Looker Studio ì—°ê²°í•´ì„œ ëŒ€ì‹œë³´ë“œ ì‘ì„±í•˜ê¸° bigquery-public-data.ml_datasets.census_adult_income ë°ì´í„° ì‚¬ìš© ëª©í‘œ ëŒ€ì‹œë³´ë“œë¡œ ë°ì´í„°ë¥¼ ì‹œê°í™” í•˜ì—¬ ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•´ë³¸ë‹¤. ê°œì¸ì˜ ì—°ê°„ ì†Œë“±ì´ 50,000ë‹¬ëŸ¬ ì´ìƒì¸ì§€ ì˜ˆì¸¡í•˜ê¸° ë¥¼ ìœ„í•´ ì§€í‘œë“¤ì˜ ìƒê´€ê´€ê³„ë¥¼ í™•ì¸í•´ë³¸ë‹¤. public-dataset ê°€ì ¸ì˜¤ê¸° ì´ì „ í¬ìŠ¤íŠ¸ì˜ [0ë‹¨ê³„]https://jmj3047.github.io/2023/02/28/BQML_Pred/)ì™€ ë™ì¼í•˜ë‹¤. ë°ì´í„° í™•ì¸ ë° EDA Looker Studioë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ í™•ì¸í•˜ê³  EDAë¥¼ í•˜ëŠ” ë°©ë²• ì „ì²´ ë°ì´í„° ê°€ì ¸ì™€ì„œ ì°¨íŠ¸ ë§Œë“¤ê¸° Big Queryë¥¼ ì‚¬ìš©í•˜ì—¬ SQLë¡œ ë°ì´í„°ë¥¼ ì •ì œí•œ í›„ Looker Studioì— SQLë¡œ ì°¨íŠ¸ ë§Œë“¤ê¸° ì „ì²´ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ ì°¨íŠ¸ ë§Œë“¤ê¸° Big Queryë¥¼ ì‚¬ìš©í•  í•„ìš”ê°€ ì—†ë‹¤. ë²„íŠ¼ìœ¼ë¡œ ê°„ë‹¨í•˜ê²Œ ì¡°ì‘ì´ ê°€ëŠ¥í•˜ë‹¤. ì§€í‘œë‹¹ ê°„ë‹¨í•œ í•©ê³„, í‰ê· , ì§‘ê³„, ìµœì†Ÿê°’, ìµœëŒ€ê°’, ì¤‘ì•™ê°’, í‘œì¤€í¸ì°¨, ë¶„ì‚° ê¹Œì§€ì˜ ì—°ì‚°ì€ ë²„íŠ¼ì‹ìœ¼ë¡œ ìˆ˜ì •í• ìˆ˜ ìˆìœ¼ë©°, í•¨ìˆ˜ ë˜í•œ í•„ë“œì¶”ê°€ì˜ í•­ëª©ìœ¼ë¡œ ë§Œë“œëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ë‹¤. GCPì— ë°ì´í„° ì„¸íŠ¸ê°€ ì—…ë¡œë“œ ë˜ì–´ ìˆë‹¤ë©´, Looker Studioë¥¼ ë°”ë¡œ ì—´ì–´ì„œ ë°ì´í„° ì¶”ê°€ ë²„íŠ¼ì„ ëˆ„ë¥¸ë‹¤. Big Queryë¥¼ ëˆ„ë¥´ê³  GCP ë°ì´í„° ì„¸íŠ¸ë¥¼ ì°¾ëŠ”ë‹¤. public data setì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ë¼ë©´ ê³µê°œ ë°ì´í„° ì§‘í•©ìœ¼ë¡œ ë“¤ì–´ê°€ì•¼ í•œë‹¤. ì¶”ê°€ ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ í™”ë©´ ì™¼ìª½ì— ë°ì´í„° ì„¸íŠ¸ì˜ ì´ë¦„ê³¼ ê·¸ ì•ˆì˜ ìŠ¤í‚¤ë§ˆ ì •ë³´ë“¤ì´ ëœ¬ë‹¤. ì›í•˜ëŠ” ëŒ€ì‹œë³´ë“œë¥¼ ë§Œë“¤ê¸° ìœ„í•´ ì°¨íŠ¸ë‚˜ í‘œë¥¼ ì¶”ê°€í•˜ì—¬ ììœ ë¡­ê²Œ ëŒ€ì‹œë³´ë“œë¥¼ êµ¬ì„±í•˜ë©´ ëœë‹¤. ì˜ˆì‹œ: https://lookerstudio.google.com/reporting/10e2c716-6289-4e07-a2de-da6cdac415b6 Big Queryë¥¼ ì‚¬ìš©í•˜ì—¬ SQLë¡œ ë°ì´í„°ë¥¼ ì •ì œí•œ í›„ Looker Studioì— SQLë¡œ ì°¨íŠ¸ ë§Œë“¤ê¸° í‘œì— ìˆëŠ” ì§€í‘œë¥¼ ê°€ê³µí•˜ì—¬ ë³´ê³  ì‹¶ì„ë•Œ ìœ ìš©í•˜ë‹¤. ë²„íŠ¼ìœ¼ë¡œ ì¡°ì‘í•˜ëŠ” ê²ƒë³´ë‹¤ ë” ë§ì€ ì§€í‘œ í‘œí˜„ì´ ê°€ëŠ¥í•˜ë‹¤. ìœ ì§€ ë³´ìˆ˜ì˜ ì–´ë ¤ì›€ì´ ìˆë‹¤. ì‚¬ìš©í•  ë°ì´í„°ì˜ ì§€í‘œë“¤ì„ í™•ì¸í•´ë³´ë©´ ì•„ë˜ì²˜ëŸ¼ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤. age(ë‚˜ì´): ê°œì¸ì˜ ë‚˜ì´ë¥¼ ì—°ë‹¨ìœ„ë¡œ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤ workclass(ë…¸ë™ ê³„ê¸‰): ê°œì¸ì˜ ê³ ìš©í˜•íƒœ Private, ?, Local-gov , Self-emp-inc, Federal-gov, State-gov, Self-emp-not-inc, Never-worked, Witout-pay functional_weight: ì¼ë ¨ì˜ ê´€ì¸¡ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¸êµ¬ì¡°ì‚¬êµ­ì´ ë¶€ì—¬í•˜ëŠ” ê°œì¸ì˜ ê°€ì¤‘ì¹˜ education: ê°œì¸ì˜ ìµœì¢…í•™ë ¥ education_num: êµìœ¡ìˆ˜ì¤€ì„ ìˆ«ìë¡œ ë²”ì£¼í™” í•˜ì—¬ ì—´ê±° í•©ë‹ˆë‹¤. ìˆ«ìê°€ ë†’ì„ìˆ˜ë¡ ê°œì¸ì˜ êµìœ¡ìˆ˜ì¤€ì´ ë†’ìŠµë‹ˆë‹¤. 11: Assoc_voc: ì „ë¬¸í•™êµ ì¤€í•™ì‚¬ 13: Bachelors: í•™ì‚¬ 9: HS-grad: ê³ ë“±í•™êµ ì¡¸ì—… marital_status: ê°œì¸ì˜ ê²°í˜¼ ì—¬ë¶€ ì…ë‹ˆë‹¤. Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse occupation: ê°œì¸ì˜ ì§ì—…ì…ë‹ˆë‹¤. relationship: ê°€ì • ë‚´ ê° ê°œì¸ì˜ ê´€ê³„ì…ë‹ˆë‹¤. Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried race: ì¸ì¢…ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤ White, Asian-Pac-Islander, Amer-Indian-Eskimo, Black, Other sex: ê°œì¸ì˜ ì„±ë³„ì…ë‹ˆë‹¤. Female, Male capital_gain: ê°œì¸ì˜ ìë³¸ ì´ìµì„ ë¯¸êµ­ ë‹¬ëŸ¬ë¡œ í‘œê¸° í•©ë‹ˆë‹¤. capital_loss: ê°œì¸ì˜ ìë³¸ ì†ì‹¤ì„ ë¯¸êµ­ ë‹¬ëŸ¬ë¡œ í‘œê¸° í•©ë‹ˆë‹¤. hours_per_week: ì£¼ë‹¹ ê·¼ë¬´ì‹œê°„ì…ë‹ˆë‹¤. native_country: ê°œì¸ì˜ ì¶œì‹  êµ­ê°€ ì…ë‹ˆë‹¤. ?,Cambodia,Canada,China,Columbia,Cuba,Dominican-Republic,Ecuador,El-Salvador,England,France,Germany,Greece,Guatemala,Haiti,Holand-Netherlands,Honduras,Hong,Hungary,India,Iran,Ireland,Italy,Jamaica,Japan,Laos,Mexico,Nicaragua,Outlying-US(Guam-USVI-etc),Peru,Philippines,Poland,Portugal,Puerto-Rico,Scotland,South,Taiwan,Thailand,Trinadad&amp;Tobago,United-States,Vietnam,Yugoslavia income_bracket: ê°œì¸ì˜ ì—°ê°„ ì†Œë“ì´ ë¯¸í™” 50,000ë‹¬ëŸ¬ê°€ ë„˜ëŠ”ì§€ ì—¬ë¶€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤ ì˜ˆì¸¡ì„ ìœ„í•œ ê°€ì„¤ë“¤ì„ ì„¸ìš´ë‹¤ ì˜ˆì‹œ: native_countryë¥¼ ê¸°ì¤€ìœ¼ë¡œ í‰ê·  ì£¼ë‹¹ ê·¼ë¬´ì‹œê°„ê³¼ ê°œì¸ì˜ ìë³¸ í˜„í™©ì˜ í‰ê· ì„ ë³´ê³  ì‹¶ë‹¤. ì´ëŸ¬í•œ ë°ì´í„°ëŠ” Looker studioì˜ ê¸°ëŠ¥ìœ¼ë¡œ ì¡°íšŒê°€ ë¶ˆê°€ëŠ¥í•˜ë‹¤. â†’ GCP Big Queryë¡œ ì¡°íšŒí•˜ê¸° ì‹¤í–‰ ì¿¼ë¦¬ 123456SELECT DISTINCT native_country, AVG(hours_per_week) as avg_hours_per_week, AVG(capital_gain - capital_loss) as avg_capitalFROM `bigquery-public-data.ml_datasets.census_adult_income`GROUP BY 1ORDER BY 1 ìœ„ ë°ì´í„°ë¥¼ Looker Studioë¡œ ì˜®ê²¨ì„œ ê·¸ë˜í”„ë¥¼ ì‘ì„±í•´ ë³´ì ë°ì´í„° ì¶”ê°€ â†’ ë¹…ì¿¼ë¦¬ â†’ ë§ì¶¤ ê²€ìƒ‰ì–´ â†’ í”„ë¡œì íŠ¸ë¥¼ ì„ íƒí•œ í›„ SQLì„ ì…ë ¥í•´ ì¤€ë‹¤. ì‹œê°„ ë°ì´í„°ê°€ ìˆì„ ë•ŒëŠ” ê¸°ê°„ ë§¤ê°œë³€ìˆ˜ ì‚¬ìš© ì„¤ì •ì„ ì²´í¬ í•˜ë©´ ê¸°ê°„ ì»¨íŠ¸ë¡¤ì— ëŒ€í•œ ë¶€ë¶„ì„ ì¡°ì‘í•  ìˆ˜ ìˆë‹¤. â†’ í•˜ì§€ë§Œ ì´ ë°ì´í„°ì—ëŠ” ê¸°ê°„ì— ëŒ€í•œ ë‚´ìš©ì€ ì—†ìœ¼ë‹ˆ ì¶”í›„ ì‘ì„±í•˜ëŠ” ê²ƒìœ¼ë¡œ í•œë‹¤. ë°ì´í„°ë¥¼ ì¶”ê°€ í•˜ê³  ì°¨íŠ¸ë¥¼ ì¶”ê°€í•˜ë©´ ëŒ€ì‹œë³´ë“œê°€ ì™„ì„± ëœë‹¤. Reference https://notebook.community/google/eng-edu/ml/cc/exercises/estimators/ko/intro_to_fairness","categories":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}],"tags":[{"name":"Big Query","slug":"Big-Query","permalink":"https://jmj3047.github.io/tags/Big-Query/"},{"name":"Looker Studio","slug":"Looker-Studio","permalink":"https://jmj3047.github.io/tags/Looker-Studio/"}],"keywords":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}]},{"title":"BQMLì„ ì´ìš©í•œ ê°œì¸ ì†Œë“ ì˜ˆì¸¡","slug":"BQML_Pred","date":"2023-02-27T15:00:00.000Z","updated":"2023-03-19T07:02:10.360Z","comments":true,"path":"2023/02/28/BQML_Pred/","link":"","permalink":"https://jmj3047.github.io/2023/02/28/BQML_Pred/","excerpt":"","text":"ê°œì¸ ì—°ê°„ ì†Œë“ì´ 5ë§Œ ë‹¬ëŸ¬ ì´ìƒì¸ì§€ ì˜ˆì¸¡í•˜ê¸°ê°œìš” GCPì—ì„œ BQML ì‚¬ìš©í•˜ê¸° BQMLì˜ ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ ìœ í˜•ìœ¼ë¡œ supervised learningì„ ì§€ì›í•˜ëŠ” ê¸°ëŠ¥ ì‚¬ìš© ë°”ì´ë„ˆë¦¬&#x2F;ë©€í‹° ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨í˜•ì„ ì‚¬ìš©í•˜ë©´ ê°’ì´ ë‘&#x2F;ì—¬ëŸ¬ ë²”ì£¼ ì¤‘ í•˜ë‚˜ì— ì†í• ì§€ ì˜ˆì¸¡í•  ìˆ˜ ìˆë‹¤. ë°ì´í„°ë¥¼ ë‘˜ ì´ìƒì˜ ë²”ì£¼ë¡œ ë¶„ë¥˜í•˜ë ¤ëŠ” ë¬¸ì œ bigquery-public-data.ml_datasets.census_adult_income ë°ì´í„° ì‚¬ìš© ëª©í‘œ ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ì„ ë§Œë“¤ê³  í‰ê°€í•œë‹¤. ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡í•œë‹¤. ë¹„ìš© BigQuery ê°€ê²©ì •ì±… BigQuery ML ê°€ê²©ì •ì±… 0ë‹¨ê³„: public-dataset ê°€ì ¸ì˜¤ê¸° GCP BigQueryë¡œ ì´ë™í•˜ì—¬ í”„ë¡œì íŠ¸ë¥¼ ë§Œë“¤ê³ , í”„ë¡œì íŠ¸ ë°‘ì— +ADD DATA ë¥¼ í´ë¦­í•œë‹¤ Public Datasets ë¥¼ ì„ íƒí•œë‹¤ ë°ì´í„° ì…‹ì¤‘ ì•„ë¬´ê±°ë‚˜ ì„ íƒ í•œ í›„ VIEW DATASET ì„ ì„ íƒí•˜ë©´ ìƒˆ ì°½ì´ ëœ¨ë©´ì„œ êµ¬ê¸€ í¼ë¸”ë¦­ë°ì´í„° ì…‹ì´ ì—°ê²° ëœë‹¤. ì´ë²ˆ í¬ìŠ¤íŒ…ì—ì„œ ë‹¤ë£° ë°ì´í„°ëŠ” bigquery-public-data.ml_datasets.census_adult_income ì´ë‹¤. ì•„ë˜ì²˜ëŸ¼ ì°½ì´ ëœ¨ë©´ ë¹…ì¿¼ë¦¬SQLì„ ì‚¬ìš©í•  ì¤€ë¹„ê°€ ë‹¤ ë˜ì—ˆë‹¤. 1ë‹¨ê³„: ë°ì´í„° ì„¸íŠ¸ ë§Œë“¤ê¸° ë‚´ í”„ë¡œì íŠ¸ë¥¼ ì„ íƒí•˜ê³  ì˜†ì— ì ì„¸ê°œë¥¼ ëˆ„ë¥´ê³  Create dataset ì„ ëˆŒëŸ¬ì„œ ë°ì´í„° ì„¸íŠ¸ë¥¼ ë§Œë“ ë‹¤. ë°ì´í„° ì„¸íŠ¸ IDì— census ë¥¼ ì…ë ¥í•œë‹¤ ë¦¬ì „ì„ ë¯¸êµ­(US)ë¡œ ì„ íƒí•œë‹¤ â†’ public datasetì´ USë©€í‹° ë¦¬ì „ì— ìˆê¸° ë•Œë¬¸. ê°™ì€ ë¦¬ì „ì— ë‘ì–´ì•¼ í˜¼ì„ ì„ ë§‰ì„ìˆ˜ ìˆë‹¤. ë§Œë“¤ì–´ì§„ ë°ì´í„° ì…‹ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. 2ë‹¨ê³„: ë°ì´í„° í™•ì¸ ë° EDA ë°ì´í„° í™•ì¸ age(ë‚˜ì´): ê°œì¸ì˜ ë‚˜ì´ë¥¼ ì—°ë‹¨ìœ„ë¡œ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤ workclass(ë…¸ë™ ê³„ê¸‰): ê°œì¸ì˜ ê³ ìš©í˜•íƒœ functional_weight: ì¼ë ¨ì˜ ê´€ì¸¡ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¸êµ¬ì¡°ì‚¬êµ­ì´ ë¶€ì—¬í•˜ëŠ” ê°œì¸ì˜ ê°€ì¤‘ì¹˜ education: ê°œì¸ì˜ ìµœì¢…í•™ë ¥ education_num: êµìœ¡ìˆ˜ì¤€ì„ ìˆ«ìë¡œ ë²”ì£¼í™” í•˜ì—¬ ì—´ê±° í•©ë‹ˆë‹¤. ìˆ«ìê°€ ë†’ì„ìˆ˜ë¡ ê°œì¸ì˜ êµìœ¡ìˆ˜ì¤€ì´ ë†’ìŠµë‹ˆë‹¤. marital_status: ê°œì¸ì˜ ê²°í˜¼ ì—¬ë¶€ ì…ë‹ˆë‹¤. occupation: ê°œì¸ì˜ ì§ì—…ì…ë‹ˆë‹¤. relationship: ê°€ì • ë‚´ ê° ê°œì¸ì˜ ê´€ê³„ì…ë‹ˆë‹¤. race: ì¸ì¢…ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤ sex: ê°œì¸ì˜ ì„±ë³„ì…ë‹ˆë‹¤. capital_gain: ê°œì¸ì˜ ìë³¸ ì´ìµì„ ë¯¸êµ­ ë‹¬ëŸ¬ë¡œ í‘œê¸° í•©ë‹ˆë‹¤. capital_loss: ê°œì¸ì˜ ìë³¸ ì†ì‹¤ì„ ë¯¸êµ­ ë‹¬ëŸ¬ë¡œ í‘œê¸° í•©ë‹ˆë‹¤. hours_per_week: ì£¼ë‹¹ ê·¼ë¬´ì‹œê°„ì…ë‹ˆë‹¤. native_country: ê°œì¸ì˜ ì¶œì‹  êµ­ê°€ ì…ë‹ˆë‹¤. income_bracket: ê°œì¸ì˜ ì—°ê°„ ì†Œë“ì´ ë¯¸í™” 50,000ë‹¬ëŸ¬ê°€ ë„˜ëŠ”ì§€ ì—¬ë¶€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤ ì˜ˆì¸¡ ì‘ì—…: ê°œì¸ì˜ ì—°ê°„ ì†Œë“ì´ 50,000ë‹¬ëŸ¬ ì´ìƒì¸ì§€ í™•ì¸í•´ë³´ê¸° ì¿¼ë¦¬ ì‹¤í–‰ 123456789101112SELECT age, workclass, marital_status, education_num, occupation, hours_per_week, income_bracketFROM `bigquery-public-data.ml_datasets.census_adult_income`LIMIT 100; ì¿¼ë¦¬ ì‹¤í–‰ ê²°ê³¼ ë¶„ì„ income_bracket : &lt;=50K ë˜ëŠ” &gt;50K ê°’ì¤‘ í•˜ë‚˜ë§Œ ìˆìŒ education , education_num â†’ ë™ì¼í•œ ë°ì´í„°ê°€ ì„œë¡œ ë‹¤ë¥¸ í˜•ì‹ìœ¼ë¡œ í‘œê¸°ë˜ì–´ ìˆìŒ functional_weight : ì¸êµ¬ ì¡°ì‚¬ ê¸°ê´€ì—ì„œ íŠ¹ì •í–‰ì´ ëŒ€í‘œí•œë‹¤ê³  íŒë‹¨í•˜ëŠ” ê°œì¸ì˜ ìˆ˜ â†’ ìš°ë¦¬ê°€ ì›í•˜ëŠ” income ì˜ˆì¸¡ê³¼ëŠ” ê´€ë ¨ì´ ì—†ìŒ. 3ë‹¨ê³„: í•™ìŠµ ë°ì´í„° ì„ íƒ í•™ìŠµì— í•„ìš”í•œ ë°ì´í„° ì„ íƒ age: ì—°ë ¹ workclass: ê³ ìš©í˜•íƒœ marital_status: ê²°í˜¼ì—¬ë¶€ education_num: êµìœ¡ìˆ˜ì¤€ occupation: ì§ì—… hours_per_week: ì£¼ë‹¹ ê·¼ë¬´ì‹œê°„ í•™ìŠµ ë°ì´í„°ë¥¼ ì»´íŒŒì¼ í•˜ëŠ” ë·°ë¥¼ ë§Œë“ ë‹¤. 1234567891011121314151617CREATE OR REPLACE VIEW `census.input_view` ASSELECT age, workclass, marital_status, education_num, occupation, hours_per_week, income_bracket, CASE WHEN MOD(functional_weight, 10) &lt; 8 THEN &#x27;training&#x27; WHEN MOD(functional_weight, 10) = 8 THEN &#x27;evaluation&#x27; WHEN MOD(functional_weight, 10) = 9 THEN &#x27;prediction&#x27; END AS dataframeFROM `bigquery-public-data.ml_datasets.census_adult_income` education, education_num ë“±ê³¼ ê°™ì€ ì¤‘ë³µë˜ëŠ” ì¹´í…Œê³ ë¦¬ë¥¼ ì œì™¸í•œë‹¤. functional_weight ëŠ” income_bracketê³¼ ìƒê´€ì´ ì—†ëŠ” ì»¬ëŸ¼ì´ë¯€ë¡œ ë¼ë²¨ë§ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤. 80%training, 10% í‰ê°€, 10% ì˜ˆì¸¡ MOD(X,Y): Xë¥¼ Yë¡œ ë‚˜ëˆ´ì„ë•Œì˜ ë‚˜ë¨¸ì§€ ìœ„ ì¿¼ë¦¬ë¥¼ ìˆ˜í–‰ì‹œí‚¤ë©´ 1ë‹¨ê³„ì—ì„œ ë§Œë“  censusì— input_viewê°€ ë§Œë“¤ì–´ì¡ŒìŒì„ ì•Œìˆ˜ ìˆë‹¤. 4ë‹¨ê³„: ë¡œì§€ìŠ¤í‹± íšŒê·€ëª¨ë¸ ë§Œë“¤ê¸° Create Model ë¬¸ì„ LOGISTIC_REG ì˜µì…˜ê³¼ í•¨ê»˜ ì‚¬ìš©í•˜ë©´ ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ì„ ë§Œë“¤ê³  í•™ìŠµ ì‹œí‚¬ìˆ˜ ìˆë‹¤. 12345678910111213CREATE OR REPLACE MODEL `census.census_model`OPTIONS ( model_type=&#x27;LOGISTIC_REG&#x27;, auto_class_weights=TRUE, input_label_cols=[&#x27;income_bracket&#x27;] ) ASSELECT * EXCEPT(dataframe)FROM `census.input_view`WHERE dataframe = &#x27;training&#x27; CREATE MODELë¬¸ì€ SELECT ë¬¸ì˜ í•™ìŠµ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµ ì‹œí‚¨ë‹¤. OPTIONS ì ˆì€ ëª¨ë¸ ìœ í˜•ê³¼ í•™ìŠµ ì˜µì…˜ì„ ì§€ì •í•œë‹¤. ì—¬ê¸°ì„œ LOGISTIC_REG ì˜µì…˜ì€ ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ ìœ í˜•ì„ ì§€ì •í•œë‹¤. ë°”ì´ë„ˆë¦¬ ë¡œì§€ìŠ¤í‹± íšŒê·€ëª¨ë¸ê³¼ ë©€í‹°í´ë˜ìŠ¤ ë¡œì§€ìŠ¤í‹± íšŒê·€ëª¨ë¸ì„ êµ¬ë¶„í•˜ì—¬ ì§€ì •í•  í•„ìš”ëŠ” ì—†ë‹¤. BigQuery MLì€ ë¼ë²¨ì—´ ê³ ìœ  ê°’ ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµ ëŒ€ìƒì„ ê²°ì •í•  ìˆ˜ ìˆë‹¤. â†’ ê³ ìœ  ê°’ ìˆ˜ì— ë”°ë¼ì„œ ìë™ìœ¼ë¡œ ì„ íƒí•˜ì—¬ í•™ìŠµí•¨ input_label_cols ì˜µì…˜ì€ SELECT ë¬¸ì—ì„œ ë¼ë²¨ ì—´ë¡œ ì‚¬ìš©í•  ì—´ì„ ì§€ì •í•œë‹¤. ì—¬ê¸°ì„œ ë¼ë²¨ ì—´ì€ income_bracket ì´ë¯€ë¡œ ëª¨ë¸ì€ ê° í–‰ì— ìˆëŠ” ë‹¤ë¥¸ ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ income_bracketì˜ ë‘ ê°’ ì¤‘ ê°€ì¥ ê°€ëŠ¥ì„±ì´ ë†’ì€ ê°’ì„ í•™ìŠµ í•œë‹¤. SELECT ë¬¸ì€ 2ë‹¨ê³„ì˜ ë·°ë¥¼ ì‚¬ìš©í•œë‹¤. ì´ ë·°ì—ëŠ” ëª¨ë¸ í•™ìŠµìš© íŠ¹ì„± ë°ì´í„°ê°€ í¬í•¨ëœ ì—´ë§Œ í¬í•¨ëœë‹¤. WHERE ì ˆì€ í•™ìŠµë°ì´í„° í”„ë ˆì„ì— ì†í•˜ëŠ” í–‰ë§Œ í•™ìŠµ ë°ì´í„°ì— í¬í•¨ë˜ë„ë¡ input_viewì˜ í–‰ì„ í•„í„°ë§ í•œë‹¤. CREATE MODEL ì¿¼ë¦¬ ì‹¤í–‰1234567891011121314CREATE OR REPLACE MODEL `census.census_model`OPTIONS ( model_type=&#x27;LOGISTIC_REG&#x27;, auto_class_weights=TRUE, data_split_method=&#x27;NO_SPLIT&#x27;, input_label_cols=[&#x27;income_bracket&#x27;], max_iterations=15) ASSELECT * EXCEPT(dataframe)FROM `census.input_view`WHERE dataframe = &#x27;training&#x27; SCHEMA íƒ­ì€ BigQuery MLì´ ë¡œì§€ìŠ¤í‹± íšŒê·€ë¥¼ ìˆ˜í–‰í•˜ëŠ”ë° ì‚¬ìš©í•œ ì†ì„±ì„ ë‚˜ì—´í•œë‹¤. 5ë‹¨ê³„: ML.EVALUATE í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ í‰ê°€ 4ë‹¨ê³„ì˜ CREATE MODEL ë¬¸ì„ ìˆ˜í–‰í–ˆì„ë•Œ ëª¨ë¸ì˜ ê²°ê³¼ ì°½ì— EVALUATION íƒ­ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ML.EVALUATE í•¨ìˆ˜ëŠ” ì‹¤ì œ ë°ì´í„°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì˜ˆì¸¡ ê°’ì„ í‰ê°€í•œë‹¤. 12345678910111213SELECT *FROM ML.EVALUATE (MODEL `census.census_model`, ( SELECT * FROM `census.input_view` WHERE dataframe = &#x27;evaluation&#x27; ) ) ì•ì—ì„œ í•™ìŠµì‹œí‚¨ ëª¨ë¸ê³¼ SELECT ì„œë¸Œ ì¿¼ë¦¬ì—ì„œ ë°˜í™˜ëœ í‰ê°€ ë°ì´í„°ë¥¼ ë°›ì•„ë“¤ì¸ë‹¤. ì´ í•¨ìˆ˜ëŠ” ëª¨ë¸ì— ëŒ€í•œ ë‹¨ì¼í–‰ì˜ í†µê³„ë¥¼ ë°˜í™˜í•œë‹¤. input_view ì˜ ë°ì´í„°ë¥¼ í‰ê°€ ë°ì´í„°ë¡œ ì‚¬ìš©í•œë‹¤. ì‹¤í–‰ ê²°ê³¼ ë¡œì§€ìŠ¤í‹± íšŒê·€ë¥¼ ìˆ˜í–‰í–ˆìœ¼ë¯€ë¡œ ê²°ê³¼ì— precision, recall, accuracy, f1_score, log_loss, roc_auc ì—´ì´ í¬í•¨ëœë‹¤. 1ë²ˆê³¼ 2ë²ˆì˜ ì°¨ì´ ML.EVALUATEëŠ” í•™ìŠµ ê³¼ì •ì—ì„œ ê³„ì‚°ëœ í‰ê°€ ì¸¡ì •ê°’ì„ ê°€ì ¸ì˜¤ëŠ”ë°, ì´ë¥¼ ìœ„í•´ ìë™ìœ¼ë¡œ ì˜ˆì•½ëœ í‰ê°€ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. data_split_method í•™ìŠµ ì˜µì…˜ì— NO_SPLIT ê°€ ì§€ì •ëœ 1ë²ˆ ë°©ë²•ì˜ ê²½ìš° ì „ì²´ ì…ë ¥ ë°ì´í„° ì„¸íŠ¸ê°€ í•™ìŠµê³¼ í‰ê°€ì— ëª¨ë‘ ì‚¬ìš©ëœë‹¤. í‰ê°€ ë°ì´í„°ì™€ í›ˆë ¨ ë°ì´í„°ë¥¼ êµ¬ë¶„í•˜ì§€ ì•Šê³  ML.EVALUATE ë¥¼ í˜¸ì¶œ í•˜ë©´ í•™ìŠµ ë°ì´í„° ì„¸íŠ¸ì—ì„œ ì„ì˜ì˜ í‰ê°€ ë°ì´í„° ì„¸íŠ¸ê°€ ì¸¡ì •ë˜ê³  ì´ëŸ¬í•œ í‰ê°€ íš¨ê³¼ëŠ” ëª¨ë¸ í•™ìŠµ ë°ì´í„°ì™€ ë³„ë„ë¡œ ìœ ì§€ëœ ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•œ í‰ê°€ ì‹¤í–‰ë³´ë‹¤ ì ë‹¤. â†’ ë”°ë¼ì„œ êµ¬ë¶„í•´ì£¼ëŠ”ê²Œ ì¢‹ë‹¤. 6ë‹¨ê³„: ML.PREDICT í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì†Œë“ ê³„ì¸µ ì˜ˆì¸¡ íŠ¹ì • ì‘ë‹µìê°€ ì†í•œ ì†Œë“ ê³„ì¸µì„ ì‹ë³„í•˜ë ¤ë©´ ML.PREDICT í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤. 12345678910111213SELECT *FROM ML.PREDICT (MODEL `census.census_model`, ( SELECT * FROM `census.input_view` WHERE dataframe = &#x27;prediction&#x27; ) ) prediction ë°ì´í„° í”„ë ˆì„ì— ìˆëŠ” ëª¨ë“  ì‘ë‹µìì˜ ì†Œë“ ê³„ì¸µì„ ì˜ˆì¸¡í•œë‹¤. ì‹¤í–‰ ê²°ê³¼: predicted_income_bracketì€ income_bracketì˜ ì˜ˆì¸¡ê°’ 7ë‹¨ê³„: Explainable AI ë©”ì„œë“œë¡œ ì˜ˆì¸¡ ê²°ê³¼ ì„¤ëª… ëª¨ë¸ì—ì„œ ì´ëŸ¬í•œ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ìƒì„±í•˜ëŠ” ì´ìœ ë¥¼ ì•Œì•„ ë³´ë ¤ë©´ ML.EXPLAIN_PREDICT í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´ ëœë‹¤. ML.EXPLAIN_PREDICT ëŠ” ML.PREDICT ì˜ í™•ì¥ëœ ë²„ì „ ì˜ˆì¸¡ ê²°ê³¼ ë¿ë§Œ ì•„ë‹ˆë¼ ì˜ˆì¸¡ê²°ê³¼ë¥¼ ì„¤ëª…í•˜ëŠ” ì¶”ê°€ ì—´ì„ ì¶œë ¥í•œë‹¤. BigQueryMLì˜ Shapley ê°’ê³¼ Explainable AIì„œë¹„ìŠ¤ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš© 12345678910111213#standardSQLSELECT*FROMML.EXPLAIN_PREDICT(MODEL `census.census_model`, ( SELECT * FROM `census.input_view` WHERE dataframe = &#x27;evaluation&#x27;), STRUCT(3 as top_k_features)) ì‹¤í–‰ ê²°ê³¼ ë¡œì§€ìŠ¤í‹± íšŒê·€ëª¨ë¸ì—ì„œ Shapley ê°’ì€ ë¨¸ì‹  ëŸ¬ë‹ ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ì— ê° íŠ¹ì„±ì´ ê¸°ì—¬í•˜ëŠ” ì •ë„ë¥¼ í‰ê°€í•˜ê¸° ìœ„í•´ ì‚¬ìš©ëœë‹¤. ì´ ê°’ì„ í†µí•´ ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í•´ì„í•˜ê³ , ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ ì–´ë–¤ íŠ¹ì„±ì„ ìˆ˜ì •í•´ì•¼ í•˜ëŠ”ì§€ë¥¼ ê²°ì •í•˜ëŠ” ë° ë„ì›€ì„ ì¤€ë‹¤. top_k_features ê°€ 3ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆê¸° ë•Œë¬¸ì— ì œê³µëœ í…Œì´ë¸”ì˜ í–‰ë‹¹ íŠ¹ì„± ê¸°ì—¬ í•™ëª© 3ê°œë¥¼ ì¶œë ¥í•¨. ê¸°ì—¬í•­ëª©ì€ ì ˆëŒ“ê°’ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬ëœë‹¤. 44ë²ˆí–‰ì˜ ê²°ê³¼ ê°’ì€ education_numì´ ê°€ì¥ í° ê¸°ì—¬ë¥¼ í•˜ì˜€ë‹¤ë©´, 47ë²ˆí–‰ì˜ ê²°ê³¼ëŠ” ageê°€ ê°€ì¥ í° ê¸°ì—¬ë¥¼ í–ˆìŒì„ ì•Œìˆ˜ ìˆë‹¤. 8ë‹¨ê³„: ëª¨ë¸ì„ ì „ì—°ì ìœ¼ë¡œ ì„¤ëª… ì¼ë°˜ì ìœ¼ë¡œ ì†Œë“ê³„ì¸µì„ ì…œì •í•˜ëŠ”ë° ê°€ì¥ ì¤‘ìš”í•œ íŠ¹ì„±ì´ ë¬´ì—‡ì¸ì§€ í™•ì¸í•˜ë ¤ë©´ ML.GLOABL_EXPLAIN í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´ ëœë‹¤. ì´ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ ëª¨ë¸ì„ í•™ìŠµ ì‹œí‚¬ë•Œ ENABLE_GLOBAL_EXPLAIN &#x3D; TRUE ì˜µì…˜ì„ ì‚¬ìš©í•˜ì—¬ì•¼ í•œë‹¤. sklearnì˜ feature importanceì™€ ê°™ì€ ê¸°ëŠ¥ì„ í•˜ëŠ” í•¨ìˆ˜ í•™ìŠµ ì¿¼ë¦¬ 1234567891011121314CREATE OR REPLACE MODEL census.census_modelOPTIONS ( model_type=&#x27;LOGISTIC_REG&#x27;, auto_class_weights=TRUE, enable_global_explain=TRUE, input_label_cols=[&#x27;income_bracket&#x27;] ) ASSELECT * EXCEPT(dataframe)FROM census.input_viewWHERE dataframe = &#x27;training&#x27; ì „ì—­ ì„¤ëª…ì— Accessí•˜ëŠ” ì¿¼ë¦¬ 12345#standardSQLSELECT *FROM ML.GLOBAL_EXPLAIN(MODEL `census.census_model`) ì‹¤í–‰ ê²°ê³¼ Reference https://notebook.community/google/eng-edu/ml/cc/exercises/estimators/ko/intro_to_fairness https://cloud.google.com/bigquery-ml/docs/logistic-regression-prediction?hl=ko","categories":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}],"tags":[{"name":"Big Query","slug":"Big-Query","permalink":"https://jmj3047.github.io/tags/Big-Query/"},{"name":"BigQueryML","slug":"BigQueryML","permalink":"https://jmj3047.github.io/tags/BigQueryML/"}],"keywords":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}]},{"title":"Mac bash íŒŒì¼ë¡œ hexo, git ëª…ë ¹ì–´ ìë™í™”","slug":"Bash_Automation","date":"2023-02-26T15:00:00.000Z","updated":"2023-02-28T07:50:29.954Z","comments":true,"path":"2023/02/27/Bash_Automation/","link":"","permalink":"https://jmj3047.github.io/2023/02/27/Bash_Automation/","excerpt":"","text":"bash íŒŒì¼ë¡œ hexo, git ëª…ë ¹ì–´ ìë™í™” í•„ìê°€ ë¸”ë¡œê·¸ê¸€ì„ ì‘ì„±í•˜ëŠ”ë° hexo, git ëª…ë ¹ì–´ ìë™í™”ì˜ í•„ìš”ì„±ì„ ëŠê»´ ì´ ê¸€ì„ ì‘ì„±í•œë‹¤. macì—ì„œ ìë™í™” í•˜ëŠ” ê²½ìš° ìœˆë„ìš°ì™€ ë‹¬ë¦¬ batch íŒŒì¼ì´ ì•„ë‹ˆë¼ bash íŒŒì¼ë¡œ ì‹¤í–‰í•´ì•¼ í•œë‹¤. ìš°ì„  ë©”ëª¨ì¥ì— ìë™í™”ë¥¼ ì›í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•œë‹¤. bash íŒŒì¼ ì‘ì„±ì‹œì—ëŠ” #!/bin/bash ë¥¼ ê¼­ ì‘ì„±í•´ì£¼ì–´ì•¼ ì‹¤í–‰ì´ ê°€ëŠ¥í•˜ë‹¤. ì‹¤í–‰íŒŒì¼ ì´ë¦„ì„ ì •í•˜ê³ (í•„ìëŠ” submit ìœ¼ë¡œ ì„¤ì •í•˜ì˜€ìŒ) ì €ì¥í•œ ë’¤ í™•ì¥ìë¥¼ ì—†ì• ì£¼ì–´ì•¼ í•œë‹¤. ì €ì¥í•œ submit íŒŒì¼ì„ ë¸”ë¡œê·¸ ë¡œì»¬ í´ë” ì•ˆì— ë„£ëŠ”ë‹¤ í„°ë¯¸ë„ì— chmod +x submit ë¥¼ ì…ë ¥í•´ì„œ ê¶Œí•œì„ ë¶€ì—¬í•´ì¤€ë‹¤ ì‹¤í–‰ì‹œí‚¤ë ¤ë©´ sh submit ì„ ì…ë ¥í•´ì£¼ë©´ ì˜ ëŒì•„ê°€ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.","categories":[{"name":"Setting","slug":"Setting","permalink":"https://jmj3047.github.io/categories/Setting/"}],"tags":[{"name":"Git","slug":"Git","permalink":"https://jmj3047.github.io/tags/Git/"},{"name":"Hexo","slug":"Hexo","permalink":"https://jmj3047.github.io/tags/Hexo/"},{"name":"Automation","slug":"Automation","permalink":"https://jmj3047.github.io/tags/Automation/"},{"name":"Bash","slug":"Bash","permalink":"https://jmj3047.github.io/tags/Bash/"}],"keywords":[{"name":"Setting","slug":"Setting","permalink":"https://jmj3047.github.io/categories/Setting/"}]},{"title":"Basic ML Process","slug":"ML_Process","date":"2023-02-23T15:00:00.000Z","updated":"2023-02-24T04:50:59.756Z","comments":true,"path":"2023/02/24/ML_Process/","link":"","permalink":"https://jmj3047.github.io/2023/02/24/ML_Process/","excerpt":"","text":"Basic ML Process ì´ í¬ìŠ¤íŠ¸ì—ì„œëŠ” í•„ìê°€ ìƒê°í•œ ê¸°ë³¸ í”„ë¡œì„¸ìŠ¤ë¥¼ ì†Œê°œ í•œë‹¤. ë¨¸ì‹ ëŸ¬ë‹ì„ ì ‘í•´ë³´ì§€ ì•Šì€ ì‚¬ëŒë“¤ì—ê²Œ ëŒ€ëµì ì¸ ê°œë…ì„ ë³´ì—¬ì£¼ëŠ” í¬ìŠ¤íŠ¸ ì´ë‹¤. ìì„¸í•œ ë‚´ìš©ì€ ì¶”í›„ ì¶”ê°€ ì˜ˆì • ê°€ì„¤ ìˆ˜ë¦½ â†’ ë°ì´í„° í™•ì¸ ë° ì „ì²˜ë¦¬ â†’ ëª¨ë¸ í•™ìŠµ&#x2F; ëª¨ë¸ ê²€ì¦ â†’ ì˜ˆì¸¡í•˜ê¸° â†’ ê²°ê³¼ í™•ì¸ ê°€ì„¤ ìˆ˜ë¦½(íšŒê·€&#x2F;ë¶„ë¥˜ ì—¬ë¶€ í™•ì¸) â†’ ì ì¬ ê³ ê° ë¶„ë¥˜, ë§¤ì¶œ ì˜ˆì¸¡, ë¦¬í…ì…˜ ì˜ˆì¸¡ ë“±ë“±.. ì ì¬ ê³ ê° ë¶„ë¥˜: íŠ¹ì • ê³ ê°êµ°ì€ ì¶”í›„ ìœ ë£Œ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ê²ƒì´ë‹¤. ë§¤ì¶œ ì˜ˆì¸¡: ë‹¤ìŒë‹¬ ë§¤ì¶œ ì˜ˆìƒì•¡ì€ ì „ì›” ëŒ€ë¹„ ?? ì¼ê²ƒì´ë‹¤. ë¦¬í…ì…˜ ì˜ˆì¸¡: ë‹¤ìŒì£¼ ë¦¬í…ì…˜ ë¹„ìœ¨ì´ ?? ì¼ ê²ƒì´ë‹¤, ??ì¼ ë™ì•ˆ ë“¤ì–´ì˜¨ ê³ ê°ì€ ì¥ê¸° ê³ ê°ì´ ë  ê²ƒì´ë‹¤ ì˜ˆì‹œ ê°€ì„¤: ì´í‹€ ì´ìƒ ì ‘ì† &amp; 1 ê²½ê¸° ì´ìƒ í”Œë ˆì´ ê²½í—˜ ìœ ì € ëŒ€ìƒìœ¼ë¡œ ê·€í™˜ ìœ ì € ë¹„ìœ¨ ì˜ˆì¸¡ â†’ íšŒê·€ë¬¸ì œ ë°ì´í„° í™•ì¸ ë° ì „ì²˜ë¦¬ ë°ì´í„° feature ì„ íƒ ì˜ˆì‹œ raw data(Big Queryì—ì„œ ì¶”ì¶œí•œ ë¶€ë¶„) Aê·¸ë£¹ì€ 2021-12-07 ~ 2021-12-21 Bê·¸ë£¹ì€ 2022-02-14 ~ 2022-02-28 ì „ì²´: ì ‘ì†ì‹œê°„, ë ˆë²¨, ê²Œì„íšŸìˆ˜, ë¡œê·¸ì¸íšŸìˆ˜, í‰ê· ìˆœìœ„, ìµœëŒ€ìˆœìœ„, ì´í‚¬íšŸìˆ˜, ìœ ì €í‚¬íšŸìˆ˜, í—¤ë“œìƒ·í‚¬ìˆ˜, ì‹ ê³  íšŸìˆ˜, ì†”ë¡œ&#x2F;ë“€ì˜¤&#x2F;ìŠ¤ì¿¼ë“œ ì°¸ì—¬ íšŸìˆ˜, ì˜¤ì§ ì†”ë¡œëª¨ë“œë§Œ ì´ìš© ê°€ì…ì²«ë‚ : ì´ ê²Œì„ìˆ˜, í‰ê· ìˆœìœ„, ìµœëŒ€ìˆœìœ„, ì´í‚¬íšŸìˆ˜, ìœ ì €í‚¬íšŸìˆ˜, í—¤ë“œìƒ·í‚¬ìˆ˜, ì‹ ê³  íšŸìˆ˜, ì†”ë¡œ&#x2F;ë“€ì˜¤&#x2F;ìŠ¤ì¿¼ë“œ ì°¸ì—¬ íšŸìˆ˜, ì˜¤ì§ ì†”ë¡œëª¨ë“œë§Œ ì´ìš© ê°€ì… ë‹¤ìŒë‚ : ì´ ê²Œì„ìˆ˜, í‰ê· ìˆœìœ„, ìµœëŒ€ìˆœìœ„, ì´í‚¬íšŸìˆ˜, ìœ ì €í‚¬íšŸìˆ˜, í—¤ë“œìƒ·í‚¬ìˆ˜, ì‹ ê³  íšŸìˆ˜, ì†”ë¡œ&#x2F;ë“€ì˜¤&#x2F;ìŠ¤ì¿¼ë“œ ì°¸ì—¬ íšŸìˆ˜, ì˜¤ì§ ì†”ë¡œëª¨ë“œë§Œ ì´ìš© EDA ìœ ì € í‚¬ìˆ˜, ìµœëŒ€ìˆœìœ„, ë¡œê·¸ì¸íšŸìˆ˜, í—¤ë“œìƒ·í‚¬ìˆ˜, ìœ ì €ë ˆë²¨, í”Œë ˆì´ ì‹œê°„, ë¡œê·¸ì•„ì›ƒ ë‚ ì§œ, ì´ ê²Œì„ íšŸìˆ˜, ì´ í‚¬ìˆ˜, í‰ê·  ìˆœìœ„ ë°ì´í„° ìƒ˜í”Œë§ ë°ì´í„°ë¥¼ ì¼ë¶€ ì •ë¦¬í•´ì„œ ìµœì ì˜ ì…ë ¥ ë°ì´í„°ë¡œ ë§Œë“œëŠ” ê³¼ì • í™•ë¥ ì  ìƒ˜í”Œë§ ë‹¨ìˆœ ëœë¤ ìƒ˜í”Œë§ 2ë‹¨ê³„ ìƒ˜í”Œë§: ì „ì²´ nê°œì˜ ë°ì´í„°ë¥¼ mê°œì˜ ëª¨ì§‘ë‹¨ìœ¼ë¡œ ë‚˜ëˆ„ê³  mê°œì˜ ëª¨ì§‘ë‹¨ ì¤‘ì— Nê°œì˜ ë°ì´í„°ë¥¼ ë‹¨ìˆœëœë¤ ìƒ˜í”Œë§ ì¸µë³„ ìƒ˜í”Œë§: ëª¨ì§‘ë‹¨ì„ ì—¬ëŸ¬ê°œ ì¸µìœ¼ë¡œ êµ¬ë¶„í•¨ìœ¼ë¡œì¨ ê° ì¸µì—ì„œ nê°œì”© ëœë¤í•˜ê²Œ ë°ì´í„° ì¶”ì¶œ êµ°ì§‘&#x2F;ì§‘ë½ ìƒ˜í”Œë§: ëª¨ì§‘ë‹¨ì´ ì—¬ëŸ¬ê°œì˜ êµ°ì§‘ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆëŠ” ê²½ìš° êµ°ì§‘ ì¤‘ í•˜ë‚˜ or ì—¬ëŸ¬ê°œì˜ êµ°ì§‘ì„ ì„ ì •í•´ì„œ ì„ ì •ëœ êµ°ì§‘ì˜ ì „ì²´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•. Ex) í•œêµ­ì˜ ì‹œ,ë„ ë°ì´í„° ê³„í†µ ìƒ˜í”Œë§: 1ì—ì„œ nê¹Œì§€ ëª¨ë“  ë°ì´í„°ì— ë²ˆí˜¸ë¥¼ ë§¤ê²¨ì„œ ì¼ì • ê°„ê²©ë§ˆë‹¤ í•˜ë‚˜ì”© ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ëŠ” ë°©ë²•. ëŒ€í‘œì ìœ¼ë¡œ ì‹œê³„ì—´ì—ì„œ ì‚¬ìš©ë¨ ë¹„í™•ë¥ ì  ìƒ˜í”Œë§ í¸ì˜ ìƒ˜í”Œë§: ë°ì´í„°ë¥´ë¥´ ìˆ˜ì§‘í•˜ê¸° ì¢‹ì€ ì‹œì ì´ë‚˜ ìœ„ì¹˜ë¥¼ ì„ ì •í•˜ì—¬ ìƒ˜í”Œë§ íŒë‹¨ ìƒ˜í”Œë§: ëª©ì ì— ê°€ì¥ ì í•©í•œ ëŒ€ìƒì´ë¼ê³  ìƒê°í•˜ëŠëŠ ëŒ€ìƒì„ ì„ íƒ í• ë‹¹ ìƒ˜í”Œë§ ì°¸ê³ : https://jmj3047.github.io/2022/09/07/Data_Sampling&#x2F; feature ì¶•ì†Œ or í™•ëŒ€ â€˜ìœ ì € í‚¬ìˆ˜, ìµœëŒ€ìˆœìœ„, ë¡œê·¸ì¸íšŸìˆ˜, í—¤ë“œìƒ·í‚¬ìˆ˜, ìœ ì €ë ˆë²¨, í”Œë ˆì´ ì‹œê°„, ë¡œê·¸ì•„ì›ƒ ë‚ ì§œ, ì´ ê²Œì„ íšŸìˆ˜, ì´ í‚¬ìˆ˜, í‰ê·  ìˆœìœ„â€™ ì˜ ì§€í‘œë¥¼ EDA ê³¼ì •ì„ ê±°ì³ â€˜ì ‘ì†ì‹œê°„, ë ˆë²¨, ê²Œì„íšŸìˆ˜, ë¡œê·¸ì¸íšŸìˆ˜, í‰ê· ìˆœìœ„, ìµœëŒ€ìˆœìœ„, ì´í‚¬íšŸìˆ˜, ìœ ì €í‚¬íšŸìˆ˜, í—¤ë“œìƒ·í‚¬ìˆ˜, ì‹ ê³  íšŸìˆ˜, ì†”ë¡œ&#x2F;ë“€ì˜¤&#x2F;ìŠ¤ì¿¼ë“œ ì°¸ì—¬ íšŸìˆ˜, ì˜¤ì§ ì†”ë¡œëª¨ë“œë§Œ ì´ìš©â€™ ìœ¼ë¡œ ì§€í‘œë¥¼ í™•ëŒ€ ì‹œí‚´ â†’ ì•½ 64ë§Œí–‰ì˜ ë°ì´í„° í™•ë³´ ëª¨ë¸ í•™ìŠµ&#x2F; ëª¨ë¸ ê²€ì¦ ëª¨ë¸ ì„ íƒ ë° íŒŒë¼ë¯¸í„° ì¡°ì • â†’ ëª¨ë¸ í•™ìŠµ â†’ ëª¨ë¸ ê²€ì¦ ëŒ€ë¶€ë¶„ í•œì¤„ì˜ ì½”ë“œë¡œ ëª¨ë¸ ì„ íƒì´ ê°€ëŠ¥í•¨ íŒŒë¼ë¯¸í„° ì¡°ì • ê°™ì€ ê²½ìš° GridSearchCVë¡œ ì¡°ì • í• ìˆ˜ ìˆìŒ n_estimators&#x3D;500, learning_rate&#x3D;0.05, gamma&#x3D;0, subsample&#x3D;0.75, â†’ ì´ ë„¤ê°€ì§€ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ìµœì ê°’ìœ¼ë¡œ ì¡°ì •í•˜ê³  ì‹¶ë‹¤ë©´, ê°ê°ì˜ ë²”ìœ„ë¥¼ ì„ íƒí•´ì£¼ë©´ ë²”ìœ„ ë‚´ì˜ ëª¨ë“  í™•ë¥ ì„ ë‹¤ í™•ì¸í•œë‹¤: ì‹œê°„ë„ ì˜¤ë˜ ê±¸ë¦¬ê³  êµ‰ì¥íˆ ë¬´ê²ê²Œ ëŒì•„ê°. feature importanceë¡œ ì–´ë–¤ íŒŒë¼ë¯¸í„°ê°€ ëª¨ë¸ í•™ìŠµì— ê¸°ì—¬ë¥¼ ë§ì´ í–ˆëŠ”ì§€ ì•Œìˆ˜ ìˆìŒ í•™ìŠµ ì‹œ train ë°ì´í„°ë¥¼ validation setì„ ë–¼ì–´ë‘ê³  í•™ìŠµ í•´ì•¼ ëª¨ë¸ ê²€ì¦ì´ ê°€ëŠ¥ í•¨. ëª¨ë¸ ê²€ì¦ â†’ ëª¨ë¸ê³¼ ë°ì´í„° ê°„ì˜ ì í•©ë„ í™•ì¸ í•˜ëŠ” ê³¼ì • f1 score, $r^2$ score, confusion matrix ë“±ë“± ì˜ˆì¸¡í•˜ê¸° ë° ê²°ê³¼ í™•ì¸ ëª¨ë¸ ì €ì¥ í›„ test ë°ì´í„°ë¡œ ì‹¤í–‰í•´ë´„ ê²°ê³¼ì˜ ì‹ ë¢°ì„±ì´ ì–´ëŠì •ë„ ì¸ì§€ í™•ì¸í•˜ëŠ” ë°©ë²• â†’ ë¯¸ë˜ì˜ ì‚¬ê±´ì´ë¼ì„œ 100í”„ë¡œ í™•ì¸ì€ ì•ˆë˜ì§€ë§Œ ì˜¤ë¥˜ì— ëŒ€í•œ ë¶€ë¶„ì„ í™•ì¸í•˜ë©´ ìœ ì¶” ê°€ëŠ¥","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"ML Process","slug":"ML-Process","permalink":"https://jmj3047.github.io/tags/ML-Process/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}]},{"title":"Auto-correlation Function, Partial Auto-correlation Function","slug":"ACF_PACF","date":"2023-02-22T15:00:00.000Z","updated":"2023-02-23T01:50:54.354Z","comments":true,"path":"2023/02/23/ACF_PACF/","link":"","permalink":"https://jmj3047.github.io/2023/02/23/ACF_PACF/","excerpt":"","text":"ìê¸° ìƒê´€ í•¨ìˆ˜ì™€ ë¶€ë¶„ ìê¸° ìƒê´€ í•¨ìˆ˜Autocorrelation Function, ìê¸° ìƒê´€ í•¨ìˆ˜ ìê¸° ìƒê´€ í•¨ìˆ˜(Auto-correlation Function) ì–´ë–¤ ì‹ í˜¸ì˜ ì‹œê°„ì´ë™ ëœ ìê¸° ìì‹ ê³¼ì˜ â€˜ìƒê´€ì„±(Correlation)â€™ ì²™ë„ ì£¼ìš” íŠ¹ì§• ê²°ì • ì‹ í˜¸(ì£¼ê¸° ì‹ í˜¸&#x2F;ë¹„ì£¼ê¸° ì‹ í˜¸)ì´ë“ , ëœë¤ ì‹ í˜¸ ì´ë“  ëª¨ë“  ì‹ í˜¸ì— ëŒ€í•´ ì ìš© ê°€ëŠ¥ íŠ¹íˆ ëœë¤ ê³¼ì •ì¸ ê²½ìš°ì— ìê¸°ìƒê´€í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ êµ³ì´ ì‹œê°„ì‹ í˜¸ì— ëŒ€í•œ í‘¸ë¦¬ì— ë³€í™˜ì„ êµ¬í•  í•„ìš” ì—†ì´ ì£¼íŒŒìˆ˜ìƒì— ë¶„í¬ëœ ì „ë ¥(ì „ë ¥ë°€ë„ìŠ¤í™íŠ¸ëŸ¼)ì„ ì·¨ê¸‰í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì´ë¥¼ ì‚¬ìš©í•˜ê²Œ ë¨ [ì°¸ê³ ] ì„œë¡œ ë‹¤ë¥¸ ì‹ í˜¸ ê°„ì˜ ìƒê´€ì„± ì²™ë„ì— ëŒ€í•´ì„œëŠ” ìƒí˜¸ìƒê´€ ì°¸ì¡° ìƒê´€ë° ëŒ€í•œ ë³´ë‹¤ ì •í™•í•œ ì´í•´ë¥¼ ìœ„í•´ì„œëŠ” ìƒê´€ì„± ì°¸ì¡° ìƒê´€ì„± ê°œë…ì˜ ì¢…í•©í™”&#x2F;ì¼ë°˜í™”ëŠ” ë¹„êµ(ê°™ìŒ&#x2F;ë‹®ìŒ&#x2F;ë‹¤ë¦„) ì°¸ì¡° í™•ì •ì  ì‹ í˜¸(Deterministic signal)ì—ì„œ, ìê¸° ìƒê´€ í•¨ìˆ˜ ì—ë„ˆì§€ì‹ í˜¸ì˜ ìê¸°ìƒê´€í•¨ìˆ˜: ì»¨ë³¼ë£¨ì…˜(*)ì— ì˜í•´ ì •ì˜ë” x(t)ê°€ ì‹¤ìˆ˜ ì‹ í˜¸ì´ë©´, $Rx(Ï„)&#x3D;âˆ«^âˆ _{âˆ’âˆ} x(t)x(t+Ï„)dt&#x3D;x(Ï„)âˆ—x(âˆ’Ï„)$ x(t)ê°€ ë³µì†Œìˆ˜ ì‹ í˜¸ì´ë©´, $Rx(Ï„)&#x3D;âˆ«^âˆ_{âˆ’âˆ}x(t)x^âˆ—(t+Ï„)dt&#x3D;x(Ï„)âˆ—x^âˆ—(âˆ’Ï„)$ ì „ë ¥ì‹ í˜¸ì˜ ìê¸° ìƒê´€ í•¨ìˆ˜: ì‹œê°„í‰ê· (&lt;&gt;)ì— ì˜í•´ ì •ì˜ë¨ ì‹¤ìˆ˜ ì‹ í˜¸ ë³µì†Œìˆ˜ ì‹ í˜¸ ëœë¤ ê³¼ì •(Random Prodcess)ì—ì„œ, ìê¸°ìƒê´€ í•¨ìˆ˜ ì •ì˜ í†µê³„ì  í‰ê· ì— ì˜í•œ ìê¸°ìƒê´€í•¨ìˆ˜ ì •ì˜: $R_x(t_1,t_2) &#x3D; E[X(t_1)X(t_2)]$ ê²°í•© PDF(ê²°í•© í™•ë¥ ë°€ë„í•¨ìˆ˜)ì— ì˜í•œ ìê¸° ìƒê´€í•¨ìˆ˜ ì •ì˜: $R_x(t_1,t_2) &#x3D; âˆ«^âˆ_{âˆ’âˆ}âˆ«^âˆ_{âˆ’âˆ}x_1x_2fx_1x_2(x_1,t_1,x_2,t_2)dx_1dx_2$ ë§Œì¼ ëœë¤ ê³¼ì •ì´ ê´‘ì˜ì˜ ì •ìƒê³¼ì •ì´ë©´, ì‹œê°„ tì˜ í•¨ìˆ˜ê°€ ì•„ë‹ˆë¼ ì‹œê°„ì²œì´ $t-t&#x3D;Ï„$ì˜ í•¨ìˆ˜ê°€ ë¨ $R_x(t_1,t_2) &#x3D; R_x(t,t+Ï„)&#x3D;R_x(Ï„) &#x3D; E[X(t)X(t+Ï„)]$ ì´ë•Œ ì‹œê°„ ì˜ì—­ ìê¸°ìƒê´€ê³¼ ì£¼íŒŒìˆ˜ì˜ì—­ ìŠ¤í™íŠ¸ëŸ¼ë°€ë„ ê°„ì— í‘¸ë¦¬ì— ë³€í™˜ ìŒ ê´€ê³„ê°€ ìˆìŒ. $R(Ï„)$ â† í‘¸ë¦¬ì—ë³€í™˜ ìŒ ê´€ê³„ â†’ $S(f)$ ë§Œì¼, ëœë¤ê³¼ì •ì´ ì—ë¥´ê³ ë”•ê³¼ì •ì´ë¼ë©´, í†µê³„ì  í‰ê·  ë° ì‹œê°„ í‰ê· ì´ ìƒí˜¸ í˜¸í™˜ì´ ê°€ëŠ¥í•¨ $R_x(Ï„) &#x3D; E[X(t)X(t+Ï„)] &#x3D;&lt; X(t)X(t+Ï„)&gt;$ ë”°ë¼ì„œ ì´ ê²½ìš°ì—ëŠ” R(Ï„)ëŠ” ì‹œê°„ í‰ê· ì´ë‚˜ í†µê³„ì  í‰ê·  ì–´ëŠ ê²ƒìœ¼ë¡œë„ êµ¬í•  ìˆ˜ ìˆìŒ. ìê¸°ìƒê´€ í•¨ìˆ˜ì˜ ì„±ì§ˆ&#x2F;íŠ¹ì§• ì‹ í˜¸ì˜ â€˜ì‹œë³€(time-variant)â€™ íŠ¹ì„±ì´ ì–´ë–¤ê°€ë¥¼ ë³´ì—¬ì¤Œ ê·¸ ì‹ í˜¸ê°€ ê°–ëŠ” ìŠ¤í™íŠ¸ëŸ¼ì˜ íŠ¹ì„± ì •ë³´ë¥¼ ë‚˜íƒ€ëƒ„ ì‹œê°„ì ì¸(ì‹œë³€) ìƒê´€ì„± ì²™ë„ì„ ë¶„ì‚°ì´ í™•ë¥ ë³€ìˆ˜ê°€ í†µê³„ì ìœ¼ë¡œ ë¶ˆê·œì¹™í•˜ê²Œ ë¶„í¬ë˜ëŠ” ì •ë„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì²™ë„ë¼ë©´ ìê¸° ìƒê´€ì€ ë¶„ì‚°ê³¼ ìœ ì‚¬í•˜ê²Œ í™•ë¥ ê³¼ì •ì´ ì‹œê°„ì ìœ¼ë¡œ ìƒê´€ ë˜ëŠ” ë¶„ì‚°ë˜ëŠ” ì²™ë„ë¥¼ ë‚˜íƒ€ëƒ„ ì§ê´€ì ìœ¼ë¡œ ìê¸° ìì‹ ê³¼ì˜ ì‹œê°„ì²œì´(Ï„)ê°€ ì‘ì„ìˆ˜ë¡ ìƒê´€ì„±ì´ ì»¤ì§ ë”°ë¼ì„œ Ï„ &#x3D; 0 ì—ì„œ ìµœëŒ€ ìƒê´€ì„± ê°’ì„ ê°–ìŒ $|R_x(Ï„)|â‰¤R_x(0)$ì‹ Ï„ &#x3D; 0 ì¼ë•Œ ë¬¼ë¦¬ì  ì˜ë¯¸ë¡œëŠ” ì—ë„ˆì§€ ì‹ í˜¸: Ï„ &#x3D; 0 ì—ì„œì˜ ìµœëŒ€ê°’ì´ ì „ì²´ ì‹ í˜¸ì—ë„ˆì§€ì™€ ê°™ìŒ $R_x(0) &#x3D; âˆ«^âˆ_{âˆ’âˆ}|x(t)|^2dt&#x3D; E_x$ ì „ë ¥ì‹ í˜¸: Ï„ &#x3D; 0 ì—ì„œì˜ ìµœëŒ€ê°’ì´ í‰ê·  ì „ë ¥ê³¼ ê°™ìŒ $R_x(0) &#x3D;&lt; x^2(t) &gt;&#x3D; âˆ«^âˆ_{âˆ’âˆ}S_x(f)df &#x3D; P_{av}$ WSS ëœë¤ê³¼ì •: Ï„ &#x3D; 0ì—ì„œì˜ ìµœëŒ€ê°’ì´ í‰ê· ì „ë ¥ê³¼ ê°™ìŒ $R_x(0) &#x3D; E[X^2(t)] &#x3D; âˆ«^âˆ_{âˆ’âˆ}S_x(f)df &#x3D; P_{av}$ ì‹œê°„ ì˜ì—­ ìê¸° ìƒê´€ê³¼ ì£¼íŒŒìˆ˜ ì˜ì—­ ìŠ¤í™íŠ¸ëŸ¼ ë°€ë„ ê°„ì— í‘¸ë¦¬ì— ë³€í™˜ ìŒ ê´€ê³„ê°€ ìˆìŒ ìê¸° ìƒê´€ â† í‘¸ë¦¬ì—ë³€í™˜ ìŒ ê´€ê³„ â†’ ìŠ¤í™íŠ¸ëŸ¼ ë°€ë„ : ìœ„ë„ˆí‚¨ì¹œì •ë¦¬ ì°¸ì¡° Partial Autocorrelation Function, ë¶€ë¶„ ìê¸° ìƒê´€ í•¨ìˆ˜ í¸ ìê¸° ìƒê´€ í•¨ìˆ˜(ë¶€ë¶„ìê¸°ìƒê´€í•¨ìˆ˜)ëŠ” ë‹¤ë¥¸ ëª¨ë“  ì§§ì€ ì‹œì°¨ í•­ì— ë”°ë¼ ì¡°ì •í•œ í›„ k ì‹œê°„ ë‹¨ìœ„ë¡œ êµ¬ë¶„ëœ ì‹œê³„ì—´($y_{t-1},y_{t-2},â€¦,y_{t-k-1}$)ì˜ ê´€ì¸¡ì¹˜ ($y_t ë° y_{t-k}$) ê°„ì˜ ìƒê´€ì˜ ì¸¡ë„ì„ í•´ì„ ARIMA ëª¨í˜•ì—ì„œ ìê¸° íšŒê·€ ì°¨ìˆ˜ë¥¼ ì‹ë³„í•˜ëŠ” ìš©ë„ë¡œ ì‚¬ìš© ë¨. í¸ ìê¸° ìƒê´€ í•¨ìˆ˜ì—ì„œ ë‹¤ìŒê³¼ ê°™ì€ íŒ¨í„´ì„ ì°¾ìŒ. ê° ì‹œì°¨ì—ì„œ í° ê°’ì„ ì¡°ì‚¬í•˜ì—¬ ìœ ì˜í•œì§€ í™•ì¸í•¨. ìœ ì˜í•œ í° ê°’ì€ ìœ ì˜ í•œê³„ë¥¼ ë²—ì–´ë‚˜ë©´, ì´ëŠ” í•´ë‹¹ ì‹œì°¨ì— ëŒ€í•œ ìƒê´€ì´ 0ì´ ì•„ë‹ˆë¼ëŠ” ê²ƒì„ ë‚˜íƒ€ëƒ„ ì´ ê·¸ë¦¼ì—ì„œëŠ” ì‹œì°¨ 1ì— ìœ ì˜í•œ ìƒê´€ì´ ìˆê³  ê·¸ ë’¤ì—ëŠ” ìœ ì˜í•˜ì§€ ì•Šì€ ìƒê´€ì´ ìˆìŒ. ì´ íŒ¨í„´ì€ 1ì°¨ ìê¸°íšŒê·€ í•­ì„ ë‚˜íƒ€ëƒ„. Reference http://www.ktword.co.kr/test/view/view.php?m_temp1&#x3D;3547 https://support.minitab.com/ko-kr/minitab/20/help-and-how-to/statistical-modeling/time-series/how-to/partial-autocorrelation/interpret-the-results/partial-autocorrelation-function-pacf/ https://zephyrus1111.tistory.com/135","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"ACF","slug":"ACF","permalink":"https://jmj3047.github.io/tags/ACF/"},{"name":"PACF","slug":"PACF","permalink":"https://jmj3047.github.io/tags/PACF/"},{"name":"Time Series","slug":"Time-Series","permalink":"https://jmj3047.github.io/tags/Time-Series/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}]},{"title":"Probability Distribution Function & Probability Density Function","slug":"Probability_Distribution_Function","date":"2022-11-19T15:00:00.000Z","updated":"2023-02-23T01:38:48.226Z","comments":true,"path":"2022/11/20/Probability_Distribution_Function/","link":"","permalink":"https://jmj3047.github.io/2022/11/20/Probability_Distribution_Function/","excerpt":"","text":"í™•ë¥  ë¶„í¬ í•¨ìˆ˜ì™€ í™•ë¥  ë°€ë„ í•¨ìˆ˜í™•ë¥  ë¶„í¬ í•¨ìˆ˜(probability distribution function)ì™€ í™•ë¥  ë°€ë„ í•¨ìˆ˜(probability density function)ëŠ” í™•ë¥  ë³€ìˆ˜ì˜ ë¶„í¬ ì¦‰, í™•ë¥  ë¶„í¬ë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ ì •ì˜í•˜ê¸° ìœ„í•œ ìˆ˜ì‹ì´ë‹¤. ì—°ì† í™•ë¥  ë¶„í¬ìš°ì„  í™•ë¥  ë°€ë„ í•¨ìˆ˜ì— ëŒ€í•´ ë¨¼ì € ì•Œì•„ë³´ì. í™•ë¥  ë°€ë„ í•¨ìˆ˜ë¥¼ ì´í•´í•˜ë©´ í™•ë¥  ë¶„í¬ í•¨ìˆ˜ë¥¼ ì´í•´í•˜ëŠ” ê²ƒì€ ì‰½ë‹¤. í™•ë¥  ë°€ë„ í•¨ìˆ˜ëŠ” ì—°ì† í™•ë¥  ë³€ìˆ˜(continuous random variable)ë¥¼ ì •ì˜í•˜ëŠ”ë° í•„ìš”í•˜ë‹¤. ì—°ì† í™•ë¥  ë³€ìˆ˜ì˜ ê°’ì€ ì‹¤ìˆ˜(real number) ì§‘í•©ì²˜ëŸ¼ ì—°ì†ì ì´ê³  ë¬´í•œê°œì˜ ê²½ìš°ì˜ ìˆ˜ë¥¼ ê°€ì§„ë‹¤. ì—°ì† í™•ë¥  ë³€ìˆ˜ì˜ ë¶„í¬ë¥¼ ì—°ì† í™•ë¥  ë¶„í¬ë¼ê³  í•œë‹¤. ì‹œê³„ ë°”ëŠ˜ì„ ì˜ˆë¡œ ë“¤ì–´ë³´ì. ë‹¤ìŒê³¼ ê°™ì€ ì•„ë‚ ë¡œê·¸ ì‹œê³„ì˜ ì‹œê³„ ë°”ëŠ˜ì„ ëˆˆì„ ê°ê³  ì„ì˜ë¡œ ëŒë ¸ë‹¤ê³  í•˜ë©´ ì‹œê³„ ë°”ëŠ˜ì´ ì •ê° 12ì‹œ(ê°ë„ 0ë„)ë¥¼ ê°€ë¦¬í‚¬ í™•ë¥ ì€ ì–¼ë§ˆì¼ê¹Œ? ë§Œì•½ ì´ í™•ë¥  ë³€ìˆ˜ì˜ í™•ë¥  ë¶„í¬ê°€ 0 ì´ìƒ 360 ë¯¸ë§Œì˜ êµ¬ê°„ë‚´ì—ì„œ ê· ì¼ ë¶„í¬(uniform distribution) ëª¨í˜•ì„ ê°€ì§„ë‹¤ê³  ê°€ì •í•˜ë©´ ë‹µì€ 0(zero)ì´ë‹¤.ì‹œê³„ ë°”ëŠ˜ì´ ê°€ë¦¬í‚¤ëŠ” ê°ë„ì˜ ê°’ì€ 0ë„ ì´ìƒ 360ë„ ë¯¸ë§Œì˜ ëª¨ë“  ì‹¤ìˆ˜ ê°’ì„ ê°€ì§ˆ ìˆ˜ ìˆëŠ”ë°, ì´ ê²½ìš° ìˆ˜ê°€ ë¬´í•œëŒ€ì´ë¯€ë¡œ ê°ê°ì˜ ê²½ìš°ì— ëŒ€í•œ í™•ë¥ ì€ 0ì´ ë˜ì–´ì•¼ í•˜ê¸° ë•Œë¬¸ì´ë‹¤.ì‚¬ì‹¤ ê°ë„ê°€ 0ë„ê°€ ì•„ë‹ˆë¼ ì–´ë–¤ íŠ¹ì •í•œ ê°ë„ë¥¼ ì§€ì •í•˜ë”ë¼ë„ ê°™ì€ ì´ìœ ë¡œ ê·¸ ê°ë„ë¥¼ ê°€ë¦¬í‚¬ í™•ë¥ ì€ 0ì´ë‹¤. ê·¸ëŸ¼ ë„ëŒ€ì²´ ì–´ë–¤ ë°©ë²•ìœ¼ë¡œ í™•ë¥  ë¶„í¬ë¥¼ ì„¤ëª…í•´ì•¼ í• ê¹Œ? ì´ë ‡ê²Œ ê²½ìš°ì˜ ìˆ˜ê°€ ë¬´í•œëŒ€ì¸ ì—°ì† í™•ë¥  ë³€ìˆ˜ì˜ ë¶„í¬ë¥¼ ì„¤ëª…í•˜ë ¤ë©´ íŠ¹ì •í•œ ê°’ì´ ì•„ë‹ˆë¼ êµ¬ê°„ì„ ì§€ì •í•˜ì—¬ í™•ë¥ ì„ ì„¤ëª…í•´ì•¼ í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ìœ„ì™€ ê°™ì€ ì‹œê³„ë°”ëŠ˜ì˜ ì˜ˆì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë¶„í¬ì˜ ë¬˜ì‚¬ê°€ ê°€ëŠ¥í•˜ë‹¤. ì‹œê³„ ë°”ëŠ˜ì´ 12ì‹œì™€ 1ì‹œ ì‚¬ì´ì— ìˆì„ í™•ë¥ ì€ 1&#x2F;12 ì‹œê³„ ë°”ëŠ˜ì´ 1ì‹œì™€ 3ì‹œ ì‚¬ì´ì— ìˆì„ í™•ë¥ ì€ 2&#x2F;12 &#x3D; 1&#x2F;6 ì‹œê³„ ë°”ëŠ˜ì´ 6ì‹œì™€ 9ì‹œ ì‚¬ì´ì— ìˆì„ í™•ë¥ ì€ 3&#x2F;12 &#x3D; 1&#x2F;4 ì´ ë°©ë²•ì˜ ë‹¨ì  ì¤‘ í•˜ë‚˜ëŠ” ë¶„í¬ë¥¼ ì„¤ëª…í•˜ëŠ”ë° ë²”ìœ„ë¥¼ ì§€ì •í•˜ëŠ” ë‘ ê°œì˜ ìˆ«ìê°€ í•„ìš”í•˜ë‹¤ëŠ” ì ì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ â€˜1ì‹œì™€ 3ì‹œ ì‚¬ì´â€™ ë¼ëŠ” ë²”ìœ„ë¥¼ ì§€ì •í•˜ëŠ”ë°ëŠ” 1ê³¼ 3ì´ë¼ëŠ” ìˆ«ìê°€ í•„ìš”í•˜ë‹¤.ê·¸ëŸ¼ í•˜ë‚˜ì˜ ìˆ«ìë¡œ í™•ë¥  ë³€ìˆ˜ì˜ ë²”ìœ„ë¥¼ ì§€ì •í•˜ëŠ” ë°©ë²•ì€ ì—†ì„ê¹Œ? ê°€ëŠ¥í•œ ë°©ë²• ì¤‘ì˜ í•˜ë‚˜ëŠ” ë²”ìœ„ë¥¼ ì§€ì •í•˜ëŠ” ë‘ ê°œì˜ ìˆ«ì ì¤‘ ì‘ì€ ìˆ«ì ì¦‰, ë²”ìœ„ê°€ ì‹œì‘í•˜ëŠ” ìˆ«ìë¥¼ ë¯¸ë¦¬ ê°€ì¥ ì‘ì€ ìˆ«ìë¡œ ê³ ì •í•˜ëŠ” ë°©ë²•ì´ë‹¤. ì´ ë°©ë²•ì„ ì“°ë©´ ë‹¤ìŒê³¼ ê°™ì´ í•˜ë‚˜ì˜ ìˆ«ìë¡œ ëœë¤ ë³€ìˆ˜ì˜ ë²”ìœ„ì™€ í•´ë‹¹ í™•ë¥ ì„ ì„œìˆ í•  ìˆ˜ ìˆë‹¤. ìˆ«ì&#x3D;1 -&gt; ë²”ìœ„&#x3D;12ì‹œë¶€í„° 1ì‹œê¹Œì§€ -&gt; í™•ë¥  1&#x2F;12 ìˆ«ì&#x3D;2 -&gt; ë²”ìœ„&#x3D;12ì‹œë¶€í„° 2ì‹œê¹Œì§€ -&gt; í™•ë¥  2&#x2F;12 ìˆ«ì&#x3D;5 -&gt; ë²”ìœ„&#x3D;12ì‹œë¶€í„° 5ì‹œê¹Œì§€ -&gt; í™•ë¥  5&#x2F;12 ëˆ„ì  í™•ë¥  ë¶„í¬ìœ„ì™€ ê°™ì€ ë°©ë²•ìœ¼ë¡œ ì„œìˆ ëœ í™•ë¥  ë¶„í¬ë¥¼ ëˆ„ì  í™•ë¥  ë°€ë„ í•¨ìˆ˜ (cumulative probability density function) ë˜ëŠ” ëˆ„ì  í™•ë¥  ë¶„í¬ë¼ê³  í•˜ê³  ì•½ìë¡œ cdfë¼ê³  ì“´ë‹¤. ì¼ë°˜ì ìœ¼ë¡œ cdfëŠ” ëŒ€ë¬¸ìë¥¼ ì‚¬ìš©í•˜ì—¬ F(x)ì™€ ê°™ì€ ê¸°í˜¸ë¡œ í‘œì‹œí•˜ë©° ì´ ë•Œ ë…ë¦½ ë³€ìˆ˜ xëŠ” ë²”ìœ„ì˜ ëì„ ëœ»í•œë‹¤. ë²”ìœ„ì˜ ì‹œì‘ì€ ì¼ë°˜ì ìœ¼ë¡œ ìŒì˜ ë¬´í•œëŒ€(negative infinity, -âˆ)ê°’ì„ ì‚¬ìš©í•œë‹¤.ëª‡ê°€ì§€ ëˆ„ì  í™•ë¥  ë¶„í¬ í‘œì‹œì˜ ì˜ˆë¥¼ ë“¤ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. F(-1) : í™•ë¥  ë³€ìˆ˜ê°€ -âˆ ì´ìƒ -1 ë¯¸ë§Œì¸ êµ¬ê°„ ë‚´ì— ì¡´ì¬í•  í™•ë¥  F(10) : í™•ë¥  ë³€ìˆ˜ê°€ -âˆ ì´ìƒ 10 ë¯¸ë§Œì¸ êµ¬ê°„ ë‚´ì— ì¡´ì¬í•  í™•ë¥  í™•ë¥  ë³€ìˆ˜ Xì— ëŒ€í•œ ëˆ„ì  í™•ë¥  ë¶„í¬ F(x)ì˜ ìˆ˜í•™ì  ì •ì˜ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. $$F(x) &#x3D; P(X &lt; x) &#x3D; P(X&lt;x)$$ ì¼ë¡€ë¡œ í‘œì¤€ ì •ê·œ ë¶„í¬ì˜ ëˆ„ì  í™•ë¥ ì„ ê·¸ë¦¬ë©´ ì•„ë˜ì™€ ê°™ë‹¤. 123x = np.linspace(-4,4)y = sp.stats.norm.cdf(x)plt.plot(x,y) ëˆ„ì  ë°€ë„ í•¨ìˆ˜ ì¦‰, cdfëŠ” ë‹¤ìŒê³¼ ê°™ì€ íŠ¹ì§•ì„ ê°€ì§„ë‹¤. F(-âˆ) &#x3D; 0 F(âˆ) &#x3D; 1 F(x) â‰¥ F(y) if x &gt; y í™•ë¥  ë°€ë„ í•¨ìˆ˜ëˆ„ì  ë°€ë„ í•¨ìˆ˜ì˜ ë‹¨ì  ì¤‘ì˜ í•˜ë‚˜ëŠ” ì–´ë–¤ ê°’ì´ ë” ìì£¼ ë‚˜ì˜¤ë“ ê°€ í˜¹ì€ ë” ê°€ëŠ¥ì„±ì´ ë†’ì€ì§€ì— ëŒ€í•œ ì •ë³´ë¥¼ ì•Œê¸° í˜ë“¤ë‹¤ëŠ” ì ì´ë‹¤. ì´ë¥¼ ì•Œê¸° ìœ„í•´ì„œëŠ” í™•ë¥  ë³€ìˆ˜ê°€ ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” ì „ì²´ êµ¬ê°„ (-âˆ ~ âˆ)ì„ ì•„ì£¼ ì‘ì€ í­ì„ ê°€ì§€ëŠ” êµ¬ê°„ë“¤ë¡œ ë‚˜ëˆˆ ë‹¤ìŒ ê° êµ¬ê°„ì˜ í™•ë¥ ì„ ì‚´í´ë³´ëŠ” ê²ƒì´ í¸ë¦¬í•˜ë‹¤. ë‹¤ë§Œ ì´ë ‡ê²Œ ë˜ë©´ êµ¬ê°„ì˜ í­ì„ ì–¼ë§ˆë¡œ ì •í•´ì•¼ í•˜ëŠ”ì§€ì— ëŒ€í•œ ì˜ë¬¸ì´ ìƒê¸´ë‹¤. 123x = np.linspace(-4,4,20)y = sp.stats.norm.cdf(x)z = np.insert(np.diff(y), 0, None) 12w = (4-(-4))/20plt.bar(x-w, z/w, width = w) ì´ ë•Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ìˆ˜í•™ì  ë°©ë²•ì´ ë°”ë¡œ ë¯¸ë¶„(differentiation)ì´ë‹¤. ë¯¸ë¶„ì€ í•¨ìˆ˜ì˜ êµ¬ê°„ì„ ë¬´í•œëŒ€ ê°¯ìˆ˜ë¡œ ìª¼ê°œì–´ ê° êµ¬ê°„ ë³€í™”ì˜ ì •ë„ ì¦‰, ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ë²•ì´ë‹¤. ëˆ„ì  ë°€ë„ í•¨ìˆ˜ë¥¼ ë¯¸ë¶„í•˜ì—¬ ë‚˜ì˜¨ ë„í•¨ìˆ˜(derivative)ë¥¼ í™•ë¥  ë°€ë„ í•¨ìˆ˜(probability density function)ë¼ê³  í•œë‹¤. ëˆ„ì  ë°€ë„ í•¨ìˆ˜ëŠ” ë³´í†µ f(x)ì™€ ê°™ì´ ì†Œë¬¸ì í•¨ìˆ˜ ê¸°í˜¸ë¥¼ ì‚¬ìš©í•˜ì—¬ í‘œê¸°í•œë‹¤. $$f(x)&#x3D;\\frac{dF(x)}{dx} \\text ë˜ëŠ” F(x) &#x3D; \\int_{-\\infty}^{x}f(u)du$$ í™•ë¥  ë°€ë„ í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ì€ íŠ¹ì§•ì„ ê°€ì§„ë‹¤. -âˆ ë¶€í„° âˆê¹Œì§€ ì ë¶„í•˜ë©´ ê·¸ ê°’ì€ 1ì´ ëœë‹¤. Reference https:&#x2F;&#x2F;velog.io&#x2F;@groovallstar&#x2F;í™•ë¥ -ë¶„í¬-í•¨ìˆ˜ì™€-í™•ë¥ -ë°€ë„-í•¨ìˆ˜ì˜-ì˜ë¯¸","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"Probability Distribution Function","slug":"Probability-Distribution-Function","permalink":"https://jmj3047.github.io/tags/Probability-Distribution-Function/"},{"name":"Probability Density Function","slug":"Probability-Density-Function","permalink":"https://jmj3047.github.io/tags/Probability-Density-Function/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}]},{"title":"Difference between Normal Distribution & Standard Normal Distribution","slug":"Normal_Distribution","date":"2022-11-10T15:00:00.000Z","updated":"2023-02-23T01:38:44.907Z","comments":true,"path":"2022/11/11/Normal_Distribution/","link":"","permalink":"https://jmj3047.github.io/2022/11/11/Normal_Distribution/","excerpt":"","text":"ì •ê·œë¶„í‘œì™€ í‘œì¤€ì •ê·œë¶„í¬í•¨ìˆ˜ì˜ ì°¨ì´ë³¸ í¬ìŠ¤íŒ…ì—ì„œëŠ” ì •ê·œë¶„í¬(Normal distribution)ì™€ í‘œì¤€ ì •ê·œ ë¶„í¬(Standard normal distribution)ì— ëŒ€í•´ ë‹¤ë£¨ë„ë¡ í•œë‹¤. ì •ê·œ ë¶„í¬ì˜ í™•ë¥ ë°€ë„ í•¨ìˆ˜ì™€ ì˜ˆìƒì¹˜(í‰ê· ), ë¶„ì‚° ê·¸ë¦¬ê³  ì¦ëª…ì— ëŒ€í•´ ë‹¤ë£¨ë©° í‘œì¤€ì •ê·œë¶„í¬ì— ëŒ€í•´ì„œëŠ” í™•ë¥ ë°€ë„í•¨ìˆ˜, ëˆ„ì ë¶„í¬í•¨ìˆ˜, ê·¸ë¦¬ê³  í‘œì¤€ì •ê·œë¶„í¬ë¥¼ ì´ìš©í•œ ì •ê·œë¶„í¬ì˜ í™•ë¥ ê³„ì‚° ë“±ì˜ ë‚´ìš©ì´ ë‹¤ë¤„ì§„ë‹¤. 1. ì •ê·œë¶„í¬(Normal distribution)**ì •ê·œ ë¶„í¬(Normal distribution)**ëŠ” ì—°ì†í™•ë¥ ë¶„í¬ ì¤‘ í•˜ë‚˜ì´ë©° ê´‘ë²”ìœ„í•˜ê²Œ ì‚¬ìš©ëœë‹¤. í™•ë¥  ë¶„í¬ ì¤‘ ê°€ì¥ ìœ ëª…í•˜ë©° ê°€ì¥ ì¤‘ìš”í•˜ê²Œ ë‹¤ë£¨ëŠ” í™•ë¥  ë¶„í¬ì´ë‹¤. **ì˜¤ë¥˜ ë¶„í¬(Error distribution)**ì™€ ë‹¤ë¥¸ ë§ì€ ìì—°í˜„ìƒì„ ì§ì ‘ ëª¨ë¸ë§ í•˜ê¸° ìœ„í•œ í™•ë¥  ë¶„í¬ì´ë‹¤. ì¤‘ì‹¬ê·¹í•œì •ë¦¬(Central Limit Theorm)ë¡œ ì¸í•´ ë§¤ìš° ìœ ìš©í•˜ë©° ë‹¨ìˆœí•˜ê³  ì •í™•í•œ ê·¼ì‚¬ê°€ ê°€ëŠ¥í•˜ë‹¤. ì •ê·œ ë¶„í¬ëŠ” ê°€ìš°ìŠ¤ ë¶„í¬(Gaussian distribution)ë¼ê³ ë„ ë¶ˆë¦°ë‹¤. ë¹„ìœ¨ê³¼ ê°œë³„ì ì¸ í™•ë¥ ì„ ëª¨ë¸ë§í•˜ëŠ”ë°ë„ ìœ ìš©í•˜ë‹¤. ì •ê·œë¶„í¬ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„ëœë‹¤. : X~N(Î¼, Ïƒ2) ì•„ë˜ ê·¸ë¦¼ì€ ì •ê·œ ë¶„í¬ì˜ í™•ë¥ ë°€ë„ í•¨ìˆ˜ë¥¼ ë³´ì—¬ì¤€ë‹¤ í™•ë¥ ë°€ë„í•¨ìˆ˜(PDF)ì •ê·œë¶„í¬ì˜ í™•ë¥  ë°€ë„ í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤ ì˜ˆìƒì¹˜(Expectation)ì™€ ë¶„ì‚°(Variance) ì •ê·œë¶„í¬ì˜ ì ë¥ ìƒì„±í•¨ìˆ˜(MGF) ì˜ˆìƒì¹˜ì™€ ë¶„ì‚°ì˜ ì¦ëª… ë³¸ ì¦ëª…ì—ì„œëŠ” ì ë¥ ìƒì„±í•¨ìˆ˜(MGF)ë¥¼ ì´ìš©í•˜ì—¬ ì¦ëª…ì„ ìˆ˜í–‰í•´ë³´ë„ë¡ í•˜ê² ë‹¤. ì ë¥ ìƒì„±í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤ ì ë¥ ìƒì„±í•¨ìˆ˜ì˜ ë¯¸ë¶„ê°’ê³¼ ê·¸ ë¯¸ë¶„ê°’ì—ì„œ t&#x3D;0ì¸ ê²½ìš°ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤ ì ë¥ ìƒì„±í•¨ìˆ˜ì˜ 2ê³„ ë¯¸ë¶„ê°’ê³¼ ê·¸ ë¯¸ë¶„ê°’ì—ì„œ t&#x3D;0ì¸ ê²½ìš°ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤ ë”°ë¼ì„œ ë¶„ì‚°ì€ ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°ëœë‹¤ í‘œì¤€ì •ê·œë¶„í¬(Standard Normal distribution)ì •ê·œ ë¶„í¬ì—ì„œëŠ” Î¼ëŠ” 0ìœ¼ë¡œ Ïƒ2ì€ 1ë¡œ ì„¤ì •í•˜ì—¬ í‘œì¤€í™”ë¥¼ ìˆ˜í–‰í•œ ì •ê·œë¶„í¬ë¥¼ í‘œì¤€ì •ê·œë¶„í¬ë¼ê³  í•œë‹¤. ì¦‰, ì˜ˆìƒì¹˜(í‰ê· )ëŠ” 0, ë¶„ì‚°ì€ 1ì´ë‹¤. ì•„ë˜ ê·¸ë¦¼ì€ í‘œì¤€ ì •ê·œ ë¶„í¬ì˜ í™•ë¥ ë°€ë„í•¨ìˆ˜ì™€ ëˆ„ì ë¶„í¬í•¨ìˆ˜ë¥¼ ë³´ì—¬ì¤€ë‹¤. í™•ë¥ ë°€ë„í•¨ìˆ˜(PDF) í‘œì¤€ì •ê·œë¶„í¬ì˜ í™•ë¥ ë°€ë„í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. ëˆ„ì ë¶„í¬í•¨ìˆ˜(CDF) í‘œì¤€ì •ê·œë¶„í¬ì˜ ëˆ„ì ë¶„í¬í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. ì •ê·œë¶„í¬ì˜ ì ë¥ ìƒì„±í•¨ìˆ˜(MGF) í‘œì¤€ì •ê·œë¶„í¬ë¥¼ ì´ìš©í•œ ì •ê·œë¶„í¬ì˜ í™•ë¥ ê³„ì‚°í™•ë¥ ë¶„í¬ê°€ ì •ê·œë¶„í¬ì¸ ê²½ìš° í‘œì¤€ì •ê·œë¶„í¬ë¡œ ì¹˜í™˜í•˜ì—¬ íŠ¹ì •ë²”ìœ„ì˜ í™•ë¥ ì„ ê³„ì‚°í• ìˆ˜ ìˆë‹¤. ì¹˜í™˜ì€ ë‹¤ìŒê³¼ ê°™ì´ ìˆ˜í–‰ í•˜ë©´ ëœë‹¤. í‘œì¤€ì •ê·œë¶„í¬ì˜ ëˆ„ì í™•ë¥ ë¶„í¬(CDF) ê°’ì€ ë¹„êµì  ì‰½ê²Œ ê³„ì‚°ì´ ê°€ëŠ¥í•˜ë©° ë‹¤ìŒê³¼ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤. í‘œì¤€ ì •ê·œ ë¶„í¬ì˜ ëˆ„ì í™•ë¥  ë¶„í¬(CDF) í‘œ Excel, R, Python ëª¨ë“ˆ ë“±ì„ í™œìš©í•œ ë°ì´í„° ì·¨ë“ ë‹¤ìŒì€ ì •ê·œë¶„í¬ì˜ íŠ¹ì • ë²”ìœ„ì—ì„œì— ëŒ€í•œ í™•ë¥ ì„ í‘œì¤€ì •ê·œë¶„í¬ë¡œ ì¹˜í™˜í•˜ëŠ” ê³¼ì •ì„ ë³´ì—¬ì¤€ë‹¤. ì •ê·œë¶„í¬ëŠ” ì¤‘ì•™ê°’ì—ì„œ ì¢Œìš° ëŒ€ì¹­ì´ë‹¤. ë”°ë¼ì„œ ì¤‘ì•™ê°’ì—ì„œ ë‹¤ìŒê³¼ ê°™ì´ íŠ¹ì • ë²”ìœ„ì˜ í™•ë¥ ì„ í‘œí˜„í•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ë‹¤. ë˜í•œ í‘œì¤€ í¸ì°¨ë¥¼ ì´ìš©í•˜ì—¬ í‘œì¤€í™” ì‹œì¼œ íŠ¹ì • ë²”ìœ„ë¥¼ í‘œí˜„í•  ìˆ˜ ìˆë‹¤. Reference https://color-change.tistory.com/m/61 https://kongdols-room.tistory.com/145","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"Normal Distribution","slug":"Normal-Distribution","permalink":"https://jmj3047.github.io/tags/Normal-Distribution/"},{"name":"Standard Normal Distribution","slug":"Standard-Normal-Distribution","permalink":"https://jmj3047.github.io/tags/Standard-Normal-Distribution/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}]},{"title":"SpeakerGAN, Speaker identification with conditional generative adversarial network","slug":"SpeakerGAN","date":"2022-11-04T15:00:00.000Z","updated":"2022-11-10T15:27:17.845Z","comments":true,"path":"2022/11/05/SpeakerGAN/","link":"","permalink":"https://jmj3047.github.io/2022/11/05/SpeakerGAN/","excerpt":"","text":"Journal&#x2F;Conference : NeurocomputingYear(published year): 2020Author: Liyang Chen, Yifeng Liu , Wendong Xiao, Yingxue Wang, Haiyong XieSubject: Speaker GAN, Generative Adversarial Network SpeakerGAN: Speaker identification with conditional generative adversarial network Summary This paper proposes a novel approach, SpeakerGAN, for speaker identification with the conditional generative adversarial network (CGAN). We configure the generator and the discriminator in SpeakerGAN with the gated convolutional neural network (CNN) and the modified residual network (ResNet) to obtain generated samples of high diversity as well as increase the network capacity. Under the scenario of limited training data, SpeakerGAN obtains significant improvement over the baselines. IntroductionThe x-vector in [13,14] is proposed as a strong contender for the speaker representation and is considered to supplant the i-vector system by many researchers. We evaluate our approach on the dataset of Librispeech-100 for the text-independent SI task. The baselines include i-vector, xvector, CNN and LSTM. Generative Adversarial NetworksConditional GANThe CGAN is a variant of GAN, which aims to let the generator $G$ produce $G(c,z)$ from the condition $c$ and random noise $z$. In this paper, the real samples x are directly utilized as the condition c and the random noise z is abandoned for showing no effectiveness in the experiments. SpeakerGAN for speaker identificationBasic principle of SpeakerGANWe investigate the CGAN as a classifier by enabling the network to learn on additional unlabeled examples. The SpeakerGAN combines the discriminator and classifier by letting the classifier take samples from the generator as inputs and have $N+1$ output units, where $l_{N+1}$corresponds to the probability $P_{model}(y&#x3D;N+1|x)$ that the inputs are real. To stabilize the training and overcome the problem of vanishing gradients, the least square GAN (LSGAN) [40] is adopted. $L_{adv}$ is then split into the losses for the discriminator and generator. Network architecture of SpeakerGAN Fig. 1. Framework of SpeakerGAN. The front extraction part extracts FBanks $x$ from the real speech samples. The generator takes the real FBanks x as inputs and produces fake samples $G(x)$. The discriminator is then fed with the real and generated FBanks to predict class labels and distinguish between the real and fake samples. The adversarial loss in Multiple loss actually denotes the formulation of LSGAN [40]. The dashed lines with arrows denote calculating loss between two objects, and the solid lines denote the flow of information. The generator takes real sequences as the condition for inputs, and produces the fake samples of the same size after passing through a series of convolutional and shuffler layers that progressively downsample and upsample. The discriminator takes the generated samples and real acoustic features from the corpus as inputs, and outputs the discrimination of real&#x2F;fake along with the $N$ classes. Generator design These generators only capture relationships among feature dimension and the generated samples are in lack of consistency. An effective way to solve this problem would be to introduce the RNN, but it is timeconsuming due to the difficulty of parallel computing. For these reasons, we configure the generator using gated CNNs [43], which allow for both sequential structure and faster convergence. This idea was previously explored in [25], and achieved competitive performance. Discriminator design As for the discriminator, we prefer deeper networks to classify speakers. However, training deep neural networks is computationally expensive and difficult. This paper modifies the ResNet [45] toaccelerate the training. ResNets have been applied in many SI systems [16,17] and are known for good classification performance on image data. To reduce parameters and improve calculation efficiency, the variant ResBlock is adopted, which comprises a stack of three convolutional layers. Four ResBlocks are stacked and a softmax classifier is used to predict the speaker identity in the final layer. ExperimentsDataset and basic setupTo evaluate the performance of the proposed approach, we conduct experiments on the Librispeech [46] dataset. We use the train-clean-100 subset in it that contains 251 speakers of 125 females and 126males. Each speaker is provided with an average of 113 utterances lasting 1â€“15 s. The same acoustic features are used for the baselines of CNN, LSTM and GAN classifier. Speech samples are shuffled before training. Sixty percentage of all utterances are randomly selected as training data and the rest are used as test data. Conclustions and future workIt directly utilizes the discriminator as a classifier using the fake samples produced by the generator as the additional class. The Hybrid loss function includes the adversarial loss of the regular GAN, thecross-entropy loss of the labeled data and the Huber loss between the real samples and generation. Experimental results demonstrate that SpeakerGAN can achieve higher identification accuracy than other state-of-the-art DL based methods and the traditional i-vector and x-vector systems. Link: SpeakerGAN, Speaker identification with conditional generative adversarial network","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Generative Model","slug":"Paper/Generative-Model","permalink":"https://jmj3047.github.io/categories/Paper/Generative-Model/"}],"tags":[{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Speaker GAN","slug":"Speaker-GAN","permalink":"https://jmj3047.github.io/tags/Speaker-GAN/"},{"name":"Speaker Identification","slug":"Speaker-Identification","permalink":"https://jmj3047.github.io/tags/Speaker-Identification/"},{"name":"Generative Adversarial Network","slug":"Generative-Adversarial-Network","permalink":"https://jmj3047.github.io/tags/Generative-Adversarial-Network/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Generative Model","slug":"Paper/Generative-Model","permalink":"https://jmj3047.github.io/categories/Paper/Generative-Model/"}]},{"title":"Multi-Task Learning for Voice Trigger Detection","slug":"MTL_for_VTD","date":"2022-11-01T15:00:00.000Z","updated":"2022-11-10T15:18:26.813Z","comments":true,"path":"2022/11/02/MTL_for_VTD/","link":"","permalink":"https://jmj3047.github.io/2022/11/02/MTL_for_VTD/","excerpt":"","text":"Journal&#x2F;Conference : ICASSP IEEEYear(published year): 2020Author: Siddharth Sigtia, Pascal Clark, Rob Haynes, Hywel Richards, John BridleSubject: Multi-Task Learning Multi-Task Learning for Voice Trigger Detection Summary We start by training a general acoustic model that produces phonetic transcriptions given a large labelled training dataset. ìš°ë¦¬ëŠ” ë ˆì´ë¸”ì´ ì§€ì •ëœ ëŒ€ê·œëª¨ í›ˆë ¨ ë°ì´í„° ì„¸íŠ¸ê°€ ì£¼ì–´ì§€ë©´ phonetic transcriptionsë¥¼ ìƒì„±í•˜ëŠ” ì¼ë°˜ì ì¸ ìŒí–¥ ëª¨ë¸ì„ í›ˆë ¨í•˜ëŠ” ê²ƒìœ¼ë¡œ ì‹œì‘í•œë‹¤. Next, we collect a much smaller dataset of examples that are challenging for the baseline system. ë‹¤ìŒìœ¼ë¡œ, ìš°ë¦¬ëŠ” ê¸°ì¤€ ì‹œìŠ¤í…œì— ë„ì „í•˜ëŠ” í›¨ì”¬ ë” ì‘ì€ ì˜ˆì œì˜ ë°ì´í„° ì„¸íŠ¸ë¥¼ ìˆ˜ì§‘í•œë‹¤. We then use multi-task learning to train a model to simultaneously produce accurate phonetic transcriptions on the larger dataset and discriminate between true and easily confusable examples using the smaller dataset. ê·¸ëŸ° ë‹¤ìŒ ë‹¤ì¤‘ ì‘ì—… í•™ìŠµì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í›ˆë ¨ì‹œì¼œ ë” í° ë°ì´í„° ì„¸íŠ¸ì—ì„œ ì •í™•í•œ phonetic transcriptionsë¥¼ ìƒì„±í•˜ê³  ë” ì‘ì€ ë°ì´í„° ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ì˜ˆì œì™€ ì‰½ê²Œ í˜¼ë™í•  ìˆ˜ ìˆëŠ” ì˜ˆì œë¥¼ êµ¬ë³„í•œë‹¤. IntroductionSignificant challenge is that unlike automatic speech recognition (ASR) systems, collecting training examples for a specific keyword or phrase in a variety of conditions is a difficult problem. ì¤‘ìš”í•œ ê³¼ì œëŠ” ìë™ ìŒì„± ì¸ì‹(ASR) ì‹œìŠ¤í…œê³¼ ë‹¬ë¦¬ ë‹¤ì–‘í•œ ì¡°ê±´ì—ì„œ íŠ¹ì • í‚¤ì›Œë“œ ë˜ëŠ” êµ¬ë¬¸ì— ëŒ€í•œ í›ˆë ¨ ì˜ˆì œë¥¼ ìˆ˜ì§‘í•˜ëŠ” ê²ƒì€ ì–´ë ¤ìš´ ë¬¸ì œë¼ëŠ” ê²ƒì´ë‹¤. In the literature, the problem of detecting a speech trigger phrase is interchangeably referred to as voice trigger detection [3], keyword spotting [4], wake-up word detection [5] or hotword detection [6]. In the rest of this paper, we refer to this problem as voice trigger detection. ë¬¸í—Œì—ì„œ ìŒì„± íŠ¸ë¦¬ê±° êµ¬ë¬¸ì„ ê²€ì¶œí•˜ëŠ” ë¬¸ì œëŠ” voice trigger detection [3], keyword spotting [4], wake-up word detection [5] ë˜ëŠ” hotword detection [6]ìœ¼ë¡œ ìƒí˜¸ êµí™˜ì ìœ¼ë¡œ ì–¸ê¸‰ëœë‹¤. ì´ ë…¼ë¬¸ì˜ ë‚˜ë¨¸ì§€ ë¶€ë¶„ì—ì„œëŠ” ì´ ë¬¸ì œë¥¼ voice trigger detectionë¼ê³  í•©ë‹ˆë‹¤. In the multi-stage approach (Figure 1), the first stage comprises a low-power DNN-HMM system that is always on [3]. ê·¸ë¦¼1ì— ë³´ë©´, ì²« ë²ˆì§¸ ë‹¨ê³„ëŠ” í•­ìƒ [3]ì— ìˆëŠ” ì €ì „ë ¥ DNN-HMM ì‹œìŠ¤í…œì„ í¬í•¨í•œë‹¤. In this design, it is the second stage that determines the final accuracy of the system and the models used in this stage are the subject of this paper. ì´ ì„¤ê³„ì—ì„œ ì‹œìŠ¤í…œì˜ ìµœì¢… ì •í™•ë„ë¥¼ ê²°ì •í•˜ëŠ” ê²ƒì€ ë‘ ë²ˆì§¸ ë‹¨ê³„ì´ë©°, ì´ ë‹¨ê³„ì—ì„œ ì‚¬ìš©ë˜ëŠ” ëª¨ë¸ì´ ì´ ë…¼ë¬¸ì˜ ì£¼ì œì´ë‹¤. Our main contribution is to propose a multi-task learning strategy where a single model is trained to optimise 2 objectives simultaneously. ìš°ë¦¬ì˜ ì£¼ìš” ê¸°ì—¬ëŠ” ë‹¨ì¼ ëª¨ë¸ì´ ë‘ ê°€ì§€ ëª©í‘œë¥¼ ë™ì‹œì— ìµœì í™”í•˜ë„ë¡ í›ˆë ¨ë˜ëŠ” ë‹¤ì¤‘ ì‘ì—… í•™ìŠµ ì „ëµì„ ì œì•ˆí•˜ëŠ” ê²ƒì´ë‹¤. The first objective is to assign the highest score to the correct sequence of phonetic labels given a speech recording. ì²« ë²ˆì§¸ ëª©í‘œëŠ” ì£¼ì–´ì§„ ìŒì„± ë…¹ìŒì˜ ìŒì„± ë ˆì´ë¸”ì´ ì˜¬ë°”ë¥¸ ìˆœì„œë¡œ ë˜ì–´ ìˆë‹¤ë©´ ê°€ì¥ ë†’ì€ ì ìˆ˜ë¥¼ í• ë‹¹í•˜ëŠ” ê²ƒì´ë‹¤. This objective is optimised on a large labelled training dataset which is also used for training the main speech recogniser and is therefore easy to obtain. ì´ ëª©í‘œëŠ” ì£¼ìš” ìŒì„± ì¸ì‹ê¸°ë¥¼ í›ˆë ¨ì‹œí‚¤ëŠ” ë° ì‚¬ìš©ë˜ë¯€ë¡œ ì‰½ê²Œ ì–»ì„ ìˆ˜ ìˆëŠ” ëŒ€ê·œëª¨ ë ˆì´ë¸”ë§ëœ í›ˆë ¨ ë°ì´í„° ì„¸íŠ¸ì— ìµœì í™”ëœë‹¤. The second objective is to discriminate between utterances that contain the trigger phrase and those that are phonetically similar and easily confusable. ë‘ ë²ˆì§¸ ëª©í‘œëŠ” trigger phraseë¥¼ í¬í•¨í•˜ëŠ” ë°œí™”ì™€ ìŒì„±í•™ì ìœ¼ë¡œ ìœ ì‚¬í•˜ê³  ì‰½ê²Œ í˜¼ë™ë˜ëŠ” ë°œí™”ë¥¼ êµ¬ë³„í•˜ëŠ” ê²ƒì´ë‹¤. BaselineThe baseline model architecture comprises an acoustic model (AM) with four bidirectional LSTM layers with 256 units each, followed by an output affine transformation + softmax layer over context independent (CI) phonemes, word and sentence boundaries, resulting in 53 output symbols (Figure 2). Firstly, the fact that the second-pass model is used for re-scoring and not in a continuous streaming setting allows us to use bidirectional LSTM layers. ì²«ì§¸, 2ì°¨ í†µê³¼ ëª¨ë¸ì´ ì—°ì† ìŠ¤íŠ¸ë¦¬ë° ì„¤ì •ì´ ì•„ë‹Œ ì¬ ë“ì ì— ì‚¬ìš©ëœë‹¤ëŠ” ì‚¬ì‹¤ì€ ì–‘ë°©í–¥ LSTM ë ˆì´ì–´ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤. Secondly, using context-independent phones as targets allows us to share training data with the main ASR. ë‘˜ì§¸, context-independent phones ë¥¼ targetìœ¼ë¡œ ì‚¬ìš©í•˜ë©´ ì£¼ìš” ASRê³¼ training ë°ì´í„°ë¥¼ ê³µìœ í• ìˆ˜ ìˆìŠµë‹ˆë‹¤. This is particularly important since in many cases it is not possible to obtain a large number of training utterances with the trigger phrase, for example when developing a trigger detector for a new language. ì´ëŠ” íŠ¹íˆ ì¤‘ìš”í•˜ë‹¤. ë§ì€ ê²½ìš°ì— ê°€ë ¹ ìƒˆë¡œìš´ ì–¸ì–´ë¡œ íŠ¸ë¦¬ê±° ê°ì§€ë¥¼ ê°œë°œí• ë•Œ training ë°œí™”ë¥¼ trigger phraseë¡œ ë‹¤ëŸ‰ì˜ ë°ì´í„°ë¥¼ ì–»ëŠ”ê²ƒì´ ë¶ˆê°€ëŠ¥í•˜ê¸° ë•Œë¬¸ì´ë‹¤. Furthermore, having CI phones as targets results in a flexible model that can be used for detecting any keyword. ë” ë‚˜ì•„ê°€ì„œ ìœ ì—°í•œ ëª¨ë¸ì—ì„œ CI ìŒì†Œë“¤ì„ íƒ€ê²Ÿ ê²°ê³¼ë¡œ ê°–ëŠ” ê²ƒì€ ì–´ëŠ í‚¤ì›Œë“œë¥¼ ê°ì§€í•˜ëŠ”ë° ì‚¬ìš©ë ìˆ˜ ìˆë‹¤. Given an audio segment x from the first pass, we are interested in calculating the probability of the phone sequence in the trigger phrase, $P(TriggerPhrasePhoneSeq|x)$. segment x ìŒì„±ì´ 1ì°¨ì—ì„œ ì£¼ì–´ì¡Œì„ë•Œ, ìš°ë¦¬ëŠ” trigger phraseì˜ ìŒì„± ì‹œí€€ìŠ¤, $P(TriggerPhrasePhoneSeq|x)$,ì˜ í™•ë¥ ì„ ê³„ì‚°í•˜ëŠ” ê²ƒì— ê´€ì‹¬ì´ ìˆë‹¤. Multi-Task LearningThe question we really want to answer is, â€œgiven an audio segment from the first pass, does it contain the trigger phrase or not?â€ ìš°ë¦¬ê°€ ì‹¤ì œë¡œ ë‹µí•˜ê³  ì‹¶ì€ ì§ˆë¬¸ì€ â€œ1ì°¨ì—ì„œ í†µê³¼ëœ ì£¼ì–´ì§„ ìŒì„± segmentê°€ trigger phraseë¥¼ í¬í•¨í•˜ê³  ìˆëŠ”ê°€ ì•„ë‹Œê°€?â€ ì´ë‹¤. We would like the second-pass model to be a binary classifier which determines the presence or absence of the trigger phrase. ìš°ë¦¬ëŠ” 2ì°¨ ëª¨ë¸ì´ trigger phraseë¥¼ í¬í•¨í•˜ëŠ”ì§€ í•˜ì§€ ì•ŠëŠ”ì§€ë¥¼ ê²°ì •í•˜ëŠ” ì´ì§„ ë¶„ë¥˜ê¸°ì˜€ìœ¼ë©´ í•œë‹¤. However the issue with this design is that collecting a large number of training examples that result in false detections by the baseline system is a difficult problem (c.f. Section 4). ê·¸ëŸ¬ë‚˜, ì´ ë””ìì¸ì˜ ì´ìŠˆëŠ” baseline ì‹œìŠ¤í…œì— ì˜í•´ ë‹¤ëŸ‰ì˜ training ì˜ˆì‹œë¥¼ ìˆ˜ì§‘í•˜ëŠ” ê²ƒì´ ì–´ë ¤ìš´ ë¬¸ì œë¼ëŠ” ê²ƒì´ë‹¤. Furthermore, the second pass models have millions of parameters, so they can easily overfit a small training set resulting in poor generalisation. ë” ë‚˜ì•„ê°€ 2ì°¨ ëª¨ë¸ì€ ìˆ˜ë°±ë§Œê°œì˜ íŒŒë¼ë¯¸í„°ê°€ ìˆë‹¤. ê·¸ë˜ì„œ ê·¸ë“¤ì€ ì‘ì€ training setì—ë„ ì‰½ê²Œ ê³¼ì í•© ë˜ë©° ì•ˆì¢‹ì€ ì¼ë°˜í™”ë¥¼ ê²°ê³¼ë¡œ ë‚¸ë‹¤. Therefore, we are faced with the choice between a more general phonetic AM that can be trained on a large, readily available dataset but is optimised for the wrong criterion or a trigger phrase specific detector that is trained on the correct criterion but with a significantly smaller training dataset. ë”°ë¼ì„œ, ìš°ë¦¬ëŠ” í¬ê³  ì†ì‰½ê²Œ ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ìœ¼ë¡œ trainëœ í•˜ì§€ë§Œ ì˜ëª»ëœ ê¸°ì¤€ìœ¼ë¡œ ìµœì í™”ëœ ì¼ë°˜í™”ëœ ìŒì„± AMê³¼ ì˜¬ë°”ë¥¸ ê¸°ì¤€ìœ¼ë¡œ trainëì§€ë§Œ ë§¤ìš° ì ì€ training ë°ì´í„°ë¡œ í•™ìŠµëœ íŠ¹ì • trigger phrase ê°ì§€ê¸°, ë‘˜ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•´ì•¼ í–ˆë‹¤. One solution to this problem is to use multi-task learning (MTL) [19] ìš°ë¦¬ì˜ í•´ê²°ì±…ì€ multi-task learningì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ì—ˆë‹¤. Note that predicting the sequence of phonetic labels in an utterance and deciding whether an utterance contains a specific trigger phrase or not, are related tasks. ë°œí™” ì†ì—ì„œ phonetic labelì˜ ì‹œí€€ìŠ¤ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒ ê·¸ë¦¬ê³  ë°œí™”ê°€ trigger phraseë¥¼ í¬í•¨í•˜ê³  ìˆëŠ”ì§€ ì•„ë‹Œì§€ ê²°ì •í•˜ëŠ” ê²ƒì€ ì„œë¡œ ì—°ê´€ì„±ì´ ìˆëŠ” ì¼ì´ë‹¤. We train a single network with a stack of shared&#x2F;tied biLSTM layers with two seperate output layers (one for each task) and train the network jointly on both sets of training data (Figure 2). ìš°ë¦¬ëŠ” ë‘ê°œì˜ ì¶œë ¥ ë ˆì´ì–´(ê°ê° í•˜ë‚˜ì˜ taskì”©)ë¥¼ ê°–ê³  ìˆê³  ê³µìœ í•˜ëŠ”&#x2F;ë¬¶ì¸ biLSTM ë ˆì´ì–´ë“¤ì˜ ë¬¶ìŒìœ¼ë¡œ ì´ë£¨ì–´ì§„ í•˜ë‚˜ì˜ ë„¤íŠ¸ì›Œí¬ë¥¼ í›ˆë ¨í–ˆê³  ë‘ ì„¸íŠ¸ì˜ training data(Figure 2)ì— í•©ë™ìœ¼ë¡œ ì—®ì¸ ë„¤íŠ¸ì›Œí¬ë¥¼ í›ˆë ¨í–ˆë‹¤. We hypothesise that the joint network is able to learn useful features from both tasks: a) the network can be trained to predict phone labels on a large labelled dataset of general speech which covers a wide distribution of complex acoustic conditions, b) the same network can also learn to discriminate between examples of true triggers and confusable examples on a relatively smaller dataset. ìš°ë¦¬ëŠ” í•©ë™ ë„¤íŠ¸ì›Œí¬ê°€ ë‘ taskì˜ ìœ ìš©í•œ featureë“¤ì„ í•™ìŠµì´ ê°€ëŠ¥í•˜ë‹¤ê³  ê°€ì •í–ˆë‹¤: a) ë„¤íŠ¸ì›Œí¬ëŠ” ë³µì¡í•œ ìŒí–¥ ì¡°ê±´ì˜ ê´‘ë²”ìœ„í•œ ë¶„í¬ë¥¼ ë‹¤ë£¨ëŠ” ì¼ë°˜ì ì¸ ìŒì„±ì˜ í° labelledëœ ë°ì´í„° ì…‹ì„ ê¸°ë°˜ìœ¼ë¡œ phone labelë“¤ì„ ì˜ˆì¸¡í•˜ê²Œë” í›ˆë ¨ë ìˆ˜ ìˆë‹¤. b) ê°™ì€ ë„¤íŠ¸ì›Œí¬ëŠ” ë˜í•œ ìƒëŒ€ì ìœ¼ë¡œ ì ì€ ë°ì´í„° ì–‘ìœ¼ë¡œ ì‹¤ì œì˜ trigger phraseì™€ í—·ê°ˆë¦¬ëŠ” ì˜ˆì‹œë“¤ì„ êµ¬ë¶„í•˜ëŠ” ë²•ì„ ë°°ìš¸ìˆ˜ ìˆë‹¤. An alternative view of this process is that the phonetic transcription task with a significantly larger training set acts as a regulariser for the trigger phrase discrimination task with a much smaller dataset. ì´ processì— ëŒ€í•œ ë‹¤ë¥¸ ì‹œê°ì€ ì—„ì²­ë‚˜ê²Œ ë§ì€ training setë¥¼ ì‚¬ìš©í•˜ëŠ” phonetic transcription taskê°€ regulariserë¡œ í›¨ì”¬ ì ì€ ë°ì´í„°ë¥¼ ê°€ì§€ê³  trigger pharseë¥¼ êµ¬ë¶„í• ë•Œ ì‚¬ìš©ëœë‹¤. The objective function for the phrase specific&#x2F;discriminative output layer is defined as follows: the softmax output layer contains two output units, one for the trigger phrase and the other one for the blank symbol used by the CTC loss function [8, 16] phraseë¥¼ íŠ¹ì •í™”&#x2F;ì°¨ë³„í™” í•˜ëŠ” output layerì˜ ëª©ì í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ëœë‹¤: softmax output layerì€ ë‘ê°œì˜ output unitì„ í¬í•¨í•˜ëŠ”ë° í•˜ë‚˜ëŠ” trigger phraseë¥¼ ìœ„í•¨ì´ê³  ë˜ ë‹¤ë¥¸ í•˜ë‚˜ëŠ” CTC loss functionì—ì„œ ì‚¬ìš©ë˜ëŠ” blanck symbolì„ ìœ„í•¨ì´ë‹¤. EvaluationThere were 100 subjects, approximately balanced between male and female adults. Distances from the device were controlled, ranging from 8 to 15 feet away. ëŒ€ëµì ìœ¼ë¡œ ë°˜ë°˜ì˜ ì„±ë¹„ë¡œ 100ëª…ì˜ ì°¸ê°€ìë“¤ì´ ì°¸ì—¬í–ˆë‹¤. ê¸°ê¸°ì™€ì˜ ê±°ë¦¬ë„ 8-15í”¼íŠ¸ ì •ë„ë¡œ í†µì œë˜ì—ˆë‹¤. There are over 13K utterances overall, evenly divided between four acoustic conditions: (a) quiet room, (b) external noise from a TV or kitchen appliance in the room, (c) music playback from the recording device at medium volume, and (d) music playback from the recording device at loud volume. ì „ë°˜ì ìœ¼ë¡œ ë§Œ 3ì²œê°œì˜ ë°œí™”ë“¤ì´ ìˆëŠ”ë° ë„¤ê°€ì§€ ì»¨ë””ì…˜ìœ¼ë¡œ ë™ì¼í•˜ê²Œ ë‚˜ëˆ„ì–´ì¡Œë‹¤: (a) ì¡°ìš©í•œ ë°©, (b) ë°©ì˜ TV ë˜ëŠ” ì£¼ë°©ê¸°ê¸°ì˜ ì™¸ë¶€ ì†ŒìŒ. (c) ì¤‘ê°„ ë³¼ë¥¨ì˜ ë…¹ìŒ ì¥ì¹˜ì—ì„œ ìŒì•… ì¬ìƒ, (d) í° ë³¼ë¥¨ì˜ ë…¹ìŒ ì¥ì¹˜ì—ì„œ ìŒì•… ì¬ìƒ These examples are used to measure the proportion of false rejections (FRs). ì´ ì˜ˆì‹œë“¤ì€ FR ë¹„ìœ¨ì„ ì¸¡ì •í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë˜ì—ˆë‹¤. In addition to these recordings, this test set also consists of almost 2,000 hours of continuous audio recordings from TV, radio, and podcasts. This allows the measurement of the false-alarm (FA) rate in terms of FAs per hour of active external audio. ê·¸ ë…¹ìŒë“¤ì— ê´€í•´ ë§ë¶™ì´ìë©´, ì´ test set ë˜í•œ TV, ë¼ë””ì˜¤, íŒŸìºìŠ¤íŠ¸ë¡œë¶€í„° 2ì²œ ì‹œê°„ì˜ ì—°ì†ì ì¸ ìŒì„± ë…¹ìŒì„ í¬í•¨í•˜ê³  ìˆë‹¤. ì´ë¥¼ í†µí•´ activeí•œ ì™¸ë¶€ ìŒì„±ì˜ ì‹œê°„ë‹¹ FA, FAë¹„ìœ¨ì„ ì¸¡ì •í•  ìˆ˜ ìˆë‹¤. The second test set is an unstructured data collection at home by our employees, designed to be more representative of realistic, spontaneous usage of the smart speaker. ë‘ë²ˆì§¸ test setì€ ì‚¬ì›ë“¤ì˜ ì§‘ì—ì„œ ë‚˜ëŠ” ì •ì œë˜ì§€ ì•Šì€ ë°ì´í„° ëª¨ìŒì´ë‹¤. ì´ëŠ” ì¡°ê¸ˆë” í˜„ì‹¤ì ì´ê³  ì¦‰í¥ì ì¸ ìŠ¤ë§ˆíŠ¸í° ìŠ¤í”¼ì»¤ì˜ ì‚¬ìš©ì„ ëŒ€í‘œí•œë‹¤. With this data, it is possible to measure nearly unbiased false-reject and false-alarm rates for realistic in-home scenarios similar to customer usage. ì´ ë°ì´í„°ë¡œ í˜„ì‹¤ì ì¸ ì§‘ ë‚´ë¶€ì˜ ì‹œë‚˜ë¦¬ì˜¤ë“¤(ì†Œë¹„ìì˜ ì‚¬ìš©ê³¼ ë¹„ìŠ·í•œ)ì„ ìœ„í•œ í¸í–¥ë˜ì§€ ì•Šì€ FAì™€ FA ë¹„ìœ¨ ì¸¡ì •ì´ ê°€ëŠ¥í•˜ë‹¤. We use detection error trade-off (DET) curves to compare the accuracy between models. Each curve displays the FA rate and the proportion of FRs associated with sweeping the trigger threshold for aparticular model. ìš°ë¦¬ëŠ” DET ê³¡ì„ ì„ ë‘ ëª¨ë¸ì˜ ì •í™•ì„±ì„ ë¹„êµí•˜ê¸° ìœ„í•´ ì‚¬ìš©í•œë‹¤. ê° ê³¡ì„ ì€ íŠ¹ì • ëª¨ë¸ì— ëŒ€í•œ íŠ¸ë¦¬ê±° ì„ê³„ê°’ì„ ìŠ¤ìœ•í•˜ëŠ” ê²ƒê³¼ ì—°ê´€ëœ FA ë¹„ìœ¨ ë° FR ë¹„ìœ¨ì„ í‘œì‹œí•œë‹¤. In practice, we compare the shapes of the DET curves for different models in the vicinity of viable operating points. ì‹¤ì œë¡œ ìš°ë¦¬ëŠ” ì‹¤í–‰ê°€ëŠ¥í•œ ì‘ë™ì§€ì  ê·¼ì²˜ì—ì„œ ì„œë¡œ ë‹¤ë¥¸ ëª¨ë¸ë“¤ì˜ DET ê³¡ì„ ì˜ ëª¨ì–‘ì„ ë¹„êµí–ˆë‹¤. We compare five models: the baseline phonetic CTC model trained on the ASR dataset (blue), the baseline phrase specific model trained on the much smaller training set with randomly initialised weights (red), the same phrase specific model but with weights initialised with the learned weights from the baseline phonetic CTC model (yellow), the phonetic (purple) and phrase specific (green) branches of the proposed MTL model. ìš°ë¦¬ëŠ” ë‹¤ì„¯ê°œì˜ ëª¨ë¸ì„ ë¹„êµí–ˆë‹¤ íŒŒë‘: ASR ë°ì´í„°ë¡œ í›ˆë ¨í•œ baseline phonetic CTC ëª¨ë¸ ë¹¨ê°•: ë¬´ì‘ìœ„ë¡œ ì´ˆê¸°í™”ëœ ê°€ì¤‘ì¹˜ë¥¼ ê°–ëŠ” í›¨ì”¬ ë” ì‘ì€ training ì„¸íŠ¸ ìƒì—ì„œ í›ˆë ¨ëœ baseline phrase íŠ¹ì • ëª¨ë¸ ë…¸ë‘: ë™ì¼í•œ phraseì˜ íŠ¹ì • ëª¨ë¸ì´ì§€ë§Œ ê¸°ë³¸ ìŒì„± CTC ëª¨ë¸ì˜ í•™ìŠµëœ ê°€ì¤‘ì¹˜ë¥¼ ì´ˆê¸°í™”ëœ ê°€ì¤‘ì¹˜ë¥¼ ê°–ëŠ” ëª¨ë¸ ë³´ë¼: ì œì•ˆëœ MTLëª¨ë¸ ì¤‘ phonetic branches ì´ˆë¡: ì œì•ˆëœ MTL ëª¨ë¸ ì¤‘ phrase specific ëª¨ë¸ Note that the phrase specific model with weight initialisation from the baseline phonetic model (yellow) is effectively trained using both datasets. baseline phonetic modelì˜ ì´ˆê¸°í™”ëœ ê°€ì¤‘ì¹˜ë¥¼ ê°€ì§€ëŠ” phrase specific ëª¨ë¸ì€ íš¨ê³¼ì ìœ¼ë¡œ ë‘ datasetì„ ì‚¬ìš©í•˜ë©´ì„œ í›ˆë ¨ë˜ì—ˆë‹¤. In both test sets, the MTL phonetic (purple) and phrase-specific (green) models outperform the baseline phonetic CTC (blue), reducing the FR rate by almost half at many points along the curve. ì–‘ìª½ test setì—ì„œ ë³´ë¼ìƒ‰ ê·¸ë¦¬ê³  ì´ˆë¡ìƒ‰ ëª¨ë¸ì„ì€ íŒŒë€ìƒ‰ë³´ë‹¤ FRë¹„ìœ¨ì„ ë‹¤ë¥¸ ê³¡ì„ ì— ë¹„í•´ ë°˜ì ˆì´ë‚˜ ê°ì†Œì‹œí‚¤ë©´ì„œ ê²°ê³¼ê°€ ì¢‹ì•˜ë‹¤. The non-MTL phrase specific models (red and yellow) yield significantly worse accuracies in comparison, which is unsurprising given that the training dataset is two orders of magnitude smaller compared to the phonetic baseline (blue). MTLì„ ì‚¬ìš©í•˜ì§€ ì•Šì€ specific ëª¨ë¸(ë¹¨ê°•, ë…¸ë‘) ë¶€ë¶„ì€ ì‹¬ê°í•˜ê²Œ ì•ˆì¢‹ì€ ì •í™•ì„±ì„ ìƒëŒ€ì ìœ¼ë¡œ ë³´ì˜€ìœ¼ë©° ì´ê²ƒì€ training datasetì´ phonetic baseline(íŒŒë‘)ì— ë¹„í•´ ë‘ìë¦¿ìˆ˜ ë” ì‘ë‹¤ëŠ” ì ì„ ê°ì•ˆí–ˆì„ë•Œ ë‹¹ì—°í•˜ë‹¤. Comparing the structured data evaluation (left) and the take-home data evaluation (right), it is also striking how the error rates are generally much higher for the latter. êµ¬ì¡°í™”ëœ ë°ì´í„° evaluation(ì™¼ìª½)ê³¼ take-home data evaluation(ì˜¤ë¥¸ìª½)ì„ ë¹„êµí•´ë³´ë©´ ì¼ë°˜ì ìœ¼ë¡œ í›„ìì˜ ê²½ìš° ì˜¤ë¥˜ìœ¨ì´ ë” ë†’ì€ê²ƒë„ ëˆˆì— ëˆë‹¤. ConclusionsWe trained the model to simultaneously produce phonetic transcriptions on a large ASR dataset and to discriminate between difficult examples on a much smaller trigger phrase specific training set. ìš°ë¦¬ëŠ” í° ASR datasetì—ì„œ phonetic transcriptionì„ ì œê³µí•˜ê³  ë™ì‹œì— í›¨ì”¬ ë” ì‘ì€ trigger phrase training setì—ì„œ ì–´ë ¤ìš´ ìƒ˜í”Œë“¤ì„ êµ¬ë³„í•˜ëŠ” ëª¨ë¸ì„ í›ˆë ¨í–ˆë‹¤. We evaluate the proposed model on two challenging test sets and find the proposed method is able to almost halve errors and does not require any extra model parameters. ìš°ë¦¬ëŠ” ì œì•ˆëœ ëª¨ë¸ì˜ ë‘ test setì„ í‰ê°€í–ˆê³  ì œì•ˆëœ ë°©ë²•ì´ ì˜¤ë¥˜ë¥¼ ë°˜ê°í•˜ê¸° ìœ„í•´ ë” ë§ì€ íŒŒë¼ë¯¸í„°ë¥¼ í•„ìš”ë¡œ í•˜ì§€ ì•Šì€ë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆë‹¤. Link: Multi-Task Learning for Voice Trigger Detection","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Multi-Task Learning","slug":"Paper/Multi-Task-Learning","permalink":"https://jmj3047.github.io/categories/Paper/Multi-Task-Learning/"}],"tags":[{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Multi-Task Learning","slug":"Multi-Task-Learning","permalink":"https://jmj3047.github.io/tags/Multi-Task-Learning/"},{"name":"Voice Trigger Detection","slug":"Voice-Trigger-Detection","permalink":"https://jmj3047.github.io/tags/Voice-Trigger-Detection/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Multi-Task Learning","slug":"Paper/Multi-Task-Learning","permalink":"https://jmj3047.github.io/categories/Paper/Multi-Task-Learning/"}]},{"title":"Multi-Task Learning for Speaker Verification and Voice Trigger Detection","slug":"MTL_for_SV&VTD","date":"2022-10-30T15:00:00.000Z","updated":"2023-05-17T11:50:35.488Z","comments":true,"path":"2022/10/31/MTL_for_SV&VTD/","link":"","permalink":"https://jmj3047.github.io/2022/10/31/MTL_for_SV&VTD/","excerpt":"","text":"Journal&#x2F;Conference : ICASSP IEEEYear(published year): 2020Author: Siddharth Sigtia, Erik Marchi, Sachin Kajarekar, Devang Naik, John BridleSubject: Multi-Task Learning Multi-Task Learning for Speaker Verification and Voice Trigger Detection Summary In this study, we investigate training a single network to perform automatic speech transcription and speaker recognition, both tasks jointly. ë³¸ ì—°êµ¬ì—ì„œëŠ” ë‹¨ì¼ ë„¤íŠ¸ì›Œí¬ë¥¼ í›ˆë ¨í•˜ì—¬ automatic speech transcriptionì™€ speaker recognitionì˜ ë‘ ê°€ì§€ ì‘ì—…ì„ ê³µë™ìœ¼ë¡œ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì„ ì—°êµ¬í•©ë‹ˆë‹¤. We train the network in a supervised multi-task learning setup, where the speech transcription branch of the network is trained to minimise a phonetic connectionist temporal classification (CTC) loss while the speaker recognition branch of the network is trained to label the input sequence with the correct label for the speaker. ìš°ë¦¬ëŠ” ë„¤íŠ¸ì›Œí¬ì˜ speech transcription ë¸Œëœì¹˜ê°€ ìŒì„± CTC ì†ì‹¤ì„ ìµœì†Œí™”í•˜ë„ë¡ í›ˆë ¨ë˜ëŠ” ê°ë… ëœ ë©€í‹° íƒœìŠ¤í‚¹ í•™ìŠµ ì„¤ì •ì—ì„œ ë„¤íŠ¸ì›Œí¬ë¥¼ í›ˆë ¨ì‹œí‚¤ëŠ” ë°˜ë©´, ë„¤íŠ¸ì›Œí¬ì˜ í™”ì ì¸ì‹ ë¸Œëœì¹˜ëŠ” ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ í™”ìì— ëŒ€í•œ ì˜¬ë°”ë¥¸ ë¼ë²¨ë¡œ ë¼ë²¨ë§í•˜ë„ë¡ í›ˆë ¨ëœë‹¤. Results demonstrate that the network is able to encode both phonetic and speaker information in its learnt representations while yielding accuracies at least as good as the baseline models for each task, with the same number of parameters as the independent models. ê²°ê³¼ëŠ” ë„¤íŠ¸ì›Œí¬ê°€ í•™ìŠµëœ í‘œí˜„ì—ì„œ ìŒì„± ë° í™”ì ì •ë³´ë¥¼ ëª¨ë‘ ì¸ì½”ë”©í•  ìˆ˜ ìˆìœ¼ë©° ë…ë¦½ ëª¨ë¸ê³¼ ë™ì¼í•œ ìˆ˜ì˜ ë§¤ê°œ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê° ì‘ì—…ì˜ ê¸°ë³¸ ëª¨ë¸ë§Œí¼ ì •í™•ë„ë¥¼ ì‚°ì¶œí•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. IntroductionVoice trigger detection, which is interchangeably known as keyword spotting [4], wake-up word detection [5], or hotword detection [6], is treated as an acoustic modeling problem. keyword spotting [4], wake-up word detection [5] ë˜ëŠ” hotword detection [6]ë¡œ ìƒí˜¸ êµí™˜ ê°€ëŠ¥í•˜ê²Œ ì•Œë ¤ì§„ ìŒì„± íŠ¸ë¦¬ê±° ê²€ì¶œì€ ìŒí–¥ ëª¨ë¸ë§ ë¬¸ì œì— ì†í•œë‹¤ Their primary aim is to recognise the phonetic content (or the trigger phrase directly) in the input audio, with no regard for the identity of the speaker. ê·¸ë“¤ì˜ ì£¼ëœ ëª©í‘œëŠ” í™”ìì˜ ì‹ ì›ì„ ê³ ë ¤í•˜ì§€ ì•Šê³  ì…ë ¥ ì˜¤ë””ì˜¤ì˜ ìŒì„± ë‚´ìš©(ë˜ëŠ” íŠ¸ë¦¬ê±° ë¬¸êµ¬)ì„ ì¸ì‹í•˜ëŠ” ê²ƒì´ë‹¤. On the other hand, speaker verification systems aim to confirm the identity of the speaker by comparing an input utterance with a set of enrolment utterances which are collected when a user sets up their device. í•œí¸, ìŠ¤í”¼ì»¤ ê²€ì¦ ì‹œìŠ¤í…œì€ ì‚¬ìš©ìê°€ ì¥ì¹˜ë¥¼ ì„¤ì •í•  ë•Œ ìˆ˜ì§‘ë˜ëŠ” ë“±ë¡ ë°œí™” ì§‘í•©ê³¼ ì…ë ¥ ë°œí™”ë¥¼ ë¹„êµí•˜ì—¬ í™”ìì˜ ì‹ ì›ì„ í™•ì¸í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤. Speaker verification algorithms can be characterised based on whether the phonetic content in the inputs is limited, which is known as text-dependent speaker verification [9]. í™”ì ê²€ì¦ ì•Œê³ ë¦¬ì¦˜ì€ ì…ë ¥ì˜ ìŒì„± ì½˜í…ì¸ ê°€ ì œí•œë˜ì–´ ìˆëŠ”ì§€ ì—¬ë¶€ì— ë”°ë¼ íŠ¹ì„±í™”í•  ìˆ˜ ìˆëŠ”ë°, ì´ë¥¼ text-dependent speaker verification[9]ì´ë¼ê³  í•œë‹¤. We believe that knowledge of the speaker would help determine the phonetic content in the acoustic signal and vice versa, therefore estimating both properties is similar to solving simultaneous equations. ìš°ë¦¬ëŠ” í™”ìì— ëŒ€í•œ ì§€ì‹ì´ ìŒí–¥ ì‹ í˜¸ì˜ ìŒì„± ë‚´ìš©ì„ ê²°ì •í•˜ëŠ” ë° ë„ì›€ì´ ë˜ê³  ê·¸ ë°˜ëŒ€ì˜ ê²½ìš°ë„ ë§ˆì°¬ê°€ì§€ë¼ê³  ìƒê°í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ë‘ ì†ì„±ì„ ëª¨ë‘ ì¶”ì •í•˜ëŠ” ê²ƒì€ ì—°ë¦½ ë°©ì •ì‹ì„ í‘¸ëŠ” ê²ƒê³¼ ìœ ì‚¬í•©ë‹ˆë‹¤. In this study, the main research question we try to answer is â€œcan a single network efficiently represent both phonetic and speaker specific information?â€. ì´ ì—°êµ¬ì—ì„œ ìš°ë¦¬ê°€ ëŒ€ë‹µí•˜ë ¤ê³ í•˜ëŠ” ì£¼ìš” ì—°êµ¬ ì§ˆë¬¸ì€ â€œí•˜ë‚˜ì˜ ë„¤íŠ¸ì›Œí¬ê°€ ìŒì„± ë° í™”ì íŠ¹ì • ì •ë³´ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆê¹Œ?â€ì…ë‹ˆë‹¤. From a practical standpoint, being able to share computation between the two tasks can save on-device memory, computation time or latency and the amount of power&#x2F;battery consumed. ì‹¤ìš©ì ì¸ ê´€ì ì—ì„œ ë‘ ì‘ì—…ê°„ì— ê³„ì‚°ì„ ê³µìœ í•  ìˆ˜ ìˆìœ¼ë©´ ì¥ì¹˜ ë©”ëª¨ë¦¬, ê³„ì‚° ì‹œê°„ ë˜ëŠ” ëŒ€ê¸° ì‹œê°„ ë° ì†Œë¹„ë˜ëŠ” ì „ë ¥ &#x2F; ë°°í„°ë¦¬ ì–‘ì„ ì ˆì•½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. More generally, we are interested in studying whether a single model can perform multiple speech understanding tasks rather than designing a separate model for each task. ë³´ë‹¤ ì¼ë°˜ì ìœ¼ë¡œ, ìš°ë¦¬ëŠ” ê° ì‘ì—…ì— ëŒ€í•´ ë³„ë„ì˜ ëª¨ë¸ì„ ì„¤ê³„í•˜ê¸°ë³´ë‹¤ëŠ” ë‹¨ì¼ ëª¨ë¸ì´ ì—¬ëŸ¬ ê°œì˜ ìŒì„± ì´í•´ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ”ì§€ ì—°êµ¬í•˜ëŠ” ë° ê´€ì‹¬ì´ ìˆìŠµë‹ˆë‹¤. We train a joint network to perform a phonetic labelling task and a speaker recognition task. ìš°ë¦¬ëŠ” ìŒì„± ë¼ë²¨ë§ ì‘ì—…ê³¼ í™”ì ì¸ì‹ ì‘ì—…ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ ê³µë™ ë„¤íŠ¸ì›Œí¬ë¥¼ í›ˆë ¨ì‹œì¼°ìŠµë‹ˆë‹¤. We evaluate the 2 branches of the model on a voice trigger detection task and a speaker verification task, respectively. ìš°ë¦¬ëŠ” ìŒì„± íŠ¸ë¦¬ê±° ê²€ì¶œ ì‘ì—…ê³¼ í™”ì ê²€ì¦ ì‘ì—…ì—ì„œ ëª¨ë¸ì˜ ë‘ ê°€ì§€ë¥¼ ê°ê° í‰ê°€í•©ë‹ˆë‹¤. It is possible for a single network to encode both speaker and phonetic information and yield similar accuracies as the baseline models without requiring any additional parameters. ë‹¨ì¼ ë„¤íŠ¸ì›Œí¬ê°€ ìŠ¤í”¼ì»¤ ë° ìŒì„± ì •ë³´ë¥¼ ëª¨ë‘ ì¸ì½”ë”©í•˜ê³  ì¶”ê°€ ë§¤ê°œ ë³€ìˆ˜ë¥¼ í•„ìš”ë¡œí•˜ì§€ ì•Šê³  ê¸°ì¤€ ëª¨ë¸ê³¼ ìœ ì‚¬í•œ ì •í™•ë„ë¥¼ ì‚°ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Voice Trigger Detection BaselineWe extract 40-dimensional log-filterbanks from the audio at 100 frame-per-second (FPS). At every step, 7 frames are spliced together to form symmetric windows and finally this sequence of windows is sub-sampled by a factor of 3, yielding a 280-dimensional input vector to the model at a rate of 33 FPS. The features are input to a stack of 4 bidirectional LSTM layers with 256 units in each layer (Figure 1). This is followed by a fully connected layer and an output softmax layer over context-independent phonemes and additional sentence and word boundary symbols, resulting in a total of 53 output symbols and 6 million model parameters. This model is then trained by minimising the CTC loss function [16]. The training data for this model is 5000 hours of anonymised audio data that is manually transcribed, where all of the recordings are sampled from intentional voice assistant invocations and are assumed to be near-field. Fig. 1. The left branch of the model represents the voice trigger detector, the right branch is the speaker verification model. Solid horizontal arrows represent layers with tied weights, dashed arrows represent layers with weights that may or may not be tied. ëª¨ë¸ì˜ ì™¼ìª½ ë¶„ê¸°ëŠ” ìŒì„± íŠ¸ë¦¬ê±° ê²€ì¶œê¸°ë¥¼ ë‚˜íƒ€ë‚´ê³ , ì˜¤ë¥¸ìª½ ë¶„ê¸°ëŠ” í™”ì ê²€ì¦ ëª¨ë¸ì´ë‹¤. ì‹¤ì„  í™”ì‚´í‘œëŠ” ë¬¶ì¸ ê°€ì¤‘ì¹˜ê°€ ìˆëŠ” ë ˆì´ì–´ë¥¼ ë‚˜íƒ€ë‚´ê³  ì ì„  í™”ì‚´í‘œëŠ” ë¬¶ì¼ ìˆ˜ë„ ìˆê³  ë¬¶ì´ì§€ ì•Šì„ ìˆ˜ë„ ìˆëŠ” ê°€ì¤‘ì¹˜ê°€ ìˆëŠ” ë ˆì´ì–´ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. Speaker Verification BaselineWe use a simple location-based attention mechanism [18] to summarise the encoder activations as a fixed-dimensional vector. ìš°ë¦¬ëŠ” ì¸ì½”ë” í™œì„±í™”ë¥¼ ê³ ì • ì°¨ì› ë²¡í„°ë¡œ ìš”ì•½í•˜ê¸° ìœ„í•´ ê°„ë‹¨í•œ ìœ„ì¹˜ ê¸°ë°˜ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ [18]ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. We found the attention mechanism to be particularly effective in the text-independent setting. ìš°ë¦¬ëŠ” attention ë©”ì»¤ë‹ˆì¦˜ì´ í…ìŠ¤íŠ¸ ë…ë¦½ì ì¸ í™˜ê²½ì—ì„œ íŠ¹íˆ íš¨ê³¼ì ì´ë¼ëŠ” ê²ƒì„ ë°œê²¬í–ˆë‹¤. During inference, given a test utterance x, the speaker embedding is obtained by removing the final softmax layer and using the 128-dimensional activations of the previous layer. ì¶”ë¡  ì¤‘ì— í…ŒìŠ¤íŠ¸ ë°œí™” xê°€ ì£¼ì–´ì§€ë©´ ìŠ¤í”¼ì»¤ ì‚½ì…ì€ ìµœì¢… ì†Œí”„íŠ¸ ë§¥ìŠ¤ ë ˆì´ì–´ë¥¼ ì œê±°í•˜ê³  ì´ì „ ë ˆì´ì–´ì˜ 128 ì°¨ì› í™œì„±í™”ë¥¼ ì‚¬ìš©í•˜ì—¬ ì–»ì–´ì§‘ë‹ˆë‹¤. Each training utterance is of the form â€œTrigger phrase, payloadâ€ for e.g.â€œHey Siri (HS), play me something Iâ€™d likeâ€. For every training example, we generate 3 segments: the trigger phrase, the payload and the whole utterance. We found that breaking the utterances up this way results in models that generalise significantly better. Evaluation ConclustionsOur results demonstrate that sharing the first two layers of the model between the speaker and phonetic tasks gives accuracies that are as good as the individual baselines. ìš°ë¦¬ì˜ ê²°ê³¼ëŠ” í™”ìì™€ ìŒì„± ì‘ì—… ì‚¬ì´ì— ëª¨ë¸ì˜ ì²˜ìŒ ë‘ ë ˆì´ì–´ë¥¼ ê³µìœ í•˜ë©´ ê°œë³„ ê¸°ì¤€ì„ ë§Œí¼ ì •í™•ë„ê°€ ë†’ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. This result indicates that it is possible to share some of the lowlevel computation between speech processing tasks without hurting accuracies. ì´ ê²°ê³¼ëŠ” ì •í™•ë„ë¥¼ í•´ì¹˜ì§€ ì•Šìœ¼ë©´ ì„œ ìŒì„± ì²˜ë¦¬ ì‘ì—…ê°„ì— ì €ìˆ˜ì¤€ ê³„ì‚°ì˜ ì¼ë¶€ë¥¼ ê³µìœ í•  ìˆ˜ ìˆìŒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. Link: Multi-Task Learning for Speaker Verification and Voice Trigger Detection","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Multi-Task Learning","slug":"Paper/Multi-Task-Learning","permalink":"https://jmj3047.github.io/categories/Paper/Multi-Task-Learning/"}],"tags":[{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Multi-Task Learning","slug":"Multi-Task-Learning","permalink":"https://jmj3047.github.io/tags/Multi-Task-Learning/"},{"name":"Speaker Verification","slug":"Speaker-Verification","permalink":"https://jmj3047.github.io/tags/Speaker-Verification/"},{"name":"Voice Trigger Detection","slug":"Voice-Trigger-Detection","permalink":"https://jmj3047.github.io/tags/Voice-Trigger-Detection/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Multi-Task Learning","slug":"Paper/Multi-Task-Learning","permalink":"https://jmj3047.github.io/categories/Paper/Multi-Task-Learning/"}]},{"title":"Hexo Blog ìƒì„± ë° ì¬ì—°ê²°","slug":"Hexo_Create","date":"2022-10-05T15:00:00.000Z","updated":"2023-02-28T07:50:08.127Z","comments":true,"path":"2022/10/06/Hexo_Create/","link":"","permalink":"https://jmj3047.github.io/2022/10/06/Hexo_Create/","excerpt":"","text":"Hexo Blog ìƒì„± ê°„ë‹¨í•˜ê²Œ Hexo ë¸”ë¡œê·¸ë¥¼ ë§Œë“¤ì–´ ë³¸ë‹¤. I. í•„ìˆ˜ íŒŒì¼ ì„¤ì¹˜ 1ë‹¨ê³„: nodejs.org ë‹¤ìš´ë¡œë“œ ì„¤ì¹˜ê°€ ì™„ë£Œ ë˜ì—ˆë‹¤ë©´ ê°„ë‹¨í•˜ê²Œ í™•ì¸í•´ë³¸ë‹¤. 1$ node -v 2ë‹¨ê³„: git-scm.com ë‹¤ìš´ë¡œë“œ ì„¤ì¹˜ê°€ ì™„ë£Œ ë˜ì—ˆë‹¤ë©´ ê°„ë‹¨í•˜ê²Œ í™•ì¸í•´ë³¸ë‹¤. 1$ git --version 3ë‹¨ê³„: hexo ì„¤ì¹˜ hexoëŠ” npmì„ í†µí•´ì„œ ì„¤ì¹˜ê°€ ê°€ëŠ¥í•˜ë‹¤. 1$ npm install -g hexo-cli II. ê¹ƒí—ˆë¸Œ ì„¤ì • ë‘ê°œì˜ ê¹ƒí—ˆë¸Œ Repoë¥¼ ìƒì„±í•œë‹¤. í¬ìŠ¤íŠ¸ ë²„ì „ê´€ë¦¬ (name: myblog) í¬ìŠ¤íŠ¸ ë°°í¬ìš© ê´€ë¦¬ (name: rain0430.github.io) rain0430 ëŒ€ì‹ ì— ê°ìì˜ usernameì„ ì…ë ¥í•˜ë©´ ëœë‹¤. ì´ ë•Œ, myblog repoë¥¼ git cloneì„ í†µí•´ ì ë‹¹í•œ ê²½ë¡œë¡œ ë‚´ë ¤ ë°›ëŠ”ë‹¤. $ git clone your_git_repo_address.git III. ë¸”ë¡œê·¸ ë§Œë“¤ê¸° (ì˜µì…˜) ì ë‹¹í•œ ê³³ì— ê²½ë¡œë¥¼ ì§€ì •í•œ ë‹¤ìŒ ë‹¤ìŒê³¼ ê°™ì´ í´ë”ë¥¼ ë§Œë“ ë‹¤. 12$ mkdir makeBlog # ë§Œì•½ Powershell ì´ë¼ë©´ mkdir ëŒ€ì‹ ì— mdë¥¼ ì“´ë‹¤. $ cd makeBlog ì„ì˜ì˜ ë¸”ë¡œê·¸ íŒŒì¼ëª…ì„ ë§Œë“ ë‹¤. 12345$ hexo init myblog$ cd myblog$ npm install$ npm install hexo-server --save$ npm install hexo-deployer-git --save ERROR Deployer not found: git hexo-deployer-gitì„ ì„¤ì¹˜ í•˜ì§€ ì•Šìœ¼ë©´ deployì‹œ ìœ„ì™€ ê°™ì€ ERRORê°€ ë°œìƒí•©ë‹ˆë‹¤. _config.yml íŒŒì¼ ì„¤ì • ì‹¸ì´íŠ¸ ì •ë³´ ìˆ˜ì • 1234title: ì œëª©ì„ ì§€ì–´ì£¼ì„¸ìš”subtitle: ë¶€ì œëª©ì„ ì§€ì–´ì£¼ì„¸ìš”description: descriptionì„ ì§€ì–´ì£¼ì„¸ìš”author: YourName ë¸”ë¡œê·¸ URL ì •ë³´ ì„¤ì • 1234url: https://rain0430.github.ioroot: /permalink: :year/:month/:day/:title/permalink_defaults: ê¹ƒí—ˆë¸Œ ì—°ë™ 12345# Deploymentdeploy: type: git repo: https://github.com/rain0430/rain0430.github.io.git branch: main IV. ê¹ƒí—ˆë¸Œì— ë°°í¬í•˜ê¸° ë°°í¬ ì „, í„°ë¯¸ë„ì—ì„œ localhost:4000 ì ‘ì†ì„ í†µí•´ í™”ë©´ì´ ëœ¨ëŠ”ì§€ í™•ì¸í•´ë³¸ë‹¤. 1234$ hexo generate$ hexo serverINFO Start processingINFO Hexo is running at http://localhost:4000 . Press Ctrl+C to stop. í™”ë©´ í™•ì¸ì´ ëœ ì´í›„ì—ëŠ” ê¹ƒí—ˆë¸Œì— ë°°í¬í•œë‹¤. ì‚¬ì „ì—, gitignore íŒŒì¼ì—ì„œ ì•„ë˜ì™€ ê°™ì´ ì„¤ì •ì„ ì§„í–‰í•œë‹¤. 1234567.DS_StoreThumbs.dbdb.json*.lognode_modules/public/.deploy*/ ìµœì¢…ì ìœ¼ë¡œ ë°°í¬ë¥¼ ì§„í–‰í•œë‹¤. 1$ hexo deploy ë°°í¬ê°€ ì™„ë£Œê°€ ë˜ë©´ ë¸Œë¼ìš°ì €ì—ì„œ USERNAME.github.ioë¡œ ì ‘ì†í•´ ì •ìƒì ìœ¼ë¡œ ë°°í¬ê°€ ë˜ì—ˆëŠ”ì§€ í™•ì¸í•œë‹¤. Hexo Blog ì¬ì—°ê²° ê¸°ì¡´ ë¸”ë¡œê·¸ í´ë” íŒŒì¼ ì••ì¶•í•´ì„œ ë°±ì—…í•œ í›„ ì§„í–‰í•´ì•¼ í•œë‹¤. â†’ theme ê°™ì€ ê²½ìš° ë°›ì•„ì˜¤ëŠ”ê±°ë¶€í„° ë‹¤ì‹œ í•´ì•¼ í•˜ê¸° ë•Œë¬¸ ì¬ì—°ê²°ë³´ë‹¤ëŠ” ì¬ìƒì„±ì´ë¼ê³  ë§í•˜ëŠ”ê²Œ ë” ì í•©í•˜ë‹¤. ë‹¤ë¥¸ ë¡œì»¬ì—ì„œ ë¸”ë¡œê·¸ë¥¼ ì¬ì—°ê²°í•´ì„œ ì‚¬ìš©í•  ê²½ìš° ì•„ë˜ì™€ ê°™ì´ ìˆœì°¨ì ìœ¼ë¡œ ì§„í–‰í•˜ë©´ ëœë‹¤. 1234$ hexo init your_blog_repo # ì—¬ê¸°ëŠ” ê°ì ì†ŒìŠ¤ ë ˆí¬ í™•ì¸$ cd myblog$ git init $ git remote add origin https://github.com/your_name/your_blog_repo.git # ê°ì ì†ŒìŠ¤ ë ˆí¬ ì£¼ì†Œ ì•„ë˜ ëª…ë ¹ì–´ì—ì„œ ì—ëŸ¬ê°€ ë°œìƒì´ ìˆë‹¤. 1$ git pull --set-upstream origin main # ì—ëŸ¬ ë°œìƒ ê·¸ëŸ° ê²½ìš°, ì•„ë˜ ëª…ë ¹ì–´ë¥¼ ì¶”ê°€í•œë‹¤. ê¸°ì¡´ì˜ ë””ë ‰í† ë¦¬ì™€ íŒŒì¼ì„ ëª¨ë‘ ì‚­ì œí•œë‹¤ëŠ” ëœ»ì´ë‹¤. 1$ git clean -d -f ê·¸ë¦¬ê³  ì—ëŸ¬ê°€ ë°œìƒí–ˆë˜ ëª…ë ¹ì–´ë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•œë‹¤. ì´ ë•Œì—ëŠ” ì´ì œ ì •ìƒì ìœ¼ë¡œ ì‹¤í–‰ë˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. 1$ git pull --set-upstream origin main # ì—ëŸ¬ ë°œìƒ ì•ˆí•¨ / ì†ŒìŠ¤ í™•ì¸ ì´ì œ ì •ìƒì ìœ¼ë¡œ í™˜ê²½ ì„¸íŒ…ì€ ëœ ê²ƒì´ë‹¤. ìˆœì°¨ì ìœ¼ë¡œ ì•„ë˜ì™€ ê°™ì´ ì§„í–‰í•˜ë„ë¡ í•œë‹¤. ì´ ë•Œ, theme í´ë”ì— ë³¸ì¸ì˜ í…Œë§ˆ ì†ŒìŠ¤ì½”ë“œê°€ ì˜ ìˆëŠ”ì§€ í™•ì¸ì„ í•˜ë„ë¡ í•œë‹¤. 1234$ npm install $ hexo clean$ hexo generate$ hexo server reference ìƒì„± ì¬ì—°ê²°","categories":[{"name":"Setting","slug":"Setting","permalink":"https://jmj3047.github.io/categories/Setting/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://jmj3047.github.io/tags/Hexo/"}],"keywords":[{"name":"Setting","slug":"Setting","permalink":"https://jmj3047.github.io/categories/Setting/"}]},{"title":"Big Query","slug":"Big_Query","date":"2022-09-28T15:00:00.000Z","updated":"2023-03-19T07:01:57.700Z","comments":true,"path":"2022/09/29/Big_Query/","link":"","permalink":"https://jmj3047.github.io/2022/09/29/Big_Query/","excerpt":"","text":"1. ì¿¼ë¦¬ ì‹¤í–‰ìˆœì„œFROM â†’ WHERE â†’ GROUP BY, Aggregation â†’ HAVING â†’ WINDOW â†’ QUALIFY â†’ DISTINCT â†’ ORDER BY â†’ LIMIT 2. JOIN 3. WINDOW í•¨ìˆ˜ 4. DECLARE ë³€ìˆ˜ë¥¼ ì„ ì–¸ í˜¹ì€ ì´ˆê¸°í™”í•  ë•Œ ì‚¬ìš© DECLARE variable_name[, ...] [variable_type] [ DEFAULT expression]; 1234567-- ì•„ë˜ ì¿¼ë¦¬ë¥¼ ì‹¤í–‰í•˜ë©´ ê°’ì´ ëª¨ë‘ 1ë¡œ ë‚˜ì˜¤ëŠ”ê±¸ í™•ì¸ í•  ìˆ˜ ìˆìŒ-- DEFAULTë¥¼ ì•ˆì£¼ë©´ NULLë¡œ ì§€ì • ë¨DECLARE x, y, z INT64 DEFAULT 1;SELECT x,y,z-- í˜„ì¬ ë‚ ì§œë¡œ dí• ë‹¹DECLARE d DATE DEFAULT CURRENT_DATE(); ìœ„ ì˜ˆì‹œ ë§ê³  ì¿¼ë¦¬ì˜ ê²°ê³¼ë¥¼ ì‚¬ìš©í•´ ë³€ìˆ˜ë¥¼ ì´ˆê¸°í™” í•  ìˆ˜ë„ ìˆìŒ 5. SET DECLAREì™€ ê°™ì´ ì‚¬ìš© ë˜ì–´ì§. DECLAREì—ì„œ ë³€ìˆ˜ íƒ€ì…ì„ ì§€ì •í•˜ê³  SETìœ¼ë¡œ ê°’ í• ë‹¹ì´ ê°€ëŠ¥, DECLAREì—ì„œ ë‘ ê³¼ì • ëª¨ë‘ í•  ìˆ˜ ìˆì§€ë§Œ SETì€ ì¿¼ë¦¬ ë‚´ ì–´ëŠ ìœ„ì¹˜ì—ì„œë‚˜ ì‚¬ìš© ê°€ëŠ¥ í•¨ 6. UDF - User Define Function ì˜êµ¬ UDFëŠ” ì—¬ëŸ¬ ì¿¼ë¦¬ì—ì„œ ì¬ì‚¬ìš© í•  ìˆ˜ ìˆìŒCREATE TEMP FUNCTION â†’ ì„ì‹œ UDF ìƒì„±CREATE FUNCTION â†’ ì˜êµ¬ UDF ìƒì„±CREATE OR REPLACE FUNCTION â†’ ì˜êµ¬ UDF ìƒì„± ë° ìˆ˜ì • 12345678910111213141516CREATE OR REPLACE FUNCTION ps-datateam.cbt_global_ceo.í•¨ìˆ˜ëª…(ë³€ìˆ˜ëª… ë³€ìˆ˜íƒ€ì…)RETURNS INT64 --ë¦¬í„´ íƒ€ì…LANGUAGE js -- ì‘ì„± ì–¸ì–´(ì„ì‹œëŠ” SQLë¡œë„ ê°€ëŠ¥í•œ ë“¯)AS &quot;&quot;&quot;if (mode == &#x27;solo&#x27;) &#123;return 1;&#125; else if (mode == &#x27;duo&#x27;) &#123;return 2;&#125; else if (mode == &#x27;trio&#x27;)&#123;return 3;&#125; else if (mode == &#x27;squad&#x27;)&#123;return 4;&#125; else &#123;return 0;&#125;&quot;&quot;&quot;; -- í•¨ìˆ˜ 7. íŒŒì´ì¬ì—ì„œ ë¹…ì¿¼ë¦¬ ë°ì´í„° ì‚¬ìš©12345678910111213141516171819from google.cloud import bigqueryfrom google.oauth2 import service_accountkey_path = &quot;í‚¤íŒŒì¼ ê²½ë¡œ&quot;credentials = service_account.Credentials.from_service_account_file(key_path, scopes=[&quot;https://www.googleapis.com/auth/cloud-platform&quot;])client = bigquery.Client(credentials=credentials, project=&#x27;í”„ë¡œì íŠ¸ëª…&#x27;)# ì¿¼ë¦¬ ê²°ê³¼ë¥¼ CSVíŒŒì¼ë¡œ -----------------------------------------------------------------query = client.query(&quot;&quot;&quot;QUERY&quot;&quot;&quot;)# ì¿¼ë¦¬ -&gt; DataFramedf_result = query.to_dataframe ()# DataFrame -&gt; csvdf_result.to_csv(&quot;filename.csv&quot;, index=False, encoding=&#x27;euc-kr&#x27;)# í”„ë¡œì íŠ¸&gt;ë°ì´í„°ì„¸íŠ¸ ë‚´ì˜ í…Œì´ë¸” ëª©ë¡ ì¡°íšŒ------------------------------------------------dataset_ref = client.dataset(&quot;kr_dict&quot;) # kr_dictë¼ëŠ” ë°ì´í„° ì„¸íŠ¸ì˜ ì •ë³´ë¥¼ ì¡°íšŒdataset = client.get_dataset(dataset_ref)list(client.list_tables(dataset)) # ë°ì´í„° ì„¸íŠ¸ì˜ í…Œì´ë¸” ëª©ë¡ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ê°€ì ¸ì˜´ Reference1 Reference2","categories":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}],"tags":[{"name":"Big Query","slug":"Big-Query","permalink":"https://jmj3047.github.io/tags/Big-Query/"}],"keywords":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"GCP","slug":"Data-Platform-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/GCP/"}]},{"title":"Pandas Dataframe ì‚¬ìš©ë²• ì •ë¦¬","slug":"Pandas","date":"2022-09-22T15:00:00.000Z","updated":"2022-10-18T15:05:07.992Z","comments":true,"path":"2022/09/23/Pandas/","link":"","permalink":"https://jmj3047.github.io/2022/09/23/Pandas/","excerpt":"","text":"ë°ì´í„° í•©ì¹˜ê¸° https://yganalyst.github.io/data_handling&#x2F;Pd_12&#x2F; https://seong6496.tistory.com/122 https://datascienceschool.net/01 python/04.06 ë°ì´í„°í”„ë ˆì„ í•©ì„±.html https://hyunmin1906.tistory.com/132 onì— ëŒ€í•œ ì„¤ëª…: https://wikidocs.net/153875 íŠ¹ì • ì¹¼ëŸ¼ì˜ ë°ì´í„° ì¢…ë¥˜ë³„ë¡œ í‰ê· &#x2F;í•© êµ¬í•˜ëŠ” ë°©ë²• https://www.dinolabs.ai/72 í–‰&#x2F;ì—´ ìˆœì„œ ë° ì´ë¦„ ë³€ê²½ https://k-glory.tistory.com/21 ì‹œë¦¬ì¦ˆ, ë°ì´í„° í”„ë ˆì„ìœ¼ë¡œ ë§Œë“¤ê¸° https://hyefungi.tistory.com/68 ë¦¬ìŠ¤íŠ¸, ë”•ì…”ë„ˆë¦¬ë¥¼ ë°ì´í„°í”„ë ˆì„, ì‹œë¦¬ì¦ˆë¡œ ë°”ê¾¸ëŠ” ë²• https://jimmy-ai.tistory.com/89 í–‰ì—´ ì „í™˜ https://computer-science-student.tistory.com/158 ë°ì´í„° í”„ë ˆì„ index ë³€ê²½í•˜ê¸° https://cosmosproject.tistory.com/337 ë°ì´í„° í”„ë ˆì„ index ì¡°ì‘ https://datascienceschool.net/01 python/04.05 ë°ì´í„°í”„ë ˆì„ ì¸ë±ìŠ¤ ì¡°ì‘.html ë°ì´í„° í”„ë ˆì„ í•„ìš”í•œ ì—´ ì¶”ì¶œ https://zephyrus1111.tistory.com/43 ë°ì´í„° í”„ë ˆì„ sum í•¨ìˆ˜ https://www.delftstack.com/ko/api/python-pandas/pandas-dataframe-dataframe.sum-function/ í–‰ì¶”ê°€, ì—´ ì‚­ì œ&#x2F;ì¶”ê°€ ë°©ë²• https://jimmy-ai.tistory.com/210 https://ldgeao99.tistory.com/8 https://blog.naver.com/rising_n_falling&#x2F;221631637822 ë°ì´í„°í”„ë ˆì„ì— ë¦¬ìŠ¤íŠ¸ë¥¼ í–‰ ì¶”ê°€ https://emilkwak.github.io/dataframe-list-row-append-ignore-index ëœë¤ ìƒ˜í”Œë§ https://rfriend.tistory.com/602 íŠ¹ì • ì¡°ê±´ì— ë§ëŠ” ë°ì´í„° ì¶”ì¶œ https://computer-science-student.tistory.com/375","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Python","slug":"Python/Python","permalink":"https://jmj3047.github.io/categories/Python/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/tags/Python/"},{"name":"Pandas Dataframe","slug":"Pandas-Dataframe","permalink":"https://jmj3047.github.io/tags/Pandas-Dataframe/"}],"keywords":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Python","slug":"Python/Python","permalink":"https://jmj3047.github.io/categories/Python/Python/"}]},{"title":"ì„ê³„ì¹˜ ì¡°ì ˆ","slug":"Threshold","date":"2022-09-19T15:00:00.000Z","updated":"2023-02-23T01:38:08.678Z","comments":true,"path":"2022/09/20/Threshold/","link":"","permalink":"https://jmj3047.github.io/2022/09/20/Threshold/","excerpt":"","text":"&lt; ë¶„ë¥˜ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì„±ëŠ¥ì§€í‘œ &gt; 1. Confusion Matrix ë¶„ë¥˜ì—ì„œ ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” ì˜¤ë¶„ë¥˜í‘œì´ë‹¤. í–‰ë ¬ì˜ ë°°ì¹˜ëŠ” ê·¸ë¦¬ëŠ” ì‚¬ëŒì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìœ¼ë©°, Scikit learnì— ê¸°ë°˜í•œ confusion matrixëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. FP: ì˜ˆì¸¡ì€ ì°¸ì´ë‚˜ ì‹¤ì œëŠ” ê±°ì§“, ì œ 1ì¢… ì˜¤ë¥˜FN: ì‹¤ì œëŠ” ì°¸ì´ë‚˜ ì˜ˆì¸¡ì€ ê±°ì§“, ì œ 2ì¢… ì˜¤ë¥˜ ì •ë°€ë„ì—ì„œëŠ” FPë¥¼ ì¤„ì´ëŠ” ê²ƒ, ì¬í˜„ìœ¨ì—ì„œëŠ” FNì„ ì¤„ì´ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤.ì¦‰ FP, FNì´ ì»¤ì§€ë©´ ì •ë°€ë„, ì¬í˜„ìœ¨ ê°ê° ì‘ì•„ì§„ë‹¤. ì •í™•ë„(Accuracy): ì „ì²´ ë°ì´í„° ì¤‘ì—ì„œ ì˜ˆì¸¡í•œ ê°’ì´ ì‹¤ì œ ê°’ê³¼ ë¶€í•©í•˜ëŠ” ë¹„ìœ¨ ì •ë°€ë„, ì¬í˜„ìœ¨, íŠ¹ì´ë„ ì •ë°€ë„(precision): ì˜ˆì¸¡ì´ ì°¸ì¸ ê°’ ì¤‘ ì‹¤ì œ ì°¸ì¸ ê°’ì¬í˜„ìœ¨(recall,Sensitivity,TPR): ì‹¤ì œ ì°¸ì¸ ê°’ ì¤‘ ì˜ˆì¸¡ë„ ì°¸ì¸ ê°’íŠ¹ì´ë„(specificity): ì˜ˆì¸¡ì´ ê±°ì§“ì¸ ê°’ ì¤‘ ì‹¤ì œ ê±°ì§“ì¸ ê°’ 2. Precision-Recall Trade-offë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨í˜•ì—ì„œ íŠ¹ì • ì´ë©”ì¼ì´ ìŠ¤íŒ¸ì¼ í™•ë¥ ì´ 0.9ì´ë©´ ìŠ¤íŒ¸ì¼ ê°€ëŠ¥ì„±ì´ ë§¤ìš° ë†’ë‹¤ê³  ì˜ˆì¸¡í•  ìˆ˜ ìˆë‹¤. ì´ì™€ ë°˜ëŒ€ë¡œ 0.03ì´ë¼ë©´ ìŠ¤íŒ¸ì´ ì•„ë‹ ê°€ëŠ¥ì„±ì´ ë†’ë‹¤. ê·¸ë ‡ë‹¤ë©´ 0.6ì¸ ì´ë©”ì¼ì˜ ê²½ìš° ì–´ë–»ê²Œ ë¶„ë¥˜í•´ì•¼ í• ê¹Œ? ì´ë•Œ ë¶„ë¥˜í•  ê¸°ì¤€ì´ í•„ìš”í•œë°, ì´ ê¸°ì¤€ì„ ì„ê³„ê°’(Threshold)ì´ë¼ í•œë‹¤. ì–´ë–¤ ë¶„ë¥˜ê¸°ì˜ ì„ê³„ê°’ì— ë”°ë¥¸ ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì„ ê·¸ë˜í”„ë¡œ ë‚˜íƒ€ë‚´ë©´ ìœ„ê³¼ ê°™ìœ¼ë©°, Precisionê³¼ Recallì´ ë§Œë‚˜ëŠ” ì ì´ ìµœì ì˜ ì„ê³„ê°’ì´ë‹¤. ì„ê³„ê°’ì„ ë†’ì´ë©´(Positiveë¡œ íŒë³„í•˜ëŠ” ê¸°ì¤€ì„ ë¹¡ë¹¡í•˜ê²Œ ì¡ìœ¼ë©´) ì •ë°€ë„ëŠ” ì˜¬ë¼ê°€ê³  ì¬í˜„ìœ¨ì€ ë‚®ì•„ì§„ë‹¤. ë°˜ëŒ€ë¡œ ì„ê³„ê°’ì„ ë‚®ì¶”ë©´(ê¸°ì¤€ì„ ë„ë„í•˜ê²Œ ì¡ìœ¼ë©´) ì •ë°€ë„ëŠ” ë‚®ì•„ì§€ê³  ì¬í˜„ìœ¨ì€ ë†’ì•„ì§„ë‹¤. ì´ë¥¼ ì •ë°€ë„-ì¬í˜„ìœ¨ íŠ¸ë ˆì´ë“œ ì˜¤í”„(Precision-Recall Trade-off)ë¼ í•œë‹¤. ì–´ë–¤ ê²ƒì„ ë¶„ë¥˜í•˜ëŠëƒì— ë”°ë¼ ì •ë°€ë„ê°€ ë” ì¤‘ìš”í•  ë•Œê°€ ìˆê³ , ì¬í˜„ìœ¨ì´ ë” ì¤‘ìš”í•  ë•Œë„ ìˆë‹¤. ìŠ¤íŒ¸ ë©”ì¼ ë¶„ë¥˜ê¸°ëŠ” FPë¥¼ ì¤„ì´ëŠ” ì¦‰ ì •ë°€ë„, ì•” í™˜ì ë¶„ë¥˜ê¸°ëŠ” FNì„ ì¤„ì´ëŠ” ì¦‰ ì¬í˜„ìœ¨ì„ ê°ê° ë” ì¤‘ìš”í•˜ê²Œ ìƒê°í•´ì•¼ í•œë‹¤. 3. G-mean, F1 measure G-mean, F1 measure ì‹¤ì œ ë°ì´í„°ì˜ íŠ¹ì„±ìƒ ì •í™•ë„ë³´ë‹¤ëŠ” ì œ1ì¢… ì˜¤ë¥˜ì™€ ì œ2ì¢… ì˜¤ë¥˜ ì¤‘ ì„±ëŠ¥ì´ ë‚˜ìœ ìª½ì— ë” ê°€ì¤‘ì¹˜ë¥¼ ì£¼ëŠ” G-meanì§€í‘œë‚˜ ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ë§Œ ê³ ë ¤í•˜ëŠ” F1 measureê°€ ë” ê³ ë ¤í•´ë³¼ ìˆ˜ ìˆëŠ” ì§€í‘œì´ë‹¤. ë‘˜ ë‹¤ ë†’ìœ¼ë©´ ë†’ì„ ìˆ˜ë¡ ì¢‹ì€ ì§€í‘œì´ë‹¤. (F1 measureê°€ ë” ìì£¼ ì“°ì¸ë‹¤.) 4. ROC curve, AUC ROC curve : ì–‘ì„±ì— ëŒ€í•œ ì˜¤ë‹µ&#x2F;ì •ë‹µ ë¹„ìœ¨ ì‹œê°í™” ê°€ë¡œì¶•ì„ 1-íŠ¹ì´ë„(FPR), ì„¸ë¡œì¶•ì„ ì¬í˜„ìœ¨**(TPR)**ë¡œ í•˜ì—¬ ì‹œê°í™”í•œ ê·¸ë˜í”„ì´ë‹¤.FPRê³¼ TPRì€ ì˜¤ì°¨ í–‰ë ¬ ë‚´ 4ê°œì˜ ìš”ì†Œë¥¼ ì‚¬ìš©í•œ ìˆ˜ì¹˜ì´ë©° ë‹¤ìŒê³¼ ê°™ë‹¤. FPR: ì‹¤ì œ Negative í´ë˜ìŠ¤ì¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì–¼ë§ˆë‚˜ ì˜ëª» ë¶„ë¥˜í–ˆëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ìˆ˜ì¹˜.TPR: ì‹¤ì œ Positiveì¸ í´ë˜ìŠ¤ì¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì–¼ë§ˆë‚˜ ì œëŒ€ë¡œ ë¶„ë¥˜í–ˆëŠ” ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ìˆ˜ì¹˜. ì„ê³„ê°’(Threshold)ì„ ë³€í™”ì‹œí‚¤ë©´ FPRì´ ë³€í•˜ê²Œ ëœë‹¤. ì„ê³„ê°’ì´ ë†’ìœ¼ë©´(1) ì •ë°€ë„(Precision)ê°€ ë†’ì•„ì§€ë©° FPê°€ ë‚®ì•„ì§€ë¯€ë¡œ(FNì´ ë†’ì•„ì§€ë¯€ë¡œ) FPRì€ 0ì´ë‹¤. ë°˜ëŒ€ë¡œ, ì„ê³„ê°’ì´ ë‚®ìœ¼ë©´(0) FPê°€ ë†’ì•„ì§€ê³ (FN ë‚®ì•„ì§€ë¯€ë¡œ)TNì€ 0ì´ë¯€ë¡œ FPRì€ 1ì´ë‹¤. ì¦‰ ì„ê³„ê°’ì´ ë‚®ì¶”ë©´ ë” ë§ì€ í•­ëª©ì´ ì–‘ì„±ìœ¼ë¡œ ë¶„ë¥˜ë˜ë¯€ë¡œ FPRê³¼ TPRì´ ëª¨ë‘ ì¦ê°€í•œë‹¤. ì´ë ‡ê²Œ ì„ê³„ê°’ì— ë”°ë¼ FPRì„ 0~1ê¹Œì§€ ë³€í™”ì‹œì¼œê°€ë©° ê·¸ì— ë”°ë¼ TPRì´ ì–´ë–»ê²Œ ë³€í™”í•˜ëŠ”ì§€ ê¸°ë¡í•œ ê²ƒì´ ROC curveì´ë‹¤. AUC(area under the curve): ROCê³¡ì„  ì•„ë«ë¶€ë¶„ ë©´ì : 0~1ì‚¬ì´ì˜ ê°’ì„ ê°€ì§€ë©°, AUCê°’ì€ í´ìˆ˜ë¡ ì¢‹ë‹¤. 123456from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, precision_score,recall_score,f1_scorefrom sklearn.metrics import precision_recall_curvefrom sklearn.metrics import roc_auc_score,roc_curve#ì˜¤ë¶„ë¥˜í‘œconfusion_matrix(y_test,y_vld) 24ëŠ” FNìœ¼ë¡œ ì‹¤ì œëŠ” ì°¸ì´ë‚˜, ì˜ˆì¸¡ì€ ê±°ì§“ì´ë‹¤. ì¦‰ ìƒì¡´ìë¥¼ ì‚¬ë§ìë¡œ ì˜ëª» ì˜ˆì¸¡í•œ ê²½ìš°ì´ë‹¤. ë°˜ëŒ€ë¡œ 15ëŠ” FPì´ë©° ì‚¬ë§ìë¥¼ ìƒì¡´ìë¡œ ì˜ëª» ì˜ˆì¸¡í–ˆë‹¤. ì´ ë•Œë¬¸ì— ì •í™•ë„ê°€ ë–¨ì–´ì§„ë‹¤. 12# ê°„ëµí•˜ê²Œ í•œ ë²ˆì— ë³´ê³  ì‹¶ì„ ë•Œ ì‚¬ìš©classification_report(y_test,y_vld) 123456789101112131415161718# ìœ„ì—(cr)ë³´ë‹¤ ì•„ë˜ ì½”ë“œë¥¼ ë” ìì£¼ ì‚¬ìš©í•œë‹¤.accuracy_score(y_test,y_vld)precision_score(y_test,y_vld)recall_score(y_test,y_vld)f1_score(y_test,y_vld)# ì˜ˆì¸¡ í™•ë¥ proba= model.predict_proba(X_test)# precision,recallì€ trade offê´€ê³„, precision_recall_curve( )precision,recall,th = precision_recall_curve(y_test,proba[:,1])plt.xlabel(&#x27;threadhold&#x27;) #ì„ê³„ê°’plt.ylabel(&#x27;score&#x27;)plt.plot(th,precision[:len(th)],&#x27;red&#x27;,linestyle = &#x27;--&#x27;,label = &#x27;precision&#x27;)plt.plot(th,recall[:len(th)],&#x27;blue&#x27;,label = &#x27;recall&#x27;)plt.legend()plt.show() 12345678# roc_curve( )fpr,tpr,th = roc_curve(y_test,proba[:,1])plt.xlabel(&#x27;FPR-False Positive&#x27;)plt.ylabel(&#x27;TPR-True Positive &#x27;)plt.plot(fpr,tpr,&#x27;red&#x27;,label = &#x27;rate&#x27;)plt.plot([0,1],[0,1],&#x27;blue&#x27;,linestyle = &#x27;--&#x27;,label = &#x27;th 0.5&#x27;)plt.legend()plt.show() ì„ê³„ê°’ ì¡°ì •ì„ê³„ê°’(Threshold)ì„ ì¡°ì •í•´ ì •ë°€ë„ ë˜ëŠ” ì¬í˜„ìœ¨ì˜ ìˆ˜ì¹˜ë¥¼ ë†’ì¼ ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì€ trade offê´€ê³„ì´ê¸° ë•Œë¬¸ì— í•œìª½ì´ ì˜¬ë¼ê°€ë©´ ë‹¤ë¥¸ í•œìª½ì´ ë–¨ì–´ì§€ê¸° ì‰½ë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ì„ê³„ê°’ì„ 0.5ë¡œ ì •í•˜ê³  ì´ë³´ë‹¤ í¬ë©´ positive, ì‘ìœ¼ë©´ negativeì´ë‹¤. predict_proba()ë¥¼ í†µí•´ ë ˆì´ë¸”ë³„ ì˜ˆì¸¡í™•ë¥ ì„ ë°˜í™˜í•œë‹¤. Binarizer: threshold ê¸°ì¤€ê°’ë³´ë‹¤ ê°™ê±°ë‚˜ ì‘ìœ¼ë©´ 0ì„, í¬ë©´ 1ì„ ë°˜í™˜í•œë‹¤. &lt;ì„ê³„ê°’ì„ 0.5ë¡œ ì„¤ì •í•œ ê²½ìš°&gt; 123456789101112131415from sklearn.preprocessing import Binarizer# Binarizerì˜ threshold ê°’ì„ 0.5ë¡œ ì„¤ì •custom_threshold = 0.5#ì¦‰ Positive í´ë˜ìŠ¤ì˜ ì»¬ëŸ¼ í•˜ë‚˜ë§Œ ì¶”ì¶œí•˜ì—¬ Binarizerë¥¼ ì ìš©proba_1 = proba[:,1].reshape(-1,1)binarizer=Binarizer(threshold=custom_threshold).fit(proba_1)custom_pred = binarizer.transform(proba_1)recall=recall_score(y_test, custom_pred)acc=accuracy_score(y_test, custom_pred)print(f&quot;ì¬í˜„ìœ¨:&#123;recall&#125;, ì •í™•ë„:&#123;acc:.4f&#125;&quot;) &lt;ì„ê³„ê°’ì„ 0.4ë¡œ ì„¤ì •í•œ ê²½ìš°&gt; 12345678910111213141516from sklearn.preprocessing import Binarizer# Binarizerì˜ threshold ê°’ì„ 0.4ë¡œ ì„¤ì •custom_threshold = 0.4# predict_proba() ê²°ê³¼ ê°’ì˜ ë‘ ë²ˆì§¸ ì»¬ëŸ¼,#ì¦‰ Positive í´ë˜ìŠ¤ì˜ ì»¬ëŸ¼ í•˜ë‚˜ë§Œ ì¶”ì¶œí•˜ì—¬ Binarizerë¥¼ ì ìš©proba_1 = proba[:,1].reshape(-1,1)binarizer=Binarizer(threshold=custom_threshold).fit(proba_1)custom_pred = binarizer.transform(proba_1)recall=recall_score(y_test, custom_pred)acc=accuracy_score(y_test, custom_pred)print(f&quot;ì¬í˜„ìœ¨:&#123;recall:.4f&#125;, ì •í™•ë„:&#123;acc:.4f&#125;&quot;) ì„ê³„ê°’ì„ ë‚®ì¶”ë©´ recallì´ ìƒìŠ¹í•˜ê³ , precisionì´ ë–¨ì–´ì§„ë‹¤. Reference","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/tags/Python/"},{"name":"Threshold Adjustment","slug":"Threshold-Adjustment","permalink":"https://jmj3047.github.io/tags/Threshold-Adjustment/"},{"name":"Confusion Matrix","slug":"Confusion-Matrix","permalink":"https://jmj3047.github.io/tags/Confusion-Matrix/"},{"name":"Precision-Recall","slug":"Precision-Recall","permalink":"https://jmj3047.github.io/tags/Precision-Recall/"},{"name":"F1 measure","slug":"F1-measure","permalink":"https://jmj3047.github.io/tags/F1-measure/"},{"name":"ROC curve","slug":"ROC-curve","permalink":"https://jmj3047.github.io/tags/ROC-curve/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}]},{"title":"Grid Search CV","slug":"Grid_Search","date":"2022-09-15T15:00:00.000Z","updated":"2023-02-23T01:37:20.820Z","comments":true,"path":"2022/09/16/Grid_Search/","link":"","permalink":"https://jmj3047.github.io/2022/09/16/Grid_Search/","excerpt":"","text":"Grid search finds the optimal parameters; each model has its own parameters, and it compares which combination yields the best score. This time, we will see a combination of two parameters and use decision tree. 123456789101112from sklearn.datasets import load_irisfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.model_selection import GridSearchCV, train_test_splitimport pandas as pd#í›ˆë ¨ë°ì´í„° ê²€ì¦ë°ì´í„° ë¶„ë¥˜iris = load_iris()data = iris.datatarget = iris.targetX_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=999) Bring the iris data Data and target are classified into training data and validation data, and the ratio of validation data is set to 20%. Random_state &#x3D; 999 means that you will continue to use random data that you have picked once. It does not matter if it is not 999 or any other number. 123456789101112#ê·¸ë¦¬ë“œì„œì¹˜dtree = DecisionTreeClassifier()grid_parameters = &#123;&quot;max_depth&quot;: [1, 2, 3], &quot;min_samples_split&quot;: [2, 3] &#125;grid_dtree = GridSearchCV(dtree, param_grid=grid_parameters, cv=3, refit=True)grid_dtree.fit(X_train, y_train) Create a decision tree model, save the value of the desired parameter in dictionary form. Cv is the number of sets of training data divided for cross-validation. Refit &#x3D; True means finding the optimal parameter and then learning based on it. It is added another step before fitting 12345678910111213#ê²°ê³¼ë¥¼ ë°ì´í„° í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜scores_df = pd.DataFrame(grid_dtree.cv_results_)print(scores_df)ì¶œë ¥mean_fit_time std_fit_time mean_score_time std_score_time param_max_depth param_min_samples_split params split0_test_score split1_test_score split2_test_score mean_test_score std_test_score rank_test_score0 0.001321 4.587829e-04 0.000997 3.893359e-07 1 2 &#123;&#x27;max_depth&#x27;: 1, &#x27;min_samples_split&#x27;: 2&#125; 0.675 0.675 0.700 0.683333 0.011785 51 0.001676 4.799690e-04 0.001196 2.498432e-04 1 3 &#123;&#x27;max_depth&#x27;: 1, &#x27;min_samples_split&#x27;: 3&#125; 0.675 0.675 0.700 0.683333 0.011785 52 0.002052 8.137361e-05 0.000613 4.378794e-04 2 2 &#123;&#x27;max_depth&#x27;: 2, &#x27;min_samples_split&#x27;: 2&#125; 0.925 0.925 0.975 0.941667 0.023570 33 0.000998 8.104673e-07 0.000332 4.694597e-04 2 3 &#123;&#x27;max_depth&#x27;: 2, &#x27;min_samples_split&#x27;: 3&#125; 0.925 0.925 0.975 0.941667 0.023570 34 0.001271 2.113806e-04 0.000563 4.170978e-04 3 2 &#123;&#x27;max_depth&#x27;: 3, &#x27;min_samples_split&#x27;: 2&#125; 0.975 0.950 0.975 0.966667 0.011785 15 0.001044 4.221438e-05 0.000665 4.699172e-04 3 3 &#123;&#x27;max_depth&#x27;: 3, &#x27;min_samples_split&#x27;: 3&#125; 0.975 0.950 0.975 0.966667 0.011785 1 To see the results fitted above, use cv_results_. Then there are a number of indicators. 123456789101112131415161718#ìµœì ì˜ íŒŒë¼ë¯¸í„° ì¶œë ¥scores_df = scores_df[[&quot;params&quot;, &quot;mean_test_score&quot;, &quot;rank_test_score&quot;, &quot;split0_test_score&quot;, &quot;split1_test_score&quot;, &quot;split2_test_score&quot;]]print(scores_df)# ìµœê³ ì˜ íŒŒë¼ë¯¸í„° ì €ì¥í•´ì¤Œprint(f&quot;ìµœì ì˜ íŒŒë¼ë¯¸í„°: &#123;grid_dtree.best_params_&#125;&quot;)print(f&quot;ìµœê³  ì •í™•ë„: &#123;grid_dtree.best_score_&#125;&quot;)ì¶œë ¥ params mean_test_score rank_test_score split0_test_score split1_test_score split2_test_score0 &#123;&#x27;max_depth&#x27;: 1, &#x27;min_samples_split&#x27;: 2&#125; 0.683333 5 0.675 0.675 0.7001 &#123;&#x27;max_depth&#x27;: 1, &#x27;min_samples_split&#x27;: 3&#125; 0.683333 5 0.675 0.675 0.7002 &#123;&#x27;max_depth&#x27;: 2, &#x27;min_samples_split&#x27;: 2&#125; 0.941667 3 0.925 0.925 0.9753 &#123;&#x27;max_depth&#x27;: 2, &#x27;min_samples_split&#x27;: 3&#125; 0.941667 3 0.925 0.925 0.9754 &#123;&#x27;max_depth&#x27;: 3, &#x27;min_samples_split&#x27;: 2&#125; 0.966667 1 0.975 0.950 0.9755 &#123;&#x27;max_depth&#x27;: 3, &#x27;min_samples_split&#x27;: 3&#125; 0.966667 1 0.975 0.950 0.975ìµœì ì˜ íŒŒë¼ë¯¸í„°: &#123;&#x27;max_depth&#x27;: 3, &#x27;min_samples_split&#x27;: 2&#125;ìµœê³  ì •í™•ë„: 0.9666666666666667 Print out the optimal parameters which is max_depth:3, min_samples_split:2 and the score is 0.96 In this case, the function that gives the best parameter and the highest score is best_params_, best_score_. Korean reference","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/tags/Python/"},{"name":"Grid Search CV","slug":"Grid-Search-CV","permalink":"https://jmj3047.github.io/tags/Grid-Search-CV/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}]},{"title":"Ensemble Model","slug":"Ensemble","date":"2022-09-14T15:00:00.000Z","updated":"2023-02-23T01:36:55.728Z","comments":true,"path":"2022/09/15/Ensemble/","link":"","permalink":"https://jmj3047.github.io/2022/09/15/Ensemble/","excerpt":"","text":"1. Ensemble Modelì–´ë– í•œ í•œ í˜„ìƒì— ëŒ€í•œ ë‹µì„ ì–»ëŠ”ë‹¤ê³  ê°€ì •í•´ë³´ì, ë§ì€ ê²½ìš°ì— í•œ ëª…ì˜ ì „ë¬¸ê°€ë³´ë‹¤ ì—¬ë ¤ ëª…ì˜ ì¼ë°˜ì¸ë“¤ì˜ ì˜ê²¬ì´ ë” ë‚˜ì€ ê²½ìš°ê°€ ìˆë‹¤. ìœ„ ì˜ˆì œì™€ ë¹„ìŠ·í•˜ê²Œ, í•˜ë‚˜ì˜ ì¢‹ì€ ëª¨í˜•(íšŒê·€,ë¶„ë¥˜)ìœ¼ë¡œë¶€í„° ì˜ˆì¸¡ì„ í•˜ëŠ” ê²ƒë³´ë‹¤ ì—¬ëŸ¬ ê°œì˜ ëª¨í˜•ìœ¼ë¡œë¶€í„° ì˜ˆì¸¡ì„ ìˆ˜ì§‘í•˜ëŠ” ê²ƒì´ ë” ì¢‹ì€ ì˜ˆì¸¡ì„ í•  ìˆ˜ ìˆë‹¤. ì´ëŸ¬í•œ ì—¬ëŸ¬ ê°œì˜ ëª¨í˜•ì„ ì•™ìƒë¸”ì´ë¼ê³  ë¶€ë¥´ê³ , ì—¬ëŸ¬ ê°œì˜ ëª¨í˜•ì„ ì¡°í™”ë¡­ê²Œ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì„ ì•™ìƒë¸” í•™ìŠµì´ë¼ê³  í•œë‹¤. ê·¸ë¦¬ê³  6ì£¼ì°¨ì—ì„œ ë°°ìš´ ê²°ì • íŠ¸ë¦¬ ëª¨í˜•ì´ í•˜ë‚˜ê°€ ì•„ë‹ˆë¼, í›ˆë ¨ ì„¸íŠ¸ë¥¼ ë¬´ì‘ìœ„ë¡œ ë‹¤ë¥¸ ì„œë¸Œì…‹ìœ¼ë¡œ ë§Œë“¤ì–´ì„œ ê²°ì • íŠ¸ë¦¬ ë¶„ë¥˜ê¸°ë¥¼ ë§Œë“¤ê³ , ë§ì€ ëª¨í˜•ë“¤ ì¤‘ì—ì„œ ê°€ì¥ ë§ì€ ì„ íƒì„ ë°›ì€ í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ì•™ìƒë¸” ëª¨í˜•ì„ ëœë¤í¬ë ˆìŠ¤íŠ¸ë¼ê³  í•œë‹¤. ì˜¤ëŠ˜ë‚ ì˜ ëœë¤í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ì€ ê°€ì¥ ê°•ë ¥í•œ ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ í•˜ë‚˜ì´ë‹¤. ê·¸ë¦¬ê³  ë¨¸ì‹ ëŸ¬ë‹ ëŒ€íšŒì—ì„œ ìš°ìŠ¹í•˜ëŠ” ì†”ë£¨ì…˜ë“¤ì€ ëŒ€ë¶€ë¶„ ì•™ìƒë¸” ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ì„œ ìµœê³  ì„±ëŠ¥ì„ ë‚¸ë‹¤. ë’¤ì—ì„œ ì•™ìƒë¸” ë°©ë²•ë“¤ ì¤‘ ë°°ê¹…ì„ ì„¤ëª…í•  ê²ƒì´ë‹¤. íˆ¬í‘œ ê¸°ë°˜ ë¶„ë¥˜ê¸° í•˜ë‚˜ì˜ ë°ì´í„°ì…‹ì„ ì—¬ëŸ¬ì¢…ë¥˜ì˜ ë¶„ë¥˜ê¸°ë“¤ë¡œ í›ˆë ¨ì‹œì¼°ë‹¤ê³  ê°€ì •í•´ë³´ì. ìœ„ì—ì„œ ì–¸ê¸‰í•œëŒ€ë¡œ í•˜ë‚˜ì˜ ì¢‹ì€ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒë³´ë‹¤, ì—¬ëŸ¬ ì¢…ë¥˜ì˜ ë¶„ë¥˜ê¸°ë“¤ì´ ê°€ì¥ ë§ì´ ì˜ˆì¸¡í•œ í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ ë” ì¢‹ì€ ë¶„ë¥˜ê¸°ë¥¼ ë§Œë“œëŠ” ë§¤ìš° ê°„ë‹¨í•œ ë°©ë²•ì´ë‹¤. ì´ë ‡ê²Œ ë‹¤ìˆ˜ê²°ì˜ íˆ¬í‘œë¡œ ì •í•´ì§€ëŠ” ë¶„ë¥˜ê¸°ë¥¼ hard voting(ì§‘ì ‘ íˆ¬í‘œ) ë¶„ë¥˜ê¸°ë¼ê³  í•œë‹¤. ë†€ëê²Œë„ ìœ„ ëª¨ë¸ ì¤‘ ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ ëª¨ë¸ì˜ ì •í™•ë„ë³´ë‹¤ ë‹¤ìˆ˜ê²°ì„ í†µí•´ ì˜ˆì¸¡í•œ ì•™ìƒë¸” ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ë†’ì€ ê²½ìš°ê°€ ë§ë‹¤. ì´ë ‡ê²Œ ëœë¤ ì¶”ì¸¡ë³´ë‹¤ ì¡°ê¸ˆ ë” ë†’ì€ ì„±ëŠ¥ì„ ë‚´ëŠ” weak learner(ì•½í•œ í•™ìŠµê¸°) ê°€ ì¶©ë¶„íˆ ë§ê³  ë‹¤ì–‘í•˜ë‹¤ë©´ strong learner(ê°•í•œ í•™ìŠµê¸°)ê°€ ë  ìˆ˜ ìˆë‹¤. ì–´ë–»ê²Œ ì•½í•œ í•™ìŠµê¸°ê°€ ê°•í•œ í•™ìŠµê¸°ê°€ ë˜ì–´ì„œ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆì„ê¹Œ?, ì´ ì§ˆë¬¸ì€ &quot;í° ìˆ˜ì˜ ë²•ì¹™&quot;ìœ¼ë¡œ ì„¤ëª…ë  ìˆ˜ ìˆë‹¤. ë¨¼ì €, 50:50ì˜ ë™ì „ì´ ì•„ë‹ˆë¼, 51:49ì˜ ë¶ˆê· í˜•í•˜ê²Œ ì•ë©´ê³¼ ë’·ë©´ì´ ë‚˜ì˜¤ëŠ” ë™ì „ì´ ìˆë‹¤ê³  ê°€ì •ì„ í•´ë³´ì. ì´ ë™ì „ì„ 1,000ë²ˆì„ ë˜ì§„ë‹¤ë©´ ê±°ì˜ ì•ë©´ 510ë²ˆê³¼ ë’·ë©´ 490ë²ˆì´ ë‚˜ì˜¬ ê²ƒì´ë‹¤. ìˆ˜í•™ì ìœ¼ë¡œ 1,000ë²ˆì„ ë˜ì¡Œì„ ë•Œ ì•ë©´ì´ ë” ë§ê²Œ ë‚˜ì˜¤ëŠ” í™•ë¥ ì€ ê±°ì˜ 75% ì •ë„ ëœë‹¤. ìˆ˜í•™ì ìœ¼ë¡œ 10,000ë²ˆì„ ë˜ì¡Œì„ ë•Œ ì•ë©´ì´ ë” ë§ê²Œ ë‚˜ì˜¤ëŠ” í™•ë¥ ì€ ê±°ì˜ 97% ì •ë„ ëœë‹¤. ìœ„ ìˆ˜í•™ì  ê³„ì‚°ì€ ì´í•­ë¶„í¬ì˜ í™•ë¥  ì§ˆëŸ‰ í•¨ìˆ˜ë¡œ ê³„ì‚° ê°€ëŠ¥í•˜ë‹¤. ex) 1-scipy.stats.binom.cdf(499,1000,0.51) &#x3D; 0.747 ìœ„ì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ìš°ë¦¬ì˜ ì•½í•œ ë¶„ë¥˜ê¸°(51%) 1,000ê°œë¡œ ì•™ìƒë¸” ëª¨í˜•ì„ êµ¬ì¶•í•˜ê³ , ê°€ì¥ ë§ì€ í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡ìœ¼ë¡œ ì‚¼ëŠ”ë‹¤ë©´ 75%, 10,000ê°œë¡œ ëª¨í˜•ì„ ë§Œë“¤ë©´ 97% ì •ë„ì˜ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ..! ìœ„ì˜ ê³¼ì •ì€ ëª¨ë“  ë¶„ë¥˜ê¸°ê°€ ì™„ë²½í•˜ê²Œ ë…ë¦½ì´ê³ , ëª¨ë¸ì˜ ì˜ˆì¸¡ ì˜¤ì°¨ì— ëŒ€í•´ì„œ ìƒê´€ê´€ê³„ê°€ ì—†ì„ë•Œë§Œ ê°€ëŠ¥í•˜ë‹¤. ğŸŒ TIP : ì•™ìƒë¸”ì—ì„œ ì˜ˆì¸¡ê¸°ê°€ ê°€ëŠ¥í•œ ì„œë¡œ ë…ë¦½ì¼ ë•Œ ìµœê³  ì„±ëŠ¥ì„ ë°œíœ˜í•œë‹¤. ê·¸ë˜ì„œ ê°€ëŠ¥í•œ ë‹¤ì–‘í•œ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•´ì„œ í•™ìŠµì„ í•˜ë©´ ë‹¤ì–‘í•œ ì¢…ë¥˜ì˜ ì˜¤ì°¨ë¥¼ ë§Œë“¤ê¸° ë•Œë¬¸ì— ì•™ìƒë¸” ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë†’ì¼ ìˆ˜ ìˆë‹¤. ì—¬ëŸ¬ ì¢…ë¥˜ì˜ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•´ì„œ íˆ¬í‘œê¸°ë°˜ ë¶„ë¥˜ê¸°ë¥¼ ë§Œë“œëŠ” ì˜ˆì œë¥¼ í•´ë³´ì. 123456789101112131415161718192021222324252627282930313233343536373839404142from sklearn.datasets import load_irisfrom sklearn.ensemble import RandomForestClassifier,VotingClassifierfrom sklearn.linear_model import LogisticRegressionfrom sklearn.svm import SVCfrom sklearn.metrics import accuracy_scorefrom sklearn.model_selection import train_test_split# ë°ì´í„°ì…‹ ë¡œë“œiris = load_iris()X = iris.data[:,2:] # ê½ƒìì˜ ê¸¸ì´, ë„ˆë¹„Y = iris.targetx_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.3,random_state=2021,shuffle=True)# ì•½í•œ í•™ìŠµê¸° êµ¬ì¶•log_model = LogisticRegression()rnd_model = RandomForestClassifier()svm_model = SVC()# ì•™ìƒë¸” ëª¨ë¸ êµ¬ì¶•# ë§Œì•½ì— ëª¨ë“  ëª¨ë¸ì´ predict_proba() ë©”ì„œë“œê°€ ìˆìœ¼ë©´, ì˜ˆì¸¡ì˜ í‰ê· ì„ ë‚´ì–´ soft voting(ê°„ì ‘ íˆ¬í‘œ)ë„ í• ìˆ˜ ìˆë‹¤.# ê°„ì ‘ íˆ¬í‘œ ë°©ì‹ì€ í™•ë¥ ì´ ë†’ì€ íˆ¬í‘œì— ë¹„ì¤‘ì„ ë‘ê¸° ë•Œë¬¸ì— ì„±ëŠ¥ì´ ë” ë†’ë‹¤. (voting=&#x27;soft&#x27; ì‚¬ìš©)# svcëŠ” ê¸°ë³¸ì ìœ¼ë¡œ predict_probaë¥¼ ì œê³µí•˜ì§€ ì•Šì•„, probability = True ì§€ì • í•´ì•¼ ì‚¬ìš© ê°€ëŠ¥# ëŒ€ì‹  svcì—ì„œ probability = Trueë¥¼ ì§€ì •í•˜ë©´ êµì°¨ ê²€ì¦ì„ ì‚¬ìš©í•´ì„œ í™•ë¥ ì„ ì¶”ì •í•˜ê¸° ë•Œë¬¸ì— í›ˆë ¨ ì†ë„ ëŠë ¤ì§# ëŒ€ì‹  ì„±ëŠ¥ì„ ì˜¬ë¼ê°voting_model = VotingClassifier( estimators=[(&#x27;lr&#x27;,log_model),(&#x27;rf&#x27;,rnd_model),(&#x27;svc&#x27;,svm_model)], # 3ê°œì˜ ì•½í•œ í•™ìŠµê¸° voting=&#x27;hard&#x27; # ì§ì ‘ íˆ¬í‘œ(hard voting))# ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµvoting_model.fit(x_train,y_train)# ëª¨ë¸ ë¹„êµfor model in (log_model,rnd_model,svm_model,voting_model): model.fit(x_train,y_train) y_pred = model.predict(x_test) print(model.__class__.__name__,&quot; : &quot;,accuracy_score(y_test,y_pred))&gt; LogisticRegression : 1.0 RandomForestClassifier : 0.9555555555555556 SVC : 1.0 VotingClassifier : 1.0 2.ë°°ê¹…ê³¼ í˜ì´ìŠ¤íŒ… ì•™ìƒë¸” ëª¨í˜•ì˜ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ê¸° ìœ„í•´ì„œëŠ” ë‹¤ì–‘í•œ ì¢…ë¥˜ì˜ ì˜¤ì°¨ë¥¼ ë§Œë“¤ì–´ì•¼ í•˜ê³ , ê·¸ëŸ¬ê¸° ìœ„í•´ì„œëŠ” ë‹¤ì–‘í•œ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•´ì•¼ í•œë‹¤ê³  ë°°ì› ë‹¤. ë‹¤ì–‘í•œ ì˜¤ì°¨ë¥¼ ë§Œë“¤ê¸°ìœ„í•œ ë‹¤ë¥¸ í•˜ë‚˜ì˜ ë°©ë²•ìœ¼ë¡œëŠ” í›ˆë ¨ ì„¸íŠ¸ì˜ ì„œë¸Œì…‹ì„ ë¬´ì‘ìœ„ë¡œ êµ¬ì„±í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì´ ìˆë‹¤. ì´ë¥¼ ë°°ê¹…ê³¼ í˜ì´ìŠ¤íŒ…ì´ë¼ê³  ë¶€ë¥¸ë‹¤. ë°°ê¹… : í›ˆë ¨ ì„¸íŠ¸ì˜ ì¤‘ë³µì„ í—ˆìš©í•˜ì—¬ ìƒ˜í”Œë§ì„ í•˜ëŠ” ë°©ì‹ (í†µê³„í•™ì—ì„œëŠ” â€œë¶€íŠ¸ìŠ¤íŠ¸ë˜í•‘â€ì´ë¼ê³ ë„ ë¶€ë¦„) í˜ì´ìŠ¤íŒ… : í›ˆë ¨ ì„¸íŠ¸ì˜ ì¤‘ë³µì„ í—ˆìš©í•˜ì§€ ì•Šê³  ìƒ˜í”Œë§ í•˜ëŠ” ë°©ì‹ ë°°ê¹…ì€ ê° ì˜ˆì¸¡ê¸°ê°€ í•™ìŠµí•˜ëŠ” ì„œë¸Œì…‹ì— ë‹¤ì–‘ì„±ì„ ì¦ê°€ì‹œí‚¤ë¯€ë¡œ í˜ì´ìŠ¤íŒ…ë³´ë‹¤ í¸í–¥ì´ ì¡°ê¸ˆ ë” ë†’ë‹¤. í•˜ì§€ë§Œ ë°°ê¹…ì€ ì˜ˆì¸¡ê¸°ë“¤ì˜ ìƒê´€ê´€ê³„ë¥¼ ì¤„ì´ë¯€ë¡œ ì•™ìƒë¸”ì˜ ë¶„ì‚°ì„ ê°ì†Œ ì‹œí‚¨ë‹¤. ì „ë°˜ì ìœ¼ë¡œ ë°°ê¹…ì´ ë” ë‚˜ì€ ëª¨ë¸ì„ ë§Œë“¤ì§€ë§Œ, ì‹œê°„ê³¼ ì¥ë¹„ê°€ ì¢‹ë‹¤ë©´ êµì°¨ê²€ì¦ìœ¼ë¡œ ë°°ê¹…ê³¼ í˜ì´ìŠ¤íŒ…ì„ ë‘˜ë‹¤ í•´ë³´ë©´ ì¢‹ë‹¤. 1. ì‚¬ì´í‚·ëŸ°ì˜ ë°°ê¹…ê³¼ í˜ì´ìŠ¤íŒ… 12345678910111213141516171819202122from sklearn.ensemble import BaggingClassifierfrom sklearn.tree import DecisionTreeClassifier# ëª¨ë¸ êµ¬ì¶•# BaggingClassifierì—ì„œ ì‚¬ìš©í•œ ë¶„ë¥˜ê¸°ê°€ í´ë˜ìŠ¤ í™•ë¥ ì¶”ì •(predict_proba)ì´ ê°€ëŠ¥í•˜ë©´ ìë™ìœ¼ë¡œ ê°„ì ‘ íˆ¬í‘œ ì‚¬ìš© bag_model = BaggingClassifier( DecisionTreeClassifier(), # ì•½í•œ í•™ìŠµê¸°(ê²°ì • íŠ¸ë¦¬) n_estimators=500, # ì•½í•œ í•™ìŠµê¸°(ê²°ì • íŠ¸ë¦¬) 500ê°œ ìƒì„± max_samples=0.05, # 0.0~1.0 ì‚¬ì´ ì‹¤ìˆ˜ ì„ íƒ(ì‹¤ìˆ˜ x ìƒ˜í”Œ ìˆ˜) í˜¹ì€ ìƒ˜í”Œìˆ˜ ì§€ì • bootstrap=True, # True : ë°°ê¹…, False : í˜ì´ìŠ¤íŒ… n_jobs=-1 # í›ˆë ¨ê³¼ ì˜ˆì¸¡ì— ì‚¬ìš©í•  CPU ì½”ì–´ ìˆ˜ (-1 : ê°€ìš©í•œ ëª¨ë“  ì½”ì–´ ì‚¬ìš©))# ëª¨ë¸ í•™ìŠµbag_model.fit(x_train,y_train)# ëª¨ë¸ ì˜ˆì¸¡y_pred = bag_model.predict(x_test)# ëª¨ë¸ í‰ê°€print(bag_model.__class__.__name__,&quot; : &quot;,accuracy_score(y_test,y_pred))&gt; BaggingClassifier : 0.9777777777777777 ë‹¨ì¼ ê²°ì • íŠ¸ë¦¬ì™€ ë°°ê¹…ì„ ì‚¬ìš©í•œ ê²°ì •íŠ¸ë¦¬ ì•™ìƒë¸”ì˜ ê²°ì •ê²½ê³„ë¥¼ ë¹„êµí•´ë³´ë©´ íŠ¸ë¦¬ ì•™ìƒë¸”ì´ ë”ìš± ì¼ë°˜í™”ê°€ ì˜ ëœê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. 2. oob í‰ê°€ ë°°ê¹…(ì¤‘ë³µ í—ˆìš© ìƒ˜í”Œë§)ì„ í•˜ë‹¤ë³´ë©´ í‰ê· ì ìœ¼ë¡œ í›ˆë ¨ ìƒ˜í”Œì˜ ì•½ 63%ì •ë„ë§Œ ì¶”ì¶œë˜ê³  ë‚˜ë¨¸ì§€ ì•½ 37%ëŠ” ì¶”ì¶œë˜ì§€ ì•Šê³ , ì´ë ‡ê²Œ ì¶”ì¶œë˜ì§€ ì•Šì€ ìƒ˜í”Œë“¤ì„ oob(out-of-bag)ìƒ˜í”Œì´ë¼ê³  ë¶€ë¥¸ë‹¤. ì˜ˆì¸¡ê¸°ê°€ í›ˆë ¨ë˜ëŠ” ë™ì•ˆì—ëŠ” oobìƒ˜í”Œì„ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, ê²€ì¦ ì„¸íŠ¸ë‚˜ êµì°¨ ê²€ì¦ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  oobìƒ˜í”Œë§Œì„ ê°€ì§€ê³  ëª¨ë¸ ìµœì í™”ë¥¼ ìœ„í•œ í‰ê°€ë¥¼ í•  ìˆ˜ ìˆë‹¤. ì•™ìƒë¸”ì˜ í‰ê°€ëŠ” ê° ì˜ˆì¸¡ê¸°ì˜ oobí‰ê°€ì˜ í‰ê· ìœ¼ë¡œ í™•ì¸í•œë‹¤. 1234567891011121314151617181920# ëª¨ë¸ êµ¬ì¶•bag_model = BaggingClassifier( base_estimator = DecisionTreeClassifier(), n_estimators = 500, bootstrap = True, n_jobs = -1, oob_score = True # oobí‰ê°€ë¥¼ ìœ„í•´ Trueë¥¼ ì§€ì •í•œë‹¤.)# ëª¨ë¸ í•™ìŠµbag_model.fit(x_train,y_train)# ëª¨ë¸ í‰ê°€(oob_score_)print(&#x27;oob_score : &#x27;,bag_model.oob_score_)# ëª¨ë¸ í‰ê°€y_pred = bag_model.predict(x_test)print(&#x27;test_score : &#x27;,accuracy_score(y_test,y_pred))&gt;oob_score : 0.9523809523809523 test_score : 0.9333333333333333 3. ëœë¤ í¬ë ˆìŠ¤íŠ¸ ëœë¤í¬ë ˆìŠ¤íŠ¸ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë°°ê¹…ë°©ë²•ì„ ì‚¬ìš©í•œ ê²°ì •íŠ¸ë¦¬ ì•™ìƒë¸” ëª¨ë¸ì´ë‹¤. ê·¸ë˜ì„œ BaggingClassifierì— DecisionTreeClassifierë¥¼ ë„£ëŠ” ëŒ€ì‹ , RandomForestClassifierë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ê·¸ë˜ì„œ RandomForestClassifierëŠ” DecisionTreeClassifierì™€ BaggingClassifier ë§¤ê°œë³€ìˆ˜ ëª¨ë‘ ê°€ì§€ê³  ìˆë‹¤. ëœë¤í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ì€ íŠ¸ë¦¬ì˜ ë…¸ë“œë¥¼ ë¶„í• í•  ë•Œ ì „ì²´ íŠ¹ì„± ì¤‘ì—ì„œ ìµœì„ ì˜ íŠ¹ì„±ì„ ì°¾ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ë¬´ì‘ìœ„ë¡œ ì„ íƒí•œ íŠ¹ì„±ë“¤ ì¤‘ì—ì„œ ìµœì„ ì˜ íŠ¹ì„±ì„ ì°¾ëŠ” ë°©ì‹ì„ ì±„íƒí•˜ì—¬ ë¬´ì‘ìœ„ì„±ì„ ë” ê°€ì§€ê²Œ ëœë‹¤. ì´ë¥¼ í†µí•´ ì•½ê°„ì˜ í¸í–¥ì€ ì†í•´ë³´ì§€ë§Œ, ë”ìš± ë‹¤ì–‘í•œ íŠ¸ë¦¬ë¥¼ ë§Œë“¤ë¯€ë¡œ ë¶„ì‚°ì„ ì „ì²´ì ìœ¼ë¡œ ë‚®ì¶”ì–´ì„œ ë” í›Œë¥­í•œ ëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ ìˆë‹¤. 123456789101112131415161718from sklearn.ensemble import RandomForestClassifier# ëœë¤í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ êµ¬ì¶•rnd_model = RandomForestClassifier( n_estimators = 500, # ì˜ˆì¸¡ê¸° 500ê°œ max_leaf_nodes = 16, # ìì‹ë…¸ë“œì˜ ìµœëŒ€ ê°œìˆ˜ n_jobs = -1 # CPU ì½”ì–´ êµ¬ë™ ê°œìˆ˜)# ëª¨ë¸ í•™ìŠµrnd_model.fit(x_train,y_train)# ëª¨ë¸ ì˜ˆì¸¡y_pred_rf = rnd_model.predict(x_test)# ëª¨ë¸ í‰ê°€print(&quot;rnd_model : &quot;,accuracy_score(y_pred_rf,y_test))&gt; rnd_model : 0.9333333333333333 Reference https:&#x2F;&#x2F;velog.io&#x2F;@changhtun1&#x2F;ensemble#-ëœë¤-í¬ë ˆìŠ¤íŠ¸","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/tags/Python/"},{"name":"Ensemble Model","slug":"Ensemble-Model","permalink":"https://jmj3047.github.io/tags/Ensemble-Model/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}]},{"title":"Decision Tree Classifier","slug":"Decision_Tree","date":"2022-09-12T15:00:00.000Z","updated":"2023-02-23T01:40:32.401Z","comments":true,"path":"2022/09/13/Decision_Tree/","link":"","permalink":"https://jmj3047.github.io/2022/09/13/Decision_Tree/","excerpt":"","text":"1. ì˜ì‚¬ê²°ì •íŠ¸ë¦¬ ë°ì´í„°ì— ìˆëŠ” ê·œì¹™ì„ í•™ìŠµì„ í†µí•´ ìë™ìœ¼ë¡œ ì°¾ì•„ë‚´ íŠ¸ë¦¬ ê¸°ë°˜ì˜ ë¶„ë¥˜ ê·œì¹™ì„ ë§Œë“œëŠ” ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤. ì¡°ê¸ˆ ë” ì‰½ê²Œ í•˜ìë©´ if elseë¥¼ ìë™ìœ¼ë¡œ ì°¾ì•„ë‚´ ì˜ˆì¸¡ì„ ìœ„í•œ ê·œì¹™ì„ ë§Œë“œëŠ” ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ Decision Treeì—ì„œ ë§ì€ ê·œì¹™ì´ ìˆë‹¤ëŠ” ê²ƒì€ ë¶„ë¥˜ ë°©ì‹ì´ ë³µì¡í•´ì§„ë‹¤ëŠ” ê²ƒì´ê³ ì´ëŠ” ê³¼ì í•©(Overfitting)ìœ¼ë¡œ ì´ì–´ì§€ê¸° ì‰½ìŠµë‹ˆë‹¤. íŠ¸ë¦¬ì˜ ê¹Šì´(depth)ê°€ ê¹Šì–´ì§ˆìˆ˜ë¡ ê²°ì •íŠ¸ë¦¬ëŠ” ê³¼ì í•©ë˜ê¸° ì‰¬ì›Œ ì˜ˆì¸¡ ì„±ëŠ¥ì´ ì €í•˜ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê°€ëŠ¥í•œ ì ì€ ê·œì¹™ë…¸ë“œë¡œ ë†’ì€ ì„±ëŠ¥ì„ ê°€ì§€ë ¤ë©´ ë°ì´í„° ë¶„ë¥˜ë¥¼ í•  ë•Œ ìµœëŒ€í•œ ë§ì€ ë°ì´í„° ì„¸íŠ¸ê°€ í•´ë‹¹ ë¶„ë¥˜ì— ì†í•  ìˆ˜ ìˆë„ë¡ ê·œì¹™ ë…¸ë“œì˜ ê·œì¹™ì´ ì •í•´ì ¸ì•¼ í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ìµœëŒ€í•œ ê· ì¼í•œ ë°ì´í„° ì„¸íŠ¸ê°€ êµ¬ì„±ë˜ë„ë¡ ë¶„í• (Split)í•˜ëŠ” ê²ƒì´ í•„ìš”í•©ë‹ˆë‹¤.(ë¶„í• ëœ ë°ì´í„°ê°€ íŠ¹ì • ì†ì„±ì„ ì˜ ë‚˜íƒ€ë‚´ì•¼ í•œë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.) ê·œì¹™ ë…¸ë“œëŠ” ì •ë³´ê· ì¼ë„ê°€ ë†’ì€ ë°ì´í„° ì„¸íŠ¸ë¡œ ìª¼ê°œì§€ë„ë¡ ì¡°ê±´ì„ ì°¾ì•„ ì„œë¸Œ ë°ì´í„° ì„¸íŠ¸ë¥¼ ë§Œë“¤ê³ , ì´ ì„œë¸Œ ë°ì´í„°ì—ì„œ ì´ëŸ° ì‘ì—…ì„ ë°˜ë³µí•˜ë©° ìµœì¢… í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡í•˜ê²Œ ë©ë‹ˆë‹¤. ì‚¬ì´í‚·ëŸ°ì—ì„œëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ì§€ë‹ˆê³„ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ë¶„í• í•©ë‹ˆë‹¤. ì§€ë‹ˆê³„ìˆ˜ : ê²½ì œí•™ì—ì„œ ë¶ˆí‰ë“±ì§€ìˆ˜ë¥¼ ë‚˜íƒ€ë‚¼ ë•Œ ì‚¬ìš©í•˜ëŠ” ê²ƒìœ¼ë¡œ 0ì¼ ë•Œ ì™„ì „ í‰ë“±, 1ì¼ ë•Œ ì™„ì „ ë¶ˆí‰ë“±ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ë¨¸ì‹ ëŸ¬ë‹ì—ì„œëŠ” ë°ì´í„°ê°€ ë‹¤ì–‘í•œ ê°’ì„ ê°€ì§ˆìˆ˜ë¡ í‰ë“±í•˜ë©° íŠ¹ì • ê°’ìœ¼ë¡œ ì ë¦´ ë•Œ ë¶ˆí‰ë“±í•œ ê°’ì´ ë©ë‹ˆë‹¤. ì¦‰, ë‹¤ì–‘ì„±ì´ ë‚®ì„ìˆ˜ë¡ ê· ì¼ë„ê°€ ë†’ë‹¤ëŠ” ì˜ë¯¸ë¡œ 1ë¡œ ê°ˆìˆ˜ë¡ ê· ì¼ë„ê°€ ë†’ì•„ ì§€ë‹ˆê³„ìˆ˜ê°€ ë†’ì€ ì†ì„±ì„ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• ëœë‹¤. 2. íŒŒë¼ë¯¸í„°ì •ë³´ ê· ì¼ë„ ì¸¡ì • ë°©ë²• ì •ë³´ ì´ë“ ë°©ì‹: ì—”íŠ¸ë¡œí”¼ëŠ” ë°ì´í„°ì˜ í˜¼ì¡ë„ë¥¼ ì˜ë¯¸í•œë‹¤. ì—”íŠ¸ë¡œí”¼ê°€ ë†“ë‹¤ëŠ” ê²ƒì€ í˜¼ì¡ë„ê°€ ë†’ë‹¤ëŠ” ê²ƒ ì§€ë‹ˆê³„ìˆ˜: ë¶ˆí‰ë“±ì§€ìˆ˜&#x2F; ì´ ê°’ì´ 0ì´ë©´ í‰ë“±í•˜ë‹¤ëŠ” ê²ƒì„(ë¶„ë¥˜ê°€ ì˜ëë‹¤) ëœ»í•œë‹¤. ì´ ê°’ì´ ë¦¬í”„ë…¸ë“œê°€ ëœë‹¤ min_samples_split ë…¸ë“œë¥¼ ë¶„í• í•˜ê¸° ìœ„í•œ ìµœì†Œí•œì˜ ìƒ˜í”Œ ë°ì´í„°ìˆ˜ â†’ ê³¼ì í•©ì„ ì œì–´í•˜ëŠ”ë° ì‚¬ìš© Default &#x3D; 2 â†’ ì‘ê²Œ ì„¤ì •í•  ìˆ˜ë¡ ë¶„í•  ë…¸ë“œê°€ ë§ì•„ì ¸ ê³¼ì í•© ê°€ëŠ¥ì„± ì¦ê°€ min_samples_leaf ë¦¬í”„ë…¸ë“œê°€ ë˜ê¸° ìœ„í•´ í•„ìš”í•œ ìµœì†Œí•œì˜ ìƒ˜í”Œ ë°ì´í„°ìˆ˜ min_samples_splitê³¼ í•¨ê»˜ ê³¼ì í•© ì œì–´ ìš©ë„ ë¶ˆê· í˜• ë°ì´í„°ì˜ ê²½ìš° íŠ¹ì • í´ë˜ìŠ¤ì˜ ë°ì´í„°ê°€ ê·¹ë„ë¡œ ì‘ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì‘ê²Œ ì„¤ì • í•„ìš” max_features ì£¼ìš” íŒŒë¼ë¯¸í„° ìµœì ì˜ ë¶„í• ì„ ìœ„í•´ ê³ ë ¤í•  ìµœëŒ€ feature ê°œìˆ˜ Default &#x3D; None â†’ ë°ì´í„° ì„¸íŠ¸ì˜ ëª¨ë“  í”¼ì²˜ë¥¼ ì‚¬ìš© intí˜•ìœ¼ë¡œ ì§€ì • â†’í”¼ì²˜ ê°¯ìˆ˜ &#x2F; floatí˜•ìœ¼ë¡œ ì§€ì • â†’ë¹„ì¤‘ sqrt ë˜ëŠ” auto : ì „ì²´ í”¼ì²˜ ì¤‘ âˆš(í”¼ì²˜ê°œìˆ˜) ë§Œí¼ ì„ ì • log : ì „ì²´ í”¼ì²˜ ì¤‘ log2(ì „ì²´ í”¼ì²˜ ê°œìˆ˜) ë§Œí¼ ì„ ì • max_depth íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´ default &#x3D; None â†’ ì™„ë²½í•˜ê²Œ í´ë˜ìŠ¤ ê°’ì´ ê²°ì •ë  ë•Œ ê¹Œì§€ ë¶„í•  ë˜ëŠ” ë°ì´í„° ê°œìˆ˜ê°€ min_samples_splitë³´ë‹¤ ì‘ì•„ì§ˆ ë•Œê¹Œì§€ ë¶„í•  ê¹Šì´ê°€ ê¹Šì–´ì§€ë©´ ê³¼ì í•©ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì ì ˆíˆ ì œì–´ í•„ìš” max_leaf_nodes ë¦¬í”„ë…¸ë“œì˜ ìµœëŒ€ ê°œìˆ˜ random_state ì£¼ë¡œ ê²€ìƒ‰í•´ì„œ ë‚˜ì˜¤ëŠ” ì†ŒìŠ¤ì½”ë“œì— random_state &#x3D; 42 ë¼ê³  ë˜ì–´ìˆì–´ì„œ ì—„ì²­ë‚œ ì˜ë¯¸ë¥¼ ê°€ì§„ ê²ƒ ê°™ì§€ë§Œ ì‚¬ì‹¤ 42ë¼ëŠ” random_stateì— í• ë‹¹ëœ ìˆ«ì ìì²´ì— í° ì˜ë¯¸ëŠ” ì—†ìŠµë‹ˆë‹¤. ì¤‘ìš”í•œê±´ ì´ random_stateë¥¼ Noneìœ¼ë¡œ ë‘ëƒ ì •ìˆ˜ë¥¼ ë„£ëŠëƒë”ë¼êµ¬ìš”. random_statusê°€ Noneì¸ ê²½ìš° í•œë²ˆ Decision treeë¥¼ ìƒì„±í•  ë•Œ 1,3,47,5â€¦ë²ˆ ë°ì´í„°ë¥¼ ì´ìš©í–ˆë‹¤ê³  í•´ì„œ ë‹¤ì‹œ ì´ Decision treeë¥¼ ìƒì„±í•  ë•Œ 1,3,47â€¦ë²ˆì§¸ ë°ì´í„°ë¥¼ ì´ìš©í•˜ì§€ëŠ” ì•ŠìŠµë‹ˆë‹¤. ë˜ ë‹¤ë¥¸ ì–´ë–¤ ë‚œìˆ˜ë²ˆì§¸ì˜ ë°ì´í„°ë¥¼ ì´ìš©í•˜ê²Œ ë˜ëŠ”ê±°ì£ . ë§Œì•½ random_stateê°€ Noneì´ë©´ ì •ë§ ê·œì¹™ì—†ëŠ” ì–´ë–¤ ë°ì´í„°ë¥¼ ë½‘ì•„ì„œ Decision treeë¥¼ ìƒì„±í•˜ê²Œ ë˜ì§€ë§Œ random_stateì— ì–´ë–¤ ê°’ì´ ìˆë‹¤ë©´ ë‚œìˆ˜ ìƒì„±ì— ì–´ë– í•œ ê·œì¹™(ì´ê±´ ë§Œë“ ì‚¬ëŒ ë§ê³ ëŠ” ì•Œ ìˆ˜ ì—†ìŒ)ì„ ë„£ì–´ì„œ ë™ì¼í•œ ê²°ê³¼ê°€ ë‚˜ì˜¤ê²Œ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ 1ì´ë¼ëŠ” ê°’ì„ ë„£ì–´ì„œ 1,3,47,5â€¦ë²ˆì§¸ ë°ì´í„°ë¥¼ ì´ìš©í–ˆë‹¤ë©´ ë˜ë‹¤ì‹œ 1ì„ ë„£ìœ¼ë©´ 1,3,47,5â€¦ë²ˆì§¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ê²Œ ë˜ëŠ”ê±°ì£ . ì´ê±¸ ì¼ë°˜ì ìœ¼ë¡œëŠ” random seedë¼ê³  í•©ë‹ˆë‹¤. random seed ì¼ë°˜ì ìœ¼ë¡œ ì‹œìŠ¤í…œì´ ë‚œìˆ˜ë¥¼ ë§Œë“¤ ë•Œ ë§ì´ ë‚œìˆ˜ì§€ ì¼ì •í•œ íŒ¨í„´ì˜ ìˆ˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œ ì¸ìë¡œ random seedë¼ëŠ”ê±¸ ë„£ì–´ì„œ ì–´ë– í•œ ê·œì¹™ì„ ë§Œë“¤ì–´ì£¼ëŠ”ê±´ë° Cì—ì„œëŠ” ê¸°ë³¸ì ìœ¼ë¡œ random seedê°€ ì •í•´ì ¸ìˆì–´ì„œ ì¼ë°˜ì ìœ¼ë¡œ ì‹œê°„ì„ random seedë¡œ ì“°ëŠ” ë°˜ë©´ íŒŒì´ì¬ì—ì„œëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ì´ random seedê°€ ì—†ëŠ” ê²½ìš° ì™„ì „ ëœë¤ì´ ë˜ë”ë¼êµ¬ìš”. ê·¸ë˜ì„œ ì´ Random Seedë¼ëŠ”ê±´ ë¶ˆê·œì¹™ì†ì— ê·œì¹™ì„ ë§Œë“¤ì–´ì£¼ëŠ” ë§¤ê°œë³€ìˆ˜ë¼ê³  ìƒê°í•´ì£¼ì‹œë©´ ë©ë‹ˆë‹¤. random_stateë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ  ë§Œì•½ Random_stateë¥¼ Noneìœ¼ë¡œ ë‘ëŠ” ê²½ìš° Decisiontreeclassifier í•¨ìˆ˜ë¥¼ ì´ìš©í•´ Decision treeë¥¼ ìƒì„±í•˜ë©´ ê·¸ë•Œê·¸ë•Œ ë‹¤ë¥¸ ë°ì´í„°ë¥¼ ì´ìš©í•˜ê¸° ë•Œë¬¸ì— ê²°ê³¼ê°€ ë°”ë€ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ Random_stateì— ë³€ìˆ˜ë¥¼ ì…ë ¥í•  ê²½ìš° íŠ¹ì •í•œ ê·œì¹™ì„ ê°–ê²Œ ë˜ê³  Aë¼ëŠ” ì‚¬ëŒì´ random_state&#x3D;1ë¡œ Decision treeë¥¼ ìƒì„±í•  ë•Œì™€ Bë¼ëŠ” ì‚¬ëŒì´ random_state&#x3D;1ë¡œ Decision treeë¥¼ ìƒì„±í•  ë•Œì˜ ê²°ê³¼ê°€ ë™ì¼í•´ì§€ë„ë¡ í•˜ëŠ”ê±°ì£ . ê·¸ëŸ¬ë‹ˆê¹Œ random_stateê°€ 0ì¸ì§€ 1ì¸ì§€ 42ì¸ì§€ë³´ë‹¤ëŠ” ê°™ì€ ë³€ìˆ˜ë¥¼ ì´ìš©í•´ ê°™ì€ ê²°ê³¼ë¥¼ ë„ì¶œí•´ë‚´ëŠ” ë°ì— í° ì˜ë¯¸ê°€ ìˆìŠµë‹ˆë‹¤. 3. ì¥,ë‹¨ì ì¥ì  ì‰½ê³  ì§ê´€ì ì…ë‹ˆë‹¤. ê° í”¼ì²˜ì˜ ìŠ¤ì¼€ì¼ë§ê³¼ ì •ê·œí™” ê°™ì€ ì „ì²˜ë¦¬ ì‘ì—…ì˜ ì˜í–¥ë„ê°€ í¬ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¨ì  ê·œì¹™ì„ ì¶”ê°€í•˜ë©° ì„œë¸ŒíŠ¸ë¦¬ë¥¼ ë§Œë“¤ì–´ ë‚˜ê°ˆìˆ˜ë¡ ëª¨ë¸ì´ ë³µì¡í•´ì§€ê³ , ê³¼ì í•©ì— ë¹ ì§€ê¸° ì‰½ìŠµë‹ˆë‹¤ â†’ íŠ¸ë¦¬ì˜ í¬ê¸°ë¥¼ ì‚¬ì „ì— ì œí•œí•˜ëŠ” íŠœë‹ì´ í•„ìš”í•©ë‹ˆë‹¤. 4. ì¿¼ë¦¬ êµ¬í˜„123456789101112131415161718192021222324252627282930313233343536373839404142from sklearn.tree import DecisionTreeClassifierfrom sklearn.linear_model import LogisticRegressionfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.preprocession import LabelEncoder, OneHotEncoder, StandardScaler, MinMaxScaler, Binarizerfrom sklearn.model_selection import train_test_split, GridSearchCVfrom skelearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_scorefrom sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curveimport pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as snsimport warningswarnings.filterwarnings(&#x27;ignore&#x27;)from sklearn.datasets import load_iris#ë¶„ë¥˜ê¸° ìƒì„±dtc_iris = DecisionTreeClassifier(random_state=100)#ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬#í•™ìŠµ, í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë¶„ë¦¬iris_data = load_iris()X_train, X_test, y_train, y_test, = train_test_split(iris_data.data, iris_data.target, test_size=0.2, random_state=100)#í•™ìŠµdtc_iris.fit(X_train, y_train)from sklearn.tree import export_graphviz#export_graphiz()ì˜ í˜¸ì¶œ ê²°ê³¼ë¡œ out_fileë¡œ ì§€ì •ëœ tree.dotíŒŒì¼ì„ ìƒì„±í•¨export_graphiz(dtc_iris, out_file=&quot;tree.dot&quot;, class_names = iris_data.target_names, feature_names = iris_data.feature_names, impurity=True, filled=True)print(&#x27;===============max_depthì˜ ì œì•½ì´ ì—†ëŠ” ê²½ìš°ì˜ Decision Tree ì‹œê°í™”==================&#x27;)import graphviz# ìœ„ì—ì„œ ìƒì„±ëœ tree.dot íŒŒì¼ì„ Graphiviz ê°€ ì½ì–´ì„œ ì‹œê°í™”with open(&quot;tree.dot&quot;) as f: dot_graph = f.read()graphviz.Source(dot_graph) 12345678910111213import seaborn as snsimport numpy as np%matplotlib inline# feature importance ì¶”ì¶œprint(&quot;Feature Importances:\\n&#123;0&#125;\\n&quot;.format(np.round(dt_clf.feature_importances_, 3)))# feature ë³„ feature importance ë§¤í•‘for name, value in zip(iris_data.feature_names, dt_clf.feature_importances_): print(&#x27;&#123;0&#125;: &#123;1:.3f&#125;&#x27;.format(name, value)) # feature importance ì‹œê°í™”sns.barplot(x=dt_clf.feature_importances_, y=iris_data.feature_names) Reference https://injo.tistory.com/15 https://continuous-development.tistory.com/173 https://jerry-style.tistory.com/entry/Decisiontreeclassifier-í•¨ìˆ˜ì˜-íŒŒë¼ë¯¸í„°-randomstateë€","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/tags/Python/"},{"name":"Decision Tree Classifier","slug":"Decision-Tree-Classifier","permalink":"https://jmj3047.github.io/tags/Decision-Tree-Classifier/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}]},{"title":"Density-Based Spatial Clustering of Applications with Noise","slug":"DBSCAN","date":"2022-09-11T15:00:00.000Z","updated":"2023-02-23T01:36:29.696Z","comments":true,"path":"2022/09/12/DBSCAN/","link":"","permalink":"https://jmj3047.github.io/2022/09/12/DBSCAN/","excerpt":"","text":"1. What is Density-Based Spatial Clustering of Applications with Noise DBSCAN (Density-based spatial clustering of applications with noise) uses density-based clustering among clustering algorithms. In the case of K-Means or Hierarchical clustering, clustering is performed using the distance between clusters. Density-based clustering is a method of clustering high-density parts because the dots are densely clustered. To put it simply, if there are â€˜nâ€™(or more) points within a radius â€˜xâ€™ of a certain point, it is recognized as a single cluster. For example, suppose that there is a point â€˜pâ€™, and if there are â€˜mâ€™(minPts) points within a distance â€˜eâ€™(epsilon) from the point â€˜pâ€™, it is recognized as a cluster. In this condition, that is, a point â€˜pâ€™ having â€˜mâ€™ points within a distance â€˜eâ€™ is called a core point. To use the DBSCAN algorithm, the distance epsilon value from the reference point and the number of points(minPts) within this radius, should be passed as a factor. In the figure below, if minPts &#x3D; 4, if there are more than four points in the radius of epsilon around the blue point â€˜Pâ€™, it can be judged as one cluster, and in the figure below â€˜Pâ€™ becomes a core point because there are five points. In the figure below, since the gray point P2 has three points within the epsilon radius based on the point P2, it does not reach minPts&#x3D;4, so it does not become the core point of the cluster, but it is called a border point because it belongs to the cluster with the previous point P as the core point. In the figure below, P3 becomes a core point because it has four points within the epsilon radius. However, another core point P is included in the radius around P3, and in this case, core point P and P3 are considered to be connected and are grouped into one cluster. Finally, P4 in the figure below is not included in the range that satisfies minPts&#x3D;4 no matter what point is centered. In other words, it becomes an outlier that does not belong to any cluster, which is called noise point. Putting it all together, we get the following picture: In summary, if there are more points than the number of minPts in the epsilon radius around a point, it becomes a cluster around that point, and that point is called a core point. When a core point becomes part of a cluster of different core points, it became one big cluster. A point belonging to a cluster but not a core point by itself is called a border point, and is mainly a point forming the outer edge of the cluster. A point that does not belong to any cluster becomes a noise point. 2. Key Points The advantage of the DBSCAN algorithm is that it does not have to set the number of clusters like K Means, and because clusters are connected to each other according to the density of clusters, clusters with geometric shapes can be found well, and outlier detection is possible through noise point. It is oftenly used for learning but not in the field â†’ If it is small data, you can use it, but because there is a lot of data in the field, use efficiency is low. Example of clustering geometry Sample Code Korean reference","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"Clustering","slug":"Clustering","permalink":"https://jmj3047.github.io/tags/Clustering/"},{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"DBSCAN","slug":"DBSCAN","permalink":"https://jmj3047.github.io/tags/DBSCAN/"},{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/tags/Python/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}]},{"title":"Comparison K means & GMM","slug":"Kmeans_VS_GMM","date":"2022-09-10T15:00:00.000Z","updated":"2023-02-23T01:36:04.330Z","comments":true,"path":"2022/09/11/Kmeans_VS_GMM/","link":"","permalink":"https://jmj3047.github.io/2022/09/11/Kmeans_VS_GMM/","excerpt":"","text":"1. K-Means It can be used for easy, concise, and large data. If the number of features becomes too large with distance-based algorithms, the performance of clustering is degraded. Therefore, in some cases, it is applied with reducing dimensions using PCA In addition, since it is an iterative algorithm, it is a model that is quite sensitive to outliers and learning execution time could slow down when the number of iterations increases rapidly. The K-means API in Scikit-learn provides the following key parameters. n_clusters : As the number of clusters defined in advance, it defines how many clusters K-means will cluster into, that is, the â€œKâ€ value. init : It is a method of initially setting the coordinates of the cluster center point and is usually set in the â€˜k-means++â€™ method. It is used because setting it in a random way can lead to out-of-the-way results. max_iter : It is to set the number of iterations. If clustering is completed before the set number of times is reached, it ends in the middle even if the number of iterations is not filled. The following are attribute values returned by the K-means API provided by Skikit-learn. In other words, these are values returned by K-means after performing all clustering. labels_ : Returns the cluster label to which each individual data point belongs. (However, keep in mind that this label value is not mapped to the same value as the label value of the actual original data!) cluster_centers_ : After clustering into K clusters, the coordinates of the center points of each K cluster are returned. Using this, the center point coordinates can be visualized. 1234567891011121314151617181920212223242526from sklearn.datasets import make_blobs# Xì—ëŠ” ë°ì´í„°, yì—ëŠ” í´ëŸ¬ìŠ¤í„°ë§ ëœ labelê°’ ë°˜í™˜X, y = make_blobs(n_samples=200, n_features=2, centers=3, cluster_std=0.8, random_state=1)print(X.shape, y.shape)# y Targetê°’ ë¶„í¬ í™•ì¸# return_counts=True ì¶”ê°€í•˜ë©´ arrayìš”ì†Œë§ˆë‹¤ value_counts()í•´ì¤Œunique, counts = np.unique(y, return_counts=True)# í´ëŸ¬ìŠ¤í„°ë§ìš©ìœ¼ë¡œ ìƒì„±í•œ ë°ì´í„° ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë§Œë“¤ê¸°cluster_df = pd.DataFrame(data=X, columns=[&#x27;ftr1&#x27;,&#x27;ftr2&#x27;])cluster_df[&#x27;target&#x27;] = ycluster_df.head()# ìƒì„± ë°ì´í„°í¬ì¸íŠ¸ë“¤ ì‹œê°í™”í•´ë³´ê¸°target_lst = np.unique(y)markers = [&#x27;o&#x27;,&#x27;s&#x27;,&#x27;^&#x27;,&#x27;P&#x27;,&#x27;D&#x27;]for target in target_lst: target_cluster = cluster_df[cluster_df[&#x27;target&#x27;]==target] plt.scatter(x=target_cluster[&#x27;ftr1&#x27;], y=target_cluster[&#x27;ftr2&#x27;], edgecolor=&#x27;k&#x27;, marker=markers[target])plt.show() Letâ€™s look at the distribution of individual data made with the make_blobs function that creates a clustering dataset. Now, letâ€™s apply K-means to the above data to visualize the coordinates of the center points of each cluster. 123456789101112131415161718192021222324252627282930313233# K-means í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰í•˜ê³  ê°œë³„ í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì„ ì‹œê°í™”# 1.K-means í• ë‹¹kmeans = KMeans(n_clusters=3, init=&#x27;k-means++&#x27;, max_iter=200, random_state=12) # XëŠ” cluster_dfì˜ feature arrayì„.cluster_labels = kmeans.fit_predict(X)cluster_df[&#x27;kmeans_label&#x27;] = cluster_labels# 2.K-meansì†ì„±ì˜ cluster_centers_ëŠ” ê°œë³„ í´ëŸ¬ìŠ¤í„°ì˜ ì¤‘ì‹¬ ìœ„ì¹˜ ì¢Œí‘œë¥¼ ë°˜í™˜centers = kmeans.cluster_centers_unique_labels = np.unique(cluster_labels)markers = [&#x27;o&#x27;,&#x27;s&#x27;,&#x27;^&#x27;,&#x27;P&#x27;]# 3. labelë³„ë¡œ ë£¨í”„ëŒì•„ì„œ ê°œë³„ í´ëŸ¬ìŠ¤í„°ë§ì˜ ì¤‘ì‹¬ ì‹œê°í™”for label in unique_labels: label_cluster = cluster_df[cluster_df[&#x27;kmeans_label&#x27;] == label] # ê° í´ëŸ¬ìŠ¤í„°ì˜ ì¤‘ì‹¬ ì¢Œí‘œ í• ë‹¹ center_x_y = centers[label] # ê° í´ëŸ¬ìŠ¤í„° ë°ì´í„°ë“¤ ì‹œê°í™” plt.scatter(x=label_cluster[&#x27;ftr1&#x27;], y=label_cluster[&#x27;ftr2&#x27;], marker=markers[label]) # ê° í´ëŸ¬ìŠ¤í„°ì˜ ì¤‘ì‹¬ ì‹œê°í™” # ì¤‘ì‹¬ í‘œí˜„í•˜ëŠ” ëª¨í˜• ì„¤ì • plt.scatter(x=center_x_y[0], y=center_x_y[1], s=200, color=&#x27;white&#x27;, alpha=0.9, edgecolor=&#x27;k&#x27;, marker=markers[label]) # ì¤‘ì‹¬ í‘œí˜„í•˜ëŠ” ê¸€ì ì„¤ì • plt.scatter(x=center_x_y[0], y=center_x_y[1], s=70, color=&#x27;k&#x27;, edgecolor=&#x27;k&#x27;, marker=&#x27;$%d$&#x27; % label)# labelê°’ì— ë”°ë¼ ìˆ«ìë¡œ í‘œí˜„í•œë‹¤ëŠ” ì˜ë¯¸plt.show() 2. GMM It is a parametric model and is a representative clustering model using EM algorithms. It is to estimate the probability that individual data belongs to a specific normal distribution under the assumption that they belong to a Gaussian distribution. n_components is main parameter of the API provided by Scikit-learn which refers to the number of clustering predefined GMM has a particularly well-applied data distribution, which is mainly easy to apply to elliptical elongated data distributions 1234567891011121314151617181920212223242526from sklearn.datasets import load_irisfrom sklearn.cluster import KMeansimport matplotlib.pyplot as pltimport numpy as npimport pandas as pd%matplotlib inlineiris = load_iris()feature_names = [&#x27;sepal_length&#x27;,&#x27;sepal_width&#x27;,&#x27;petal_length&#x27;,&#x27;petal_width&#x27;]# ë³´ë‹¤ í¸ë¦¬í•œ ë°ì´íƒ€ Handlingì„ ìœ„í•´ DataFrameìœ¼ë¡œ ë³€í™˜irisDF = pd.DataFrame(data=iris.data, columns=feature_names)irisDF[&#x27;target&#x27;] = iris.target# GMM ì ìš©from sklearn.mixture import GaussianMixture# n_componentsë¡œ ë¯¸ë¦¬ êµ°ì§‘ ê°œìˆ˜ ì„¤ì •gmm = GaussianMixture(n_components=3, random_state=42)gmm_labels = gmm.fit_predict(iris.data)# GMM í›„ í´ëŸ¬ìŠ¤í„°ë§ ë ˆì´ë¸”ì„ ë”°ë¡œ ì„¤ì •irisDF[&#x27;gmm_cluster&#x27;] = gmm_labels# ì‹¤ì œ ë ˆì´ë¸”ê³¼ GMM í´ëŸ¬ìŠ¤í„°ë§ í›„ ë ˆì´ë¸”ê³¼ ë¹„êµí•´ë³´ê¸°(ë‘ ë ˆì´ë¸” ìˆ˜ì¹˜ê°€ ë™ì¼í•´ì•¼ ë˜‘ê°™ì€ ë ˆì´ë¸” ì˜ë¯¸ ì•„ë‹˜!)print(irisDF.groupby(&#x27;target&#x27;)[&#x27;gmm_cluster&#x27;].value_counts()) The result values are as follows, where target is the label of the actual original data, and gmm_cluster is the label of clustering after clustering. To interpret the above results, all of the labels with a target of 0 were clustered into a clustering label of 2. It is 100% well clustered. On the other hand, among the labels with a target of 1, there are 45 clusters with 0 and 5 clusters with 1, so there are 5 incorrectly clustered data. Korean reference","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"Clustering","slug":"Clustering","permalink":"https://jmj3047.github.io/tags/Clustering/"},{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/tags/Python/"},{"name":"Gaussian Mixture Model","slug":"Gaussian-Mixture-Model","permalink":"https://jmj3047.github.io/tags/Gaussian-Mixture-Model/"},{"name":"K-Means Clustering","slug":"K-Means-Clustering","permalink":"https://jmj3047.github.io/tags/K-Means-Clustering/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}]},{"title":"Gaussian Mixture Model","slug":"GMM","date":"2022-09-09T15:00:00.000Z","updated":"2023-02-23T01:36:14.673Z","comments":true,"path":"2022/09/10/GMM/","link":"","permalink":"https://jmj3047.github.io/2022/09/10/GMM/","excerpt":"","text":"1. What is GMM It is one of several models applying the Expectation Maximum (EM) algorithm. What is EM algorithm? EM algorithm is basically an algorithm mainly used for Unsupervised learning. It is also used in clustering. The EM algorithm can be largely divided into two stages: E-step and M-step. In conclusion, it is a flow that finds the optimal parameter value by repeating E-step and M-step. E-step: Calculate the likelihood value that is as close as possible to the likelihood of the initial value of a given arbitrary parameter. M-step: Obtained a new parameter value that maximizes the likelihood calculated in the E-step The above two steps are continuously repeated until the parameter value does not change significantly. The purpose of the EM algorithm is to find parameters that maximize the likelihood. Maximum Likelihood Estimation (MLE) is a Convex or Convex function whose objective function can be differentiated (which can obtain a global optimum) because the optimal parameter must be obtained directly by partial differentiation of the parameter variable. However, EM is a process of finding a value close to the optimal parameter while repeating the E-M step. Similar to the ANN (artificial neural network) model, it does not guarantee to find the Global Optimum, and even if it does, it does not recognize that the point is the real Global Optimum. Therefore, it is very likely that the local minimum is found and the objective function is not composed of Convex or Concave functions. The EM algorithm defines the function of the parameters to be obtained (ex. probability distribution) as a probability variable and optimizes it. It is also used in clustering models and is mainly used in speech recognition modeling. This is a post that Kakao, which develops voice recognition technology using GMM algorithms. Clustering is performed on the assumption that data are generated by datasets with multiple normal distributions. Several normal distribution curves are extracted and individual data are determined to which normal distribution belongs. This process is called parameter estimation in GMM, and two typical estimates are made and the EM method is applied for this parameter estimation. Means and variance of individual normal distributions Probability of which normal distribution each data corresponds 2. Code123456789101112131415161718192021222324252627282930import numpy as npimport pandas as pdimport matplotlib as mplimport matplotlib.pyplot as pltimport seaborn as snsimport warnings%matplotlib inline%config InlineBackend.figure_format = &#x27;retina&#x27;mpl.rc(&#x27;font&#x27;, family=&#x27;NanumGothic&#x27;) # í°íŠ¸ ì„¤ì •mpl.rc(&#x27;axes&#x27;, unicode_minus=False) # ìœ ë‹ˆì½”ë“œì—ì„œ ìŒìˆ˜ ë¶€í˜¸ ì„¤ì •# ì°¨íŠ¸ ìŠ¤íƒ€ì¼ ì„¤ì •sns.set(font=&quot;NanumGothic&quot;, rc=&#123;&quot;axes.unicode_minus&quot;:False&#125;, style=&#x27;darkgrid&#x27;)plt.rc(&quot;figure&quot;, figsize=(10,8))warnings.filterwarnings(&quot;ignore&quot;)from sklearn.datasets import load_irisiris = load_iris()feature_names = [&#x27;sepal_length&#x27;,&#x27;sepal_width&#x27;,&#x27;petal_length&#x27;,&#x27;petal_width&#x27;]iris_df = pd.DataFrame(iris.data, columns = feature_names)iris_df[&quot;target&quot;] = iris.targetiris_df.head() 1234567891011from sklearn.mixture import GaussianMixture# GMM: n_components = ëª¨ë¸ì˜ ì´ ìˆ˜gmm = GaussianMixture(n_components=3, random_state=0)gmm.fit(iris.data)gmm_cluster_labels = gmm.predict(iris.data)# target, gmm_cluster ë¹„êµiris_df[&quot;gmm_cluster&quot;] = gmm_cluster_labelsiris_df.groupby([&quot;target&quot;,&quot;gmm_cluster&quot;]).size() 1234567ê²°ê³¼target gmm_cluster0 0 501 1 45 2 52 2 50dtype: int64 GMM can be performed using GaussianMixture() in sklearn.mixture() n_components is the total number of models, and like n_clusters in K-Means, determines the number of clusters. Here, when the target of gmm_cluster was 1, only five were mapped differently, and all the rest were well mapped. Korean reference1 Korean reference2","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"Clustering","slug":"Clustering","permalink":"https://jmj3047.github.io/tags/Clustering/"},{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/tags/Python/"},{"name":"Gaussian Mixture Model","slug":"Gaussian-Mixture-Model","permalink":"https://jmj3047.github.io/tags/Gaussian-Mixture-Model/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}]},{"title":"K-Means Clustering","slug":"K-Means_Clustering","date":"2022-09-08T15:00:00.000Z","updated":"2023-02-23T01:35:53.766Z","comments":true,"path":"2022/09/09/K-Means_Clustering/","link":"","permalink":"https://jmj3047.github.io/2022/09/09/K-Means_Clustering/","excerpt":"","text":"1. What is K-means Clustering The K-Means clustering algorithm does not automatically identify and group the number of clusters by looking at the data. The number of clusters should be specified and the initial value should be selected accordingly by user. How to determine the number of clusters use the indicator that quantifies how well clustering has been done. After deciding the candidates of cluster numbers and performing clustering about each cluster number, calculate the index At this time, we select the number of clusterings that optimize the index as the optimal number of clusters. The frequently used indicators include Dunn Index, and Silhouette Index. The algorithm assigning the group determines the initial value which is the initial center point of the group. Each center point determines a group, and individual data is assigned to the same group as the center point close to itself. It does not end at once (depending on the form of the data) but repeats the process of updating the center of the group and assigning groups of individual data again. It belongs to the hard clustering algorithm, which is a clustering that unconditionally assigns a group of data points to a point close to the center. 2. Pros &amp; Cons Advantages The process is intuitive, easy to understand, and the algorithm is simple so that it is easy to implement. It does not require complex calculations, so it can be applied to large-scale data. convergence is guaranteed. Disadvantages The result can be very different depending on the initial value. Can be affected by an outlier: because the distance is based on Euclidean Distance, it can be affected by an outlier in the process of updating the center value. Cannot reflect intra-group dispersion structure: Since the distance is based on Euclidean Distance, it cannot properly reflect the intra-group dispersion structure. As the dimensionality increases, the distance between individual data becomes closer, and the effect of clustering may not be effective. This has the meaning of clustering only when the distance between clusters is kept as far apart as possible, but as the dimensionality increases, the distance between individual data becomes closer, so this effect may not be observed. The number of clusters is not set automatically, but must be set in advance. However, the optimal number of clusters can be determined using indicators such as Dunn Index and Silhouette Index. Because of Euclidean Distance, it cannot be used if there is a categorical variable. In this case, it is possible to use the extended K-Modes Clustering algorithm. 3. Using python: scikit-learn Put 3 in n_clusters and init_center in init, and create a KMeans instance. If you put data(X) into the fit function, clustering is performed. The final label can be obtained through the labels_ field. 1234567import warningswarnings.filterwarnings(&#x27;ignore&#x27;)from sklearn.cluster import KMeans kmeans = KMeans(n_clusters=3, init=init_center)kmeans.fit(X)labels = kmeans.labels_ Visualization 1234567891011121314fig = plt.figure(figsize=(7,7))fig.set_facecolor(&#x27;white&#x27;)for i, label in enumerate(labels): if label == 0: color = &#x27;blue&#x27; elif label ==1: color = &#x27;red&#x27; else: color = &#x27;green&#x27; plt.scatter(X[i,0],X[i,1], color=color) plt.xlabel(&#x27;x1&#x27;)plt.ylabel(&#x27;x2&#x27;)plt.show() Korean reference1 Korean reference2","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"Clustering","slug":"Clustering","permalink":"https://jmj3047.github.io/tags/Clustering/"},{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/tags/Python/"},{"name":"K-Means Clustering","slug":"K-Means-Clustering","permalink":"https://jmj3047.github.io/tags/K-Means-Clustering/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}]},{"title":"Clustering","slug":"Clustering","date":"2022-09-07T15:00:00.000Z","updated":"2023-02-23T01:35:34.556Z","comments":true,"path":"2022/09/08/Clustering/","link":"","permalink":"https://jmj3047.github.io/2022/09/08/Clustering/","excerpt":"","text":"Clustering is an example of unsupervised learning. Without any label, those with close distances in the data are classified into clusters. It is different from classification, which is supervised learning. In other words, it identifies patterns and groups hidden in the data and binds them together. Even if thereâ€™s label in data, there is a possibility that some data with same label can be grouped into different clusters. There are K-Means Clustering, Mean Shift, Gaussian Mixture Model, DBSCAN, Agglomerative Clustering in clustering algorithms and they will be covered in the next post. Korean reference","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"Clustering","slug":"Clustering","permalink":"https://jmj3047.github.io/tags/Clustering/"},{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}]},{"title":"Data Sampling","slug":"Data_Sampling","date":"2022-09-06T15:00:00.000Z","updated":"2023-02-23T01:35:23.520Z","comments":true,"path":"2022/09/07/Data_Sampling/","link":"","permalink":"https://jmj3047.github.io/2022/09/07/Data_Sampling/","excerpt":"","text":"1. Reason why you need The more input data you have on machine learning, the slower the processing. Therefore, in order to speed up the processing speed of machine learning, acceleration of learning speed of data would be helpful, which can be done with optimization of machine learning with representative data. Then letâ€™s see how we can reduce the data so that we can only use the data we need. 2. What is data sampling The process of organizing the data and making it the best input data. For example, it can speed up the processing of machine learning by using sales of â€˜monthâ€™ rather than using sales of â€˜dayâ€™ units to analyze last yearâ€™s profits of a pizza house. This is the work of making optiml data. There are probabilistic sampling, nonprobability sampling in data sampling method. The probabilistic sampling method is a sampling method based on statistics, and the nonprobability sampling is a sampling method in which the subjectivity of a person is involved. Depending on each sampling method, you should select and use the sampling method that matches the data and the situation because there are advantages and disadvantages. 3. Probabilistic sampling Probabilistic sampling is a random sampling method that can be divided into simple random sampling, two-step sampling, stratified sampling, cluster&#x2F;collective sampling, and system sampling. Simple Random Sampling: A method of randomly extracting samples from the entire data. Two-Step Sampling: A sampling method that separates the entire n data into m subpopulations. Selects m subpopulations and provides simple random sampling of N data from m subpopulations. It is an accurate sampling method rather than simple random sampling. Stratified Sampling: By separating the population from several layers, it is a method of randomly extracting data from each layer by n. For example, it is a method of dividing the layers of Korean cities and province and extracting n data from each layer. cluster&#x2F;collective sampling: If the population is composed of multiple clusters, it is a method of selecting one or several clusters and using the entire data of the selected clusters. For example, it is a way to use all of the data by setting the Korea as itâ€™s city and province. system sampling: It is a method of extracting data one by one at a regular interval by numbering all data from 1 to n. This method is mainly used to sample representative values of the time series data 4. Nonprobability sampling This is a method of subjectively extracting the probability of being selected as a sample in advance. The advantage and disadvantage of this sampling is that the subjective intention of the sampling person is involved. The implicit population extracted by nonprobability sampling is a good sampling if it matches the ideal population, the most suitable population for the subject. Nonprobability sampling methods include convenience sampling, purpose sampling, and quota sampling methods. Convenience Sampling: A method of sampling by selecting a point or location where data is good for collecting. The sample surveyed by this sampling method has a disadvantage that it is less representative than the population. Itâ€™s not possible to go through the statistical inference process. Statistical inference is that we generalize the sample analysis results to speculation about populations. Purpose Sampling: This is how you select the object you think is the most suitable object for your purpose; you will sample the data that is subjectively appropriate for your purpose.The downside is that it has also low representative for the population. Quota Sampling: Divide the population into segments, assigning each segment a quartar that represents the number of samples. Within segments, the characteristics related to the topic must be similar, and the populations must be distributed differently between segments. This is methodologically similar to layer-by-layer sampling. But the difference is that the sample is not selected by probability but by subjective judgment. 5. Conclusion Comparing probabilistic sampling with nonprobability sampling, probabilistic sampling may look good when judged by just words. However, stochastic sampling is advantageous for data that can be analyzed based on statistics, and non-probability sampling is advantageous for data such as language and music. It is better to choose and use probabilistic and nonprobable sampling as appropriate. Korean Reference","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Probabilistic sampling","slug":"Probabilistic-sampling","permalink":"https://jmj3047.github.io/tags/Probabilistic-sampling/"},{"name":"Nonprobability sampling","slug":"Nonprobability-sampling","permalink":"https://jmj3047.github.io/tags/Nonprobability-sampling/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}]},{"title":"Growth Hacking, AARRR, Funnel, Retention","slug":"Growth_Hacking","date":"2022-08-11T15:00:00.000Z","updated":"2023-02-23T01:35:04.494Z","comments":true,"path":"2022/08/12/Growth_Hacking/","link":"","permalink":"https://jmj3047.github.io/2022/08/12/Growth_Hacking/","excerpt":"","text":"1. Growth Hacking ê·¸ë¡œìŠ¤í•´í‚¹(Growth Hacking)ì€ ì„±ì¥(Growth)ì„ ìœ„í•œ ëª¨ë“  ìˆ˜ë‹¨(Hacking)ì´ë€ ëœ»ìœ¼ë¡œ ê³µê²© ëŒ€ìƒì˜ ë¯¸ì„¸í•œ ë¹ˆí‹ˆì„ ì°¾ì•„ í•´í‚¹ì„ í•˜ë“¯ì´ ì„±ì¥ì„ ìœ„í•´ ê³ ê°ê³¼ ìœ í†µê³¼ì • ë“±ì˜ ê³µëµì§€ì ì„ ì°¾ì•„ë‚´ê³  ì´ë¥¼ ì ê·¹ì ìœ¼ë¡œ ê³µëµí•˜ëŠ” ë§ˆì¼€íŒ… ë°©ë²•ë¡  ë¸Œëœë“œ, ê¸°ì—…, ì œí’ˆ ë§¤ì¶œ ì¦ê°€ ë“±ì„ ìœ„í•œ ê°€ì„¤ì„ ìˆ˜ë¦½í•˜ê³  ì´ë¥¼ ë¹ ë¥´ê²Œ MVP ëª¨ë¸ë¡œ ì¶œì‹œí•˜ì—¬ ì‹œì¥ì˜ í‰ê°€ë¥¼ ë°›ì•„ ë³¸ í›„, ì†Œë¹„ìì™€ ì‹œì¥ì˜ ë°˜ì‘ì— ë”°ë¼ ì œí’ˆ ë˜ëŠ” ì„œë¹„ìŠ¤ê°€ ì‹œì¥ì—ì„œ ì›í•˜ëŠ” (ê³ ê°ë“¤ì´ ì›í•˜ëŠ”) ì™„ë²½í•œ ìƒí’ˆìœ¼ë¡œ ë„ë‹¬í•  ë•Œê¹Œì§€ ì‰¬ì§€ ì•Šê³  ê°œì„ í•´ ë‚˜ê°€ëŠ” ë°©ì‹ ì„±ì¥(Growth)ì„ ìœ„í•œ ëª¨ë“  ìˆ˜ë‹¨(Hacking)ì„ í†µí•´ íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•˜ê³  ì œí’ˆê³¼ ì„œë¹„ìŠ¤ë¥¼ ë¹ ë¥´ê²Œ ì„±ì¥ì‹œí‚¨ë‹¤ëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ ìˆ˜ë‹¨ì´ë€, ë°ì´í„° ê¸°ë°˜ì˜ ë¶„ì„ê³¼ ì‹œì¥ì˜ í”¼ë“œë°±ì„ ë°›ì•„ ì œí’ˆê³¼ ì„œë¹„ìŠ¤ë¥¼ ê°œì„ í•˜ê³  í™•ì¥í•´ ë‚˜ê°„ë‹¤ëŠ” ì˜ë¯¸ë¡œ, ìë³¸ê³¼ ë¦¬ì†ŒìŠ¤ê°€ í•œì •ì ì¸ ìŠ¤íƒ€íŠ¸ ê¸°ì—…ì—ê²Œì„œ íš¨ê³¼ì ì¸ ë§ˆì¼€íŒ… ì„±ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŒ êµ¬ë§¤ìì˜ í–‰ë™ íŒ¨í„´ì„ ë¶„ì„í•˜ê³  ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ê²½í—˜ì„ ìµœì í™”í•˜ëŠ” ë°©ë²•. ê·¸ë¡œìŠ¤í•´í‚¹ì€ ìƒì‚°ë¶€í„° ê´€ë¦¬ì— ì´ë¥´ê¸°ê¹Œì§€ ì†Œë¹„ìì˜ Wantsë¥¼ ì¶©ì¡±ì‹œí‚¤ëŠ” ìƒí’ˆì„ ë§Œë“œëŠ” ê²ƒ 2. AARRR Analysis Acquisition: ì–´ë–»ê²Œ ì²˜ìŒ ìš°ë¦¬ ì„œë¹„ìŠ¤ë¥¼ ì ‘í•˜ê²Œ ë˜ëŠ”ê°€(ì‚¬ìš©ì ìœ ì¹˜) Activation: ì‚¬ìš©ìê°€ ì²˜ìŒ ì„œë¹„ìŠ¤ë¥¼ ì ‘í•  ë•Œì— ê¸ì •ì ì¸ ê²½í—˜ì„ ì œê³µí•˜ê³  ìˆëŠ”ê°€(ì‚¬ìš©ì í™œì„±í™”) Retention: ì´í›„ì˜ ì„œë¹„ìŠ¤ë¥¼ ë‹¤ì‹œ ì‚¬ìš©í•˜ëŠ” ì •ë„ëŠ” ì–¼ë§ˆë‚˜ ë˜ëŠ”ê°€(ì‚¬ìš©ì ìœ ì§€) Revenue: ìµœì¢… ëª©ì (ê±°ì‹œì „í™˜)ìœ¼ë¡œ ì—°ê²°ë˜ê³  ìˆëŠ”ê°€(ë§¤ì¶œ) Referral: ì‚¬ìš©ìê°€ ìë°œì ìœ¼ë¡œ í™•ì‚°ì´ë‚˜ ê³µìœ ë¥¼ ì¼ìœ¼í‚¤ê³  ìˆëŠ”ê°€(ì¶”ì²œ) 3. Funnel Analysis í¼ë„ ë¶„ì„ì€ ì›¹ ì‚¬ì´íŠ¸ì—ì„œ íŠ¹ì • ê²°ê³¼ì— ë„ë‹¬í•˜ëŠ”ë° í•„ìš”í•œ ë‹¨ê³„ì™€, ê° ë‹¨ê³„ë¥¼ í†µê³¼í•˜ëŠ” ì‚¬ìš©ì ìˆ˜ë¥¼ íŒŒì•…í•˜ê¸° ìœ„í•œ ë°©ë²• í¼ë„ ë¶„ì„ì„ í†µí•´ ê¸°ì—…ì€ ë°©ë¬¸ìê°€ ì‚¬ì´íŠ¸ì— ê°€ì…í•˜ëŠ”ì§€, êµ¬ë§¤ìë¡œ ë³€í™˜ì´ ë˜ëŠ”ì§€ ë“±ì˜ ì •ë³´ë¥¼ íŠ¹ì •í•œ í¼ë„ì— ë§¤í•‘í•˜ê²Œ ë¨. ì´ë•Œ ì‚¬ìš©ìì˜ íë¦„ì„ ì‹œê°í™”í•˜ëŠ” ëª¨í˜•ì˜ ëª¨ìŠµì´ ë¶€ì—Œì´ë‚˜ ì°¨ê³ ì—ì„œ í”í•˜ê²Œ ì“°ì´ëŠ” ê¹”ë•Œê¸°ì™€ ìœ ì‚¬í•œ ëª¨ìŠµì„ ë ê³  ìˆì–´ â€˜í¼ë„ ë¶„ì„â€™ì´ë¼ëŠ” ì´ë¦„ì´ ë¶™ì—¬ì§ 4. Retention Analysis ë¦¬í…ì…˜ì€ ì•± ì„œë¹„ìŠ¤ ì„±ì¥ì— ìˆì–´ì„œ ë§¤ìš° ì¤‘ìš”í•œ ì§€í‘œ ë¦¬í…ì…˜ì´ë€ í•œë²ˆ íšë“í•œ ìœ ì €ë“¤ì´ ì„œë¹„ìŠ¤ë¥¼ ì´íƒˆí•˜ì§€ ì•Šê³  ê³„ì† ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•˜ëŠ” ê²ƒì„ ì˜ë¯¸ ë¦¬í…ì…˜ì´ ë†’ì€ ì„œë¹„ìŠ¤ëŠ” ë¦¬í…ì…˜ì´ ë‚®ì€ ì„œë¹„ìŠ¤ë³´ë‹¤ íšë“ë¹„ìš©ì— íˆ¬ìí•œ ë¹„ìš©ì„ ë¹ ë¥´ê²Œ íšŒìˆ˜í•  ìˆ˜ ìˆìœ¼ë©° ì´ë ‡ê²Œ íšŒìˆ˜í•œ ë¹„ìš©ìœ¼ë¡œ ë‹¤ì‹œê¸ˆ ë¹ ë¥´ê²Œ íšë“ì— íˆ¬ìí•  ìˆ˜ ìˆë„ë¡ í•˜ì—¬ ì„±ì¥ì„ ì´‰ì§„í•©ë‹ˆë‹¤. ë¦¬í…ì…˜ì´ ë‚®ë‹¤ëŠ” ê²ƒì€ íšë“ ì´í›„ ë‹¤ì‹œ ëŒì•„ì˜¤ì§€ ì•Šê³  ì´íƒˆí•˜ëŠ” ìœ ì €ê°€ ë§ì€ ê²ƒì¸ë°ìš”, ë¦¬í…ì…˜ì´ ë‚®ì€ ì„œë¹„ìŠ¤ëŠ” ì„±ì¥ì´ ë”ë”œ ë¿ë§Œ ì•„ë‹ˆë¼ í•œë²ˆ íšë“í–ˆë‹¤ê°€ ì´íƒˆí•œ ìœ ì €ëŠ” ë‹¤ì‹œ íšë“í•˜ê¸°ì—ë„ ë” ë§ì€ íšë“ ë¹„ìš©ê³¼ ì‹œê°„ì´ ì†Œìš”ë˜ë¯€ë¡œ ì•…ìˆœí™˜ì„ ë§Œë“¤ì–´ëƒ…ë‹ˆë‹¤. ë”°ë¼ì„œ ë¦¬í…ì…˜ì€ ì‚¬ì—… ì„±ì¥ì— ìˆì–´ì„œ ë°˜ë“œì‹œ ì§€ì¼œë³´ì•„ì•¼ í•  ì§€í‘œ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. ì”ì¡´ìœ¨(D+1ì§€í‘œ) Reference Retention Funnel How to measure Retention","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}]},{"title":"Deep Embedding Learning for Text-Dependent Speaker Verification","slug":"Deep_Embedding_Learning_for_Text-Dependent_Speaker_Verification","date":"2022-07-04T15:00:00.000Z","updated":"2022-10-15T08:03:47.207Z","comments":true,"path":"2022/07/05/Deep_Embedding_Learning_for_Text-Dependent_Speaker_Verification/","link":"","permalink":"https://jmj3047.github.io/2022/07/05/Deep_Embedding_Learning_for_Text-Dependent_Speaker_Verification/","excerpt":"","text":"Journal&#x2F;Conference: InterspeechYear(published year): 2020Author: Peng Zhang, Peng Hu, Xueliang ZhangSubject: Speaker Verification Deep Embedding Learning for Text-Dependent Speaker Verification Summary ì´ ë…¼ë¬¸ì€ í™”ì ê²€ì¦ ì‘ì—…ì„ ìœ„í•œ íš¨ê³¼ì ì¸ ë”¥ ì„ë² ë”© í•™ìŠµ ì•„í‚¤í…ì²˜ë¥¼ ì œì‹œí•œë‹¤. ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ì”ë¥˜ ì‹ ê²½ë§(ResNet) ë° ì‹œê°„ ì§€ì—° ì‹ ê²½ë§(TDNN) ê¸°ë°˜ ì•„í‚¤í…ì²˜ì™€ ë¹„êµí•˜ì—¬, ë‘ ê°€ì§€ ì£¼ìš” ê°œì„ ì´ ì œì•ˆëœë‹¤. ìš°ë¦¬ëŠ” í™”ìì˜ ë‹¨ê¸° ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ì¸ì½”ë”©í•˜ê¸° ìœ„í•´ ì¡°ë°€í•˜ê²Œ ì—°ê²°ëœ ì»¨ë³¼ë£¨ì…˜ ë„¤íŠ¸ì›Œí¬(DenseNet)ë¥¼ ì‚¬ìš©í•œë‹¤. ì–‘ë°©í–¥ ì£¼ì˜ í’€ë§ ì „ëµì´ ì œì•ˆëœë‹¤. ì¥ê¸°ì ì¸ ì‹œê°„ì  ë§¥ë½ì„ ëª¨ë¸ë§í•˜ê³  í™”ì ì •ì²´ì„±ì„ ë°˜ì˜í•˜ëŠ” ì¤‘ìš”í•œ í”„ë ˆì„ì„ ì§‘ê³„í•œë‹¤. ê²°ê³¼ëŠ” ì œì•ˆëœ ì•Œê³ ë¦¬ë“¬ì´ ê³¼ì œ 1ê³¼ ê³¼ì œ 3ì˜ í‰ê°€ ì„¸íŠ¸ì—ì„œ ê°ê° 8.06%, 19.70% minDCF ë° 9.26%, 16.16% EERs ìƒëŒ€ì  ê°ì†Œë¡œ FFSVC2020ì˜ ê³µì‹ ê¸°ì¤€ì„ ì„ ëŠ¥ê°€í•œë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤€ë‹¤. Introductionë”¥ ëŸ¬ë‹ ê¸°ë°˜ ë°©ë²•ì€ í™”ì ê°„ ì°¨ë³„ì— í•„ìˆ˜ì ì¸ ì •ë³´ë¥¼ í¬í•¨í•˜ëŠ” ë”¥ ìŠ¤í”¼ì»¤ ì„ë² ë”© ë˜ëŠ” ì§§ê²Œ ì„ë² ë”©ê³¼ ê°™ì€ ë°œí™” ìˆ˜ì¤€ í‘œí˜„ì„ ì–»ê¸° ìœ„í•´ ì§€ë°°ì ì´ì—ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” í™”ì ê²€ì¦ì„ ìœ„í•œ íš¨ê³¼ì ì¸ ë”¥ ì„ë² ë”© í•™ìŠµ ì•„í‚¤í…ì²˜ë¥¼ ì œì•ˆí•œë‹¤. ìµœê·¼ ì¡°ë°€í•˜ê²Œ ì—°ê²°ëœ ì»¨ë³¼ë£¨ì…˜ ë„¤íŠ¸ì›Œí¬ì˜ ì„±ê³µì— ìê·¹ë°›ì•„ (DenseNet) ì´ë¯¸ì§€ ë¶„ë¥˜ [15], ìŒì•… ì†ŒìŠ¤ ë¶„ë¦¬ [16], í™”ì ë¶„ë¦¬ [17] ë° í™”ìì¸ì‹ [18]ì—ì„œ, ìš°ë¦¬ëŠ” DenseNetì„ í”„ë ˆì„ ë ˆë²¨ í”¼ì²˜ ì¶”ì¶œê¸°ë¡œ ì±„íƒí•˜ì—¬ í›„ì† ë ˆì´ì–´ì˜ ì…ë ¥ì˜ ë³€í™”ì™€ í›ˆë ¨ íš¨ìœ¨ì„±ì„ ì¦ê°€ì‹œí‚¨ë‹¤. ê°œë…ì ìœ¼ë¡œ ê° ë°€ë„ ë¸”ë¡ì€ ì‘ì€ CNN ì‹œìŠ¤í…œ ì—­í• ì„ í•œë‹¤. ì°¨ì´ì ì€ ë ˆì´ì–´ì˜ ì¶œë ¥ì´ ì±„ë„ ì°¨ì›ì˜ ì¶œë ¥ ì—°ê²°ì— ì˜í•´ êµ¬í˜„ë˜ëŠ” í”¼ë“œ í¬ì›Œë“œ ë°©ì‹ìœ¼ë¡œ ì¡°ë°€í•˜ê²Œ ì—°ê²°ëœë‹¤ëŠ” ê²ƒì´ë‹¤. ë˜í•œ, ìš°ë¦¬ëŠ” ë°œí™” ìˆ˜ì¤€ ê¸°ëŠ¥ì˜ í‘œí˜„ë ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ì‹œê°„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì¶”ê°€ë¡œ ëª¨ë¸ë§í•˜ê¸° ìœ„í•œ ì–‘ë°©í–¥ ì£¼ì˜ í’€ë§ ê³„ì¸µì„ ì„¤ê³„í•œë‹¤. Model ArchitectureDenseNet(í”„ë ˆì„ ë ˆë²¨ ê¸°ëŠ¥ ì¶”ì¶œê¸°): ê°ê° 5ê°œì˜ CNN ë ˆì´ì–´ê°€ ìˆëŠ” 4ê°œì˜ ë°€ë„ ë¸”ë¡(DenseBlock)ìœ¼ë¡œ êµ¬ì„±. í”„ë ˆì„ ë ˆë²¨ í”¼ì²˜ ì¶”ì¶œ í›„, ì–‘ë°©í–¥ ì£¼ì˜ í’€ë§ ë ˆì´ì–´ëŠ” í”„ë ˆì„ ë ˆë²¨ í”¼ì²˜ë¥¼ ê³ ì • ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ë° ì‚¬ìš©ë˜ë©°, ì´ì–´ì„œ ì™„ì „íˆ ì—°ê²°ëœ ë‘ ê°œì˜ ìˆ¨ê²¨ì§„ ë ˆì´ì–´ê°€ ë°œí™” ë ˆë²¨ í”¼ì²˜ë¥¼ í˜•ì„±í•œë‹¤. ì¶œë ¥ì€ ì†Œí”„íŠ¸ë§¥ìŠ¤ ë¶„ë¥˜ê¸° ê³„ì¸µì´ë©°, ê° ë…¸ë“œëŠ” í™”ìIDì— ëŒ€ì‘í•©ë‹ˆë‹¤. Frame levelì˜ íŠ¹ì§• ì¶”ì¶œì„ í•˜ê³ , Inputê³¼ ë‹¤ìŒ ë ˆì´ì–´ì˜ ë³€í™”ì— ìµœì í™” ì‹œí‚¤ê³ , í›ˆë ¨ íš¨ìœ¨ì„ ë†’ì´ê¸°ìœ„í•´ì„œ ì‚¬ìš©í•¨.ê°œë…ì ìœ¼ë¡œ ê°ê°ì˜ dense blockë“¤ ì´ ì‘ì€ CNNì˜ ì—­í• ì„ ìˆ˜í–‰í•˜ê³  ìˆëŠ” ê²ƒ. ê°ê°ì˜ ë ˆì´ì–´ë“¤ì´ ë°”ë¡œ ì „ì— ìˆëŠ” ëª¨ë“  ë ˆì´ì–´ë“¤ê³¼ feature map(CNNì—ì„œì˜ í•©ì„±ê³±ê³¼ ê°™ì€ ì›ë¦¬)ì„ í†µí•´ì„œ ì—°ê²°ì´ ë˜ì–´ìˆëŠ” ë°©ì‹.ì´ëŸ¬í•œ ì—°ê²° íŒ¨í„´ì€ í›ˆë ¨ ì¤‘ ë ˆì´ì–´ë“¤ ì‚¬ì´ì—ì„œ ë” ë‚˜ì€ ê¸°ìš¸ê¸° flowì™€ ê° ë ˆì´ì–´ë“¤ì— ëŒ€í•œ ì ‘ê·¼ì„ ëª¨ë“  featureì—ê²Œ ì „ë‹¬í•¨ìœ¼ë¡œì¨ ì¼ì‹œì ì¸ ë¬¸ë§¥ ì •ë³´ë¥¼ capture ê°€ëŠ¥. í”„ë ˆì„ ë ˆë²¨ ë‹¨ê³„ì˜ ê²½ìš° ê° DenseBlockì€ 5ê°œì˜ ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´(Conv2D), ì§€ìˆ˜ ì„ í˜• ë‹¨ìœ„(ELU) ë° ì¸ìŠ¤í„´ìŠ¤ ì •ê·œí™”(IN)ë¡œ êµ¬ì„±ëœë‹¤. ê° ë°€ë„ ë¸”ë¡ ë’¤ì˜ í…ì„œ ëª¨ì–‘ì€ featureMaps Ã— timeSteps Ã— ì£¼íŒŒìˆ˜ ì±„ë„ í˜•ì‹ì´ë‹¤. ê° Conv2D ë° Conv2D+IN+ELUëŠ” kernelSize í˜•ì‹ìœ¼ë¡œ ì§€ì •ë©ë‹ˆë‹¤. ì‹œê°„ Ã— kernelSizeFreq, (ë³´í–‰)ì‹œê°„, ë³´í­Freq), (íŒ¨ë”©ì‹œê°„, íŒ¨ë”©Freq), ë§µì„ íŠ¹ì§•ìœ¼ë¡œ í•œë‹¤. ê° ë°€ë„ ë¸”ë¡(g)ì€ 5ê°œì˜ Conv2D+ì¦ê°€ìœ¨ì´ gì¸ IN+ELU ë¸”ë¡ì„ í¬í•¨í•©ë‹ˆë‹¤. ë°œí™” ìˆ˜ì¤€ ë‹¨ê³„ì˜ ê²½ìš° ìˆ«ìëŠ” ìš°ë¦¬ì˜ êµ¬í˜„ì—ì„œ ì¶œë ¥ ê¸°ëŠ¥ ë§µ ë˜ëŠ” ì„ë² ë”© ì°¨ì›ì˜ ì±„ë„ì„ ë‚˜íƒ€ë‚¸ë‹¤. DenseBlock[15]ì—ì„œ ì²˜ìŒ ì œì‹œëœ DenseBlock ì•„í‚¤í…ì²˜ì˜ ì£¼ìš” ì•„ì´ë””ì–´ëŠ” CNNì˜ ë„¤íŠ¸ì›Œí¬ êµ¬ì¶• ë¸”ë¡ì—ì„œ ê° ê³„ì¸µì—ì„œ ëª¨ë“  í›„ì† ê³„ì¸µì— ì§ì ‘ ì—°ê²°ì„ ë„ì…í•˜ëŠ” ê²ƒì´ë‹¤. ì‹œê°„ ë¹ˆë„ê°€ ê¸´ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ìº¡ì²˜í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆë‹¤. ê° ê³„ì¸µì€ í”¼ì³ ë§µ ì—°ê²°ì„ í†µí•´ ëª¨ë“  ë‹¤ìŒ ê³„ì¸µì— ì§ì ‘ ì—°ê²°ë©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì—°ê²° íŒ¨í„´ì€ í›ˆë ¨ ì¤‘ì— ê³„ì¸µ ê°„ì— ë” ë‚˜ì€ ê·¸ë ˆì´ë””ì–¸íŠ¸ íë¦„ì„ ìƒì„±í•˜ê³  ê° ê³„ì¸µì´ ì´ì „ ê³„ì¸µì˜ ëª¨ë“  íŠ¹ì§• í‘œí˜„ì— ì•¡ì„¸ìŠ¤í•  ìˆ˜ ìˆë„ë¡ í•˜ì—¬ ì‹œê°„ì  ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ìº¡ì²˜í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆë‹¤. Bidirectional Attentive Pooling ì–‘ë°©í–¥ ì£¼ì˜ í’€ë§(bidirectional attentive pooling, BAP)ì˜ ë„ì‹ì…ë‹ˆë‹¤. hiëŠ” BAP ì…ë ¥ì˜ ië²ˆì§¸ ë²¡í„°ë¥¼ ë‚˜íƒ€ë‚´ë©° wfì™€ wbëŠ” ê°ê° BGRUì˜ ì „ë°©ê³¼ í›„ë°© ìˆ¨ê²¨ì§„ ìƒíƒœë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. â†’U ë° â†UëŠ” BGRU ë ˆì´ì–´ì˜ ì–‘ë°©í–¥ ì¶œë ¥ì…ë‹ˆë‹¤. bidirectional gated recurrent unit (BGRU) layer + attentive pooling : utterance level feature ì¼ë°˜ì ì¸ average poolingì„ ì‚¬ìš©í•˜ì§€ ì•Šì€ ì´ìœ : í‰ê· í™” ëŒ€ì‹  ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜[10, 12]ì€ ìˆ¨ê²¨ì§„ í‘œí˜„ì„ ì ê·¹ì ìœ¼ë¡œ ì„ íƒí•˜ê³  í™”ì ì°¨ë³„ ì •ë³´ë¥¼ ê°•ì¡°í•˜ê¸° ìœ„í•œ ë” ë‚˜ì€ ëŒ€ì•ˆì´ë‹¤. ë³´ë‹¤ ì°¨ë³„ì ì¸ ê³ ì • ì°¨ì› ë°œí™” ìˆ˜ì¤€ í‘œí˜„ì„ ì–»ê³  ì¥ê¸° ì‹œí€€ìŠ¤ ì •ë³´ë¥¼ ìº¡ì²˜í•˜ê¸° ìœ„í•´ [19]ëŠ” CNN-BLSTM ëª¨ë¸ê³¼ ì£¼ì˜ ê¹Šì€ í’€ë§ ë ˆì´ì–´ë¥¼ í•¨ê»˜ ê²°í•©í•˜ëŠ” ì£¼ì˜ ê¸°ë°˜ CNN-BLSTM í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•˜ì˜€ë‹¤. BLSTMì„ ì£¼ì˜ ê¹Šì€ í’€ë§ ê³„ì¸µì— ì§ì ‘ ì—°ê²°í•˜ëŠ” [19]ì™€ëŠ” ë‹¬ë¦¬ ì£¼ì˜ ê¹Šì€ í’€ë§ì„ ì‚¬ìš©í•˜ì—¬ ì–‘ë°©í–¥ ë°˜ë³µ ì‹ ê²½ë§ì— ì˜í•´ ì¶œë ¥ë˜ëŠ” ì–‘ë°©í–¥ ì‹œê°„ ì •ë³´ë¥¼ ìº¡ì²˜í•œ ë‹¤ìŒ, ì–‘ë°©í–¥ ë°œí™” ìˆ˜ì¤€ ê¸°ëŠ¥ì„ ì—°ê²°í•œë‹¤. ì œì•ˆëœ í’€ë§ ë°©ë²•ì¸ ì–‘ë°©í–¥ ì£¼ì˜ í’€ë§(BAP)ì€ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„ë  ìˆ˜ ìˆë‹¤. BAP ê³„ì¸µì€ ì–‘ë°©í–¥ ìˆœì°¨ ëª¨ë¸ë§ê³¼ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì„ ëª¨ë‘ í™œìš©í•˜ì—¬ ì¥ê¸°ì ì¸ ì‹œê°„ì  ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ìº¡ì²˜í•œë‹¤. Result Dataset: FFSVC2020The first 30 utterances are of fixed content: â€˜ni hao mi yaâ€™ in Mandarin Chinese for TD-SV tasks. The remaining utterances are text-independent.In total, training data sets have nearly 1,139,671 utterances and the total duration approximately 950 hours with 374 speakers Conclusion í…ìŠ¤íŠ¸ ì˜ì¡´ì  ìŠ¤í”¼ì»¤ ê²€ì¦ì„ ìœ„í•œ ë”¥ ì„ë² ë”© í•™ìŠµ ì•„í‚¤í…ì²˜ë¥¼ ì œì•ˆ ì•„í‚¤í…ì²˜ëŠ” í”„ë ˆì„ ìˆ˜ì¤€ì—ì„œ ìŠ¤í”¼ì»¤ IDë¥¼ ìº¡ì²˜í•˜ê¸° ìœ„í•œ DenseBlockì˜ ìŠ¤íƒê³¼ ë°œí™” ìˆ˜ì¤€ì—ì„œ ìŠ¤í”¼ì»¤ ì„ë² ë”©ì„ í˜•ì„±í•˜ê¸° ìœ„í•œ ì–‘ë°©í–¥ ì£¼ì˜ í’€ë§ êµ¬ì¡°ë¡œ êµ¬ì„±ë¨ ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ì˜ ì¶œë ¥ì„ ì¡°ë°€í•˜ê²Œ ì—°ê²°í•¨ìœ¼ë¡œì¨, ì‹œê°„ ì£¼íŒŒìˆ˜ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ì˜ ë‹¤ì–‘í•œ ì¸¡ë©´ì„ ê°€ì§„ ë³´ë‹¤ ì˜ë¯¸ ìˆëŠ” í”„ë ˆì„ ë ˆë²¨ í‘œí˜„ì´ ìƒì„±ë¨ ì–‘ë°©í–¥ ì£¼ì˜ í’€ë§ ê³„ì¸µì€ BGRU ê³„ì¸µê³¼ ì£¼ì˜ í’€ë§ì˜ ì¡°í•©ìœ¼ë¡œ ì–‘ë°©í–¥ì—ì„œ ì‹œê°„ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ì¶”ê°€ë¡œ ìº¡ì²˜ FFSVC2020ì—ì„œ ì ìˆ˜ ì œì¶œì˜ ê²½ìš°, ìš°ë¦¬ê°€ ì œì•ˆí•œ ë°©ë²•ì€ í‰ê°€ ì„¸íŠ¸ì˜ ê³¼ì œ 1ê³¼ ê³¼ì œ 3ì— ëŒ€í•œ ìµœì†Œ DCFì™€ EERì—ì„œ ê°ê° 0.52ì™€ 4.72%, 0.14%ë¥¼ ë‹¬ì„±. ë˜í•œ ì´ ê²°ê³¼ëŠ” x-ë²¡í„° ë° ResNet ê¸°ì¤€ì„  ì‹œìŠ¤í…œì— ë¹„í•´ ìƒë‹¹í•œ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì—¬ì¤Œ Pdf: Deep Embedding Learning for Text-Dependent Speaker Verification","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speaker Verification","slug":"Paper/Speaker-Verification","permalink":"https://jmj3047.github.io/categories/Paper/Speaker-Verification/"}],"tags":[{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"},{"name":"TD-SV","slug":"TD-SV","permalink":"https://jmj3047.github.io/tags/TD-SV/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speaker Verification","slug":"Paper/Speaker-Verification","permalink":"https://jmj3047.github.io/categories/Paper/Speaker-Verification/"}]},{"title":"Attention is all you need","slug":"Attention_is_all_you_need","date":"2022-05-09T15:00:00.000Z","updated":"2022-10-15T08:03:40.527Z","comments":true,"path":"2022/05/10/Attention_is_all_you_need/","link":"","permalink":"https://jmj3047.github.io/2022/05/10/Attention_is_all_you_need/","excerpt":"","text":"Journal&#x2F;Conference: NIPSYear(published year): 2017Author: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia PolosukhinSubject: NLP Attention is all you need Summary Attention ë§Œìœ¼ë¡œ ì‹œí€€ì…œ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ë³‘ë ¬í™”ì™€ ì—°ì‚° ì†ë„ í–¥ìƒì„ ê°€ëŠ¥í•˜ê²Œ í•œ ìƒˆë¡œìš´ ëª¨ë¸ ì œì‹œ Seq2Seq ê³¼ Attention ì„ ê²°í•©í•œ ëª¨ë¸(Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. CoRR, abs&#x2F;1409.0473, 2014.)ì—ì„œ í•œì¸µ ë” ë°œì „í•œ ëª¨ë¸ì…ë‹ˆë‹¤. Recurrent model(ì¬ê·€ êµ¬ì¡°)ì—†ì´ Self-attention ë§Œìœ¼ë¡œ êµ¬ì„±í•œ ì²«ë²ˆì§¸ ëª¨ë¸ì…ë‹ˆë‹¤. ì¬ê·€ êµ¬ì¡° ì œê±°ë¡œ ëª¨ë¸ì„ ë³‘ë ¬í™”(Parallelization)í•˜ì—¬ ìì—° ì–¸ì–´ ì²˜ë¦¬ í•™ìŠµ&#x2F;ì¶”ë¡  ì‹œê°„ì„ íšê¸°ì ìœ¼ë¡œ ë‹¨ì¶•ì‹œì¼°ìŠµë‹ˆë‹¤. Introductionê¸°ì¡´ì˜ ìì—°ì–¸ì–´ ì²˜ë¦¬ ëª¨ë¸ì€ RNN, LSTM, GLU ëª¨ë¸ë¡œ ëŒ€í‘œë˜ëŠ” ì¬ê·€ ëª¨ë¸(Recurrent Model)ì„ encoder-decoder êµ¬ì¡°ë¡œ ê²°í•©í•˜ëŠ” seq2seq ê³¼ ê°™ì€ ëª¨ë¸ì„ ì£¼ë¡œ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì¬ê·€ ëª¨ë¸ì€ ìˆœì°¨ ì²˜ë¦¬ë¡œ ì¸í•´ì„œ ë³‘ë ¬í™”ê°€ ì–´ë µë‹¤ëŠ” ì•½ì ì´ ìˆê³ , ë©”ëª¨ë¦¬ í¬ê¸°ê°€ ì œí•œë˜ì–´ ê¸´ ë¬¸ì¥ì„ ì²˜ë¦¬í•˜ê¸°ë„ ì–´ë µë‹¤ëŠ” ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ ë³´ì™„í•˜ê¸° ìœ„í•œ ì–´í…ì…˜(attention) ë§¤ì»¤ë‹ˆì¦˜ì´ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤. ì–´í…ì…˜ì€ ì¬ê·€ ê³¼ì •ì—ì„œ ì…ë ¥ì—ì„œ ì¶œë ¥ê¹Œì§€ì˜ ê±°ë¦¬ê°€ ê¸¸ì–´ì§€ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆì–´ ì…ì¶œë ¥ì˜ ì „ì—­ ì˜ì¡´ì„±ì„ ë†’ì—¬ì£¼ì—ˆì§€ë§Œ, ì¬ê·€ ëª¨ë¸ê³¼ ê²°í•©í•´ì„œë§Œ ì‚¬ìš©ë˜ì–´ ì™”ìŠµë‹ˆë‹¤. ì´ì— í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ”, ì–´í…ì…˜ë§Œìœ¼ë¡œ ëª¨ë¸ì„ êµ¬ì„±í•˜ì—¬ ì‰½ê²Œ ë³‘ë ¬í™” í•  ìˆ˜ ìˆê³  ìì—°ì–¸ì–´ì²˜ë¦¬ ê³¼ì œì˜ ì„±ëŠ¥ì„ ë†’ì¸ Transformer ëª¨ë¸ì„ ì œì‹œí•©ë‹ˆë‹¤. ì´ëŠ” ê¸°ì¡´ì˜ ì¬ê·€ ëª¨ë¸ê³¼ ë‹¤ë¥´ê²Œ 8ëŒ€ì˜ P100 GPUë¡œ 12ì‹œê°„ ì •ë„ë§Œ í•™ìŠµí–ˆìŒì—ë„ ë‹¹ì‹œ ê¸°ì¤€ SOTAë¥¼ ë‹¬ì„±í•˜ì˜€ìŠµë‹ˆë‹¤. WMT 2014 English-to-German Translation task -&gt; 28.4 BLEU WMT 2014 English-to-French Translation task -&gt; 41.0 BLEU Model ArchitectureTransformer ëª¨ë¸ì€ seq2seqìœ¼ë¡œ ëŒ€í‘œë˜ëŠ” ì¸ì½”ë”-ë””ì½”ë” êµ¬ì¡°ë¥¼ self-attention ìœ¼ë¡œ ìŒ“ì€ ë’¤, fully connected layer ë¡œ ì¶œë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤. Encoder and Decoder Stacks ì¸ì½”ë”(Encoder) $N$(&#x3D;6)ê°œì˜ ë™ì¼í•œ ë ˆì´ì–´ë¡œ êµ¬ì„± ê° ë ˆì´ì–´ëŠ” 2ê°œì˜ í•˜ìœ„ ë ˆì´ì–´ë¡œ êµ¬ì„± Multi-head self-attention position-wise fully connected feed-forward í•˜ìœ„ ë ˆì´ì–´ë¥¼ ê±°ì¹  ë•Œë§ˆë‹¤ Residual connection(Resnet) ê³¼ layer normalization ì„ ì‹¤í–‰ ê° ë ˆì´ì–´ ì¶œë ¥ì˜ í¬ê¸°ëŠ” $d_{model}$(&#x3D;512)ë¡œ ê³ ì • ë””ì½”ë”(Decoder) ì¸ì½”ë”ì™€ ê°™ì´ $N$(&#x3D;6)ê°œì˜ ë™ì¼í•œ ë ˆì´ì–´ë¡œ êµ¬ì„± ì¸ì½”ë”ì™€ ë™ì¼í•œ 2ê°œì˜ í•˜ìœ„ ë ˆì´ì–´ì— í•œê°€ì§€ë¥¼ ë” ì¶”ê°€í•˜ì—¬ 3ê°œì˜ í•˜ìœ„ ë ˆì´ì–´ë¡œ êµ¬ì„± Multi-head self-attention position-wise fully connected feed-forward ì¸ì½”ë”ì˜ ì¶œë ¥ìœ¼ë¡œ ì‹¤í–‰í•˜ëŠ” multi-head attention ìˆœì°¨ì ìœ¼ë¡œ ê²°ê³¼ë¥¼ ë§Œë“¤ì–´ ë‚¼ ìˆ˜ ìˆë„ë¡ Self-attention ë ˆì´ì–´ì— Masking ì„ì¶”ê°€ : $i$ ë²ˆì§¸ ì¶œë ¥ì„ ë§Œë“¤ ë•Œ, $i$ë²ˆì§¸ë³´ë‹¤ ì•ì„  ì¶œë ¥($i-1, i-2,\\dots$) ë§Œì„ ì°¸ê³ í•˜ë„ë¡ í•¨ Attentionattentionì€ queryì™€ key-value pairë“¤ì„ outputì— ë§µí•‘í•´ì£¼ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. ì¶œë ¥ì€ valuesë“¤ì˜ weighted sumìœ¼ë¡œ, valueì— í• ë‹¹ëœ weightëŠ” queryì™€ ëŒ€ì‘ë˜ëŠ” keyì˜ compatibility functionìœ¼ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤. Scaled Dot-Product Attention ì—¬ê¸°ì„œ ì‚¬ìš©í•˜ëŠ” attentionì€ Scaled Dot-Product Attention(SDPA)ë¼ ë¶€ë¥´ëŠ”ë°, inputì€ dimensionì´ $d_k$ì¸ queryì™€ key, dimensionì´ $d_v$ì¸ valueë“¤ë¡œ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤. ëª¨ë“  queryì™€ ëª¨ë“  keyë“¤ì— ëŒ€í•´ dot productë¡œ ê³„ì‚°ë˜ëŠ”ë° ê°ê°ì˜ ê²°ê³¼ì— dkë¡œ ë‚˜ëˆ„ì–´ì§„ë‹¤. ë‹¤ìŒ valueì˜ ê°€ì¤‘ì¹˜ë¥¼ ì–»ê¸° ìœ„í•´ softmax í•¨ìˆ˜ë¥¼ ì ìš©í•©ë‹ˆë‹¤. attention í•¨ìˆ˜ëŠ” additive attentionê³¼ dot-product attentionì´ ì‚¬ìš©ë©ë‹ˆë‹¤. additive attentionì€ single hidden layerì™€ í•¨ê»˜ feed-forward networkì— compatibility functionì„ ê³„ì‚°í•˜ëŠ”ë° ì‚¬ìš©ë©ë‹ˆë‹¤. dot-product attentionì´ ì¢€ ë” ë¹ ë¥´ê³  ì‹¤ì œë¡œ space-efficientí•©ë‹ˆë‹¤. ì™œëƒí•˜ë©´ optimized matrix multiplication codeë¥¼ ì‚¬ìš©í•´ì„œ êµ¬í˜„ë˜ì—ˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. $d_k$ê°€ ì‘ì€ ê²½ìš° additive attentionì´ dot product attentionë³´ë‹¤ ì„±ëŠ¥ì´ ì¢‹ìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ $d_k$ê°€ í° ê°’ì¸ ê²½ìš° softmax í•¨ìˆ˜ì—ì„œ ê¸°ìš¸ê¸° ë³€í™”ê°€ ê±°ì˜ ì—†ëŠ” ì˜ì—­ìœ¼ë¡œ ì´ë™í•˜ê¸° ë•Œë¬¸ì— dot productë¥¼ ì‚¬ìš©í•˜ë©´ì„œ $d_k$ìœ¼ë¡œ ë‚˜ëˆ„ì–´ scalingì„ ì ìš©í–ˆìŠµë‹ˆë‹¤. Multi-Head Attention $d_{model}$ dimensionì˜ query, key, value ë“¤ë¡œ í•˜ë‚˜ì˜ attentionì„ ìˆ˜í–‰í•˜ëŠ” ëŒ€ì‹ , query, key, valueë“¤ì— ê°ê° í•™ìŠµëœ linear projectionì„ $h$ë²ˆ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ ë” ì¢‹ìŠµë‹ˆë‹¤. ì¦‰, Q,K,Vì— ê°ê° ë‹¤ë¥¸ weightë¥¼ ê³±í•´ì£¼ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ë•Œ, projectionì´ë¼ í•˜ëŠ” ì´ìœ ëŠ” ê°ê°ì˜ ê°’ë“¤ì´ parameter matrixì™€ ê³±í•´ì¡Œì„ ë•Œ, $d_k,d_v,d_{model}$ì°¨ì›ìœ¼ë¡œ projectë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. query, key, valueë“¤ì„ ë³‘ë ¬ì ìœ¼ë¡œ attention functionì„ ê±°ì³ dimensionì´ $d_v$ì¸ output ê°’ìœ¼ë¡œ ë‚˜ì˜¤ê²Œ ë©ë‹ˆë‹¤. ì´í›„ ì—¬ëŸ¬ê°œì˜ headë¥¼ concatenateí•˜ê³  ë‹¤ì‹œ $W^O$ì™€ projectioní•˜ì—¬ dimensionì´ $d_{model}$ì¸ output ê°’ìœ¼ë¡œ ë‚˜ì˜¤ê²Œ ë©ë‹ˆë‹¤. Position-wise Feed-Forward Networksì¸ì½”ë”ì™€ ë””ì½”ë”ëŠ” fully connected feed-forward networkë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ë˜í•œ, ë‘ ë²ˆì˜ linear transformationsê³¼ activation function ReLUë¡œ êµ¬ì„±ë˜ì–´ì§‘ë‹ˆë‹¤. ê°ê°ì˜ positionë§ˆë‹¤ ê°™ì€ $W,b$ ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ layerê°€ ë‹¬ë¼ì§€ë©´ ë‹¤ë¥¸ parameterë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. Positional Encodingëª¨ë¸ì´ recurrenceì™€ convolutionì„ ì‚¬ìš©í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ë¬¸ì¥ì•ˆì— ìƒëŒ€ì ì¸ í˜¹ì€ ì ˆëŒ€ì ì¸ ìœ„ì¹˜ì˜ tokenë“¤ì— ëŒ€í•œ ì •ë³´ë¥¼ ì£¼ì…í•´ì•¼ë§Œ í–ˆìŠµë‹ˆë‹¤. ì´í›„ positional encodingì´ë¼ëŠ” ê²ƒì„ encoderì™€ decoder stack ë°‘ input embeddingì— ë”í•´ì¤¬ìŠµë‹ˆë‹¤. positional encodingì€ $d_{model}$ì¸ dimensionì„ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸ì— ë‘˜ì„ ë”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. $pos$ëŠ” position, $i$ëŠ” dimension $pos$ëŠ” sequenceì—ì„œ ë‹¨ì–´ì˜ ìœ„ì¹˜ì´ê³  í•´ë‹¹ ë‹¨ì–´ëŠ” $i$ì— 0ë¶€í„° $2d_{model}$ê¹Œì§€ ëŒ€ì…í•´ dimensionì´ $d_{model}$ì¸ positional encoding vectorë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Why Self-Attention Self-Attentionì„ ì‚¬ìš©í•˜ëŠ” ì²« ë²ˆì§¸ ì´ìœ ëŠ” layerë§ˆë‹¤ total computational complexityê°€ ì‘ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ë‘ ë²ˆì§¸ ì´ìœ ëŠ” computationì˜ ì–‘ì´ parallelizedí•˜ê¸°ë•Œë¬¸ì— sequential operationì˜ minimumìœ¼ë¡œ ì¸¡ì •ë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì„¸ ë²ˆì§¸ ì´ìœ ë¡œëŠ” ë„¤íŠ¸ì›Œí¬ì—ì„œì˜ long-range dependenciesì‚¬ì´ì˜ path lengthë•Œë¬¸ì…ë‹ˆë‹¤. long-range dependenciesë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì€ ë§ì€ ë¬¸ì¥ ë²ˆì—­ ë¶„ì•¼ì—ì„œì˜ key challengeê°€ ë©ë‹ˆë‹¤. input sequenceì™€ output sequenceì˜ ê¸¸ì´ê°€ ê¸¸ì–´ì§€ë©´ ë‘ positionê°„ì˜ ê±°ë¦¬ê°€ ë©€ì–´ì ¸ long-range dependenciesë¥¼ í•™ìŠµí•˜ëŠ”ë° ì–´ë ¤ì›Œì§‘ë‹ˆë‹¤. í…Œì´ë¸”ì„ ë³´ë©´ Recurrent layerì˜ ê²½ìš° Sequential operationì—ì„œ $O(n)$ì´ í•„ìš”í•˜ì§€ë§Œ Self-Attentionì˜ ê²½ìš° ìƒìˆ˜ì‹œê°„ì— ì‹¤í–‰ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ Self-Attentionì€ interpretable(ì„¤ëª…ê°€ëŠ¥í•œ) modelì¸ ê²ƒì´ ì´ì ì…ë‹ˆë‹¤. Traning OptimizerAdam optimizer ì— íŒŒë¼ë¯¸í„°ë¡œ $\\beta_1&#x3D;0.9, \\beta_2&#x3D;0.98, \\epsilon&#x3D;10^{-9}$ ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. í•™ìŠµë™ì•ˆ ì•„ë˜ì˜ ê³µì‹ì„ í†µí•´ learning rateë¥¼ ë³€í™”ì‹œì¼°ìŠµë‹ˆë‹¤. ì´ëŠ” warmup_stepì— ë”°ë¼ linearí•˜ê²Œ ì¦ê°€ì‹œí‚¤ê³  step numberì— ë”°ë¼ square rootí•œ ê°’ì„ í†µí•´ ì ì§„ì ìœ¼ë¡œ ì¤„ì—¬ê°”ìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  warmup_step &#x3D; 4000ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. Residual Dropoutê° sub-layerì—ì„œ inputì„ ë”í•˜ëŠ” ê²ƒê³¼ normalizationì„ í•˜ê¸°ì „ì— outputì— dropoutì„ ì„¤ì •í–ˆìŠµë‹ˆë‹¤. ë˜í•œ encoderì™€ decoder stacksì— embeddingì˜ í•©ê³„ì™€ positional encodingì—ë„ dropoutì„ ì„¤ì •í–ˆìŠµë‹ˆë‹¤. dropout rate $P_{drop}&#x3D;0.1$ ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. Label Smoothingí•™ìŠµí•˜ëŠ” ë™ì•ˆ label smoothing value $\\epsilon_{ls}&#x3D;0.1$ì„ ì ìš©í–ˆìŠµë‹ˆë‹¤. Result Machine Translation ì˜ì–´â†’ë…ì¼ì–´ ë²ˆì—­ì—ì„œëŠ” ê¸°ì¡´ ëª¨ë¸ë“¤ë³´ë‹¤ ë†’ì€ ì ìˆ˜ê°€ ë‚˜ì™”ê³  ì˜ì–´â†’í”„ë‘ìŠ¤ì–´ ë²ˆì—­ì—ì„œëŠ” single ëª¨ë¸ë³´ë‹¤ ì¢‹ê³  ensemble ëª¨ë¸ë“¤ê³¼ ë¹„ìŠ·í•œ ì„±ëŠ¥ì„ ë‚´ì£¼ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ ì¤‘ìš”í•œ ì ì€ Training Costì¸ë° ê¸°ì¡´ ëª¨ë¸ë“¤ë³´ë‹¤ í›¨ì”¬ ì ì€ Costê°€ ë“¤ì–´ê°€ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Model Variations (A)ë¥¼ ë³´ë©´ single-head attentionì€ head&#x3D;16ì¼ë•Œë³´ë‹¤ 0.9 BLEU ë‚®ê³  head&#x3D;32ë¡œ ëŠ˜ë ¸ì„ ë•Œë„ head&#x3D;16ì¼ë•Œë³´ë‹¤ BLEUê°€ ë‚®ìŠµë‹ˆë‹¤. (B)ë¥¼ ë³´ë©´ dkë¥¼ ë‚®ì¶”ëŠ” ê²ƒì´ model qualityë¥¼ ë‚®ì¶”ê²Œ í•©ë‹ˆë‹¤. (C), (D)ë¥¼ ë³´ë©´ ë” í° ëª¨ë¸ì¼ìˆ˜ë¡ ì¢‹ê³ , dropoutì´ overfittingì„ í”¼í•˜ëŠ”ë° ë„ì›€ì´ ë˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. (E)ë¥¼ ë³´ë©´ sinusoidal positionëŒ€ì‹  learned positional embeddingsë¥¼ ë„£ì—ˆì„ ë•Œì˜ ê²°ê³¼ê°€ base modelê³¼ ë™ì¼í•œ ê²°ê³¼ì¸ ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Conclusion ì¬ê·€ êµ¬ì¡° ì—†ì´ Multi-headed Self-attention ìœ¼ë¡œ ì¸ì½”ë”-ë””ì½”ë”ë¥¼ ëŒ€ì²´í•œ Transformer ëª¨ë¸ì„ ì œì‹œí•˜ì˜€ìŠµë‹ˆë‹¤. ì¬ê·€êµ¬ì¡°ê°€ ì—†ìœ¼ë¯€ë¡œ recurrent ë˜ëŠ” convolutional ë ˆì´ì–´ ê¸°ë°˜ ëª¨ë¸ë³´ë‹¤ ë¹ ë¥´ê²Œ í•™ìŠµì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•´ë‹¹ ëª¨ë¸ì€ WMT 2014 ì˜ì–´â†’ë…ì–´, ì˜ì–´â†’ë¶ˆì–´ ë²ˆì—­ ë¶„ì•¼ì—ì„œ ê¸°ì¡´ ëª¨ë“  ì•™ìƒë¸” ëª¨ë¸ë“¤ì„ ëŠ¥ê°€í•˜ëŠ” SOTAë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. Link: Attention is all you need","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"NLP","slug":"Paper/NLP","permalink":"https://jmj3047.github.io/categories/Paper/NLP/"}],"tags":[{"name":"Transformer","slug":"Transformer","permalink":"https://jmj3047.github.io/tags/Transformer/"},{"name":"Attention","slug":"Attention","permalink":"https://jmj3047.github.io/tags/Attention/"},{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"NLP","slug":"Paper/NLP","permalink":"https://jmj3047.github.io/categories/Paper/NLP/"}]},{"title":"Light Gradient Boosting Machine","slug":"LGBM","date":"2022-05-09T15:00:00.000Z","updated":"2023-02-23T01:34:43.368Z","comments":true,"path":"2022/05/10/LGBM/","link":"","permalink":"https://jmj3047.github.io/2022/05/10/LGBM/","excerpt":"","text":"1. DefinitionEnsembleâ†’ ì—¬ëŸ¬ ì˜ˆì¸¡ê¸°ë¥¼ ìˆ˜ì§‘í•´ì„œ ë‹¨ì¼ ì˜ˆì¸¡ê¸° ë³´ë‹¤ ë” ì¢‹ì€ ì˜ˆì¸¡ê¸°ë¥¼ ë§Œë“œëŠ” ê²ƒ. ì¼ë°˜ì ìœ¼ë¡œ ì•™ìƒë¸” ê¸°ë²•ì„ ì‚¬ìš©í•˜ë©´ , ì˜ˆì¸¡ê¸° í•˜ë‚˜ë¡œ í›ˆë ¨í•˜ì˜€ì„ë•Œ ë³´ë‹¤ , í¸í–¥ì€ ë¹„ìŠ·í•˜ì§€ë§Œ ë¶„ì‚°ì´ ì¤„ì–´ë“ ë‹¤ê³  ì•Œë ¤ì ¸ ìˆë‹¤. ë°°ê¹…(bagging) ì›ë°ì´í„° ì§‘í•©ìœ¼ë¡œë¶€í„° í¬ê¸°ê°€ ê°™ì€ í‘œë³¸ì„ ì—¬ëŸ¬ ë²ˆ ë‹¨ìˆœì„ì˜ ë³µì›ì¶”ì¶œí•˜ì—¬ ê° í‘œë³¸(ë¶“ìŠ¤íŠ¸ë© í‘œë³¸) ì— ëŒ€í•´ ë¶„ë¥˜ê¸°ë¥¼ ìƒì„±í•œ í›„ ê·¸ ê²°ê³¼ë¥¼ ì•™ìƒë¸”í•˜ëŠ” ë°©ë²• ë°˜ë³µì¶”ì¶œ ë°©ë²•ì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ê°™ì€ ë°ì´í„°ê°€ í•œ í‘œë³¸ì— ì—¬ëŸ¬ ë²ˆ ì¶”ì¶œë  ìˆ˜ë„ ìˆê³ , ì–´ë–¤ ë°ì´í„°ëŠ” ì¶”ì¶œë˜ì§€ ì•Šì„ìˆ˜ë„ ìˆë‹¤. ë¶€ìŠ¤íŒ…(boosting) ë°°ê¹…ì˜ ê³¼ì •ê³¼ ìœ ì‚¬í•˜ë‚˜ ë¶“ìŠ¤íŠ¸ë© í‘œë³¸ì„ êµ¬ì„±í•˜ëŠ” sampling ê³¼ì •ì—ì„œ ê° ìë£Œì— ë™ì¼í•œ í™•ìœ¨ì„ ë¶€ì—¬í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ë¶„ë¥˜ê°€ ì˜ëª»ëœ ë°ì´í„°ì—- ë” í° ê°€ì¤‘ì„ ì£¼ì–´ í‘œë³¸ì„ ì¶”ì¶œí•œë‹¤. Bagging ê³¼ Boosting ì˜ ì°¨ì´ Bagging ì€ ë…ë¦½ëœ ì˜ˆì¸¡ê¸°ë¥¼ í†µí•´ ë” ë‚˜ì€ ì˜ˆì¸¡ê¸°ë¥¼ ì–»ëŠ”ë‹¤ Boosting ì€ ì•ì˜ ì˜ˆì¸¡ê¸°ë¥¼ ë³´ì™„í•´ ë‚˜ê°€ë©´ì„œ ë” ë‚˜ì€ ì˜ˆì¸¡ê¸°ë¥¼ ì–»ëŠ”ë‹¤ ëœë¤í¬ë ˆìŠ¤íŠ¸(random forest) ë°°ê¹…ì— ëœë¤ ê³¼ì •ì„ ì¶”ê°€í•œ ë°©ë²•ì´ë‹¤. ê° ë…¸ë“œë§ˆë‹¤ ëª¨ë“  ì˜ˆì¸¡ë³€ìˆ˜ ì•ˆì—ì„œ ìµœì ì˜ ë¶„í• ì„ ì„ íƒí•˜ëŠ” ë°©ë²• ëŒ€ì‹  ì˜ˆì¸¡ë³€ìˆ˜ë“¤ì„ ì„ì˜ë¡œ ì¶”ì¶œí•˜ê³ , ì¶”ì¶œëœ ë³€ìˆ˜ ë‚´ì—ì„œ ìµœì ì˜ ë¶„í• ì„ ë§Œë“¤ì–´ ë‚˜ê°€ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•œë‹¤. ë¶€íŠ¸ìŠ¤íŠ¸ë©(Bootstrap) ë¶€íŠ¸ìŠ¤íŠ¸ë©ì€ í‰ê°€ë¥¼ ë°˜ë³µí•œë‹¤ëŠ” ì¸¡ë©´ì—ì„œ êµì°¨ê²€ì¦ê³¼ ìœ ì‚¬í•˜ë‚˜, í›ˆë ¨ìš© ìë£Œë¥¼ ë°˜ë³µ ì¬ì„ ì •í•œë‹¤ëŠ” ì ì—ì„œ ì°¨ì´ê°€ ìˆë‹¤. ì¦‰ ë¶€íŠ¸ìŠ¤íŠ¸ë©ì€ ê´€ì¸¡ì¹˜ë¥¼ í•œë²ˆ ì´ìƒ í›ˆë ¨ìš© ìë£Œë¡œ ì‚¬ìš©í•˜ëŠ” ë³µì›ì¶”ì¶œë²•ì— ê¸°ë°˜í•œë‹¤. ë¶€íŠ¸ìŠ¤íŠ¸ë©ì€ ì „ì²´ ë°ì´í„°ì˜ ì–‘ì´ í¬ì§€ì•Šì€ ê²½ìš°ì˜ ëª¨í˜•í‰ê°€ì— ê°€ì¥ ì í•©í•˜ë‹¤. 2. GBM Gradient Boosting Machine(GBM)ì€ Ensemble Learningì˜ ì¼í™˜ Gradient Boosting&#x3D;Gradient Descent+Boosting Gradient Descent ì²«ë²ˆì§¸ ë°ì´í„°ì—ì„œ ì˜ ëª» ë§ì¶˜ ë°ì´í„°ë“¤ì— ê°€ì¤‘ì¹˜ë¥¼ ì£¼ì–´, ë‘ë²ˆì§¸ ëª¨ë¸ ì—ì„œëŠ” ë” ë§ì€ì–‘ì„ ë§Œë“¤ì–´ ì¤€ë‹¤. ë˜, ë‘ë²ˆì§¸ ëª¨ë¸ì—ì„œ ì˜ëª» ë§¤ì¹­í•œ ë°ì´í„°ë“¤ ì—ê²Œ ê°€ì¤‘ì¹˜ë¥¼ ì£¼ì–´ì„œ, ì„¸ë²ˆì§¸ ëª¨ë¸ì„ ë§Œë“¤ì–´ì£¼ëŠ” ê·¸ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œ ëª¨ë¸ì„ ë°˜ë³µí•œë‹¤. Fit an addictive model(ensemble) in a foward stage-wise manner bagging ì²˜ëŸ¼ í•œë²ˆì— ë”± í•™ìŠµì„ ì‹œí‚¤ëŠ”ê²Œ ì•„ë‹ˆê³  ìœ„ì— ì–¸ê¸‰í•œ ê²ƒ ì²˜ëŸ¼ í•˜ë‚˜ì”© í•˜ë‚˜ì”© ë”í•´ê°€ë©´ì„œ ëª¨ë¸ì„ í•™ìŠµ ì‹œì¼œë‚˜ê°€ëŠ” ëª¨ë¸ Adaptive boosting ëª¨ë¸ì„ ê±°ë“­í• ìˆ˜ë¡, weak leanerê°€ ë§Œë“¤ì–´ ì§€ë©´ì„œ ì´ì „ì— ê°€ì¡Œë˜ ì˜¤ë¥˜ì— ëŒ€í•´ í•´ê²°í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì„ ë§Œë“¦ ì´ëŸ¬í•œ weak learner ë¥¼ ë§Œë“¤ê¸° ìœ„í•´ , ì•ì„œ ë³´ì˜€ë˜ ëª¨ë¸ì˜ shortcoming ì„ ë” ë§ì´ ìƒ˜í”Œë§ í•˜ë¼ëŠ” ëœ» Gradient Boosting ê·¸ë¼ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…ì€, Regression,Classification,Ranking ì„ ë‹¤ í•  ìˆ˜ ìˆë‹¤. ì´ ì…‹ì˜ ë‹¤ë¥´ê¸°ëŠ” loss function ì—ì„œ ì°¨ì´ê°€ ë‚  ë¿, concept ì€ ë™ì¼ ğŸ’¡ Regression ìœ¼ë¡œ ì„¤ëª…í•˜ê¸°ê°€ ê°€ì¥ ì§ê´€ì ì´ê¸° ë•Œë¬¸ì—, Regression ëª¨ë¸ë¡œì¨ concept ì„ ì„¤ëª…í•˜ê² ë‹¤. ì–˜ëŠ” adaptive boosting ê³¼ëŠ” ë‹¬ë¦¬, ìƒ˜í”Œë§ì„ ë”°ë¡œ ì‹œí–‰í•˜ì§€ëŠ” ì•ŠëŠ”ë‹¤. ì”ì°¨ë¥¼ ëª©í‘¯ê°’ (y) ìœ¼ë¡œ ë†“ê³  ê³„ì†í•´ì„œ ë°˜ë³µí•˜ë©´ì„œ ì”ì°¨ì— ëŒ€í•œ ì‹ì„ ë§Œë“¤ì–´ ë‚¸ë‹¤. ì”ì°¨ë¥¼ ëª©í‘œê°’ìœ¼ë¡œ ì¡ì•„ë‘ë©´, ì•ì„  ëª¨ë¸ì´ ë§ì¶”ì§€ ëª»í•œ ë§Œí¼ë§Œ ë§ì¶”ë ¤ê³  ë…¸ë ¥ì„ í•˜ê¸°ë•Œë¬¸ì—, ì•ì„  ëª¨ë¸ì˜ ê²°ê³¼ë¬¼ê³¼ ë’· ëª¨ë¸ì˜ ê²°ê³¼ë¬¼ì„ ë”í•˜ë©´ ì •ë‹µì´ ë‚˜ì˜¨ë‹¤. ê²½ì‚¬ë„ë¥¼ í†µí•´ì„œ Weak learner ë¥¼ boosting ì‹œí‚´ ì†ì‹¤í•¨ìˆ˜ì˜ gradient(ê²½ì‚¬ë„) ê°€ 0 ì— ê°€ê¹Œìš¸ë•Œ ê¹Œì§€ ë¯¸ë¶„ì„ í•´ì¤€ë‹¤. Gradient ê°€ 0ì´ ì•„ë‹ˆë¼ë©´ weight ë¥¼ gradinet ì˜ ë°˜ëŒ€ë°©í–¥ìœ¼ë¡œ ì›€ì§ì´ë˜ ì–¼ë§ˆë§Œí¼ ì›€ì§ì´ëƒì— ë”°ë¼ì„œ ë‹¬ë¼ì§€ë‹ˆ ì¡°ê¸ˆì”© ì›€ì§ì¸ë‹¤. ì²˜ìŒ, decision tree ë¡œ split point ë¥¼ ì¡ì•„, regression í•´ì¤€ ë¶€ë¶„ì˜ ì”ì°¨ë¥¼ ë³´ë©´ ë†’ì€ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. GBMì—ì„œëŠ” ì´ ì”ì°¨ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë˜ Split point ë¥¼ ì¡ì•„ì£¼ë©´ì„œ(ì´ëŸ¬í•œ ê³¼ì •ì—ì„œ ì†ì‹¤í•¨ìˆ˜ê°€ ë“¤ì–´ê°€ë©° ì†ì‹¤í•¨ìˆ˜ì˜ gradient ë¥¼ ì¤„ì—¬ë‚˜ê°€ëŠ” ê³¼ì •ì—ì„œ gradient descent ê°œë…ì´ ë“¤ì–´ê°€ëŠ” ê²ƒ) ì ì  ì”ì°¨ë¥¼ ì¤„ì—¬ ë‚˜ê°€ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ğŸ’¡ ì²˜ìŒì—ëŠ” íšŒê·€ì‹ì´ ì•ˆì¢‹ê²Œ ë‚˜ì˜¤ëŠ”ë° iteration ì´ ë°˜ë³µ ë ìˆ˜ë¡ íšŒê·€ì‹ì´ ì¢‹ì•„ì§€ëŠ”ê²ƒìœ¼ë¡œ ë³¼ ìˆ˜ ìˆë‹¤. Overfitting problem in GBM GBM ì˜ ê°€ì¥í° ë¬¸ì œì ì€ ì˜¤ì°¨ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ì„ í˜•ì„± í•˜ê¸°ë•Œë¬¸ì—(ì• ì´ˆì— ëª¨ë¸ ìì²´ê°€ ë°˜ë³µì„ ê±°ë“­ í• ìˆ˜ë¡, ì „ ëª¨ë¸ì˜ ì˜¤ì°¨ë¥¼ ì¤„ì—¬ë‚˜ê°€ëŠ” ëª¨ë¸ì´ê¸° ë•Œë¬¸ì— ì˜¤ë²„í”¼íŒ… ë¬¸ì œëŠ” í•„ì—°ì ) ìš°ë¦¬ê°€ ì–´ì°Œ í• ìˆ˜ ì—†ëŠ” ì˜¤ì°¨ê¹Œì§€ë„ ëª¨ë¸ì— í•™ìŠµì‹œí‚¤ì–´ì„œ ì˜¤ë²„í”¼íŒ… ë¬¸ì œë¥¼ ë¶ˆëŸ¬ ì¼ìœ¼í‚¨ë‹¤ ê³¼ì í•© í•´ê²°ë²• Subsamplingâ†’ê°ê°ì˜ ëª¨ë¸ì„ ë§Œë“¤ë•Œ ìƒ˜í”Œë§ì„ ëœë¤ìœ¼ë¡œ 80% ë§Œí•´ì„œ ëª¨ë¸ì„ ë§Œë“¤ì–´ì¤€ë‹¤ Shrinkage- original ì•Œê³ ë¦¬ì¦˜ë“¤ì€ ì „ì— ë§Œë“¤ì–´ì§„ ëª¨ë¸ë“¤ê³¼ ë’¤ë¡œ ê°ˆìˆ˜ë¡ ë§Œë“¤ì–´ì§€ëŠ” ëª¨ë¸ë“¤ì—, ì˜í–¥ë ¥ì´ ë™ë“±í–ˆëŠ”ë° , shrinkage ë¥¼ ì“°ë©´ , ë’¤ì— ë§Œë“¤ì–´ì§€ëŠ” ëª¨ë¸ë“¤ì— ëŒ€í•´ì„œ , ê°€ì¤‘ì¹˜ë¥¼ ì ê²Œ ë‘ì–´ ë§Œë“¤ì–´ì¤€ë‹¤. Early Stopping- validation error ê°€ ì¦ê°€ í•  ê²ƒ ê°™ìœ¼ë©´ ë¯¸ë¦¬ ì¤‘ì§€ë¥¼ ì‹œí‚¤ëŠ”ê²ƒ Information Gain:Split pointë¥¼ í†µí•´ì„œ ì–¼ë§ˆë‚˜ í˜¼ì¡ë„,ë¶ˆìˆœë„ê°€ ë‚®ì•„ì§€ëŠ”ê°€. Information Gain ì„ í†µí•´ ê·¸ ë³€ìˆ˜ì˜ ì˜í–¥ë„ë¥¼ ì²´í¬ í•  ìˆ˜ ìˆë‹¤. 3. LightGBMGOSS ëª¨ë“  í”¼ì³ë“¤ì„ ê²€ì‚¬í•˜ë©´ ì‹œê°„ì´ ë§ì´ ê±¸ë¦¬ê¸° ë•Œë¬¸ì— ì´ë¥¼ ë§‰ê¸°ìœ„í•´ì„œ, Gradient-based One-side Sampling (GOSS) ë¥¼ ì‚¬ìš©â†’ Large gradient ëŠ” keep í•˜ê³  small gradient ëŠ” ë“œë í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ, 1000ê°œ ë°ì´í„°ë¥¼ ëª¨ë‘ íƒìƒ‰í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ gradient ê°€ í° ê²ƒ ìœ„ì£¼ë¡œ íƒìƒ‰í•˜ëŠ” ë°©ì‹ â†’ íƒìƒ‰íšŸìˆ˜ë¥¼ ì¤„ì´ëŠ” ê²ƒ EFB (Exclusive Feature Bundleing) ëª¨ë“ í”¼ì³ë¥¼ íƒìƒ‰í•  í•„ìš”ë¥¼ ì—†ì• ëŠ” ê²ƒ Bundle ì„ ì°¾ëŠ” ë°©ë²• Graph coloring problem ìœ¼ë¡œ í•´ê²°ê°€ëŠ¥ ê°ê°ì˜ ë…¸ë“œëŠ” í”¼ì³ì´ê³ , edge ëŠ” í”¼ì³ë“¤ê°„ì˜ conflict â†’ conflict ê°€ ë§ì€ ì• ë“¤ì€ ì¤‘ë³µì´ ë§ì´ ë“¤ì–´ê°€ì„œ bundling ì´ ë˜ë©´ ì•ˆë¨ conflict ê°€ ì—†ëŠ” ì• ë“¤ ë¼ë¦¬ëŠ” bundling ì„ í•´ë„ ë¨ Greed bundling ê³„ì‚°ë²• edge ì˜ ê°•ë„: ë‘ ë³€ìˆ˜ì˜ conflict ê°•ë„ edge: ë™ì‹œì— 0ì´ì•„ë‹Œ ê°ì²´ì˜ ìˆ˜. Degree ì‹œì‘ì ì„ degreeì˜ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë¦¬ í•´ì¤€ ë‹¤ìŒ, degreeê°€ ë†’ì€ê²ƒ ë¶€í„° ì‹œì‘í•œë‹¤. cutoff ëŠ” hyperparameter ì¸ë°, cut-offê°€ 0.2ë¼ëŠ” ë§ì€, N&#x3D;10 ì´ê¸° ë•Œë¬¸ì— 2íšŒ ì´ìƒ Nonzero value ê°€ ê²¹ì¹˜ê²Œ ë˜ë©´, bundleingì´ ì•ˆë˜ëŠ” ê²ƒ. cut-off ê¸°ì¤€ì— ë§ì§€ ì•Šê¸° ë•Œë¬¸ì— x5ëŠ” ê³ ë¦½ì´ ëœë‹¤. feature merge ë¥¼ ì‰½ê²Œ í•´ì£¼ê¸° ìœ„í•´ featureì˜ ìœ„ì¹˜ë¥¼ ì‚´ì§ ì¡°ì • í•˜ì—¬ì¤€ë‹¤. feature ë¥¼ merge í•˜ëŠ”ë°©ë²•. Add. offset add offsetâ†’bundling ì„ í•˜ê¸°ìœ„í•œ ëŒ€ìƒì´ ë˜ëŠ” ë³€ìˆ˜ì—ë‹¤ê°€ ê¸°ì¤€ì´ ë˜ëŠ” ë³€ìˆ˜ê°€ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ìµœëŒ€ ê°’ì„ ë”í•´ì¤€ë‹¤. conflict ê°€ ì¼ì–´ë‚œ ë¶€ë¶„ì€ ê·¸ëŒ€ë¡œ ê¸°ì¤€ ë³€ìˆ˜ê°€ ê°€ì§€ëŠ” ê°’ì„ ë”í•´ì¤€ë‹¤. Reference: ê³ ë ¤ëŒ€í•™êµ ì‚°ì—…ê²½ì˜ê³µí•™ë¶€ DSBA ì—°êµ¬ì‹¤","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}],"tags":[{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}]},{"title":"Making Chatbot with doc2vec tutorial(1)","slug":"Making_Chatbot_with_doc2vec_tutorial(1)","date":"2022-05-05T15:00:00.000Z","updated":"2022-10-15T08:07:56.742Z","comments":true,"path":"2022/05/06/Making_Chatbot_with_doc2vec_tutorial(1)/","link":"","permalink":"https://jmj3047.github.io/2022/05/06/Making_Chatbot_with_doc2vec_tutorial(1)/","excerpt":"","text":"ëª¨ë¸ ë§Œë“¤ê¸°ë°ì´í„° ë§Œë“¤ê¸°doc2vecì„ ì´ìš©í•´ì„œ FAQë°ì´í„°ë“¤ì˜ ì§ˆë¬¸ë“¤ì„ ë²¡í„°í™”í•˜ëŠ” ëª¨ë¸ì„ ë§Œë“¤ì–´ ë³¸ë‹¤. word2vecì´ ë‹¨ì–´ë¥¼ ë²¡í„°í™” í•˜ëŠ” ê²ƒì´ë¼ë©´ doc2vecì€ ë‹¨ì–´ê°€ ì•„ë‹ˆë¼ ë¬¸ì„œë¥¼ ê¸°ì¤€ìœ¼ë¡œ (ì—¬ê¸°ì„œëŠ” ë¬¸ì¥)ë²¡í„°ë¥¼ ë§Œë“œëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì´ë‹¤. doc2vecì„ ì‚¬ìš©í•˜ë©´ ì„œë¡œ ë‹¤ë¥¸ ë¬¸ì„œë“¤ì´ ê°™ì€ ì°¨ì›ì˜ ë²¡í„°ê°’ì„ ê°–ê²Œ ëœë‹¤. ê° ë¬¸ì„œë¼ ê°–ëŠ” ë²¡í„°ê°’ì„ ë¹„êµí•´ ê°™ìœ¼ë©´ ê°™ì„ ìˆ˜ë¡ ìœ ì‚¬í•œ ë¬¸ì„œë¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ doc2vecì„ ì´ìš©í•´ FAQì˜ ì§ˆë¬¸ë“¤ì„ ë²¡í„°í™” í•œë‹¤ë©´ ì–´ë–¤ ì§ˆë¬¸ì´ ë“¤ì–´ì™”ì„ ë•Œ ë™ì¼ ëª¨ë¸ë¡œ ì§ˆë¬¸ì„ ë²¡í„°í™” í•œë‹¤ìŒ, ì €ì¥ë¼ ìˆëŠ” ì§ˆë¬¸ë“¤ì˜ ë²¡í„°ì™€ ë¹„êµí•´ì„œ ê°€ì¥ ìœ ì‚¬í•œ ì§ˆë¬¸ì„ ì°¾ì„ ìˆ˜ ìˆë‹¤. ê°€ì¥ ìœ ì‚¬í•œ ì§ˆë¬¸ì„ ì°¾ì€ ë‹¤ìŒ ê·¸ ì§ˆë¬¸ì˜ ë‹µì„ ì¶œë ¥í•˜ë©´ FAQì±—ë´‡ì„ ë§Œë“¤ ìˆ˜ ìˆë‹¤. **GPUì‚¬ìš© í•„ìˆ˜..! 1234567891011import osimport warningsfrom gensim.models import doc2vecfrom gensim.models.doc2vec import TaggedDocumentfaqs = [[&quot;1&quot;, &quot;ë‹¹í•´ë…„ë„ ë‚©ì…ì•¡ì€ ìˆ˜ì • ê°€ëŠ¥ í•œê°€ìš”?&quot;, &quot;ë„¤, ë‹¹í•´ë…„ë„ ë‚©ì…ì•¡ì€ 12464 í™”ë©´ ë“±ë¡ì „ê¹Œì§€ ìˆ˜ì • ê°€ëŠ¥í•©ë‹ˆë‹¤.&quot;], [&quot;2&quot;, &quot;ëŒ€ë¦¬ì¸í†µë³´ ëŒ€ìƒê³„ì¢Œ ê¸°ì¤€ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?&quot;, &quot;ëª¨ê³„ì¢Œ ê¸°ì¤€ ê°€ì¥ ìµœê·¼ì— ê°œì„¤ëœ ê³„ì¢Œì˜ ê´€ë¦¬ì ì—ì„œ ì¡°íšŒ ë©ë‹ˆë‹¤. ì˜ì›íì‡„ëœ ìê³„ì¢ŒëŠ” ì¡°íšŒëŒ€ìƒ ê³„ì¢Œì—ì„œ ì œì™¸ë©ë‹ˆë‹¤. ê³„ì¢Œì£¼ ê³„ì¢Œê°€ ì‚¬ì ˆì› ê³„ì¢Œê°€ ì•„ë‹Œ ê²½ìš°ë§Œ ì¡°íšŒë©ë‹ˆë‹¤&quot;], [&quot;3&quot;, &quot;ë“±ë¡ê°€ëŠ¥ ë‹¨ë§ê¸°ìˆ˜ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?&quot;, &quot;5ëŒ€ê¹Œì§€ ë“±ë¡ ê°€ëŠ¥ì…ë‹ˆë‹¤.&quot;], [&quot;4&quot;, &quot;ëª¨ë°”ì¼ê³„ì¢Œê°œì„¤ ê°€ëŠ¥í•œ ì‹œê°„ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?&quot;, &quot;08:00 ~ 20:00(ì˜ì—…ì¼ë§Œ ê°€ëŠ¥&quot;], [&quot;5&quot;, &quot;ë¯¸êµ­ì¸ì¼ë•Œ ë¯¸êµ­ë‚©ì„¸ìë“±ë¡ë²ˆí˜¸ ì‘ì„± ë°©ë²•ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?&quot;, &quot;ê³„ì¢Œì£¼ê°€ ë¯¸êµ­ì¸ì¼ ë•Œ ê³„ì¢Œì£¼ì˜ ë¯¸êµ­ë‚©ì„¸ìë“±ë¡ë²ˆí˜¸(ì‚¬íšŒë³´ì¥ë²ˆí˜¸(Social Security Number), ê³ ìš©ì£¼ì‹ë³„ë²ˆí˜¸(Employer Identification Number), ê°œì¸ë‚©ì„¸ìë²ˆí˜¸(Individual Taxpayer Identification Number))ë¥¼ ê¸°ì¬í•©ë‹ˆë‹¤..&quot;]] ìœ„ì™€ ê°™ì´ 5ê°œì˜ FAQë°ì´í„°ë¥¼ ì„ì˜ë¡œ ë§Œë“¤ì—ˆë‹¤. ì´ì œ ì—¬ê¸°ì— 5ê°œì˜ ì§ˆë¬¸ì„ ë²¡í„°í™” í• ê±´ë° ì‚¬ì‹¤ ë²¡í„°í™” í•  ë•Œ ë°ì´í„°ëŠ” ë§ì„ ìˆ˜ë¡ ì¢‹ë‹¤. ì ìœ¼ë©´ ì„œë¡œ ë¶„ê°„ì´ ì˜ ì•ˆë¨. í˜•íƒœì†Œ ë¶„ì„doc2vecìœ¼ë¡œ ë¬¸ì¥ì„ ë²¡í„°í™”í•˜ê¸° ì „ì— ì•½ê°„ì˜ ì „ì²˜ë¦¬ ê³¼ì •ì´ í•„ìš”í•˜ë‹¤. ê° ë¬¸ì¥ì„ tokenizeí•´ì•¼ í•œë‹¤. í† í°í™” í•˜ëŠ” ê³¼ì •ì´ ì˜ì–´ë‘ í•œêµ­ì–´ë‘ ì¡°ê¸ˆ ë‹¤ë¥¸ë° í•œêµ­ì–´ì˜ ê²½ìš° í˜•íƒœì†Œ ë¶„ì„(pos tagging)ì„ í†µí•´ í˜•íƒœì†Œ ë‹¨ìœ„ë¡œ ë‚˜ëˆˆë’¤ í† í°ìœ¼ë¡œ ì‚¬ìš©í•  í˜•íƒœì†Œë¥¼ ê²°ì •í•˜ê³  ë‚˜ëˆˆë‹¤. ì¦‰ ê° ë¬¸ì¥ì„ í˜•íƒœì†Œ ë‹¨ìœ„ì˜ ë°°ì—´ë¡œ ë§Œë“ ë‹¤. í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸°ëŠ” konlpyë¥¼ ì‚¬ìš©í•œë‹¤. 123456789101112#í˜•íƒœì†Œ ë¶„ì„import jpypefrom konlpy.tag import Kkmakkma = Kkma()def tokenize_kkma(doc): jpype.attachThreadToJVM() #ìë°”ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ì†ŒìŠ¤ ì½”ë“œ token_doc = [&#x27;/&#x27;.join(word) for word in kkma.pos(doc) ] #í˜•íƒœì†Œ ë¶„ì„í•œ ë‹¨ì–´ì™€ í˜•íƒœì†Œ ëª…ì„ &#x27;ë‹¨ì–´/í˜•íƒœì†Œ&#x27;í˜•íƒœë¡œ ì¶œë ¥í•˜ê¸° ìœ„í•œ ì½”ë“œ return token_doctokenize_kkma(faqs[0][1]) Kkmaë¥¼ import í•˜ê³  jpypeë„ import í•œë‹¤. jpypeëŠ” íŒŒì´ì¬ì—ì„œ ìë°”ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” íŒ¨í‚¤ì§€ì¸ë° ê¸°ë³¸ì ìœ¼ë¡œ kkmaê°€ ìë°” ë² ì´ìŠ¤ë¼ì„œ ê¼­ í•„ìš”í•˜ë‹¤. Kkma()ë¡œ í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ë¶ˆëŸ¬ì˜¨ë‹¤ìŒ kkma.pos(doc)ë¡œ í˜•íƒœì†Œ ë¶„ì„ì„ í•œë‹¤. 123456789101112ì¶œë ¥ ê²°ê³¼:[&#x27;ë‹¹í•´/NNG&#x27;, &#x27;ë…„ë„/NNM&#x27;, &#x27;ë‚©ì…/NNG&#x27;, &#x27;ì•¡/XSN&#x27;, &#x27;ì€/JX&#x27;, &#x27;ìˆ˜ì •/NNG&#x27;, &#x27;ê°€ëŠ¥/NNG&#x27;, &#x27;í•œ/MDN&#x27;, &#x27;ê°€ìš”/NNG&#x27;, &#x27;?/SF&#x27;] í˜•íƒœì†Œ ë¶„ì„ì„ í•˜ë©´ ë¬¸ì¥ì´ ë‹¨ì–´&#x2F;í˜•íƒœì†Œ í˜•íƒœì˜ ë°°ì—´ë¡œ ì¶œë ¥ëœë‹¤. 1ë²ˆ ë¬¸ì¥ì€ ì´ 10ê°œì˜ í˜•íƒœì†Œë¡œ ë‚˜ë‰˜ì—ˆë‹¤. í˜•íƒœì†Œ ë¶„ì„ê¸°ì¢…ë¥˜ì— ë”°ë¼ ê²°ê³¼ê°€ ì¡°ê¸ˆì”© ë‹¤ë¥¼ìˆ˜ ìˆë‹¤. Doc2Vec ëª¨ë¸ ë§Œë“¤ê¸°Doc2Vecì„ ì´ìš©í•´ ëª¨ë¸ì„ ë§Œë“¤ê¸° ìœ„í•´ì„œëŠ” í† í°í™” ëœ ë¦¬ìŠ¤íŠ¸ì™€ íƒœê·¸ ê°’ì´ í•„ìš”í•˜ë‹¤. ì—¬ê¸°ì„œ íƒœê·¸ëŠ” ë¬¸ì¥ ë²ˆí˜¸. [ë¬¸ì¥ì˜ ë²ˆí˜¸, ë¬¸ì¥ì„ í† í°í™”í•œ ë°°ì—´] ì´ë ‡ê²Œ ë‘ ê°œì˜ ê°’ì„ ê°€ì§„ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•´ doc2vec ëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ ìˆë‹¤. ì‹¤ì œë¡œ ëª¨ë¸ì„ ë§Œë“œëŠ”ë° ì‚¬ìš©í•˜ëŠ” ê±´ í† í° ê°’ì´ì§€ë§Œ ë¹„ìŠ·í•œ ë¬¸ì¥ì´ ë¬´ì—‡ì¸ì§€ ì°¾ê¸° ìœ„í•œ ì¸ë±ìŠ¤ë¡œ íƒœê·¸ ê°’ì„ ì‚¬ìš©í•˜ê²Œ ëœë‹¤. 1234567# ë¦¬ìŠ¤íŠ¸ì—ì„œ ê° ë¬¸ì¥ë¶€ë¶„ í† í°í™”token_faqs = [(tokenize_kkma(row[1]), row[0]) for row in faqs]# Doc2Vecì—ì„œ ì‚¬ìš©í•˜ëŠ” íƒœê·¸ë¬¸ì„œí˜•ìœ¼ë¡œ ë³€ê²½tagged_faqs = [TaggedDocument(d, [c]) for d, c in token_faqs]tagged_faqs 12345[TaggedDocument(words=[&#x27;ë‹¹í•´/NNG&#x27;, &#x27;ë…„ë„/NNM&#x27;, &#x27;ë‚©ì…/NNG&#x27;, &#x27;ì•¡/XSN&#x27;, &#x27;ì€/JX&#x27;, &#x27;ìˆ˜ì •/NNG&#x27;, &#x27;ê°€ëŠ¥/NNG&#x27;, &#x27;í•œ/MDN&#x27;, &#x27;ê°€ìš”/NNG&#x27;, &#x27;?/SF&#x27;], tags=[&#x27;1&#x27;]), TaggedDocument(words=[&#x27;ëŒ€ë¦¬ì¸/NNG&#x27;, &#x27;í†µë³´/NNG&#x27;, &#x27;ëŒ€ìƒ/NNG&#x27;, &#x27;ê³„ì¢Œ/NNG&#x27;, &#x27;ê¸°ì¤€/NNG&#x27;, &#x27;ì€/JX&#x27;, &#x27;ì–´ë–»/VA&#x27;, &#x27;ê²Œ/ECD&#x27;, &#x27;ë˜/VV&#x27;, &#x27;ë‚˜ìš”/EFQ&#x27;, &#x27;?/SF&#x27;], tags=[&#x27;2&#x27;]), TaggedDocument(words=[&#x27;ë“±ë¡/NNG&#x27;, &#x27;ê°€ëŠ¥/NNG&#x27;, &#x27;ë‹¨ë§/NNG&#x27;, &#x27;ê¸°ìˆ˜/NNG&#x27;, &#x27;ëŠ”/JX&#x27;, &#x27;ì–´ë–»/VA&#x27;, &#x27;ê²Œ/ECD&#x27;, &#x27;ë˜/VV&#x27;, &#x27;ë‚˜ìš”/EFQ&#x27;, &#x27;?/SF&#x27;], tags=[&#x27;3&#x27;]), TaggedDocument(words=[&#x27;ëª¨ë°”ì¼/NNG&#x27;, &#x27;ê³„ì¢Œ/NNG&#x27;, &#x27;ê°œì„¤/NNG&#x27;, &#x27;ê°€ëŠ¥/NNG&#x27;, &#x27;í•˜/XSV&#x27;, &#x27;ã„´/ETD&#x27;, &#x27;ì‹œê°„/NNG&#x27;, &#x27;ì€/JX&#x27;, &#x27;ì–´ë–»/VA&#x27;, &#x27;ê²Œ/ECD&#x27;, &#x27;ë˜/VV&#x27;, &#x27;ë‚˜ìš”/EFQ&#x27;, &#x27;?/SF&#x27;], tags=[&#x27;4&#x27;]), TaggedDocument(words=[&#x27;ë¯¸êµ­ì¸/NNG&#x27;, &#x27;ì¼/NNG&#x27;, &#x27;ë•Œ/NNG&#x27;, &#x27;ë¯¸êµ­/NNP&#x27;, &#x27;ë‚©ì„¸ì/NNG&#x27;, &#x27;ë“±ë¡/NNG&#x27;, &#x27;ë²ˆí˜¸/NNG&#x27;, &#x27;ì‘ì„±/NNG&#x27;, &#x27;ë°©ë²•/NNG&#x27;, &#x27;ì€/JX&#x27;, &#x27;ì–´ë–»/VA&#x27;, &#x27;ê²Œ/ECD&#x27;, &#x27;ë˜/VV&#x27;, &#x27;ë‚˜ìš”/EFQ&#x27;, &#x27;?/SF&#x27;], tags=[&#x27;5&#x27;])] TaggedDocument functionì„ ì‚¬ìš©í•˜ë©´ doc2vecì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” íƒœê·¸ ëœ ë¬¸ì„œ í˜•ì‹ìœ¼ë¡œ ë³€ê²½í•œë‹¤. ì¶œë ¥í•´ ë³´ë©´ wordsë°°ì—´ê³¼ tagsê°’ì„ ê°–ëŠ” Dicí˜•íƒœì˜ ìë£Œí˜•ì´ ë˜ì—ˆìŒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. 1234567891011121314151617181920212223# make model import multiprocessing cores = multiprocessing.cpu_count() d2v_faqs = doc2vec.Doc2Vec(vector_size=50, alpha=0.025, min_alpha=0.025, hs=1, negative=0, dm=0, dbow_words = 1, min_count = 1, workers = cores, seed=0) d2v_faqs.build_vocab(tagged_faqs) # train document vectors for epoch in range(10): d2v_faqs.train(tagged_faqs, total_examples = d2v_faqs.corpus_count, epochs = d2v_faqs.epochs ) d2v_faqs.alpha -= 0.0025 # decrease the learning rate d2v_faqs.min_alpha = d2v_faqs.alpha # fix the learning rate, no decay ëª¨ë¸ì„ ë§Œë“¤ê³  í•™ìŠµ ì‹œí‚¨ë‹¤. doc2vecëª¨ë¸ì„ ë§Œë“¤ ë•Œ íŒŒë¼ë¯¸í„°ëŠ” ì—¬ëŸ¬ê°€ì§€ê°€ ë“¤ì–´ê°€ëŠ”ë° ì—¬ê¸°ì„œëŠ” vector_sizeì™€ min_countì •ë„ë¥¼ ìˆ˜ì •í–ˆë‹¤. vector_sizeëŠ” ë§Œë“¤ì–´ì§€ëŠ” ë²¡í„° ì°¨ì›ì˜ í¬ê¸°ì´ê³ , min_countëŠ” ìµœì†Œ ëª‡ ë²ˆ ì´ìƒ ë‚˜ì˜¨ ë‹¨ì–´ì— ëŒ€í•´ í•™ìŠµí• ì§€ ì •í•˜ëŠ” íŒŒë¼ë¯¸í„°ì´ë‹¤. ì—¬ê¸°ì„œëŠ” ì¼ë‹¨ ì‚¬ì´ì¦ˆ 50ì— ìµœì†Œ íšŸìˆ˜ëŠ” 1íšŒë¡œ ì •í–ˆë‹¤. epochëŠ” 10ë²ˆìœ¼ë¡œ í•´ì„œ trainí–ˆë‹¤. ìœ ì‚¬ ë¬¸ì¥ ì°¾ê¸°ì´ì œ ì´ ëª¨ë¸ë¡œ ì–´ë–¤ ë¬¸ì¥ì´ ë“¤ì–´ì™”ì„ ë•Œ 1~5ë²ˆ ì¤‘ì— ë¬´ì—‡ê³¼ ë¹„ìŠ·í•œì§€ ì•Œì•„ë³´ì. ë¨¼ì € ì–´ë–¤ ë¬¸ì¥ì´ ë“¤ì–´ì˜¤ë©´ ê·¸ ë¬¸ì¥ì„ ë²¡í„°í™” í•˜ê³  ê·¸ ë²¡í„°ê°€ ì–´ë–¤ ë¬¸ì¥ê³¼ ë¹„ìŠ·í•œì§€ íƒœê·¸ ê°’ì„ ì°¾ì•„ë³¸ë‹¤. 12predict_vector = d2v_faqs.infer_vector([&quot;ë‹¹í•´ë…„ë„ ë‚©ì…ì•¡ì€ ìˆ˜ì • ê°€ëŠ¥ í•œê°€ìš”?&quot;])d2v_faqs.docvecs.most_similar([predict_vector], topn=2) 1[(&#x27;2&#x27;, 0.21605531871318817), (&#x27;3&#x27;, 0.10707802325487137)] ì œëŒ€ë¡œ ëëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•´ 1ë²ˆ ë¬¸ì¥ì„ ê·¸ëŒ€ë¡œ ë„£ì—ˆì§€ë§Œ ë‹µì€ 2, 3ë²ˆì´ ë‚˜ì™”ë‹¤. í•™ìŠµì´ ì œëŒ€ë¡œ ë˜ì§€ ì•Šì•„ì„œ ì´ëŸ° ê²°ê³¼ê°€ ë‚˜ì™”ë‹¤. í…ŒìŠ¤íŠ¸í•  ë¬¸ì¥ì„ ë²¡í„°í™” í•  ë•Œë„ í˜•íƒœì†Œ ë¶„ì„ì„ í•´ì¤˜ì•¼ í•œë‹¤. ì™œëƒí•˜ë©´ ëª¨ë¸ì„ í•™ìŠµ í•  ë•Œ ë¬¸ì¥ë“¤ì„ í˜•íƒœì†Œë¡œ ë¶„ì„í•´ì„œ ë„£ì–´ì¤¬ê¸° ë•Œë¬¸ì´ë‹¤. 123test_string = &quot;ëŒ€ë¦¬ì¸í†µë³´ ëŒ€ìƒê³„ì¢Œ ê¸°ì¤€ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?&quot;tokened_test_string = tokenize_kkma(test_string)tokened_test_string 1234567891011[&#x27;ëŒ€ë¦¬ì¸/NNG&#x27;, &#x27;í†µë³´/NNG&#x27;, &#x27;ëŒ€ìƒ/NNG&#x27;, &#x27;ê³„ì¢Œ/NNG&#x27;, &#x27;ê¸°ì¤€/NNG&#x27;, &#x27;ì€/JX&#x27;, &#x27;ì–´ë–»/VA&#x27;, &#x27;ê²Œ/ECD&#x27;, &#x27;ë˜/VV&#x27;, &#x27;ë‚˜ìš”/EFQ&#x27;, &#x27;?/SF&#x27;] 12test_vector = d2v_faqs.infer_vector(tokened_test_string)d2v_faqs.docvecs.most_similar([test_vector], topn=2) 1[(&#x27;1&#x27;, 0.1448383331298828), (&#x27;3&#x27;, 0.0218462273478508)] 2ë²ˆ ë¬¸ì¥ìœ¼ë¡œ í–ˆì„ ë•Œ ê²°ê³¼ì…ë‹ˆë‹¤. doc2vecì´ë¼ëŠ” ëª¨ë¸ì€ ë¬¸ì„œë‹¨ìœ„ë¡œ ë²¡í„°í™” í•˜ëŠ” ê²ƒì´ê¸° ë•Œë¬¸ì— ë¬¸ì„œê°€ ë§ì•„ì•¼ í•œë‹¤. ì—¬ê¸°ì„œëŠ” ë¬¸ì¥ì´ ë§ì•„ì•¼ í•œë‹¤. ë¬¸ì¥ì´ ë§ìœ¼ë©´ ë§ì„ ìˆ˜ë¡ ë¬¸ì¥ ê°„ì˜ ê±°ë¦¬ë¥¼ ê³„ì‚°í•´ì„œ ë” ì˜ êµ¬ë¶„í•´ì¤€ë‹¤. ê°„ë‹¨í•˜ê²Œ ìƒê°í•˜ë©´ ë¬¸ì¥ì´ ì ìœ¼ë©´ ì ì¤‘ë¥ ì´ ë†’ì„ ê²ƒ ê°™ì§€ë§Œ ì‚¬ì‹¤ì€ ê·¸ ë°˜ëŒ€ì´ë‹¤. ë°ì´í„°ê°€ ë§ì„ìˆ˜ë¡ ê·¸ ë°ì´í„°ê°„ì˜ ì°¨ì´ë¥¼ êµ¬ë¶„í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ë” ì˜ ì˜ˆì¸¡í•˜ê²Œ ëœë‹¤. Local path :C:\\Users\\jmj30\\Dropbox\\ì¹´ë©”ë¼ ì—…ë¡œë“œ\\Documentation\\2022\\2022 ìƒë°˜ê¸°\\íœ´ë¨¼êµìœ¡ì„¼í„°\\mj_chatbot_prac\\faq_chatbot Reference: https://cholol.tistory.com/466","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Python","slug":"Python/Python","permalink":"https://jmj3047.github.io/categories/Python/Python/"}],"tags":[{"name":"Doc2vec","slug":"Doc2vec","permalink":"https://jmj3047.github.io/tags/Doc2vec/"},{"name":"Chatbot","slug":"Chatbot","permalink":"https://jmj3047.github.io/tags/Chatbot/"},{"name":"NLP","slug":"NLP","permalink":"https://jmj3047.github.io/tags/NLP/"}],"keywords":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Python","slug":"Python/Python","permalink":"https://jmj3047.github.io/categories/Python/Python/"}]},{"title":"Making English Chatbot with Django(4)","slug":"Making_English_Chatbot_with_Django(4)","date":"2022-05-05T15:00:00.000Z","updated":"2022-05-06T08:28:34.000Z","comments":true,"path":"2022/05/06/Making_English_Chatbot_with_Django(4)/","link":"","permalink":"https://jmj3047.github.io/2022/05/06/Making_English_Chatbot_with_Django(4)/","excerpt":"","text":"ì‹¤ì œ ì„œë¹„ìŠ¤ êµ¬í˜„í•´ë³´ê¸°**code: https://github.com/jmj3047/faq_chatbot_example.git vs codeë¡œ django ì„¤ì •í•˜ê¸°: https://integer-ji.tistory.com/81 ì±„íŒ…ì°½ ë§Œë“¤ê¸° html&#x2F;cssë¥¼ ì‚¬ìš©í•´ ê°„ë‹¨í•œ ì±„íŒ…í™”ë©´ì„ ë§Œë“¤ì—ˆë‹¤. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465&lt;!-- //templates/addresses/chat_test.html --&gt;&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;/static/jquery-3.2.1.min.js&quot;&gt;&lt;/script&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt; &lt;link rel=&quot;preconnect&quot; href=&quot;https://fonts.googleapis.com&quot;&gt; &lt;link rel=&quot;preconnect&quot; href=&quot;https://fonts.gstatic.com&quot; crossorigin&gt; &lt;link href=&quot;https://fonts.googleapis.com/css2?family=Inconsolata:wght@400;600&amp;display=swap&quot; rel=&quot;stylesheet&quot;&gt;&lt;/head&gt;&lt;style&gt;* &#123;font-family: &#x27;Inconsolata&#x27;, monospace;&#125;.chat_wrap &#123;display:none;width: 350px;height: 500px;position: fixed;bottom: 30px;right: 95px;background: #a9bdce;&#125;.chat_content &#123;font-size:16pt; position:relative; height: 600px;width: 500px;overflow-y:scroll;padding:10px 15px;background: cornflowerblue&#125;.chat_input &#123;border:solid 0.5px lightgray; padding:2px 5px;&#125;.chat_header &#123;padding: 10px 15px; width: 500px; border-bottom: 1px solid #95a6b4;&#125;.chat_header .close_btn &#123;border: none;background: lightgray;float: right;&#125;.send_btn &#123;border: none; background: #ffeb33;height: 100%; color: #0a0a0a;&#125;.msg_box:after &#123;content: &#x27;&#x27;;display: block;clear:both;&#125;.msg_box &gt; span &#123;padding: 3px 5px;word-break: break-all;display: block;max-width: 300px;margin-bottom: 10px;border-radius: 4px&#125;.msg_box.send &gt; span &#123;background:#ffeb33;float: right;&#125;.msg_box.receive &gt; span &#123;background:#fff;float: left;&#125;&lt;/style&gt;&lt;body&gt;&lt;div class=&quot;chat_header&quot;&gt; &lt;span style=&quot;font-size:20pt;&quot;&gt;EDITH&lt;/span&gt; &lt;button type=&quot;button&quot; id=&quot;close_chat_btn&quot; class=&quot;close_btn&quot;&gt;X&lt;/button&gt;&lt;/div&gt;&lt;div id=&quot;divbox&quot; class=&quot;chat_content&quot;&gt;&lt;/div&gt;&lt;form id=&quot;form&quot; style=&quot;display: inline&quot;&gt; &lt;input type=&quot;text&quot; placeholder=&quot;write message..&quot; name=&quot;input1&quot; class=&quot;chat_input&quot; id=&quot;input1&quot; size=&quot;74&quot; style=&quot;margin:-3px; display: inline; width: 468px; height: 32px; font-size: 16pt;&quot; /&gt; &lt;input type=&quot;button&quot; value=&quot;SEND&quot; id=&quot;btn_submit&quot; class=&quot;send_btn&quot; style=&quot;margin:-5px; display: inline; width: 53px; height: 38px; font-size: 14pt;&quot; /&gt;&lt;/form&gt;&lt;script&gt; $(&#x27;#btn_submit&#x27;).click(function () &#123; send(); &#125;); $(&#x27;#form&#x27;).on(&#x27;submit&#x27;, function(e)&#123; e.preventDefault(); send(); &#125;); $(&#x27;#close_chat_btn&#x27;).on(&#x27;click&#x27;, function()&#123; $(&#x27;#chat_wrap&#x27;).hide().empty(); &#125;); function send()&#123; $(&#x27;#divbox&#x27;).append(&#x27;&lt;div class=&quot;msg_box send&quot;&gt;&lt;span&gt;&#x27;+$(&#x27;#input1&#x27;).val()+&#x27;&lt;span&gt;&lt;/div&gt;&#x27;); $(&quot;#divbox&quot;).scrollTop($(&quot;#divbox&quot;)[0].scrollHeight); console.log(&quot;serial&quot;+$(&#x27;form&#x27;).serialize()) $.ajax(&#123; url: &#x27;http://127.0.0.1:8000/chat_service/&#x27;, //ì±—ë´‡ api url type: &#x27;post&#x27;, dataType: &#x27;json&#x27;, data: $(&#x27;form&#x27;).serialize(), success: function(data) &#123; &lt;!--$(&#x27;#reponse&#x27;).html(data.reponse);--&gt; $(&#x27;#divbox&#x27;).append(&#x27;&lt;div class=&quot;msg_box receive&quot;&gt;&lt;span&gt;&#x27;+ data.response +&#x27;&lt;span&gt;&lt;/div&gt;&#x27;); $(&quot;#divbox&quot;).scrollTop($(&quot;#divbox&quot;)[0].scrollHeight); &#125; &#125;); $(&#x27;#input1&#x27;).val(&#x27;&#x27;); &#125;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; ê°„ë‹¨í•˜ê²Œ ì„¤ëª…í•˜ë©´ Djangoë¡œ restfulAPIë¥¼ êµ¬í˜„í•˜ê¸° ìœ„í•œ ì†ŒìŠ¤ ìœ„ì— ì±—ë´‡ì„ ë¶™ì´ê¸° ìœ„í•œ í™”ë©´ê³¼ ëª¨ë¸ì´ ë“¤ì–´ê°€ ìˆëŠ” ë²„ì „ì´ë‹¤. ìœ„ì— ì†ŒìŠ¤ëŠ” í™”ë©´ ì—­í• ì„ í•˜ëŠ” chat_test.html íŒŒì¼ì´ë‹¤. jquery ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í–ˆê¸° ë•Œë¬¸ì— jqueryë¥¼ import í•´ì•¼ í•œë‹¤. jquery fileì´ static í´ë”ì— ìˆì–´ì•¼ í•œë‹¤. jqueryëŠ” ì†ŒìŠ¤ í•˜ë‹¨ë¶€ì— ìˆëŠ” scriptë¥¼ ìœ„í•´ í•„ìš”í•˜ë‹¤. ì±„íŒ…ì—ì„œ ì „ì†¡ ë²„íŠ¼ì„ ëˆ„ë¥´ê±°ë‚˜ ì—”í„°ë¥¼ ëˆ„ë¥´ë©´ send()ë¼ëŠ” í•¨ìˆ˜ê°€ ì‹¤í–‰ë˜ê³  ì´ í•¨ìˆ˜ëŠ” ajaxë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ë°›ì•„ì˜¤ëŠ” APIë¥¼ í˜¸ì¶œí•œë‹¤. ì—¬ê¸°ì„œëŠ” localhost&#x2F;chat_serviceë¥¼ í˜¸ì¶œí•œë‹¤. ì±„íŒ…ì„ ìœ„í•œ APIí™”ë©´ì´ ë§Œë“¤ì–´ì¡Œìœ¼ë©´ ì´ì œ ì§ˆë¬¸ì„ ë°›ì•„ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” APIë¥¼ ë§Œë“ ë‹¤. ì•„ì§ FAQë°ì´í„°ë¥¼ í•™ìŠµí•œ ëª¨ë¸ì€ ë„£ì§€ ì•Šì•˜ìœ¼ë‹ˆ ì¸í’‹ì´ ë“¤ì–´ì˜¤ë©´ ë”ë¯¸ë°ì´í„°(dummy)ë¥¼ ë¦¬í„´í•˜ëŠ” APIë¥¼ ë§Œë“ ë‹¤. ì´ëŸ° API ë™ì‘ë“¤ì€ view.pyì—ì„œ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤. 1234567891011#faq_chatbot_example/addresses/views.py@csrf_exemptdef chat_service(request): if request.method == &#x27;POST&#x27;: input1 = request.POST[&#x27;input1&#x27;] output = dict() output[&#x27;response&#x27;] = &quot;ì´ê±´ ì‘ë‹µ&quot; return HttpResponse(json.dumps(output), status=200) else: return render(request, &#x27;addresses/chat_test.html&#x27;) Django í”„ë¡œì íŠ¸ ì•ˆì— addresses ì•±ì— ìˆëŠ” views.pyë¥¼ ë³´ë©´ chat_service í•¨ìˆ˜ë¥¼ ë§Œë“¤ì—ˆë‹¤. POSTí˜•ì‹ìœ¼ë¡œ ì½œì´ ì˜¤ë©´ responseì— ì•„ì›ƒí’‹ ë©”ì„¸ì§€ë¥¼ ë‹´ì•„ì„œ jsoní˜•íƒœë¡œ ë¦¬í„´í•œë‹¤. views.pyì— í•¨ìˆ˜ë¥¼ ë§Œë“¤ê³  urlë¡œ ì—°ê²°í•˜ê¸° ìœ„í•´ì„œ urls.pyì— chat_serviceë¥¼ ì…ë ¥í•œë‹¤. 123456789101112###django 3.8.3 ë²„ì „ ë§ì¶°ì¤˜ì•¼ í•¨#/faq_chatbot_example/restfulapiserver/urls.py# from django.conf.urls import url, includefrom addresses import viewsfrom django.urls import path, re_path, includefrom django.contrib import adminurlpatterns = [ ... path(&#x27;chat_service/&#x27;, views.chat_service), ...] urls.pyì—ì„œ ~&#x2F;chat_serviceë¥¼ views.chat_serviceì— ì—°ê²°ì‹œí‚¨ë‹¤. ì´ì œ ~&#x2F;chat_serviceë¡œ ì½œí•˜ë©´ views.chat_serviceê°€ ì‹¤í–‰ëœë‹¤. ì•„ê¹Œ ìœ„ì—ì„œ ë§Œë“  ì±„íŒ…í˜ì´ì§€ì— ì „ì†¡ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ ajaxë¥¼ ì´ìš©í•´ chat_serviceë¥¼ í˜¸ì¶œí–ˆë‹¤. ì •ìƒì ìœ¼ë¡œ ë˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸ í•´ë³¸ë‹¤. FAQ ëª¨ë¸ ë„£ê¸°addresses ì•± ì•ˆì— ìƒˆë¡œìš´ py ëª¨ë¸ì„ ë§Œë“¤ì–´ì„œ ë„£ê¸° 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374#/faq_chatbot_example/addresses/faq_chatbot.pyfrom gensim.models import doc2vec, Doc2Vecfrom gensim.models.doc2vec import TaggedDocumentimport pandas as pdfrom nltk.tokenize import word_tokenizefrom nltk.stem import WordNetLemmatizerfrom nltk.corpus import stopwordsimport nltk# íŒŒì¼ë¡œë¶€í„° ëª¨ë¸ì„ ì½ëŠ”ë‹¤. ì—†ìœ¼ë©´ ìƒì„±í•œë‹¤.try: d2v_faqs = Doc2Vec.load(&#x27;d2v_faqs_size200_min5_epoch20_jokes.model&#x27;) lemmatizer = WordNetLemmatizer() stop_words = stopwords.words(&#x27;english&#x27;) faqs = pd.read_csv(&#x27;jokes.csv&#x27;)except: faqs = pd.read_csv(&#x27;jokes.csv&#x27;) nltk.download(&#x27;punkt&#x27;) # í† ê·¼í™” tokened_questions = [word_tokenize(question.lower()) for question in faqs[&#x27;Question&#x27;]] lemmatizer = WordNetLemmatizer() nltk.download(&#x27;wordnet&#x27;) # lemmatization lemmed_questions = [[lemmatizer.lemmatize(word) for word in doc] for doc in tokened_questions] nltk.download(&#x27;stopwords&#x27;) # stopword ì œê±° ë¶ˆìš©ì–´ ì œê±°í•˜ê¸° stop_words = stopwords.words(&#x27;english&#x27;) questions = [[w for w in doc if not w in stop_words] for doc in lemmed_questions] # ë¦¬ìŠ¤íŠ¸ì—ì„œ ê° ë¬¸ì¥ë¶€ë¶„ í† í°í™” index_questions = [] for i in range(len(faqs)): index_questions.append([questions[i], i ]) # Doc2Vecì—ì„œ ì‚¬ìš©í•˜ëŠ” íƒœê·¸ë¬¸ì„œí˜•ìœ¼ë¡œ ë³€ê²½ tagged_questions = [TaggedDocument(d, [int(c)]) for d, c in index_questions] # make model import multiprocessing cores = multiprocessing.cpu_count() d2v_faqs = doc2vec.Doc2Vec( vector_size=200, hs=1, negative=0, dm=0, dbow_words=1, min_count=5, workers=cores, seed=0, epochs=20 ) d2v_faqs.build_vocab(tagged_questions) d2v_faqs.train(tagged_questions, total_examples=d2v_faqs.corpus_count, epochs=d2v_faqs.epochs) d2v_faqs.save(&#x27;d2v_faqs_size200_min5_epoch20_jokes.model&#x27;)# FAQ ë‹µë³€def faq_answer(input): # í…ŒìŠ¤íŠ¸í•˜ëŠ” ë¬¸ì¥ë„ ê°™ì€ ì „ì²˜ë¦¬ë¥¼ í•´ì¤€ë‹¤. tokened_test_string = word_tokenize(input) lemmed_test_string = [lemmatizer.lemmatize(word) for word in tokened_test_string] test_string = [w for w in lemmed_test_string if not w in stop_words] topn = 5 test_vector = d2v_faqs.infer_vector(test_string) result = d2v_faqs.docvecs.most_similar([test_vector], topn=topn) print(result) for i in range(topn): print(&quot;&#123;&#125;ìœ„. &#123;&#125;, &#123;&#125; &#123;&#125; &#123;&#125;&quot;.format(i + 1, result[i][1], result[i][0], faqs[&#x27;Question&#x27;][result[i][0]], faqs[&#x27;Answer&#x27;][result[i][0]])) return faqs[&#x27;Answer&#x27;][result[0][0]]faq_answer(&quot;What do you call a person who is outside a door and has no arms nor legs?&quot;) ìœ„ ì†ŒìŠ¤ì—ì„œ ìƒë‹¨ì— ìˆëŠ” ëª¨ë¸ì„ ë§Œë“œëŠ” ì½”ë“œëŠ” API ì„œë²„ë¥¼ ì‹¤í–‰í•˜ëŠ” ì‹œì ì—ì„œ í˜¸ì¶œëœë‹¤. ë¬´ì¡°ê±´ í˜¸ì¶œí•˜ëŠ” ê±´ ì•„ë‹ˆê³  views.pyì—ì„œ importë¥¼ ì¨ ë„£ìœ¼ë©´ ìµœì´ˆ 1ë²ˆì€ ì‹¤í–‰ë˜ê²Œ ëœë‹¤. ì±„íŒ… ì›¹í˜ì´ì§€ë¡œë¶€í„° faq_chatbot.pyì— ìˆëŠ” faq_answerë¥¼ í˜¸ì¶œí•˜ëŠ” ê²ƒ ê¹Œì§€ flowë¥¼ ê·¸ë ¤ë³´ë©´ chat_test.htmlâ†’view.py(chat_service)â†’faq_chatbot.py(faq_answer) ìˆœì„œì´ë‹¤. ë”°ë¼ì„œ views.pyì—ì„œ faq_answerí•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ê¸° ìœ„í•´ importë¥¼ í•˜ê²Œ ë˜ëŠ”ë° djangoëŠ” ìµœì´ˆ ì‹¤í–‰ì‹œ views.pyë¥¼ í•œë²ˆ ì½ê¸° ë•Œë¬¸ì— faq_chatbot.pyì— ì ì–´ë†“ì€ ì†ŒìŠ¤ê°€ í•œë²ˆ ì‹¤í–‰ë˜ê²Œ ëœë‹¤. ë§¤ë²ˆì„œë²„ë¥¼ ì‹¤í–‰í•  ë•Œë§ˆë‹¤ ëª¨ë¸ì„ ìƒˆë¡œ ë§Œë“¤ê²Œ ë˜ë©´ ì„œë²„ ê¸°ë™ ì†ë„ê°€ ëŠë ¤ì§€ê³  ë¹„íš¨ìœ¨ì ì´ê¸° ë•Œë¬¸ì— ëª¨ë¸ì„ ë§Œë“¤ê³  ë‚˜ì„œ íŒ¨ì¼ë¡œ ì €ì¥í•˜ê³ , ë§Œë“¤ì–´ì§„ íŒŒì¼ì´ ì—†ë‹¤ë©´ ëª¨ë¸ì„ ìƒì„±í•˜ë„ë¡ try&#x2F;exceptë¥¼ ì‚¬ìš©í–ˆë‹¤. ì¶”ê°€ì ìœ¼ë¡œ í”„ë¡œì íŠ¸ìƒ ì†ŒìŠ¤ê°€ ì‹¤í–‰ë˜ê¸° ë•Œë¬¸ì— íŒŒì¼ê²½ë¡œëŠ” rootì´ë‹¤. jokes.csvê°€ ìˆì–´ì•¼ í•  ê³³ê³¼ ëª¨ë¸ì´ ìƒì„±ë˜ëŠ” ê³³ì˜ ê²½ë¡œëŠ” í”„ë¡œì íŠ¸ì˜ rootí´ë”ì´ë‹¤. ì ì´ì œ ì§ˆë¬¸ì˜ ë‹µì„ ì°¾ì•„ì£¼ëŠ” í•¨ìˆ˜ê°€ ë§Œë“¤ì–´ ì¡Œìœ¼ë‹ˆ ì•„ê¹Œ ë”ë¯¸ ë°ì´í„°ë¡œ ë¦¬í„´í•´ì£¼ë˜ views.pyì˜ í•¨ìˆ˜ë¥¼ ë°”ê¿”ë³´ì. 123456789101112#faq_chatbot_example/addresses/views.py@csrf_exemptdef chat_service(request): if request.method == &#x27;POST&#x27;: input1 = request.POST[&#x27;input1&#x27;] response = faq_answer(input1) output = dict() output[&#x27;response&#x27;] = response return HttpResponse(json.dumps(output), status=200) else: return render(request, &#x27;addresses/chat_test1.html&#x27;) ì´ì „ì—ëŠ” responseì— ë¬´ì¡°ê±´ ë”ë¯¸ ì‘ë‹µì„ ë³´ëƒˆëŠ”ë° ì´ì œëŠ” faq_answerí•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ í•´ë‹¹ ì§ˆë¬¸ì— ì•Œë§ì€ ì •ë‹µì„ ê°€ì ¸ì˜¨ë‹¤. faq_answerí•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì œì¼ ìƒë‹¨ì— from .faq_chatbot import faq_answerë¥¼ ì„ ì–¸í•´ì•¼ í•œë‹¤. ì‹¤í–‰ ê²°ê³¼(html íŒŒì¼ ìˆ˜ì •) **code: â€£https://github.com/jmj3047/faq_chatbot_example.git Reference: https://cholol.tistory.com/478","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Django","slug":"Python/Django","permalink":"https://jmj3047.github.io/categories/Python/Django/"}],"tags":[{"name":"Doc2vec","slug":"Doc2vec","permalink":"https://jmj3047.github.io/tags/Doc2vec/"},{"name":"Chatbot","slug":"Chatbot","permalink":"https://jmj3047.github.io/tags/Chatbot/"},{"name":"NLP","slug":"NLP","permalink":"https://jmj3047.github.io/tags/NLP/"}],"keywords":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Django","slug":"Python/Django","permalink":"https://jmj3047.github.io/categories/Python/Django/"}]},{"title":"Making English Chatbot with doc2vec(3)","slug":"Making_English_Chatbot_with_doc2vec(3)","date":"2022-05-05T15:00:00.000Z","updated":"2022-10-15T08:08:04.571Z","comments":true,"path":"2022/05/06/Making_English_Chatbot_with_doc2vec(3)/","link":"","permalink":"https://jmj3047.github.io/2022/05/06/Making_English_Chatbot_with_doc2vec(3)/","excerpt":"","text":"ë§ì€ ë°ì´í„°ë¡œ ì‹¤í—˜í•´ë³´ê¸°ë°ì´í„° ì‚´í´ë³´ê¸°ë” ë§ì€ í•™ìŠµ ë°ì´í„°ë¡œ ëª¨ë¸ì„ í•™ìŠµí•œë‹¤. ë°ì´í„° ì›ë³¸ ë§í¬: https://www.kaggle.com/jiriroz/qa-jokesì´ 3ë§Œ 8ì²œê°œì˜ ë¬¸ì¥ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°, ì „ì²˜ë¦¬12345678import osimport warningsfrom gensim.models import doc2vecfrom gensim.models.doc2vec import TaggedDocumentimport pandas as pdfaqs = pd.read_csv(os.path.join(&#x27;data&#x27;,&#x27;jokes.csv&#x27;))faqs í•œêµ­ì–´ì™€ ë‹¤ë¥´ê²Œ ì˜ì–´ëŠ” ë„ì–´ì“°ê¸°ë¡œ ë‹¨ì–´ê°€ ì˜ êµ¬ë¶„ë˜ê¸° ë•Œë¬¸ì— í˜•íƒœì†Œ ë¶„ì„ì€ ìƒëµí•œë‹¤. í˜•íƒœì†Œ ë¶„ì„ì„ í•˜ì§€ ì•Šì•„ë„ ë„ì–´ì“°ê¸°ë¡œ splití•˜ë©´ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ì˜ ì§¤ë¦¬ê¸° ë•Œë¬¸ì´ë‹¤. í•˜ì§€ë§Œ ì˜ì–´ ë‹¨ì–´ë¥¼ ì›í˜•ìœ¼ë¡œ ë§Œë“¤ì–´ ì£¼ëŠ” lemmatizationì´ë‚˜ theë‚˜ aê°™ì€ ê´€ì‚¬ë¥¼ ì œê±°í•˜ëŠ” stopword ì œê±°ëŠ” í•´ì¤€ë‹¤. 12345678910from nltk.tokenize import word_tokenizefrom nltk.stem import WordNetLemmatizerfrom nltk.corpus import stopwordsimport nltknltk.download(&#x27;punkt&#x27;)# í† í°í™”tokened_questions = [word_tokenize(question.lower()) for question in faqs[&#x27;Question&#x27;]]tokened_questions 123456789101112131415[[&#x27;did&#x27;, &#x27;you&#x27;, &#x27;hear&#x27;, &#x27;about&#x27;, &#x27;the&#x27;, &#x27;native&#x27;, &#x27;american&#x27;, &#x27;man&#x27;, &#x27;that&#x27;, &#x27;drank&#x27;, &#x27;200&#x27;, &#x27;cups&#x27;, &#x27;of&#x27;, &#x27;tea&#x27;, &#x27;?&#x27;], ëŒ€ë¬¸ìë¥¼ ëª¨ë‘ ì†Œë¬¸ìë¡œ ë°”ê¿”ì£¼ê³  í† í°í™”ë¥¼ í•œ ë‹¤ìŒ ë„ì–´ì“°ê¸°ë¡œ ì¶œë ¥í•´ì£¼ì—ˆë‹¤. 12345678910lemmatizer = WordNetLemmatizer()nltk.download(&#x27;wordnet&#x27;)# lemmatizationlemmed_questions = [[lemmatizer.lemmatize(word) for word in doc] for doc in tokened_questions]lemmed_questionsnltk.download(&#x27;stopwords&#x27;)# stopword ì œê±° ë¶ˆìš©ì–´ ì œê±°í•˜ê¸°stop_words = stopwords.words(&#x27;english&#x27;)questions = [[w for w in doc if not w in stop_words] for doc in lemmed_questions]questions 123456789101112131415**[[&#x27;hear&#x27;, &#x27;native&#x27;, &#x27;american&#x27;, &#x27;man&#x27;, &#x27;drank&#x27;, &#x27;200&#x27;, &#x27;cup&#x27;, &#x27;tea&#x27;, &#x27;?&#x27;], [&quot;&#x27;s&quot;, &#x27;best&#x27;, &#x27;anti&#x27;, &#x27;diarrheal&#x27;, &#x27;prescription&#x27;, &#x27;?&#x27;], [&#x27;call&#x27;, &#x27;person&#x27;, &#x27;outside&#x27;, &#x27;door&#x27;, &#x27;ha&#x27;, &#x27;arm&#x27;, &#x27;leg&#x27;, &#x27;?&#x27;], [&#x27;star&#x27;, &#x27;trek&#x27;, &#x27;character&#x27;, &#x27;member&#x27;, &#x27;magic&#x27;, &#x27;circle&#x27;, &#x27;?&#x27;], [&quot;&#x27;s&quot;, &#x27;difference&#x27;, &#x27;bullet&#x27;, &#x27;human&#x27;, &#x27;?&#x27;], [&#x27;wa&#x27;, &#x27;ethiopian&#x27;, &#x27;baby&#x27;, &#x27;cry&#x27;, &#x27;?&#x27;], [&quot;&#x27;s&quot;, &#x27;difference&#x27;, &#x27;corn&#x27;, &#x27;husker&#x27;, &#x27;epilepsy&#x27;, &#x27;hooker&#x27;, &#x27;dysentery&#x27;, &#x27;?&#x27;], [&#x27;2016&#x27;, &quot;&#x27;s&quot;, &#x27;biggest&#x27;, &#x27;sellout&#x27;, &#x27;?&#x27;],** lemmatizationí•˜ê³  ë¶ˆìš©ì–´ë¥¼ ì œê±°í•˜ê³  ë‚œ ë‹¤ìŒì˜ ê²°ê³¼ë¬¼. ì´ì œ ëª¨ë“  ì „ì²˜ë¦¬ê°€ ëë‚¬ìœ¼ë‹ˆ TaggedDocumentë¡œ ë³€í˜•ì‹œí‚¤ê³  ë‚˜ì„œ doc2vecì— ë„£ì–´ì¤€ë‹¤. 12345678# ë¦¬ìŠ¤íŠ¸ì—ì„œ ê° ë¬¸ì¥ë¶€ë¶„ í† í°í™”index_questions = []for i in range(len(faqs)): index_questions.append([questions[i], i ])# Doc2Vecì—ì„œ ì‚¬ìš©í•˜ëŠ” íƒœê·¸ë¬¸ì„œí˜•ìœ¼ë¡œ ë³€ê²½tagged_questions = [TaggedDocument(d, [int(c)]) for d, c in index_questions] doc2vec ëª¨ë¸í™”doc2vecì„ í›ˆë ¨í•˜ê¸° ì „ì— ëª¨ë¸ì— ë³€í˜•ì„ ì£¼ì—ˆë‹¤. forë¬¸ë„ ë¹¼ê³  íŒŒë¼ë¯¸í„°ë„ ë³€ê²½í•´ ì£¼ì—ˆë‹¤. 123456789101112131415161718192021222324252627# make modelimport multiprocessingcores = multiprocessing.cpu_count()d2v_faqs = doc2vec.Doc2Vec(vector_size=200, # alpha=0.025, # min_alpha=0.025, hs=1, negative=0, dm=0, dbow_words = 1, min_count = 5, workers = cores, seed=0, epochs=20)d2v_faqs.build_vocab(tagged_questions)d2v_faqs.train(tagged_questions, total_examples = d2v_faqs.corpus_count, epochs = d2v_faqs.epochs ) # # train document vectors # for epoch in range(50): # d2v_faqs.train(tagged_faqs, # total_examples = d2v_faqs.corpus_count, # epochs = d2v_faqs.epochs # ) # d2v_faqs.alpha -= 0.0025 # decrease the learning rate # d2v_faqs.min_alpha = d2v_faqs.alpha # fix the learning rate, no decay 1234567# í…ŒìŠ¤íŠ¸í•˜ëŠ” ë¬¸ì¥ë„ ê°™ì€ ì „ì²˜ë¦¬ë¥¼ í•´ì¤€ë‹¤.test_string = &quot;What&#x27;s the best anti diarrheal prescription?&quot;tokened_test_string = word_tokenize(test_string)lemmed_test_string = [lemmatizer.lemmatize(word) for word in tokened_test_string]test_string = [w for w in lemmed_test_string if not w in stop_words]test_string 12345678910111213141516# ì„±ëŠ¥ ì¸¡ì •raten = 5found = 0for i in range(len(faqs)): tstr = faqs[&#x27;Question&#x27;][i] tokened_test_string = word_tokenize(tstr) lemmed_test_string = [lemmatizer.lemmatize(word) for word in tokened_test_string] ttok = [w for w in lemmed_test_string if not w in stop_words] tvec = d2v_faqs.infer_vector(ttok) re = d2v_faqs.docvecs.most_similar([tvec], topn = raten) for j in range(raten): if i == re[j][0]: found = found + 1 breakprint(&quot;ì •í™•ë„ = &#123;&#125; % (&#123;&#125;/&#123;&#125; ) &quot;.format(found/len(faqs),found, len(faqs))) 1ì •í™•ë„ = 0.8626303274190598 % (33012/38269 ) Local path :C:\\Users\\jmj30\\Dropbox\\ì¹´ë©”ë¼ ì—…ë¡œë“œ\\Documentation\\2022\\2022 ìƒë°˜ê¸°\\íœ´ë¨¼êµìœ¡ì„¼í„°\\mj_chatbot_prac\\faq_chatbot Reference: https://cholol.tistory.com/466","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Python","slug":"Python/Python","permalink":"https://jmj3047.github.io/categories/Python/Python/"}],"tags":[{"name":"Doc2vec","slug":"Doc2vec","permalink":"https://jmj3047.github.io/tags/Doc2vec/"},{"name":"Chatbot","slug":"Chatbot","permalink":"https://jmj3047.github.io/tags/Chatbot/"},{"name":"NLP","slug":"NLP","permalink":"https://jmj3047.github.io/tags/NLP/"}],"keywords":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Python","slug":"Python/Python","permalink":"https://jmj3047.github.io/categories/Python/Python/"}]},{"title":"Making Korean Chatbot with doc2vec(2)","slug":"Making_Korean_Chatbot_with_doc2vec(2)","date":"2022-05-05T15:00:00.000Z","updated":"2022-10-15T08:08:14.439Z","comments":true,"path":"2022/05/06/Making_Korean_Chatbot_with_doc2vec(2)/","link":"","permalink":"https://jmj3047.github.io/2022/05/06/Making_Korean_Chatbot_with_doc2vec(2)/","excerpt":"","text":"ëª¨ë¸ ë‹¤ë“¬ê¸°FAQë°ì´í„° ëŠ˜ë¦¬ê¸°ë” ë§ì€ í•™ìŠµ ë°ì´í„°ë¡œ ëª¨ë¸ì„ í•™ìŠµí•œë‹¤. ë°ì´í„° ì›ë³¸ ë§í¬: https://www.data.go.kr/dataset/3068685/fileData.do 123456789import osimport warningsfrom gensim.models import doc2vecfrom gensim.models.doc2vec import TaggedDocumentimport pandas as pdfaqs = pd.read_csv(os.path.join(&#x27;data&#x27;,&#x27;kor_elec_faq2.csv&#x27;), encoding=&#x27;CP949&#x27;)faqsfaqs[[&#x27;ìˆœë²ˆ&#x27;, &#x27;ì œëª©&#x27;, &#x27;ë‚´ìš©&#x27;]] pandasë¥¼ ì‚¬ìš©í•´ csvíŒŒì¼ì„ ë°”ë¡œ ì½ì–´ì¤€ë‹¤. utf-8 ì¸ì½”ë”© ë¬¸ì œë¡œ ì—ëŸ¬ê°€ ë‚˜ë©´ cp949ë¡œ ë„£ì–´ì¤€ë‹¤. ì „ì²´ í•„ë“œì—ì„œ í•„ìš”í•œ indexì™€ ì§ˆë¬¸(ì—¬ê¸°ì„œëŠ” ì œëª©), ë‹µë³€(ì—¬ê¸°ì„œëŠ” ë‚´ìš©)ë§Œ ë½‘ì•„ë‚¸ë‹¤. ì´ 351ê°œì˜ ì§ˆë¬¸ê³¼ ë‹µë³€ ë°ì´í„°ê°€ ìƒê²¼ìœ¼ë‹ˆ ì „ ê²Œì‹œë¬¼ê³¼ ë™ì¼í•œ ë°©ë²•ìœ¼ë¡œ ëª¨ë¸í•™ìŠµì„ ì‹œí‚¨ë‹¤. ì „ ë°ì´í„°ëŠ” pandasë°ì´í„°ê°€ ì•„ë‹ˆì—ˆê¸° ë•Œë¬¸ì— ìˆ˜ì •í•´ ì¤€ë‹¤. 1234567# ë¦¬ìŠ¤íŠ¸ì—ì„œ ê° ë¬¸ì¥ë¶€ë¶„ í† í°í™”token_faqs = []for i in range(len(faqs)): token_faqs.append([tokenize_kkma_noun(faqs[&#x27;ì œëª©&#x27;][i]), faqs[&#x27;ìˆœë²ˆ&#x27;][i]])# Doc2Vecì—ì„œ ì‚¬ìš©í•˜ëŠ” íƒœê·¸ë¬¸ì„œí˜•ìœ¼ë¡œ ë³€ê²½tagged_faqs = [TaggedDocument(d, [c]) for d, c in token_faqs] ì´ë ‡ê²Œ í•˜ê³  ëª¨ë¸ì„ ëŒë ¤ë„ ì„±ëŠ¥ì´ ì¢‹ì§€ ì•ŠìŒ. doc2vecëª¨ë¸ì˜ ê²½ìš° ìµœì†Œ ë§Œë‹¨ìœ„ì˜ ë¬¸ì¥ì´ ìˆì–´ì•¼ ì œëŒ€ë¡œ ë‚˜ì˜¨ë‹¤. íŠœë‹ ì‹œë„í•´ë³´ê¸°ì´ì „ê¹Œì§€ ë°ì´í„° ë‚´ì— ìˆëŠ” ìˆœë²ˆì„ ì¸ë±ìŠ¤ë¡œ ì‚¬ìš©í–ˆëŠ”ë° ì •ìƒì ìœ¼ë¡œ ì‘ë™í•˜ì§€ ì•Šì•„ ë‹¤ì‹œ ë§Œë“¤ì–´ì¤€ë‹¤. 123456789# ë¦¬ìŠ¤íŠ¸ì—ì„œ ê° ë¬¸ì¥ë¶€ë¶„ í† í°í™”token_faqs = []for i in range(len(faqs)): token_faqs.append([tokenize_kkma_noun(faqs[&#x27;ì œëª©&#x27;][i]), i ]) # token_faqs.append([tokenize_kkma_noun(faqs[&#x27;ì œëª©&#x27;][i]), faqs[&#x27;ìˆœë²ˆ&#x27;][i]])# Doc2Vecì—ì„œ ì‚¬ìš©í•˜ëŠ” íƒœê·¸ë¬¸ì„œí˜•ìœ¼ë¡œ ë³€ê²½tagged_faqs = [TaggedDocument(d, [int(c)]) for d, c in token_faqs]# tagged_faqs = [TaggedDocument(d, [c]) for d, c in token_faqs] ë¬¸ì„œ ì›ë³¸ì„ ìˆ˜ì •í•  í•„ìš”ëŠ” ì—†ê³  TaggedDocumentë§Œë“¤ ë•Œë§Œ ì˜ ë„£ì–´ì¤€ë©´ ëœë‹¤. ê¸°ì¡´ì—ëŠ” faqs[â€™ìˆœë²ˆâ€™][i]ë¥¼ íƒœê·¸ ê°’ìœ¼ë¡œ ë„£ì–´ì£¼ì—ˆëŠ”ë° ê·¸ëƒ¥ ië¥¼ ë„£ëŠ”ë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ì¢‹ì€ ì ì´ ì›ë³¸ì˜ indexì™€ íƒœê·¸ê°’ì´ ê°™ì•„ì§€ê¸° ë•Œë¬¸ì— ë‚˜ì¤‘ì— ì›ë¬¸ì§ˆë¬¸ì„ ë³µì›í•  ë•Œ faqs[â€™ì œëª©â€™][tag]ë¡œ ì¶œë ¥ì´ ê°€ëŠ¥í•˜ë‹¤. ê·¸ë¦¬ê³  ì´ì œ ê°€ì¥ ë¨¼ì € í•  ê±°ëŠ” ì „ì²˜ë¦¬ë¥¼ ì¡°ê¸ˆ ìˆ˜ì •í•˜ëŠ” ê²ƒì´ë‹¤. í˜•íƒœì†Œ ë¶„ì„ì„ í•  ë•Œ í•„ìš” ì—†ëŠ” ë°ì´í„°ë¥¼ ì œì™¸ì‹œí‚¤ëŠ” ë°©ë²•ì´ë‹¤. ë³´í†µ ë¬¸ì¥ì—ì„œëŠ” ëª…ì‚¬ì™€ ë™ì‚¬ê°€ ì¤‘ìš”í•˜ê¸° ë•Œë¬¸ì— ëª…ì‚¬ ë™ì‚¬ ë¹¼ê³ ëŠ” ë‹¤ ë‚ ë ¤ë³¸ë‹¤. 1234567891011121314151617#íŠœë‹:í˜•íƒœì†Œ í•„í„°ë§kkma = Kkma()filter_kkma = [&#x27;NNG&#x27;, #ë³´í†µëª…ì‚¬ &#x27;NNP&#x27;, #ê³ ìœ ëª…ì‚¬ &#x27;OL&#x27; , #ì™¸êµ­ì–´ &#x27;VA&#x27;,&#x27;VV&#x27;,&#x27;VXV&#x27; ]def tokenize_kkma(doc): jpype.attachThreadToJVM() token_doc = [&#x27;/&#x27;.join(word) for word in kkma.pos(doc)] return token_docdef tokenize_kkma_noun(doc): jpype.attachThreadToJVM() token_doc = [&#x27;/&#x27;.join(word) for word in kkma.pos(doc) if word[1] in filter_kkma] return token_doc ì´ëŸ°ì‹ìœ¼ë¡œ filter_kkmaë¦¬ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ ë§Œë“¤ì–´ í˜•íƒœì†Œë¥¼ ë¶„ì„í–ˆì„ ë•Œ ë‚˜ì˜¤ëŠ” í˜•íƒœì†Œê°€ filter_kkmaì— í¬í•¨ë˜ì–´ ìˆì„ ê²½ìš°ë§Œ í•™ìŠµ ëŒ€ìƒì— ì¶”ê°€í•œë‹¤. tokenize_kkmaë¥¼ ì“°ë©´ ì „ì²´ í˜•íƒœì†Œ ë¶„ì„, tokenize_kkmk_nounì„ ì“°ë©´ ë™ì‚¬ ëª…ì‚¬ë§Œ ì¶”ì¶œí•œë‹¤. ê°€ì¥ ê²°ê³¼ê°€ ì¢‹ê²Œ ë‚˜ì˜¨ ì¡°í•©ì€ ëª…ì‚¬ë§Œ ì¶”ì¶œ, for ë¬¸ 50ë²ˆì— epochs&#x3D;100ìœ¼ë¡œ í•œ ê²°ê³¼ê°’. 12345678910111213141516#íŠœë‹:ëª…ì‚¬ë§Œ ì¶”ì¶œkkma = Kkma()filter_kkma = [&#x27;NNG&#x27;, #ë³´í†µëª…ì‚¬ &#x27;NNP&#x27;, #ê³ ìœ ëª…ì‚¬ &#x27;OL&#x27; , #ì™¸êµ­ì–´ ]def tokenize_kkma(doc): jpype.attachThreadToJVM() token_doc = [&#x27;/&#x27;.join(word) for word in kkma.pos(doc)] return token_docdef tokenize_kkma_noun(doc): jpype.attachThreadToJVM() token_doc = [&#x27;/&#x27;.join(word) for word in kkma.pos(doc) if word[1] in filter_kkma] return token_doc 123456789101112131415161718192021222324252627# make model import multiprocessing import tensorflow as tf with tf.device(&#x27;/device:GPU:0&#x27;): cores = multiprocessing.cpu_count() d2v_faqs = doc2vec.Doc2Vec(vector_size=20, #100 alpha=0.025, min_alpha=0.025, hs=1, negative=0, dm=0, window=3, dbow_words = 1, min_count = 1, workers = cores, seed=0, epochs=100) d2v_faqs.build_vocab(tagged_faqs) # train document vectors for epoch in range(50): d2v_faqs.train(tagged_faqs, total_examples = d2v_faqs.corpus_count, epochs = d2v_faqs.epochs ) d2v_faqs.alpha -= 0.0025 # decrease the learning rate d2v_faqs.min_alpha = d2v_faqs.alpha # fix the learning rate, no decay 123test_string = &quot;ë³€ì••ê¸°ê³µë™ì´ìš©(ëª¨ìê±°ë˜)ì´ë€ ë¬´ì—‡ì´ë©°, ìš”ê¸ˆê³„ì‚°ì€ ì–´ë–»ê²Œ í•©ë‹ˆê¹Œ&quot;tokened_test_string = tokenize_kkma_noun(test_string)tokened_test_string 12345678910111213# ì„±ëŠ¥ ì¸¡ì •# raten = 5 #ì •í™•ë„ = 0.5128205128205128 % (180/351 )raten = 1 #ì •í™•ë„ = 0.24216524216524216 % (85/351 ) found = 0for i in range(len(faqs)): tstr = faqs[&#x27;ì œëª©&#x27;][i] ttok = tokenize_kkma_noun(tstr) tvec = d2v_faqs.infer_vector(ttok) re = d2v_faqs.docvecs.most_similar([tvec], topn = raten) for j in range(raten): if i == re[j][0]: found = found + 1print(&quot;ì •í™•ë„ = &#123;&#125; % (&#123;&#125;/&#123;&#125; ) &quot;.format(found/len(faqs),found, len(faqs)) ëª¨ë¸ ì €ì¥, ë¶ˆëŸ¬ì˜¤ê¸°1234567891011121314151617# ëª¨ë¸ ì €ì¥d2v_faqs.save(os.path.join(&#x27;data&#x27;,&#x27;/d2v_faqs_size100_min1_batch50_epoch100_nounonly_dm0.model&#x27;))# ëª¨ë¸ loadd2v_faqs_1 = doc2vec.Doc2Vec.load(os.path.join(&#x27;data&#x27;,&#x27;/d2v_faqs_size100_min1_batch50_epoch100_nounonly_dm0.model&#x27;))#testtest_string = &quot;ê±´ë¬¼ì„ ìƒˆë¡œ ì§€ì„ ë•Œ ì„ì‹œì „ë ¥ì€ ì–´ë–»ê²Œ ì‹ ì²­í•˜ë‚˜ìš”&quot;tokened_test_string = tokenize_kkma_noun(test_string)tokened_test_stringtopn = 5# ëª¨ë¸ ì¶”ì¸¡test_vector1 = d2v_faqs_1.infer_vector(tokened_test_string)result1 = d2v_faqs_1.docvecs.most_similar([test_vector1], topn=topn)for i in range(topn): print(&quot;ëª¨ë¸ 1 &#123;&#125;ìœ„. &#123;&#125;, &#123;&#125; &#123;&#125;&quot;.format(i+1, result1[i][1], result1[i][0],faqs[&#x27;ì œëª©&#x27;][result1[i][0]] )) Local path :C:\\Users\\jmj30\\Dropbox\\ì¹´ë©”ë¼ ì—…ë¡œë“œ\\Documentation\\2022\\2022 ìƒë°˜ê¸°\\íœ´ë¨¼êµìœ¡ì„¼í„°\\mj_chatbot_prac\\faq_chatbot Reference: https://cholol.tistory.com/466","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Python","slug":"Python/Python","permalink":"https://jmj3047.github.io/categories/Python/Python/"}],"tags":[{"name":"Doc2vec","slug":"Doc2vec","permalink":"https://jmj3047.github.io/tags/Doc2vec/"},{"name":"Chatbot","slug":"Chatbot","permalink":"https://jmj3047.github.io/tags/Chatbot/"},{"name":"NLP","slug":"NLP","permalink":"https://jmj3047.github.io/tags/NLP/"}],"keywords":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Python","slug":"Python/Python","permalink":"https://jmj3047.github.io/categories/Python/Python/"}]},{"title":"Support Vector Machine","slug":"SVM","date":"2022-05-05T15:00:00.000Z","updated":"2023-02-23T01:34:32.094Z","comments":true,"path":"2022/05/06/SVM/","link":"","permalink":"https://jmj3047.github.io/2022/05/06/SVM/","excerpt":"","text":"1. ë¶„ë¥˜ì— ëŒ€í•œ ìˆ˜ì  í‘œí˜„ í•™ìŠµ ë°ì´í„° X(ë…ë¦½ë³€ìˆ˜),Y(ì¢…ì†ë³€ìˆ˜)ê°€ ìˆì„ ë•Œ (i&#x3D;1,2,3,4,5 â€¦.ë°ì´í„°ì˜ ê°¯ìˆ˜) Yâ‡’{-1,1} (ë‘ ê°œì˜ í´ë˜ìŠ¤ë¥¼ ì˜ë¯¸) â‡’ ê²½ìš°ì— ë”°ë¼ì„œ, í´ë˜ìŠ¤ë¥¼ 1ê³¼ -1 ë¡œ ë‚˜ëˆ” Y(ì •ë‹µ) * F(x)(ì˜ˆì¸¡í•œ ì •ë‹µ) &gt;0 ë¼ëŠ” ê²ƒì€ ì œëŒ€ë¡œ ë¶„ë¥˜ëœ í˜•íƒœ ( ê°™ì€ ë¶€í˜¸ë¼ë¦¬ ê³±í•˜ë©´ ì–‘ìˆ˜ì¸ ê²½ìš°ë‹ˆê¹Œ) 2. ì„ í˜• ë¶„í• (Linear Classifier) f(x)&#x3D; W transpose X + b (ì„ í˜•ì¡°í•©, ê°ê°ì˜ í•­ë“¤ì´ ë”í•˜ê¸°ë¡œ ì´ë£¨ì–´ì§„ ì¡°í•©.) ì„ í˜•ë¶„í• ì€ ì§ì„ ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒ (2ì°¨ì›ì´ê±´ 3ì°¨ì›ì´ê±´ ê·¸ ì´ìƒì´ê±´ ìƒê´€ ì—†ìŒ) b(bias) Y ì ˆí¸ì„ ì˜ë¯¸ WëŠ” ì§ì„ ì˜ ê¸°ìš¸ê¸° 3. ì´ˆí‰ë©´ ë¶„í•  ë” ë‚˜ì€(ìµœì ) ë¶„ë¥˜ë¥¼ ìœ„í•œ ì´ˆí‰ë©´(Hyperplane)â†’ì„  ë³´ë‹¤ ë” í° ì°¨ì› ì¢‹ì€ íŒë³„ì„ ì— ëŒ€í•œ ê¸°ì¤€ ìµœì í™”: ì¢‹ì€ ê²ƒ ì„ ê·¹ëŒ€í™” ì‹œí‚¤ê³  ë‚˜ìœ ê²ƒ ì„ ê·¹ì†Œí™” ì‹œí‚¤ëŠ” ê²ƒ ë¶„ë¥˜ì—ì„œì˜ ìµœì í™”: ì˜ ì•ˆë‚˜ë‰˜ëŠ”ê²ƒ , ì˜ ë‚˜ë‰˜ëŠ” ê²ƒ ë‚˜ì¤‘ì— Testing data ë¥¼ ëŒë ¸ì„ë•Œ, ê°€ì¥ ì¢‹ê²Œ ë‚˜ë‰œ ê²ƒì€ ë°˜ì ˆë¡œ ë‚˜ë‰œ ì§ì„ ì´ë‹¤. Test data ê°€ ì–´ë–»ê²Œ ë“¤ì–´ì˜¬ì§€ ëª¨ë¥´ëŠ” ê²ƒ ì´ê¸°ë•Œë¬¸ì— , ê³¼ì í•© ë˜ì–´ ìˆëŠ” ê²ƒë³´ë‹¤ í™•ì‹¤íˆ ì ˆë°˜ìœ¼ë¡œ ë‚˜ëˆ„ëŠ”ê²ƒì´ ì¢‹ë‹¤. ìµœì ì˜ ë¶„í•  ì´ˆí‰ë©´ ì°¾ê¸° Margin: cëŠ” ì„ í˜•ë¶„í• ì˜ ê° í´ë˜ìŠ¤ë³„ ê±°ë¦¬ ê° í´ë˜ìŠ¤ë³„ ê±°ë¦¬ë¥¼ í•©ì¹œ ê²ƒ Margin&#x3D;2cë¥¼ ìµœëŒ€í™” í•˜ëŠ”, w T x +b&#x3D;0 ì˜ ì§ì„ ì„ ì°¾ì•„ì•¼ í•˜ëŠ”ê²ƒ ì´ë‹¤. Marign ì„ ìµœëŒ€í™” ì‹œí‚¤ëŠ” ì´ˆí‰ë©´ì´ ìµœì  â€œLearning Theoryâ€ ì— ë”°ë¥´ë©´, Mariginì„ ìµœëŒ€í™” ì‹œí‚¤ëŠ” ì´ˆí‰ë©´ì´ ì¼ë°˜í™” ì˜¤ë¥˜ê°€ ê°€ì¥ ë‚®ê²Œ ë‚˜íƒ€ë‚¨(Test data ì—ì„œë„ ì¢‹ì€ ì ìˆ˜ê°€ ë‚˜ì˜¨ë‹¤) Margin:ì´ˆí‰ë©´ê³¼ ê°€ì¥ ê·¼ì ‘í•œ ê° í´ë˜ìŠ¤ ê´€ì¸¡ì¹˜ì™€ì˜ ê±°ë¦¬ì˜ í•©. Margin ìˆ˜ì‹ ìœ ë„ ì¼ë°˜ì ì¸ ë°©ë²• ì ê³¼ ì„  ì‚¬ì´ì˜ ê±°ë¦¬ ê±°ë¦¬ d ê°€ 2ê°œì´ë‹ˆê¹Œ 2&#x2F;||W|| Margin ìµœëŒ€í™” (ìµœì í™”) ||w|| ê°€ ë¶„ëª¨ì— ìˆê¸° ë•Œë¬¸ì— ê²°êµ­ ||w|| ë¥¼ ìµœì†Œí™” í•´ì£¼ëŠ”ê²ƒ ì´ 2&#x2F;||w|| ë¥¼ ìµœëŒ€í™” í•´ì£¼ëŠ”ê±°ë‘ ê°™ë‹¤ê³  í•  ìˆ˜ ìˆë‹¤ ìš°ë¦¬ëŠ” ê²°êµ­ w ê°’ì„ ìµœì†Œí™” ì‹œì¼œì£¼ëŠ”ê²ƒì´ ëª©ì ì´ê¸° ë•Œë¬¸ì— ì œê³±ì„ ì·¨í•´ì£¼ë“  ìƒìˆ˜ë¥¼ ê³±í•´ì£¼ëŠ” ìƒê´€ì´ ì—†ë‹¤ Lagrange Multiplier(ìˆ˜í•™ì  ê¸°ë²•) â‡’ ì œì•½ì¡°ê±´ì„ ìµœì í™” ì¡°ê±´ì— ë…¹ì—¬ë²„ë¦¬ëŠ” ê¸°ë²•. ğŸ’¡ ë¼ê·¸ë‘ì¥¬ë¥¼ ë‹¤ í’€ê³  ë‚˜ë©´ íŒë³„ì‹ì´ ë‚˜ì˜¨ë‹¤. Xi tranpose X ( í•™ìŠµë°ì´í„°ì™€ ë¶„ë¥˜í•  ë°ì´í„°ì˜ ë‚´ì ) 4. SVM(Support Vector Machine) íŒë³„ì‹ì— ì„œí¬íŠ¸ë²¡í„°ë§Œ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ì•„ì›ƒë¼ì´ì–´ì— ëŒ€í•œ ì˜í–¥ì„ ì•ˆ ë°›ìŒ(KKT ì¡°ê±´ìœ¼ë¡œ ê±¸ëŸ¬ëƒ„) KNN ë˜í•œ ì´ì›ƒì„ í™•ì¸í•˜ëŠ” ê°œìˆ˜ì¸ Kì˜ í•œê³„ê°€ ìˆì–´ì„œ ì–´ëŠ elbow point ë¥¼ ì§€ë‚˜ì¹˜ë©´ ì •í™•ë„ê°€ ë–¨ì–´ì§„ë‹¤. â†’ ë¹„ìŠ·í•œ ì›ë¦¬ â‡’ svm ë˜í•œ ë¶„ë¥˜ë¥¼ ìœ íš¨í•˜ê²Œ í•˜ê¸°ìœ„í•´ì„œ support verctor ë§Œ ì´ìš©í•´ì¤€ë‹¤. ì„ í˜•ìœ¼ë¡œ ì™„ë²½íˆ ë‚˜ëˆ ì§€ì§€ ì•ŠëŠ” ë°ì´í„°ë¼ë©´ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ê²ŒëŠ” ìœ„ì˜ ëª¨ë¸ ë³´ë‹¤ ì•„ë˜ ëª¨ë¸ì´ ë” ì¢‹ì„ ê²ƒ ìœ¼ë¡œ ë³´ì¸ë‹¤. í•˜ì§€ë§Œ SVM ì˜ ì œì•½ì¡°ê±´ì—ëŠ” íŠ¸ë ˆì¸ë°ì´í„°ê°€ ì™„ë²½í•˜ê²Œ ë‚˜ëˆ„ì–´ì ¸ì•¼ í•œë‹¤ëŠ” ì œì•½ ì¡°ê±´ì´ ê±¸ë ¤ìˆë‹¤. ì–´ë–»ê²Œ í•˜ë©´ ì¢‹ì„ê¹Œ? Slack Variable for â€œSoft Marginâ€ Soft Margin SVM Non-linear SVM Reference: í•œêµ­ê³µí•™ëŒ€í•™êµ ê²½ì˜í•™ê³¼ ê°•ì§€í›ˆ êµìˆ˜ë‹˜ ê°•ì˜","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}],"tags":[{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}]},{"title":"Setting Git & Virtualenv","slug":"Setting_Git_&_Virtualenv","date":"2022-05-05T15:00:00.000Z","updated":"2023-02-28T07:50:54.784Z","comments":true,"path":"2022/05/06/Setting_Git_&_Virtualenv/","link":"","permalink":"https://jmj3047.github.io/2022/05/06/Setting_Git_&_Virtualenv/","excerpt":"","text":"Put Local folder into git repo Make folder â€˜exampleâ€™ and git repo â€˜example 123456789101112131415161718192021222324#in local cmd example foldergit init #add remote repogit remote add origin &#x27;repo https&#x27;#bring files in repo to localgit pull origin master #bring local files to git repogit add .git commit -m &#x27;updated&#x27;git push orgin master #check remotegit remote -v#check current statusgit status#error: failed to push some refs to &#x27;https://github.com/jmj3047/.git&#x27;#force to push git push -f origin master Setting virtual env in window&#x2F;linux1234567#****use virtual env no matter what****&gt;python -m venv env_name&gt;source env_name/Scripts/activate #window&gt;source env_name/bin/activate #linux#put all the version of modules in requirements.txt&gt;pip install -r requirements.txt","categories":[{"name":"Setting","slug":"Setting","permalink":"https://jmj3047.github.io/categories/Setting/"}],"tags":[{"name":"Git","slug":"Git","permalink":"https://jmj3047.github.io/tags/Git/"},{"name":"Virtualenv","slug":"Virtualenv","permalink":"https://jmj3047.github.io/tags/Virtualenv/"}],"keywords":[{"name":"Setting","slug":"Setting","permalink":"https://jmj3047.github.io/categories/Setting/"}]},{"title":"What is Transformer","slug":"What_is_Transformer","date":"2022-05-05T15:00:00.000Z","updated":"2022-10-15T08:08:32.440Z","comments":true,"path":"2022/05/06/What_is_Transformer/","link":"","permalink":"https://jmj3047.github.io/2022/05/06/What_is_Transformer/","excerpt":"","text":"Transformerë€?íŠ¸ëœìŠ¤í¬ë¨¸(Transformer)ëŠ” êµ¬ê¸€ì—ì„œ ë°œí‘œí•œ ë…¼ë¬¸ â€œAttention is all you needâ€ì— ë‚˜ì˜¤ëŠ” ëª¨ë¸ì´ë‹¤. ì•„ë˜ ê¸€ì€ ì´ ë…¼ë¬¸ abstractì˜ ì¼ë¶€ë¶„ì´ë‹¤. The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU â€¦ ì—¬ê¸°ì„œë„ ì•Œ ìˆ˜ ìˆë“¯ì´, íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ì–´í…ì…˜(Attention) mechanismì„ ê¸°ë°˜ìœ¼ë¡œ ì—¬ëŸ¬ê°œì˜ ì¸ì½”ë”ì™€ ë””ì½”ë”ë¥¼ ì—°ê²°í•œ êµ¬ì¡°ë¥¼ ê°–ê³  ìˆë‹¤. ë˜í•œ CNN, RNN, LSTM ë“±ì˜ êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì— í•™ìŠµ ì‹œê°„ì´ í›¨ì”¬ ê°ì†Œëœ ì„±ëŠ¥ì„ ë‚´ì—ˆë‹¤ê³  í•œë‹¤. ê·¸ë ‡ë‹¤ë©´ ê·¸ êµ¬ì¡°ê°€ ë¬´ì—‡ì¸ì§€ ë” ì•Œì•„ë³´ë„ë¡ í•˜ì. (1) íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì…ë ¥ë¨¼ì € íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì…ë ¥ë¶€í„° ì•Œì•„ë³´ì, ë‹¨ì–´ ë²¡í„° ë°ì´í„°ê°€ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°€ì§€ê²Œ ë˜ëŠ”ë° ì´ ë•Œ ê° ë‹¨ì–´ì˜ ìœ„ì¹˜ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì–´ì•¼ í•œë‹¤. ì™œëƒí•˜ë©´ íŠ¸ëœìŠ¤í¬ë¨¸ì— ë‹¨ì–´ê°€ ì…ë ¥ë  ë•Œ ìˆœì°¨ì ìœ¼ë¡œ ë°›ì•„ì§€ì§€ ì•Šê¸° ë•Œë¬¸ì´ë‹¤. ë”°ë¼ì„œ ìˆœì„œ ì •ë³´ë¥¼ ë”í•´ì£¼ì–´ì•¼ í•˜ê¸° ë•Œë¬¸ì— ìœ„ì¹˜ ì •ë³´ë¥¼ ê° ë‹¨ì–´ ë²¡í„°ë§ˆë‹¤ ë”í•´ì£¼ì–´ì•¼ í•˜ëŠ”ë°, ì´ ê³¼ì •ì„ í¬ì§€ì…”ë„ ì¸ì½”ë”©(positional encoding)ì´ë¼ê³  í•œë‹¤. í¬ì§€ì…”ë„ ì¸ì½”ë”© ê°’ì„ ë”í•´ì£¼ê¸° ìœ„í•´ì„œëŠ” ì‚¬ì¸í•¨ìˆ˜ì™€ ì½”ì‚¬ì¸í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œ ì•„ë˜ ë‘ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤. ìœ„ ì‹ì—ì„œ posëŠ” ì…ë ¥ëœ ë°ì´í„°ì˜ ì„ë² ë”© ë²¡í„°(ëª‡ë²ˆì§¸ ë‹¨ì–´ì¸ì§€)ë¥¼, iëŠ” ì„ë² ë”© ë²¡í„°ë‚´ì˜ ì°¨ì›ì˜ ì¸ë±ìŠ¤(0~512)ë¥¼ ëœ»í•œë‹¤. ì„ë² ë”© ë²¡í„°ë‚´ì˜ ì°¨ì›ì´ë€ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì˜ ì¸ì½”ë”ì™€ ë””ì½”ë”ì—ì„œ ì •í•´ì§„ ì…ë ¥ê³¼ ì¶œë ¥ì˜ í¬ê¸°ë¥¼ ë§í•œë‹¤. ë…¼ë¬¸ìƒì—ì„œ ì´ ì°¨ì›ì„ 512ë¡œ ì„¤ì •í–ˆìœ¼ë©´ ì´ ì°¨ì›ì€ ì¸ì½”ë”ì˜ ê°’ì„ ë””ì½”ë”ë¡œ ë³´ë‚¼ë•Œ ê°’ì„ ìœ ì§€í•˜ë„ë¡ í•œë‹¤. ë‹¤ì‹œ ëŒì•„ì™€ì„œ, ìœ„ í•¨ìˆ˜ì—ì„œ ì°¨ì›ì´ 2i(ì§ìˆ˜)ì¸ì§€ 2i+1(í™€ìˆ˜)ì¸ì§€ì— ë”°ë¼ì„œ ì‚¬ìš©í•˜ëŠ” í•¨ìˆ˜ê°€ ë‹¤ë¥´ë‹¤. ì§ìˆ˜ ì°¨ì›ì˜ ê²½ìš° ì‚¬ì¸í•¨ìˆ˜, í™€ìˆ˜ì°¨ì›ì˜ ê²½ìš° ì½”ì‚¬ì¸ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ê²Œëœë‹¤. (2)ì¸ì½”ë”(Encoder)ì˜ êµ¬ì¡° ìœ„ ì´ë¯¸ì§€ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ë…¼ë¬¸ì— í•¨ê»˜ ì‹¤ë ¤ ìˆëŠ” ì´ë¯¸ì§€ë¡œ, íŠ¸ëœìŠ¤í¬ë¨¸ì˜ êµ¬ì¡°ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.ì—¬ê¸°ì„œ ì™¼ìª½ ë¶€ë¶„ì´ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì¸ì½”ë” ë¶€ë¶„ì¸ë°, ì¸ì½”ë”ì˜ êµ¬ì¡°ëŠ” ì–´ë–»ê²Œ ì´ë£¨ì–´ì¡Œì„ê¹Œ? ë¨¼ì € ì´ë¯¸ì§€ ì™¼ìª½ ì•„ë˜ë¥¼ ë³´ì. Input dataê°€ ë“¤ì–´ê°€ê²Œ ë˜ë©´ Input Embeddingì„ ê±°ì¹˜ê²Œ ë˜ëŠ”ë°, ì—¬ê¸°ì„œëŠ” ë¬¸ìì—´ì¸ ë‹¨ì–´ ë°ì´í„°ë¥¼ ë²¡í„°í˜•íƒœë¡œ ë³€í™˜í•´ ì¤€ë‹¤(ë‹¨ì–´ ê¸¸ì´ X ë²¡í„°ì°¨ì›ì˜ í–‰ë ¬). ê·¸ë¦¬ê³  ë‚˜ì„œ ìœ„ì—ì„œ ì„¤ëª…í•œ í¬ì§€ì…”ë„ ì¸ì½”ë”©ì„ ìˆ˜í–‰í•´ì£¼ê²Œ ëœë‹¤. ê·¸ëŸ¬ê³  ë‚˜ì„œ ë°•ìŠ¤ë¡œ í‘œí˜„ëœ ì¸ì½”ë”ì— ë“¤ì–´ê°€ê²Œ ëœë‹¤. ì¸ì½”ë” ì•ˆì—ì„œëŠ” í¬ê²Œ Multi-Head Attentionê³¼ Feed Forwardê³¼ì •ì´ ìˆ˜í–‰ë˜ëŠ”ë°, Multi-Head Attentionì€ ì…€í”„ ì–´í…ì…˜ì´ ë³‘ë ¬ì ìœ¼ë¡œ ì‚¬ìš©ëœ ê²ƒì„ ë§í•˜ë©°, Feed Forwardë€ í”¼ë“œí¬ì›Œë“œ ì‹ ê²½ë§ êµ¬ì¡°ë¥¼ ì˜ë¯¸í•œë‹¤. í•œí¸, ìœ„ì—ì„œ ì ê¹ ì–¸ê¸‰í–ˆì§€ë§Œ íŠ¸ëœìŠ¤í¬ë¨¸ì—ì„œëŠ” ì—¬ëŸ¬ ê°œì˜ ì¸ì½”ë”ì™€ ë””ì½”ë”ë¥¼ ìŒ“ì€ êµ¬ì¡°ë¥¼ ê°–ê³  ìˆë‹¤. ì¦‰, ì¸ì½”ë”ê°€ 1ê°œê°€ ì•„ë‹ˆë¼ëŠ” ëœ»ì¸ë°, ë…¼ë¬¸ì—ì„œëŠ” 6ê°œì˜ ì¸ì½”ë” ì¸µì„ ì‚¬ìš©í–ˆë‹¤ê³  í•˜ë‹ˆ, 6ê°œë¼ê³  ì„¤ì •í•˜ë„ë¡ í•˜ê² ë‹¤. ì•„ë¬´íŠ¼, ì¸ì½”ë” ê³¼ì •ì„ ì´ 6ë²ˆ ë°˜ë³µí•œë‹¤ê³  ìƒê°í•˜ë©´ ëœë‹¤. *ì…€í”„ ì–´í…ì…˜ì´ë€?ì…€í”„ ì–´í…ì…˜ì´ë€ ìê¸° ìì‹ ì—ê²Œ ì–´í…ì…˜ í•¨ìˆ˜ë¥¼ ìˆ˜í–‰í•˜ëŠ” ê²ƒì„ ë§í•˜ëŠ”ë°, ê·¸ë ‡ë‹¤ë©´ ì–´í…ì…˜ì´ë€ ë¬´ì—‡ì¼ê¹Œ? ì–´í…ì…˜ì—ì„œë„ ë‹¤ì–‘í•œ ì¢…ë¥˜ê°€ ìˆëŠ”ë° ê°„ë‹¨íˆ ë§í•˜ìë©´, ì¿¼ë¦¬(Query)ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ì´ ì¿¼ë¦¬ì™€ ì—¬ëŸ¬ê°œì˜ í‚¤(Key)ì™€ì˜ ìœ ì‚¬ë„ë¥¼ ê°ê° êµ¬í•˜ê³ , êµ¬í•œ ìœ ì‚¬ë„ë¥¼ ê°€ì¤‘ì¹˜ë¡œ ì„¤ì •í•˜ì—¬ ê°ê°ì˜ ê°’(value)ì„ êµ¬í•œ ë’¤, ì´ ê°’(ìœ ì‚¬ë„ê°€ ë°˜ì˜ëœ ê°’)ë“¤ì„ ëª¨ë‘ ê°€ì¤‘í•©í•˜ì—¬ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë§í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´, í•œ í…ìŠ¤íŠ¸ ë¬¸ì¥ì´ ì¿¼ë¦¬ë¡œ ì…ë ¥ë  ë•Œ, ê° ë‹¨ì–´ ë²¡í„°ë“¤ê³¼ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•´ ì´ ìœ ì‚¬ë„ë¥¼ ê°€ì¤‘í•©í•˜ì—¬ ë°˜í™˜ëœ ê°’ì´ ê·¸ ë¬¸ì¥ì˜ ì–´í…ì…˜ ê°’ì´ ëœë‹¤. ê·¸ë ‡ë‹¤ë©´ ì…€í”„ ì–´í…ì…˜ ê°’ì„ êµ¬í•˜ê¸° ìœ„í•´ì„œ ì…ë ¥ëœ ë¬¸ì¥ì˜ ë‹¨ì–´ ë²¡í„°(ì¿¼ë¦¬)ì— ëŒ€í•´ ì¿¼ë¦¬(query),í‚¤(key), ê°’(value) ë²¡í„°ê°€ ì •ì˜ë˜ì–´ì•¼ í•  ê²ƒì´ë‹¤. ê·¸ ê³¼ì •ì€ ì•„ë˜ ì´ë¯¸ì§€ë¥¼ í†µí•´ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë‹¤. â€˜studentâ€™ë¼ëŠ” ë‹¨ì–´ ë²¡í„°ê°€ ì…ë ¥ë˜ì—ˆì„ ë•Œ, ê°ê° ì¿¼ë¦¬, í‚¤ ê°’ì˜ ê°€ì¤‘ì¹˜ í–‰ë ¬ì„ ê³±í•´ì£¼ì–´ ì¿¼ë¦¬, í‚¤, ê°’ ë²¡í„°ë¥¼ ì–»ì–´ë‚¸ë‹¤. ì´ë ‡ê²Œ ì¿¼ë¦¬ ë²¡í„°, í‚¤ ë²¡í„°, ê°’ ë²¡í„°ë¥¼ ì–»ì–´ëƒˆë‹¤ë©´ ì¿¼ë¦¬ ë²¡í„°ëŠ” ëª¨ë“  í‚¤ ë²¡í„°ì— ëŒ€í•´ ì–´í…ì…˜ ìŠ¤ì½”ì–´(attention score)ë¥¼ êµ¬í•˜ê²Œ ë˜ê³ , ì´ë¥¼ ì´ìš©í•˜ì—¬ ëª¨ë“  ê°’ ë²¡í„°ë¥¼ ê°€ì¤‘í•© í•˜ì—¬ ì–´í…ì…˜ ê°’ì„ êµ¬í•˜ê²Œ ëœë‹¤. í•œí¸, ì´ëŸ¬í•œ ì—°ì‚°ì€ ê° ë‹¨ì–´ë§ˆë‹¤ê°€ ì•„ë‹Œ ë¬¸ì¥ ì „ì²´ì— ëŒ€í•´ì„œ í–‰ë ¬ ì—°ì‚°ìœ¼ë¡œë„ ì¼ê´„ì ìœ¼ë¡œ ì—°ì‚°ì´ ê°€ëŠ¥í•œë°, ìœ„ì™€ ê°™ì´ ë¬¸ì¥ì— ëŒ€í•œ ì¿¼ë¦¬ ë²¡í„°, í‚¤ ë²¡í„°ì˜ ì—°ì‚°ì„ í†µí•´ ê°’ ë²¡í„° í–‰ë ¬ì„ êµ¬í•  ìˆ˜ ìˆê²Œ ëœë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ ì¿¼ë¦¬ë²¡í„°ì™€ í‚¤ ë²¡í„°ê°€ ì—°ì‚°ë˜ì–´ ë‚˜ì˜¨ í–‰ë ¬ì— ì „ì²´ì ìœ¼ë¡œ íŠ¹ì • ê°’(keyë²¡í„° ì°¨ì›ì˜ ì œê³±ê·¼ ê°’)ì„ ë‚˜ëˆ„ì–´ ì¤€ ë’¤, ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ë¥¼ ì ìš©í•´ì£¼ê³ , ê°€ì¤‘ì¹˜ê°€ ê³„ì‚°ëœ ê°’ ë²¡í„°ë¥¼ ê³±í•˜ê²Œ ë˜ë©´ ìµœì¢…ì ìœ¼ë¡œ ê° ë‹¨ì–´ì˜ ì–´í…ì…˜ ê°’ì„ ê°€ì§€ëŠ” ì–´í…ì…˜ ê°’ í–‰ë ¬ì´ ë„ì¶œëœë‹¤. ì¦‰, ìš”ì•½í•˜ìë©´ ì–´í…ì…˜ í•¨ìˆ˜ëŠ” ì¿¼ë¦¬(Query)ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ì´ ì¿¼ë¦¬ì™€ ì—¬ëŸ¬ê°œì˜ í‚¤(key)ì™€ì˜ ìœ ì‚¬ë„ë¥¼ ê°ê° êµ¬í•˜ê³ , êµ¬í•œ ìœ ì‚¬ë„ë¥¼ ê°€ì¤‘ì¹˜ë¡œ ì„¤ì •í•˜ì—¬ ê°ê°ì˜ ê°’(value)ì„ êµ¬í•œ ë’¤, ìœ ì‚¬ë„ê°€ ë°˜ì˜ëœ ê°’ë“¤ì„ ëª¨ë‘ ê°€ì¤‘í•© í•˜ì—¬ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë§í•œë‹¤. *ë©€í‹° í—¤ë“œ ì–´í…ì…˜(Multi-Head Attention)ì´ë€?ì•ì—ì„œ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì¸ì½”ë”ì—ì„œëŠ” ì–´í…ì…˜ì´ ë³‘ë ¬ì ìœ¼ë¡œ ìˆ˜í–‰ë˜ëŠ” ë©€í‹° í—¤ë“œ ì–´í…ì…˜ì´ ìˆ˜í–‰ëœë‹¤ê³  í–ˆë‹¤. ë…¼ë¬¸ì—ì„œëŠ” 512ì°¨ì›ì˜ ë²¡í„°ë¥¼ 8ë¡œ ë‚˜ëˆ„ì–´ 54ì°¨ì›ì˜ Query, Key, Value ë²¡í„°ë¡œ ë°”ê¾¸ì–´ì„œ ì–´í…ì…˜ í•¨ìˆ˜ë¥¼ ë³‘ë ¬ì ìœ¼ë¡œ ìˆ˜í–‰í•œ ê²ƒì¸ë°, ê·¸ë ‡ë‹¤ë©´ ì™œ ì´ë ‡ê²Œ ìˆ˜í–‰í•œ ê²ƒì¼ê¹Œ? ì¦‰, ì°¨ì›ì„ ë‚˜ëˆ„ì–´ì„œ ì–´í…ì…˜ í•¨ìˆ˜ë¥¼ ìˆ˜í–‰í•œ ë’¤, ê°€ì¤‘ì¹˜ í–‰ë ¬ì„ ê³±í•´ì£¼ê³  ì´ë¥¼ ë‹¤ì‹œ í•©ì¹˜ê²Œ ë˜ëŠ”ê±´ë°, ë…¼ë¬¸ì— ë”°ë¥´ë©´ single attention functionì„ í•˜ëŠ” ê²ƒë³´ë‹¤ ë³‘ë ¬ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ ëª¨ë¸ì´ í•™ìŠµí•˜ëŠ” ë°ì—ëŠ” ë” íš¨ê³¼ì ì´ì—ˆìœ¼ë©°, ëª¨ë¸ì´ ë‹¤ë¥¸ ì˜ì—­(ê³¼ê±°ì‹œì ê³¼ ë¯¸ë˜ì‹œì )ì— ìˆëŠ” ì •ë³´ë“¤ì„ ì°¸ì¡°í•  ìˆ˜ ìˆë‹¤ê³ í•œë‹¤. ë”°ë¼ì„œ ì¶œë ¥ëœ ê°’ë“¤ì€ ì¸ì½”ë”ì˜ ì…ë ¥ ê°’ì˜ ì°¨ì›ê³¼ ë™ì¼í•˜ê²Œ ìœ ì§€ëœë‹¤. *í”¼ë“œ í¬ì›Œë“œ ì‹ ê²½ë§ì´ë€?ì¸ì½”ë” ì•ˆì—ì„œ Multi-Head Attentionì´ ìˆ˜í–‰ë˜ê³  ë‚˜ë©´ Feed Forwardê°€ ìˆ˜í–‰ëœë‹¤ê³  í–ˆì—ˆëŠ”ë°, Feed ForwardëŠ” ë¬´ì—‡ì¼ê¹Œ? Feed ForwardëŠ” ì¼ì¢…ì˜ ì‹ ê²½ë§ìœ¼ë¡œ Feed Forward Neural Networkë¥¼ ì¤„ì—¬ì„œ FFNNì´ë¼ê³  í•œë‹¤. FFNNì˜ ì¢…ë¥˜ë„ ì—¬ëŸ¬ê°€ì§€ê°€ ìˆëŠ”ë°, íŠ¸ëœìŠ¤ í¬ë¨¸ì˜ ì¸ì½”ë” ì¸µì—ëŠ” í¬ì§€ì…˜ ì™€ì´ì¦ˆ(Position-wise) FFNNì„ ì‚¬ìš©í•œë‹¤. í¬ì§€ì…˜ ì™€ì´ì¦ˆ FFNNì€ Fully-connected FFNNê³¼ ê°™ì€ ê¸°ëŠ¥ì„ í•˜ëŠ”ë°, ì•„ë˜ì™€ ê°™ì€ ì—°ì‚°ì„ ìˆ˜í–‰í•œë‹¤. ìœ„ ì‹ì—ì„œ xì˜ ê°’ì€ Multi-Head Attentionì—ì„œ ì¶œë ¥ëœ í–‰ë ¬ ê°’ì´ë‹¤. ë°˜ë©´, ê°€ì¤‘ì¹˜ë¥¼ ì˜ë¯¸í•˜ëŠ” W1, W2, b1, b2ëŠ” ê°€ì¤‘ì¹˜ ê°’ìœ¼ë¡œ ì¸ì½”ë” ë§ˆë‹¤ ë‹¤ë¥¸ ê°’ì„ ê°€ì§€ì§€ë§Œ í•˜ë‚˜ì˜ ì¸ì½”ë” ì¸µ ì•ˆì—ì„œëŠ” ë¬¸ì¥ê³¼ ë‹¨ì–´ë“¤ë§ˆë‹¤ ë™ì¼í•˜ê²Œ ì‚¬ìš©ëœë‹¤ê³  í•œë‹¤. ì´ë ‡ê²Œ í”¼ë“œ í¬ì›Œë“œ ì‹ ê²½ë§ê¹Œì§€ ê±°ì¹˜ê²Œ ë˜ë©´ í•œ ì¸ì½”ë”ì˜ ì¶œë ¥ê°’ì´ ë„ì¶œ ë˜ê³ , ì´ ê°’ì€ ë‹¤ì‹œ ë‘ë²ˆì§¸ ì¸ì½”ë” ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°€ê²Œ ë˜ë©° ì´ ê³¼ì •ì´ ë°˜ë³µëœë‹¤. *Add &amp; Normí•œí¸ ì¸ì½”ë”ì˜ êµ¬ì¡°ë¥¼ ë³´ì—¬ì¤€ ì´ë°ë¥¼ ë‹¤ì‹œ ë³´ê³  ì˜¤ë©´ 2ê°œì˜ ì„œë¸Œì¸µì¸ Multi-Head Attentionê³¼ Feed Forwardê°€ ê°ê° ëë‚˜ê³  ë‚˜ë©´ Add &amp; Norm ì´ë¼ëŠ” ë‹¨ê³„ê°€ ìˆ˜í–‰ëœë‹¤. ì´ê²ƒì€ ë˜ ë¬´ì—‡ì¼ê¹Œ? ë…¼ë¬¸ì˜ ì¼ë¶€ë¶„ì„ ì½ì–´ë³´ë©´ Add &amp; Normì´ë€ ë°”ë¡œ ë‘ê°œì˜ ì„œë¸Œì¸µì„ residual connection í•´ì£¼ê³  layer normalizationì„ í•´ì£¼ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. Residual connectionê³¼ layer normalizationì— ëŒ€í•´ ì§§ê²Œ ìš”ì•½í•˜ìë©´, residual connectionì€ ì„œë¸Œì¸µì˜ ì…ë ¥ê³¼ ì¶œë ¥ì„ ë” í•˜ëŠ” ê²ƒì´ë‹¤. ì´ëŸ¬í•œ ì•Œê³ ë¦¬ì¦˜ì€ RNN, VGG êµ¬ì¡°ì—ì„œë„ ë³¼ ìˆ˜ ìˆê³ , ì´ëŸ¬í•œ ì—°ì‚°ì´ ê°€ëŠ¥í•œ ê²ƒì€ ì…ë ¥ ë°ì´í„°ì™€ ì¶œë ¥ ë°ì´í„°ê°€ ë™ì¼í•œ ì°¨ì›ì„ ê°–ê³  ìˆê¸° ë•Œë¬¸ì´ë¼ê³  í•œë‹¤. ë°˜ë©´, layer normalizationì€ ì •ê·œí™”ë¥¼ í•˜ëŠ” ê³¼ì •ìœ¼ë¡œ ì¶œë ¥ëœ ê°’ë“¤ì— ëŒ€í•´ì„œ í‰ê· ê³¼ ë¶„ì‚°ì„ êµ¬í•´ì„œ ì •ê·œí™”ë¥¼ í•˜ëŠ” ê²ƒì„ ë§í•œë‹¤. ì•ì—ì„œ ì…ë ¥ë°ì´í„°ì¸ 512ì°¨ì›ì˜ ë²¡í„°ë¥¼ 8ë¡œ ë‚˜ëˆ„ì–´ ì–´í…ì…˜ í•¨ìˆ˜ë¥¼ ë³‘ë ¬ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ì˜€ë‹¤ê³  í–ˆëŠ”ë°, ê·¸ë ‡ê²Œ ì¶œë ¥ëœ 8ê°œì˜ ê°’ë“¤ë¡œ layer normalizationì„ í•˜ëŠ” ê²ƒì´ë‹¤. (3)ë””ì½”ë”(Decoder)ì˜ êµ¬ì¡° ë‹¤ì‹œ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ êµ¬ì¡°ë¥¼ ì‚´í´ë³´ì. ì§€ê¸ˆê¹Œì§€ ì™¼ìª½ì— ìˆëŠ” ì¸ì½”ë”ì— ëŒ€í•´ ì‚´í´ ë³´ì•˜ê³ , ì´ì œ ì˜¤ë¥¸ìª½ì— ìˆëŠ” ë””ì½”ë”ì— ëŒ€í•´ ì‚´í´ë³´ë„ë¡ í•˜ê² ë‹¤. ë””ì½”ë”ëŠ” ì¸ì½”ë”ì—ì„œ ë„˜ê²¨ë°›ì€ ê°’ì— ëŒ€í•´ Multi-Head Attentionê³¼ Feed Forwardë¥¼ ìˆ˜í–‰í•˜ê¸° ì „ output dataì— ëŒ€í•´ ì„ë² ë”©ê³¼ í¬ì§€ì…”ë„ ì¸ì½”ë”©ì„ í•œ ê°’ì„ ì…ë ¥ ë°›ëŠ”ë‹¤. ê·¸ë¦¬ê³  ì¸ì½”ë”ì™€ëŠ” ë‹¤ë¥´ê²Œ Masked Multi-Head Attentionì´ë¼ëŠ” ê²ƒì„ í•´ì£¼ê²Œ ëœë‹¤. *Masked Multi-Head Attentionì´ë€?Masked Multi-Head Attentionì€ ë§ê·¸ëŒ€ë¡œ Multi-Head Attentionì—ì„œ Maskê¸°ëŠ¥ì´ ë“¤ì–´ê°„ ê²ƒì´ë‹¤. ì•ì—ì„œ Multi-Head Attentionì€ ì…€í”„ì–´í…ì…˜ì„ ë³‘ë ¬ì ìœ¼ë¡œ ìˆ˜í–‰í•œ ê²ƒì„ ì˜ë¯¸í–ˆì—ˆë‹¤. ë”°ë¼ì„œ ë‹¤ë¥¸ ì˜ì—­ì— ìˆëŠ”, ì¦‰ ë¯¸ë˜ì˜ ì‹œì ì— ìˆëŠ” ë‹¨ì–´ì˜ ì •ë³´ë„ ì•Œ ìˆ˜ ìˆê²Œ ëœë‹¤ê³  í–ˆì—ˆë‹¤. ì´ëŸ¬í•œ ì´ìœ ë¡œ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ë””ì½”ë”ì—ëŠ” í˜„ì¬ì‹œì ë³´ë‹¤ ë¯¸ë˜ì— ìˆëŠ” ë‹¨ì–´ë¥¼ ì°¸ê³ í•´ ì˜ˆì¸¡í•˜ì§€ ëª»í•˜ê³  ì´ì „ì‹œì ë“¤ì— ìˆëŠ” ë‹¨ì–´ë“¤ë§Œ ì°¸ê³ í•  ìˆ˜ ìˆë„ë¡ ë§ˆìŠ¤í‚¹í•´ì¤˜ì•¼ í•œë‹¤. ì•„ë§ˆ ë¯¸ë˜ì— ìˆëŠ” ë‹¨ì–´ë¥¼ ì°¸ê³ í•´ ì˜ˆì¸¡í•˜ë„ë¡ í•œë‹¤ë©´ í•™ìŠµí•˜ëŠ”ë° ë„ì›€ì´ ë˜ì§€ ì•ŠëŠ”ê°€ ë³´ë‹¤. ë‹µì§€ë³´ê³  ë² ë¼ëŠ” ëŠë‚Œì´ë„ê¹Œ? ì•„ë¬´íŠ¼ ë§ˆìŠ¤í‚¹ì„ í•˜ê¸° ìœ„í•´ lood-ahead maskë¼ëŠ” ê²ƒì„ í•´ì£¼ëŠ”ë°, Multi-Head Attentionì„ í†µí•´ ë‚˜ì˜¨ í–‰ë ¬ ê°’ì— ëŒ€í•´ ë§ˆìŠ¤í‚¹ì„ í•˜ê³ ì í•˜ëŠ” ê°’ì—ëŠ” 1, ë§ˆìŠ¤í‚¹ì„ í•˜ì§€ ì•ŠëŠ” ê°’ì—ëŠ” 0ì„ ë¦¬í„´í•˜ë„ë¡ í•œë‹¤. ê·¸ë¦¬ê³  ë‚˜ì„œ Add &amp; Norm ê³¼ì •ì„ ìˆ˜í–‰í•´ì¤€ë’¤ ë„ì¶œëœ ê²°ê³¼ë¥¼ ë‹¤ìŒ ê²°ê³¼ë¡œ ë³´ë‚´ì¤€ë‹¤. *ë””ì½”ë”ì˜ Multi-Head Attentionê³¼ Feed Forwardë””ì½”ë”ì—ì„œ Masked Multi-Head Attentionì´ ìˆ˜í–‰ë˜ê³  ë‚˜ë©´ ê·¸ ë‹¤ìŒë¶€í„°ëŠ” ì¸ì½”ë”ì™€ ë§ˆì°¬ê°€ì§€ë¡œ Multi-Head Attentionê³¼ Feed Forwardê°€ ìˆ˜í–‰ëœë‹¤. ê·¼ë° ì´ë•Œ Multi-Head Attentionì— ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°€ëŠ” ê°’ë“¤ì„ ì˜ ì‚´í´ ë´ì•¼ í•œë‹¤. ì¸ì½”ë”ì—ì„œ ì¶œë ¥ëœ ê°’ê³¼ ë””ì½”ë” ì²«ë²ˆì§¸ ì„œë¸Œì¸µì—ì„œ ì¶œë ¥ëœ ê°’ì´ ì¸í’‹ìœ¼ë¡œ ë“¤ì–´ê°€ê¸° ë•Œë¬¸ì´ë‹¤. ë‘ë²ˆì§¸ ì„œë¸Œì¸µì¸ Multi-Head Attentionì—ì„œëŠ” ë§ˆì°¬ê°€ì§€ë¡œ ì…€í”„ì–´í…ì…˜ í•¨ìˆ˜ë¥¼ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ Query, Key, Value ë²¡í„°ê°€ ì…ë ¥ë˜ì–´ì•¼ í•œë‹¤. ì´ë•Œ QueryëŠ” ë””ì½”ë” ì²«ë²ˆì§¸ ì„œë¸Œì¸µì—ì„œ ì¶œë ¥ëœ ê°’ì´ í•´ë‹¹ë˜ê³ , Key ë²¡í„°ì™€ Value ë²¡í„°ëŠ” ë§ˆì§€ë§‰ ì¸ì½”ë”ì—ì„œ ì¶œë ¥ëœ ê°’ìœ¼ë¡œ ì…ë ¥ëœë‹¤. ê·¸ë¦¬ê³  ë˜‘ê°™ì´ Multi-Head Attentionì„ ìˆ˜í–‰í•´ì£¼ê²Œ ëœë‹¤. ì´ë ‡ê²Œ 6ê°œì˜ ë””ì½”ë”ë§ˆë‹¤ Multi-Head Attentionì˜ Query ë²¡í„°ëŠ” ë””ì½”ë” ì²«ë²ˆì§¸ ì„œë¸Œì¸µì˜ output, Keyë²¡í„°ì™€ Valueë²¡í„°ëŠ” ì¸ì½”ë”ì˜ outputì´ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°€ê²Œ ëœë‹¤. Reference attention ë…¼ë¬¸: https://arxiv.org/abs/1409.0473 https://wikidocs.net/31379 https:&#x2F;&#x2F;velog.io&#x2F;@seolini43&#x2F;íŒŒì´ì¬Transformerë¡œ-ì˜¤í”¼ìŠ¤-ì±—ë´‡-ë§Œë“¤ê¸°-ì´ë¡ í¸","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"NLP","slug":"Paper/NLP","permalink":"https://jmj3047.github.io/categories/Paper/NLP/"}],"tags":[{"name":"Transformer","slug":"Transformer","permalink":"https://jmj3047.github.io/tags/Transformer/"},{"name":"Attention","slug":"Attention","permalink":"https://jmj3047.github.io/tags/Attention/"},{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"NLP","slug":"Paper/NLP","permalink":"https://jmj3047.github.io/categories/Paper/NLP/"}]},{"title":"HTML with Python_CGI","slug":"HTML_with_Python_CGI","date":"2022-05-03T15:00:00.000Z","updated":"2022-05-04T08:38:19.000Z","comments":true,"path":"2022/05/04/HTML_with_Python_CGI/","link":"","permalink":"https://jmj3047.github.io/2022/05/04/HTML_with_Python_CGI/","excerpt":"","text":"** code: https://github.com/jmj3047/mj_chatbot_prac&#x2F;blob&#x2F;c5bec233b833b24345deeffe7391621415dc1dcb&#x2F;cgi_webpython.py Making Website with CGI Constructing Web Server: Download and Install Apche Official: Installing Apache in window(https://httpd.apache.org/docs/2.4/platform/windows.html) Beginner version: Install Bitnami Wamp Stack push 1 or 2 click this button below it takes some time to install it Constructing Web Server: Bitnami Wamp Stack Start Bitnami Wamp Stack Click Go to Application: If you can see the site like the picture below, success! Start or Stop the server: Click Manage Server If program below is shut down, go to the folder where â€˜bitnami wamp stackâ€™ installed and click â€˜manager-windows.exeâ€™ Using Python in Web(HTML): Setting Apache Install python and apache find folder where apach installed(â€˜D:\\wamp\\apache2\\confâ€™) &gt; â€˜confâ€™ folder &gt; httpd.conf open httpd.conf file and search: 1LoadModule cgid_module modules/mod_cgid.so and if â€˜#â€™ exists in front of code, delete it Find tag in httpd.conf and add some lines 1234567891011121314151617181920212223242526272829303132333435&lt;Directory &quot;/Applications/mampstack-8.0.5-0/apache2/htdocs&quot;&gt; # # Possible values for the Options directive are &quot;None&quot;, &quot;All&quot;, # or any combination of: # Indexes Includes FollowSymLinks SymLinksifOwnerMatch ExecCGI MultiViews # # Note that &quot;MultiViews&quot; must be named *explicitly* --- &quot;Options All&quot; # doesn&#x27;t give it to you. # # The Options directive is both complicated and important. Please see # http://httpd.apache.org/docs/2.4/mod/core.html#options # for more information. # Options Indexes FollowSymLinks # # AllowOverride controls what directives may be placed in .htaccess files. # It can be &quot;All&quot;, &quot;None&quot;, or any combination of the keywords: # AllowOverride FileInfo AuthConfig Limit # AllowOverride None # # Controls who can get stuff from this server. # Require all granted ********** add this code ********** &lt;Files *.py&gt; Options ExecCGI AddHandler cgi-script .py &lt;/Files&gt; *********************************** &lt;/Directory&gt; htdocs ë””ë ‰í† ë¦¬ ë‚´ í™•ì¥ìê°€ pyì¸ ëª¨ë“  íŒŒì¼ì€ CGIê¸°ëŠ¥ì„ í™œì„±ì‹œí‚¤ê³  CGIë¡œ ì‹¤í–‰í•˜ë¼ëŠ” ì˜ë¯¸ Restart Apache Web Server in manager-osx Python file setting index.py ê°€ ìˆëŠ” htdocs ë””ë ‰í† ë¦¬ì—ì„œ index.py ì‹¤í–‰ í›„ ì•„ë˜ê°™ì´ ì…ë ¥(ë‹¤ë¥¸ íŒŒì´ì¬ íŒŒì¼ì„ ë§Œë“¤ì–´ë„ ìƒê´€ ì—†ìŒ) 123#!/usr/local/bin/python3 &gt;&gt;&gt; python.exe ê²½ë¡œ í™˜ê²½ë³€ìˆ˜ì— ì €ì¥í•´ì¤¬ë‹¤ë©´ !Pythonë§Œ í•´ë„ ë¨ print(&quot;Content-Type: text/html&quot;) print() index.html ì˜ ì½”ë“œ ë„£ê¸°( ë‹¤ë¥¸ html íŒŒì¼ ì´ì–´ë„ ë¨): index.pyê°€ ì‹¤í–‰ë˜ì—ˆì„ ë•Œ index.html ì˜ ì½”ë“œê°€ ì¶œë ¥ë˜ê²Œ í•´ì£¼ëŠ” ì½”ë“œ 12345678910111213141516171819202122#!/usr/local/bin/python3print(&quot;Content-Type: text/html&quot;)print()print(&#x27;&#x27;&#x27;&lt;!doctype html&gt; # ---&gt; ì¤„ë°”ê¿ˆì„ ìœ„í•´ docsting (&#x27;&#x27;&#x27; &#x27;&#x27;&#x27;) ì‚¬ìš©&lt;html&gt;&lt;head&gt; &lt;title&gt;WEB1 - Welcome&lt;/title&gt; &lt;meta charset=&quot;utf-8&quot;&gt;&lt;/head&gt;&lt;body&gt; &lt;h1&gt;&lt;a href=&quot;index.html&quot;&gt;WEB&lt;/a&gt;&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;a href=&quot;qs-1.html&quot;&gt;HTML&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;qs-2.html&quot;&gt;CSS&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;qs-3.html&quot;&gt;JavaScript&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h2&gt;WEB&lt;/h2&gt; &lt;p&gt;The World Wide Web (abbreviated WWW or the Web) is an information space where documents and other web resources are identified by Uniform Resource Locators (URLs), interlinked by hypertext links, and can be accessed via the Internet.[1] English scientist Tim Berners-Lee invented the World Wide Web in 1989. He wrote the first web browser computer program in 1990 while employed at CERN in Switzerland.[2][3] The Web browser was released outside of CERN in 1991, first to other research institutions starting in January 1991 and to the general public on the Internet in August 1991. &lt;/p&gt;&lt;/body&gt;&lt;/html&gt;&#x27;&#x27;&#x27;) ì›¹ ë¸Œë¼ìš°ì € ì£¼ì†Œì°½ì— localhost:8080&#x2F;index.py ì…ë ¥í•˜ê³  ì ‘ì† index.html íŒŒì¼ì˜ ë‚´ìš©ì´ ì˜ ì¶œë ¥ëœë‹¤ë©´ êµ¬í˜„ ì„±ê³µ Internal Server Error ê°€ í™•ì¸ëœë‹¤ë©´ ì—ë””í„°ì—ì„œ apache2&#x2F;logs ë””ë ‰í† ë¦¬ ë‚´ error_log íŒŒì¼ì— ìˆëŠ” ì—ëŸ¬ ì½”ë“œ í™•ì¸ ë° êµ¬ê¸€ë§ EXAMPLE123456789101112131415161718192021222324252627282930#!C:\\Python310\\python.exe ---&gt;íŒŒì´ì¬ ê²½ë¡œ# í•œê¸€ì´ êº ì§€ì§€ ì•Šìœ¼ë ¤ë©´ ê¼­ ë„£ì–´ì•¼ í•¨# -*- coding:utf-8 -*-import sysimport codecssys.stdout =codecs.getwriter(&quot;utf-8&quot;)(sys.stdout.detach())import cgi# cgitbëŠ” CGI í”„ë¡œê·¸ë˜ë°ì‹œ ë””ë²„ê¹…ì„ ìœ„í•œ ëª¨ë“ˆë¡œ cgitb.enable()í•  ê²½ìš° ëŸ°íƒ€ì„ ì—ëŸ¬ë¥¼ ì›¹ë¸Œë¼ìš°ì €ë¡œ ì „ì†¡í•¨# cgitb.enable() í•˜ì§€ ì•Šì€ ìƒíƒœë¡œ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•œ ê²½ìš° ì›¹ì„œë²„ëŠ” í´ë¼ì´ì–¸íŠ¸ì—ê²Œ HTTPì‘ë‹µ ì½”ë“œ 500ì„ ì „ì†¡í•¨import cgitbcgitb.enable()# HTTP ê·œê²©ì—ì„œ í—¤ë” ì „ì†¡ ì´í›„ì—ëŠ” ë°˜ë“œì‹œ ì¤„ ë°”ê¿ˆì„ í•˜ê²Œë˜ì–´ ìˆìŒìœ¼ë¡œ ë§ˆì§€ë§‰ì— \\r\\nì„ ì „ì†¡# ë§ˆì§€ë§‰ì— \\r\\nì„ ì „ì†¡í•˜ì§€ ì•Šìœ¼ë©´ ë¸Œë¼ìš°ì € ì¸¡ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒprint(&quot;Content-type: text/html;charset=utf-8\\r\\n&quot;)print(&quot;&quot;&quot; &lt;!doctype html&gt; &lt;html&gt; &lt;head&gt; &lt;meta charset=&#x27;utf-8&#x27;&gt; &lt;h1&gt;ì•ˆë…•?&lt;/h1&gt; &lt;h2&gt;Thank you so much&lt;/h2&gt; &lt;h3&gt;This page is made by Python&lt;/h3&gt; &quot;&quot;&quot;)a = 3+4+5b = a/3print(&#x27;bëŠ” :&#x27;, b)print(&quot;&lt;/head&gt;&quot;)print(&quot;&lt;/html&gt;&quot;) Result Reference https://daekiry.tistory.com/4?category=928946 https://daekiry.tistory.com/5?category=928946 https://daekiry.tistory.com/6 https:&#x2F;&#x2F;velog.io&#x2F;@ssoulll&#x2F;python-ì›¹-í˜ì´ì§€ë¥¼-CGIë¡œ-êµ¬í˜„","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"HTML","slug":"Python/HTML","permalink":"https://jmj3047.github.io/categories/Python/HTML/"}],"tags":[{"name":"Web Server","slug":"Web-Server","permalink":"https://jmj3047.github.io/tags/Web-Server/"},{"name":"CGI","slug":"CGI","permalink":"https://jmj3047.github.io/tags/CGI/"}],"keywords":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"HTML","slug":"Python/HTML","permalink":"https://jmj3047.github.io/categories/Python/HTML/"}]},{"title":"HTML with Python_Flask & Brython","slug":"HTML_with_Python_Flask&Brython","date":"2022-05-03T15:00:00.000Z","updated":"2023-05-17T11:50:35.489Z","comments":true,"path":"2022/05/04/HTML_with_Python_Flask&Brython/","link":"","permalink":"https://jmj3047.github.io/2022/05/04/HTML_with_Python_Flask&Brython/","excerpt":"","text":"Flask íŒŒì´ì¬ ê¸°ë°˜ ë§ˆì´í¬ë¡œ ì›¹ ê°œë°œ í”„ë ˆì„ì›Œí¬ ì›¹ ê°œë°œì˜ í•µì‹¬ê¸°ëŠ¥ë§Œ ê°„ê²½í•˜ê²Œ ìœ ì§€ í•„ìš”í•œ ê¸°ëŠ¥ì€ ë‹¤ë¥¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë‚˜ í”„ë ˆì„ì›Œí¬ë¡œ ì†ì‰½ê²Œ í™•ì¥ ì‹ ì†í•˜ê²Œ ìµœì†Œí•œì˜ ë…¸ë ¥ìœ¼ë¡œ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ ê°€ëŠ¥ Installation start virtualenv pip install flask Error â†’ note: could not find a version that satisfies the requirement flask â†’ ë„¤íŠ¸ì›Œí¬ ë¬¸ì œë¡œ ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì €ì¥ì†Œì— ì ‘ê·¼í•˜ì§€ ëª»í•  ê²½ìš° ë‚˜ì˜¤ëŠ” ë¬¸ì œ, â†’ ì§ì ‘ https://github.com/mitsuhiko/flask ìœ„ì¹˜ë¡œ ê°€ì„œ ì†ŒìŠ¤ ë°›ì•„ ì„¤ì¹˜ í•´ì•¼ í•¨. Strat Flask**code: https://github.com/jmj3047/mj_chatbot_prac&#x2F;blob&#x2F;c5bec233b833b24345deeffe7391621415dc1dcb&#x2F;flask_prac&#x2F;0.flask_hello.py 12345678910from flask import Flaskapp = Flask(__name__)@app.route(&#x27;/&#x27;)def hello_world(): return &#x27;Hello World!&#x27; if __name__ == &#x27;__main__&#x27;: app.debug =True app.run() ì†ŒìŠ¤ë¥¼ ì‹¤í–‰í•˜ê³ , terminal ì—ì„œ â€˜python flask_test.pyâ€™ ì…ë ¥ í›„ http://127.0.0.1:5000 ìœ¼ë¡œ ì ‘ê·¼ Process for starting Flask Application íŠ¹ì • URL í˜¸ì¶œ(request) : http://127.0.0.1:5000/ ë˜ëŠ” http://localhost:5000 íŠ¹ì • URL ë§¤í•‘ ê²€ìƒ‰ : @app.route(â€˜&#x2F;â€˜) íŠ¹ì • URLì— ë§¤ì¹­ëœ í•¨ìˆ˜(def í•¨ìˆ˜) ì‹¤í–‰ : def hello_world() ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì‹¤í–‰ : result ê²°ê³¼ ì‘ë‹µìœ¼ë¡œ ì „ì†¡(response): return result HTML ë¡œ í™”ë©´ì— ì¶œë ¥ ì¿ í‚¤(Cookie), ì„¸ì…˜(Session), ë¡œê¹…(logging) ë“± ì œê³µ Routing**code: https://github.com/jmj3047/mj_chatbot_prac&#x2F;blob&#x2F;c5bec233b833b24345deeffe7391621415dc1dcb&#x2F;flask_prac&#x2F;1.flask_login.py URLì„ í†µí•´ ì²˜ë¦¬í•  í•¸ë“¤ëŸ¬ë¥¼ ì°¾ëŠ” ê²ƒ í”Œë¼ìŠ¤í¬ëŠ” ë³µì¡í•œ URIë¥¼ í•¨ìˆ˜ë¡œ ì—°ê²°í•˜ëŠ” ë°©ë²•ì„ ì œê³µ URI ë¥¼ ì—°ê²°í•˜ëŠ” route() ë°ì½”ë ˆì´í„° í•¨ìˆ˜ ì œê³µ &#x2F; ì ‘ì† ì‹œ root_world() ê°€ í˜¸ì¶œ ë¨ &#x2F;hello ì ‘ì† ì‹œ hello_world() ê°€ í˜¸ì¶œ ë¨ 12345678910111213141516from flask import Flask, redirect, url_forapp = Flask(__name__)@app.route(&#x27;/&#x27;) #127.0.0.1:5000ì— ê°€ë©´ í•¨ìˆ˜ ì‹¤í–‰def root_world(): result = &#x27;root world&#x27; return result@app.route(&#x27;/hello&#x27;) #127.0.0.1:5000/hello ë¥¼ ê°€ë©´ ì‹¤í–‰def hello_world(): result = &#x27;hello world&#x27; return resultif __name__ == &#x27;__main__&#x27;: app.debug =True app.run() app.debugëŠ” ê°œë°œì˜ í¸ì˜ë¥¼ ìœ„í•´ ì¡´ì¬ Trueê°’ì„ ê²½ìš° ì½”ë“œë¥¼ ë³€ê²½í•˜ë©´ ìë™ìœ¼ë¡œ ì„œë²„ê°€ ì¬ ì‹¤í–‰ ë¨ ë˜í•œ, ì›¹ìƒì—ì„œ íŒŒì´ì¬ ì½”ë“œë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆê²Œ ë˜ë¯€ë¡œ, ìš´ì˜í™˜ê²½ì—ì„œ ì‚¬ìš©ì„ ìœ ì˜í•´ì•¼ í•¨. í˜„ì¬ ì ‘ê·¼ì€ ê°œë°œ ì†ŒìŠ¤ê°€ ì¡´ì¬í•˜ëŠ” ë¡œì»¬ì—ì„œë§Œ ì ‘ê·¼ ê°€ëŠ¥ ì™¸ë¶€ì—ì„œë„ ì ‘ê·¼ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ë ¤ë©´ app.run(host&#x3D;â€™0.0.0.0â€™)ë¡œ ì„œë²„ ì‹¤í–‰ ë¶€ë¥¼ ë³€ê²½í•´ì•¼ í•¨ 1234567891011121314151617181920212223242526272829303132from flask import Flask, redirect, url_forapp = Flask(__name__)@app.route(&#x27;/users/&lt;user_id&gt;&#x27;) #ë™ì  ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ URI ì ‘ì†# &lt;ë™ì ë³€ìˆ˜&gt;ë¥¼ ë·°í•¨ìˆ˜ì˜ ì¸ìë¡œ ì‚¬ìš©# &lt;ë™ì  ë³€ìˆ˜&gt; ë‹¤ìŒì— /ë¥¼ ë„£ìœ¼ë©´ ì•ˆë¨def user_id(userid): result = &#x27;user_id = &#x27; + userid return result@app.route(&#x27;/admin&#x27;)def hello_admin(): return &#x27;Hello Admin&#x27;@app.route(&#x27;/guest/&lt;guest&gt;&#x27;)def hello_guest(guest): return &#x27;Hello %s as Guest&#x27; % guest@app.route(&#x27;/user/&lt;name&gt;&#x27;)def hello_user(name): if name == &#x27;admin&#x27;: return redirect(url_for(&#x27;hello_admin&#x27;)) else: return redirect(url_for(&#x27;hello_guest&#x27;, guest=name))# url_for(): í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ëŠ” URIë¥¼ ë°˜í™˜# redirect(): ë‹¤ë¥¸ route ê²½ë¡œ ì´ë™(ë‹¤ë¥¸ í˜ì´ì§€ ì´ë™)if __name__ == &#x27;__main__&#x27;: app.debug = True app.run() Flask GET ë°©ì‹ìœ¼ë¡œ ê°’ ì „ì†¡ ë° ì²˜ë¦¬**code: https://github.com/jmj3047/mj_chatbot_prac&#x2F;blob&#x2F;c5bec233b833b24345deeffe7391621415dc1dcb&#x2F;flask_prac&#x2F;2.flask_app.py https://github.com/jmj3047/mj_chatbot_prac&#x2F;blob&#x2F;c5bec233b833b24345deeffe7391621415dc1dcb&#x2F;flask_prac&#x2F;templates&#x2F;login&#x2F;login_form_get.html mkdir templates í´ë” ìƒì„± login_form_get.html íŒŒì¼ ì‘ì„± get ë°©ì‹ ì§€ì • : method&#x3D;â€getâ€ 1234567891011121314151617181920from flask import Flask, request, session, render_templateapp = Flask(__name__)@app.route(&#x27;/login_form_get&#x27;) def login_form_get(): return render_template(&#x27;login/login_form_get.html&#x27;)@app.route(&#x27;/login_get_proc&#x27;, methods=[&#x27;GET&#x27;]) def login_get_proc(): user_id = request.args.get(&#x27;user_id&#x27;) user_pwd = request.args.get(&#x27;user_pwd&#x27;) if len(user_id) == 0 or len(user_pwd) == 0: return &#x27;no &#123;&#125; or &#123;&#125;&#x27;.format(user_id, user_pwd) return &#x27;welcome &#123;&#125;&#x27;.format(user_id)if __name__ == &#x27;__main__&#x27;: app.debug = True app.run() 12345678910111213141516&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;meta charset = &quot;UTF-8&quot;&gt; &lt;title&gt;login_form_get.html&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt; &lt;form action=&quot;/login_get_proc&quot; method=&quot;get&quot;&gt; ID: &lt;input type=&quot;text&quot;, name=&quot;user_id&quot;&gt;&lt;br&gt; PW: &lt;input type=&quot;password&quot;, name=&quot;user_pwd&quot;&gt;&lt;br&gt; &lt;input type=&quot;submit&quot;, value=&quot;Click&quot;&gt; &lt;/form&gt; &lt;/h1&gt; &lt;/body&gt;&lt;/html&gt; Brython**code:https://github.com/jmj3047/mj_chatbot_prac&#x2F;blob&#x2F;c5bec233b833b24345deeffe7391621415dc1dcb&#x2F;flask_prac&#x2F;templates&#x2F;brython_test.html pythonì„ HTML ì½”ë“œì— ì‚½ì…í•´ì„œ ì‚¬ìš© 12345678910111213141516171819202122&lt;html&gt; &lt;head&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;/path/to/brython.js&quot;&gt;&lt;/script&gt; &lt;/head&gt; &lt;body onload=&quot;brython()&quot;&gt; &lt;script type=&quot;text/python&quot;&gt; from browser import document, alert def echo(event): alert(document[&quot;zone&quot;].value) document[&quot;mybutton&quot;].bind(&quot;click&quot;, echo) &lt;/script&gt; &lt;input id=&quot;zone&quot;&gt;&lt;button id=&quot;mybutton&quot;&gt;click !&lt;/button&gt; &lt;/body&gt;&lt;/html&gt; Reference: https://essim92.tistory.com/8 https://code-examples.net/ko/q/dc0356 https://github.com/brython-dev/brython #Test Brython online(DEMO)","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"HTML","slug":"Python/HTML","permalink":"https://jmj3047.github.io/categories/Python/HTML/"}],"tags":[{"name":"Web Server","slug":"Web-Server","permalink":"https://jmj3047.github.io/tags/Web-Server/"},{"name":"Flask","slug":"Flask","permalink":"https://jmj3047.github.io/tags/Flask/"},{"name":"Brython","slug":"Brython","permalink":"https://jmj3047.github.io/tags/Brython/"}],"keywords":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"HTML","slug":"Python/HTML","permalink":"https://jmj3047.github.io/categories/Python/HTML/"}]},{"title":"Making Office Chatbot with Transformer","slug":"Making_Office_Chatbot_with_Transformer","date":"2022-05-03T15:00:00.000Z","updated":"2022-10-15T08:08:18.515Z","comments":true,"path":"2022/05/04/Making_Office_Chatbot_with_Transformer/","link":"","permalink":"https://jmj3047.github.io/2022/05/04/Making_Office_Chatbot_with_Transformer/","excerpt":"","text":"ì¼ìƒ ëŒ€í™”ì™€ ì˜¤í”¼ìŠ¤ ëŒ€í™” ë°ì´í„°ë¥¼ Transformer ëª¨ë¸ë¡œ í•™ìŠµì‹œì¼œ, ì§ˆë¬¸ì— ëŒ€í•œ ì ì ˆí•œ ë‹µë³€ì„ í•˜ëŠ” ì±—ë´‡ Data: í•œêµ­ì–´ëŒ€í™”ë°ì´í„°ì…‹(ì˜¤í”¼ìŠ¤ë°ì´í„°) ì‚¬ìš© (AIHUBì— â€˜ê°œë°©ë°ì´í„°-ì¸ì‹ê¸°ìˆ  ì–¸ì–´ì§€ëŠ¥-í•œêµ­ì–´ëŒ€í™”ë°ì´í„°ì…‹â€™ì—ì„œ ë¡œê·¸ì¸ í›„ ë‹¤ìš´ë¡œë“œ) GPU ì‚¬ìš© ê·¸ ì™¸ í™˜ê²½ 123batch size = 64buffer size = 20000epochs = 50 1. Environments1!pip install tensorflow_datasets 12345678import pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport reimport urllib.requestimport timeimport tensorflow_datasets as tfdsimport tensorflow as tf 12import osos.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;3&quot; 1234567with tf.device(&#x27;/device:GPU:3&#x27;): # í…ì„œ ìƒì„± a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]) b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]) c = tf.matmul(a, b) print(c) tf.Tensor( [[22. 28.] [49. 64.]], shape=(2, 2), dtype=float32) 2. ë°ì´í„° ì „ì²˜ë¦¬12data = pd.read_csv(&#x27;./ChatbotData.csv&#x27;)data = data[0:5290] 1data[:10] 123456789f = open(r&#x27;./conversation_office.txt&#x27;,&quot;r&quot;)lines = f.readlines()Q = []A = []for i in range(len(lines)) : if i%2 == 0 : Q.append(lines[i][2:-1]) A.append(lines[i+1][2:-1]) 123456import pandas as pddf = pd.DataFrame()df[&#x27;Q&#x27;] = Qdf[&#x27;A&#x27;] = Adf[&#x27;label&#x27;] = 1 1df[:10] 1234#ë‘ ë°ì´í„°ë¥¼ concat() í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ í•©ì³ í•˜ë‚˜ì˜ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë‚˜íƒ€ë‚´ì£¼ë„ë¡ í•¨train_data = pd.concat([data, df],ignore_index=True)train_data = train_data.sample(frac=1).reset_index(drop=True) #ë°ì´í„°ë¥¼ ëœë¤ìœ¼ë¡œ ì„ì–´ì£¼ëŠ” ì½”ë“œ 3. ë‹¨ì–´ ì§‘í•© ìƒì„±123456789101112131415# ë¬¸ì¥ ê·¸ëŒ€ë¡œ í•™ìŠµ ëª¨ë¸ì— ë„£ìœ¼ë©´ ëª¨ë¸ì´ ì¸ì‹ì„ í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì— ë‹¨ì–´ ì§‘í•©ì„ ë§Œë“¤ì–´ ì¤˜ì•¼ í•¨.# ì •ìˆ˜ ì¸ì½”ë”©ê³¼ íŒ¨ë”©ì„ í•´ì£¼ëŠ” ì‘ì—…ì„ í•´ì£¼ì–´ì•¼ í•¨#íŠ¹ìˆ˜ê¸°í˜¸ ë„ì–´ì“°ê¸°questions = []for sentence in train_data[&#x27;Q&#x27;]: sentence = re.sub(r&quot;([?.!,])&quot;, r&quot; \\1 &quot;, sentence) sentence = sentence.strip() questions.append(sentence) answers = []for sentence in train_data[&#x27;A&#x27;]: sentence = re.sub(r&quot;([?.!,])&quot;, r&quot; \\1 &quot;, sentence) sentence = sentence.strip() answers.append(sentence) 123456789# ì„œë¸Œì›Œë“œí…ìŠ¤íŠ¸ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸, ë‹µë³€ ë°ì´í„°ë¡œë¶€í„° ë‹¨ì–´ ì§‘í•©(Vocabulary) ìƒì„±tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus( questions + answers, target_vocab_size=2**13) # ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì— ëŒ€í•œ ì •ìˆ˜ ë¶€ì—¬START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]# ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì„ ê³ ë ¤í•˜ì—¬ ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸°ë¥¼ + 2VOCAB_SIZE = tokenizer.vocab_size + 2 123456789101112131415161718192021222324252627#ì •ìˆ˜ ì¸ì½”ë”©ê³¼ íŒ¨ë”©# ì„œë¸Œì›Œë“œí…ìŠ¤íŠ¸ì¸ì½”ë” í† í¬ë‚˜ì´ì €ì˜ .encode()ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤ë¥¼ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜.print(&#x27;ì„ì˜ì˜ ì§ˆë¬¸ ìƒ˜í”Œì„ ì •ìˆ˜ ì¸ì½”ë”© : &#123;&#125;&#x27;.format(tokenizer.encode(questions[20])))#ì¶œë ¥ : ì„ì˜ì˜ ì§ˆë¬¸ ìƒ˜í”Œì„ ì •ìˆ˜ ì¸ì½”ë”© : [8656, 331]# ìµœëŒ€ ê¸¸ì´ë¥¼ 40ìœ¼ë¡œ ì •ì˜MAX_LENGTH = 40# í† í°í™” / ì •ìˆ˜ ì¸ì½”ë”© / ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í° ì¶”ê°€ / íŒ¨ë”©def tokenize_and_filter(inputs, outputs): tokenized_inputs, tokenized_outputs = [], [] for (sentence1, sentence2) in zip(inputs, outputs): # encode(í† í°í™” + ì •ìˆ˜ ì¸ì½”ë”©), ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í° ì¶”ê°€ sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN tokenized_inputs.append(sentence1) tokenized_outputs.append(sentence2) # íŒ¨ë”© tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences( tokenized_inputs, maxlen=MAX_LENGTH, padding=&#x27;post&#x27;) tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences( tokenized_outputs, maxlen=MAX_LENGTH, padding=&#x27;post&#x27;) return tokenized_inputs, tokenized_outputs ì„ì˜ì˜ ì§ˆë¬¸ ìƒ˜í”Œì„ ì •ìˆ˜ ì¸ì½”ë”© : [2704, 1081, 13, 542] 1questions, answers = tokenize_and_filter(questions, answers) 1234#sample# ì •ìˆ˜ ì¸ì½”ë”©ê³¼ íŒ¨ë”©ì´ ëœ ê²°ê³¼ê°€ ì¶œë ¥print(questions[0])print(answers[0]) [10023 31 121 4282 10024 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [10023 3607 213 13 21 1 10024 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] 12from tensorflow.python.client import device_libdevice_lib.list_local_devices() 1234567891011121314151617181920212223#ì¸ì½”ë”ì™€ ë””ì½”ë”ì˜ ì…ë ¥ ë°ì´í„°ê°€ ë˜ë„ë¡ ë°°ì¹˜ í¬ê¸°ë¡œ ë°ì´í„°ë¥¼ ë¬¶ì–´ì¤Œ with tf.device(&#x27;/device:GPU:3&#x27;): BATCH_SIZE = 64 BUFFER_SIZE = 20000 dataset = tf.data.Dataset.from_tensor_slices(( &#123; &#x27;inputs&#x27;: questions, &#x27;dec_inputs&#x27;: answers[:, :-1] # ë””ì½”ë”ì˜ ì…ë ¥ / ë§ˆì§€ë§‰ íŒ¨ë”© í† í° ì œê±° &#125;, &#123; &#x27;outputs&#x27;: answers[:, 1:] # ë§¨ ì²˜ìŒ í† í°ì´ ì œê±° = ì‹œì‘ í† í° ì œê±° &#125;, )) dataset = dataset.cache() dataset = dataset.shuffle(BUFFER_SIZE) dataset = dataset.batch(BATCH_SIZE) dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE) 4. íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ ë§Œë“¤ê¸°123456789101112131415161718192021222324252627282930313233343536373839def transformer(vocab_size, num_layers, dff, d_model, num_heads, dropout, name=&quot;transformer&quot;): # ì¸ì½”ë”ì˜ ì…ë ¥ inputs = tf.keras.Input(shape=(None,), name=&quot;inputs&quot;) # ë””ì½”ë”ì˜ ì…ë ¥ dec_inputs = tf.keras.Input(shape=(None,), name=&quot;dec_inputs&quot;) # ì¸ì½”ë”ì˜ íŒ¨ë”© ë§ˆìŠ¤í¬ enc_padding_mask = tf.keras.layers.Lambda( create_padding_mask, output_shape=(1, 1, None), name=&#x27;enc_padding_mask&#x27;)(inputs) # ë””ì½”ë”ì˜ ë£©ì–´í—¤ë“œ ë§ˆìŠ¤í¬(ì²«ë²ˆì§¸ ì„œë¸Œì¸µ) look_ahead_mask = tf.keras.layers.Lambda( create_look_ahead_mask, output_shape=(1, None, None), name=&#x27;look_ahead_mask&#x27;)(dec_inputs) # ë””ì½”ë”ì˜ íŒ¨ë”© ë§ˆìŠ¤í¬(ë‘ë²ˆì§¸ ì„œë¸Œì¸µ) dec_padding_mask = tf.keras.layers.Lambda( create_padding_mask, output_shape=(1, 1, None), name=&#x27;dec_padding_mask&#x27;)(inputs) # ì¸ì½”ë”ì˜ ì¶œë ¥ì€ enc_outputs. ë””ì½”ë”ë¡œ ì „ë‹¬ëœë‹¤. enc_outputs = encoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff, d_model=d_model, num_heads=num_heads, dropout=dropout, )(inputs=[inputs, enc_padding_mask]) # ì¸ì½”ë”ì˜ ì…ë ¥ì€ ì…ë ¥ ë¬¸ì¥ê³¼ íŒ¨ë”© ë§ˆìŠ¤í¬ # ë””ì½”ë”ì˜ ì¶œë ¥ì€ dec_outputs. ì¶œë ¥ì¸µìœ¼ë¡œ ì „ë‹¬ëœë‹¤. dec_outputs = decoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff, d_model=d_model, num_heads=num_heads, dropout=dropout, )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask]) # ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ì„ ìœ„í•œ ì¶œë ¥ì¸µ outputs = tf.keras.layers.Dense(units=vocab_size, name=&quot;outputs&quot;)(dec_outputs) return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name) 1234567891011121314151617181920212223242526272829303132class PositionalEncoding(tf.keras.layers.Layer): def __init__(self, position, d_model): super(PositionalEncoding, self).__init__() self.pos_encoding = self.positional_encoding(position, d_model) def get_angles(self, position, i, d_model): angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32)) return position * angles def positional_encoding(self, position, d_model): angle_rads = self.get_angles( position=tf.range(position, dtype=tf.float32)[:, tf.newaxis], i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :], d_model=d_model) # ë°°ì—´ì˜ ì§ìˆ˜ ì¸ë±ìŠ¤(2i)ì—ëŠ” ì‚¬ì¸ í•¨ìˆ˜ ì ìš© sines = tf.math.sin(angle_rads[:, 0::2]) # ë°°ì—´ì˜ í™€ìˆ˜ ì¸ë±ìŠ¤(2i+1)ì—ëŠ” ì½”ì‚¬ì¸ í•¨ìˆ˜ ì ìš© cosines = tf.math.cos(angle_rads[:, 1::2]) angle_rads = np.zeros(angle_rads.shape) angle_rads[:, 0::2] = sines angle_rads[:, 1::2] = cosines pos_encoding = tf.constant(angle_rads) pos_encoding = pos_encoding[tf.newaxis, ...] print(pos_encoding.shape) return tf.cast(pos_encoding, tf.float32) def call(self, inputs): return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :] 1234567891011121314151617181920212223242526272829303132333435def create_padding_mask(x): mask = tf.cast(tf.math.equal(x, 0), tf.float32) # (batch_size, 1, 1, keyì˜ ë¬¸ì¥ ê¸¸ì´) return mask[:, tf.newaxis, tf.newaxis, :]# ë””ì½”ë”ì˜ ì²«ë²ˆì§¸ ì„œë¸Œì¸µ(sublayer)ì—ì„œ ë¯¸ë˜ í† í°ì„ Maskí•˜ëŠ” í•¨ìˆ˜def create_look_ahead_mask(x): seq_len = tf.shape(x)[1] look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0) padding_mask = create_padding_mask(x) # íŒ¨ë”© ë§ˆìŠ¤í¬ë„ í¬í•¨ return tf.maximum(look_ahead_mask, padding_mask)#encoderdef encoder(vocab_size, num_layers, dff, d_model, num_heads, dropout, name=&quot;encoder&quot;): inputs = tf.keras.Input(shape=(None,), name=&quot;inputs&quot;) # ì¸ì½”ë”ëŠ” íŒ¨ë”© ë§ˆìŠ¤í¬ ì‚¬ìš© padding_mask = tf.keras.Input(shape=(1, 1, None), name=&quot;padding_mask&quot;) # í¬ì§€ì…”ë„ ì¸ì½”ë”© + ë“œë¡­ì•„ì›ƒ embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs) embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32)) embeddings = PositionalEncoding(vocab_size, d_model)(embeddings) outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings) # ì¸ì½”ë”ë¥¼ num_layersê°œ ìŒ“ê¸° for i in range(num_layers): outputs = encoder_layer(dff=dff, d_model=d_model, num_heads=num_heads, dropout=dropout, name=&quot;encoder_layer_&#123;&#125;&quot;.format(i), )([outputs, padding_mask]) return tf.keras.Model( inputs=[inputs, padding_mask], outputs=outputs, name=name) 1234567891011121314151617181920212223242526272829def encoder_layer(dff, d_model, num_heads, dropout, name=&quot;encoder_layer&quot;): inputs = tf.keras.Input(shape=(None, d_model), name=&quot;inputs&quot;) # ì¸ì½”ë”ëŠ” íŒ¨ë”© ë§ˆìŠ¤í¬ ì‚¬ìš© padding_mask = tf.keras.Input(shape=(1, 1, None), name=&quot;padding_mask&quot;) # ë©€í‹°-í—¤ë“œ ì–´í…ì…˜ (ì²«ë²ˆì§¸ ì„œë¸Œì¸µ / ì…€í”„ ì–´í…ì…˜) attention = MultiHeadAttention( d_model, num_heads, name=&quot;attention&quot;)(&#123; &#x27;query&#x27;: inputs, &#x27;key&#x27;: inputs, &#x27;value&#x27;: inputs, # Q = K = V &#x27;mask&#x27;: padding_mask # íŒ¨ë”© ë§ˆìŠ¤í¬ ì‚¬ìš© &#125;) # ë“œë¡­ì•„ì›ƒ + ì”ì°¨ ì—°ê²°ê³¼ ì¸µ ì •ê·œí™” attention = tf.keras.layers.Dropout(rate=dropout)(attention) attention = tf.keras.layers.LayerNormalization( epsilon=1e-6)(inputs + attention) # í¬ì§€ì…˜ ì™€ì´ì¦ˆ í”¼ë“œ í¬ì›Œë“œ ì‹ ê²½ë§ (ë‘ë²ˆì§¸ ì„œë¸Œì¸µ) outputs = tf.keras.layers.Dense(units=dff, activation=&#x27;relu&#x27;)(attention) outputs = tf.keras.layers.Dense(units=d_model)(outputs) # ë“œë¡­ì•„ì›ƒ + ì”ì°¨ ì—°ê²°ê³¼ ì¸µ ì •ê·œí™” outputs = tf.keras.layers.Dropout(rate=dropout)(outputs) outputs = tf.keras.layers.LayerNormalization( epsilon=1e-6)(attention + outputs) return tf.keras.Model( inputs=[inputs, padding_mask], outputs=outputs, name=name) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960class MultiHeadAttention(tf.keras.layers.Layer): def __init__(self, d_model, num_heads, name=&quot;multi_head_attention&quot;): super(MultiHeadAttention, self).__init__(name=name) self.num_heads = num_heads self.d_model = d_model assert d_model % self.num_heads == 0 # d_modelì„ num_headsë¡œ ë‚˜ëˆˆ ê°’. # ë…¼ë¬¸ ê¸°ì¤€ : 64 self.depth = d_model // self.num_heads # WQ, WK, WVì— í•´ë‹¹í•˜ëŠ” ë°€ì§‘ì¸µ ì •ì˜ self.query_dense = tf.keras.layers.Dense(units=d_model) self.key_dense = tf.keras.layers.Dense(units=d_model) self.value_dense = tf.keras.layers.Dense(units=d_model) # WOì— í•´ë‹¹í•˜ëŠ” ë°€ì§‘ì¸µ ì •ì˜ self.dense = tf.keras.layers.Dense(units=d_model) # num_heads ê°œìˆ˜ë§Œí¼ q, k, vë¥¼ splití•˜ëŠ” í•¨ìˆ˜ def split_heads(self, inputs, batch_size): inputs = tf.reshape( inputs, shape=(batch_size, -1, self.num_heads, self.depth)) return tf.transpose(inputs, perm=[0, 2, 1, 3]) def call(self, inputs): query, key, value, mask = inputs[&#x27;query&#x27;], inputs[&#x27;key&#x27;], inputs[ &#x27;value&#x27;], inputs[&#x27;mask&#x27;] batch_size = tf.shape(query)[0] query = self.query_dense(query) key = self.key_dense(key) value = self.value_dense(value) # 2. í—¤ë“œ ë‚˜ëˆ„ê¸° # q : (batch_size, num_heads, queryì˜ ë¬¸ì¥ ê¸¸ì´, d_model/num_heads) # k : (batch_size, num_heads, keyì˜ ë¬¸ì¥ ê¸¸ì´, d_model/num_heads) # v : (batch_size, num_heads, valueì˜ ë¬¸ì¥ ê¸¸ì´, d_model/num_heads) query = self.split_heads(query, batch_size) key = self.split_heads(key, batch_size) value = self.split_heads(value, batch_size) # 3. ìŠ¤ì¼€ì¼ë“œ ë‹· í”„ë¡œë•íŠ¸ ì–´í…ì…˜. ì•ì„œ êµ¬í˜„í•œ í•¨ìˆ˜ ì‚¬ìš©. # (batch_size, num_heads, queryì˜ ë¬¸ì¥ ê¸¸ì´, d_model/num_heads) scaled_attention, _ = scaled_dot_product_attention(query, key, value, mask) # (batch_size, queryì˜ ë¬¸ì¥ ê¸¸ì´, num_heads, d_model/num_heads) scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3]) # 4. í—¤ë“œ ì—°ê²°(concatenate)í•˜ê¸° # (batch_size, queryì˜ ë¬¸ì¥ ê¸¸ì´, d_model) concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model)) # 5. WOì— í•´ë‹¹í•˜ëŠ” ë°€ì§‘ì¸µ ì§€ë‚˜ê¸° # (batch_size, queryì˜ ë¬¸ì¥ ê¸¸ì´, d_model) outputs = self.dense(concat_attention) return outputs 123456789101112131415161718192021222324252627def scaled_dot_product_attention(query, key, value, mask): # query í¬ê¸° : (batch_size, num_heads, queryì˜ ë¬¸ì¥ ê¸¸ì´, d_model/num_heads) # key í¬ê¸° : (batch_size, num_heads, keyì˜ ë¬¸ì¥ ê¸¸ì´, d_model/num_heads) # value í¬ê¸° : (batch_size, num_heads, valueì˜ ë¬¸ì¥ ê¸¸ì´, d_model/num_heads) # padding_mask : (batch_size, 1, 1, keyì˜ ë¬¸ì¥ ê¸¸ì´) # Qì™€ Kì˜ ê³±. ì–´í…ì…˜ ìŠ¤ì½”ì–´ í–‰ë ¬. matmul_qk = tf.matmul(query, key, transpose_b=True) # ìŠ¤ì¼€ì¼ë§ # dkì˜ ë£¨íŠ¸ê°’ìœ¼ë¡œ ë‚˜ëˆ ì¤€ë‹¤. depth = tf.cast(tf.shape(key)[-1], tf.float32) logits = matmul_qk / tf.math.sqrt(depth) # ë§ˆìŠ¤í‚¹. ì–´í…ì…˜ ìŠ¤ì½”ì–´ í–‰ë ¬ì˜ ë§ˆìŠ¤í‚¹ í•  ìœ„ì¹˜ì— ë§¤ìš° ì‘ì€ ìŒìˆ˜ê°’ì„ ë„£ëŠ”ë‹¤. # ë§¤ìš° ì‘ì€ ê°’ì´ë¯€ë¡œ ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ë¥¼ ì§€ë‚˜ë©´ í–‰ë ¬ì˜ í•´ë‹¹ ìœ„ì¹˜ì˜ ê°’ì€ 0ì´ ëœë‹¤. if mask is not None: logits += (mask * -1e9) # ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ëŠ” ë§ˆì§€ë§‰ ì°¨ì›ì¸ keyì˜ ë¬¸ì¥ ê¸¸ì´ ë°©í–¥ìœ¼ë¡œ ìˆ˜í–‰ëœë‹¤. # attention weight : (batch_size, num_heads, queryì˜ ë¬¸ì¥ ê¸¸ì´, keyì˜ ë¬¸ì¥ ê¸¸ì´) attention_weights = tf.nn.softmax(logits, axis=-1) # output : (batch_size, num_heads, queryì˜ ë¬¸ì¥ ê¸¸ì´, d_model/num_heads) output = tf.matmul(attention_weights, value) return output, attention_weights 123456789101112131415161718192021222324252627def decoder(vocab_size, num_layers, dff, d_model, num_heads, dropout, name=&#x27;decoder&#x27;): inputs = tf.keras.Input(shape=(None,), name=&#x27;inputs&#x27;) enc_outputs = tf.keras.Input(shape=(None, d_model), name=&#x27;encoder_outputs&#x27;) # ë””ì½”ë”ëŠ” ë£©ì–´í—¤ë“œ ë§ˆìŠ¤í¬(ì²«ë²ˆì§¸ ì„œë¸Œì¸µ)ì™€ íŒ¨ë”© ë§ˆìŠ¤í¬(ë‘ë²ˆì§¸ ì„œë¸Œì¸µ) ë‘˜ ë‹¤ ì‚¬ìš©. look_ahead_mask = tf.keras.Input( shape=(1, None, None), name=&#x27;look_ahead_mask&#x27;) padding_mask = tf.keras.Input(shape=(1, 1, None), name=&#x27;padding_mask&#x27;) # í¬ì§€ì…”ë„ ì¸ì½”ë”© + ë“œë¡­ì•„ì›ƒ embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs) embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32)) embeddings = PositionalEncoding(vocab_size, d_model)(embeddings) outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings) # ë””ì½”ë”ë¥¼ num_layersê°œ ìŒ“ê¸° for i in range(num_layers): outputs = decoder_layer(dff=dff, d_model=d_model, num_heads=num_heads, dropout=dropout, name=&#x27;decoder_layer_&#123;&#125;&#x27;.format(i), )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask]) return tf.keras.Model( inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask], outputs=outputs, name=name) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def decoder_layer(dff, d_model, num_heads, dropout, name=&quot;decoder_layer&quot;): inputs = tf.keras.Input(shape=(None, d_model), name=&quot;inputs&quot;) enc_outputs = tf.keras.Input(shape=(None, d_model), name=&quot;encoder_outputs&quot;) # ë£©ì–´í—¤ë“œ ë§ˆìŠ¤í¬(ì²«ë²ˆì§¸ ì„œë¸Œì¸µ) look_ahead_mask = tf.keras.Input( shape=(1, None, None), name=&quot;look_ahead_mask&quot;) # íŒ¨ë”© ë§ˆìŠ¤í¬(ë‘ë²ˆì§¸ ì„œë¸Œì¸µ) padding_mask = tf.keras.Input(shape=(1, 1, None), name=&#x27;padding_mask&#x27;) # ë©€í‹°-í—¤ë“œ ì–´í…ì…˜ (ì²«ë²ˆì§¸ ì„œë¸Œì¸µ / ë§ˆìŠ¤í¬ë“œ ì…€í”„ ì–´í…ì…˜) attention1 = MultiHeadAttention( d_model, num_heads, name=&quot;attention_1&quot;)(inputs=&#123; &#x27;query&#x27;: inputs, &#x27;key&#x27;: inputs, &#x27;value&#x27;: inputs, # Q = K = V &#x27;mask&#x27;: look_ahead_mask # ë£©ì–´í—¤ë“œ ë§ˆìŠ¤í¬ &#125;) # ì”ì°¨ ì—°ê²°ê³¼ ì¸µ ì •ê·œí™” attention1 = tf.keras.layers.LayerNormalization( epsilon=1e-6)(attention1 + inputs) # ë©€í‹°-í—¤ë“œ ì–´í…ì…˜ (ë‘ë²ˆì§¸ ì„œë¸Œì¸µ / ë””ì½”ë”-ì¸ì½”ë” ì–´í…ì…˜) attention2 = MultiHeadAttention( d_model, num_heads, name=&quot;attention_2&quot;)(inputs=&#123; &#x27;query&#x27;: attention1, &#x27;key&#x27;: enc_outputs, &#x27;value&#x27;: enc_outputs, # Q != K = V &#x27;mask&#x27;: padding_mask # íŒ¨ë”© ë§ˆìŠ¤í¬ &#125;) # ë“œë¡­ì•„ì›ƒ + ì”ì°¨ ì—°ê²°ê³¼ ì¸µ ì •ê·œí™” attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2) attention2 = tf.keras.layers.LayerNormalization( epsilon=1e-6)(attention2 + attention1) # í¬ì§€ì…˜ ì™€ì´ì¦ˆ í”¼ë“œ í¬ì›Œë“œ ì‹ ê²½ë§ (ì„¸ë²ˆì§¸ ì„œë¸Œì¸µ) outputs = tf.keras.layers.Dense(units=dff, activation=&#x27;relu&#x27;)(attention2) outputs = tf.keras.layers.Dense(units=d_model)(outputs) # ë“œë¡­ì•„ì›ƒ + ì”ì°¨ ì—°ê²°ê³¼ ì¸µ ì •ê·œí™” outputs = tf.keras.layers.Dropout(rate=dropout)(outputs) outputs = tf.keras.layers.LayerNormalization( epsilon=1e-6)(outputs + attention2) return tf.keras.Model( inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask], outputs=outputs, name=name) 1234567891011121314151617tf.keras.backend.clear_session()# Hyper-parametersD_MODEL = 256NUM_LAYERS = 2NUM_HEADS = 8DFF = 512DROPOUT = 0.1model = transformer( vocab_size=VOCAB_SIZE, num_layers=NUM_LAYERS, dff=DFF, d_model=D_MODEL, num_heads=NUM_HEADS, dropout=DROPOUT) (1, 10025, 256) (1, 10025, 256) 123456789101112131415161718192021222324class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule): def __init__(self, d_model, warmup_steps=4000): super(CustomSchedule, self).__init__() self.d_model = d_model self.d_model = tf.cast(self.d_model, tf.float32) self.warmup_steps = warmup_steps def __call__(self, step): arg1 = tf.math.rsqrt(step) arg2 = step * (self.warmup_steps**-1.5) return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)def loss_function(y_true, y_pred): y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1)) loss = tf.keras.losses.SparseCategoricalCrossentropy( from_logits=True, reduction=&#x27;none&#x27;)(y_true, y_pred) mask = tf.cast(tf.not_equal(y_true, 0), tf.float32) loss = tf.multiply(loss, mask) return tf.reduce_mean(loss) 1234567891011learning_rate = CustomSchedule(D_MODEL)optimizer = tf.keras.optimizers.Adam( learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)def accuracy(y_true, y_pred): # ë ˆì´ë¸”ì˜ í¬ê¸°ëŠ” (batch_size, MAX_LENGTH - 1) y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1)) return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy]) 123## ëª¨ë¸ í•™ìŠµEPOCHS = 50model.fit(dataset, epochs=EPOCHS) Epoch 1/50 104/104 [==============================] - 12s 54ms/step - loss: 1.2164 - accuracy: 0.0149 Epoch 2/50 104/104 [==============================] - 6s 54ms/step - loss: 1.0725 - accuracy: 0.0285 Epoch 3/50 104/104 [==============================] - 6s 54ms/step - loss: 0.9082 - accuracy: 0.0472 Epoch 4/50 104/104 [==============================] - 6s 54ms/step - loss: 0.7714 - accuracy: 0.0482 ... 104/104 [==============================] - 6s 54ms/step - loss: 0.0160 - accuracy: 0.1321 Epoch 46/50 104/104 [==============================] - 6s 56ms/step - loss: 0.0158 - accuracy: 0.1320 Epoch 47/50 104/104 [==============================] - 6s 55ms/step - loss: 0.0152 - accuracy: 0.1320 Epoch 48/50 104/104 [==============================] - 6s 54ms/step - loss: 0.0151 - accuracy: 0.1322 Epoch 49/50 104/104 [==============================] - 6s 55ms/step - loss: 0.0149 - accuracy: 0.1321 Epoch 50/50 104/104 [==============================] - 6s 53ms/step - loss: 0.0148 - accuracy: 0.1321 &lt;keras.callbacks.History at 0x7f794c0f0880&gt; 5. ì±—ë´‡ í‰ê°€í•˜ê¸° í•™ìŠµì‹œí‚¨ ì±—ë´‡ì— ìƒˆë¡œìš´ ë¬¸ì¥ì„ ë„£ì–´ì„œ í‰ê°€ 12345678910111213141516171819202122232425262728293031323334# ìƒˆë¡œìš´ ë¬¸ì¥ë„ ì¸ì½”ë” ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ë³€í˜•í•˜ëŠ” ì½”ë“œdef preprocess_sentence(sentence): # ë‹¨ì–´ì™€ êµ¬ë‘ì  ì‚¬ì´ì— ê³µë°± ì¶”ê°€. # ex) 12ì‹œ ë•¡! -&gt; 12ì‹œ ë•¡ ! sentence = re.sub(r&quot;([?.!,])&quot;, r&quot; \\1 &quot;, sentence) sentence = sentence.strip() return sentencedef evaluate(sentence): sentence = preprocess_sentence(sentence) sentence = tf.expand_dims( START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0) output = tf.expand_dims(START_TOKEN, 0) # ë””ì½”ë”ì˜ ì˜ˆì¸¡ ì‹œì‘ for i in range(MAX_LENGTH): predictions = model(inputs=[sentence, output], training=False) # í˜„ì¬(ë§ˆì§€ë§‰) ì‹œì ì˜ ì˜ˆì¸¡ ë‹¨ì–´ë¥¼ ë°›ì•„ì˜¨ë‹¤. predictions = predictions[:, -1:, :] predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32) # ë§Œì•½ ë§ˆì§€ë§‰ ì‹œì ì˜ ì˜ˆì¸¡ ë‹¨ì–´ê°€ ì¢…ë£Œ í† í°ì´ë¼ë©´ ì˜ˆì¸¡ì„ ì¤‘ë‹¨ if tf.equal(predicted_id, END_TOKEN[0]): break # ë§ˆì§€ë§‰ ì‹œì ì˜ ì˜ˆì¸¡ ë‹¨ì–´ë¥¼ ì¶œë ¥ì— ì—°ê²°í•œë‹¤. # ì´ëŠ” forë¬¸ì„ í†µí•´ì„œ ë””ì½”ë”ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©ë  ì˜ˆì •ì´ë‹¤. output = tf.concat([output, predicted_id], axis=-1) return tf.squeeze(output, axis=0) 123456789101112def predict(sentence): prediction = evaluate(sentence) predicted_sentence = tokenizer.decode( [i for i in prediction if i &lt; tokenizer.vocab_size]) print(&#x27;Master: &#123;&#125;&#x27;.format(sentence)) # print(&#x27;Output: &#123;&#125;&#x27;.format(predicted_sentence)) print(&#x27;Chatbot: &#123;&#125;&#x27;.format(predicted_sentence)) return predicted_sentence 12#predict() í•¨ìˆ˜ì— ë¬¸ì¥ì„ ì…ë ¥í•˜ë©´ í•´ë‹¹ ë¬¸ì¥ì— ëŒ€í•œ ê²°ê³¼ê°€ ì¶œë ¥ë¨output = predict(&quot;êµ¿ëª¨ë‹&quot;) Input: êµ¿ëª¨ë‹ Output: ì¢‹ì€ ì•„ì¹¨ì´ì—ìš” . 1output = predict(&quot;ì˜¤ëŠ˜ ë‚ ì”¨&quot;) Input: ì˜¤ëŠ˜ ë‚ ì”¨ Output: ì¶©ë¶„íˆ ì•„ë¦„ë‹¤ì›Œìš” . 1output = predict(&quot;ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?&quot;) Input: ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ? Output: ì˜¤ì „ì—” í™”ì°½í•˜ì§€ë§Œ ì˜¤í›„ì—ëŠ” ë¹„ê°€ ì˜¬ ê²ƒì…ë‹ˆë‹¤ . 1output = predict(&quot;ì§‘ì¤‘ë ¥&quot;) Input: ì§‘ì¤‘ë ¥ Output: ë³‘ì› ê°€ë³´ì„¸ìš” . 1output = predict(&quot;í‡´ê·¼&quot;) Input: í‡´ê·¼ Output: ì¸ìƒì€ ì±„ì›Œë‚˜ê°€ëŠ”ê±°ì£  . 1output = predict(&quot;ì•¼ê·¼ ì‹«ì–´&quot;) Input: ì•¼ê·¼ ì‹«ì–´ Output: ì–¼ë¥¸ ì§‘ì— ê°€ì„œ ì‰¬ì‹œê¸¸ ë°”ë„ê²Œìš” . 123output = str(input(&quot;ì˜¤í”¼ìŠ¤ ì±—ë´‡ì…ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?:&quot;))output = predict(output) Master: ì¼í•˜ê¸° ì‹«ì–´ Chatbot: ì €ë„ìš” ! ! Reference https:&#x2F;&#x2F;velog.io&#x2F;@seolini43&#x2F;íŒŒì´ì¬Transformerë¡œ-ì˜¤í”¼ìŠ¤-ì±—ë´‡-ë§Œë“¤ê¸°-ì½”ë“œ **code: https://github.com/jmj3047/mj_chatbot_prac&#x2F;blob&#x2F;c5bec233b833b24345deeffe7391621415dc1dcb&#x2F;chatbot_backend.py","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Python","slug":"Python/Python","permalink":"https://jmj3047.github.io/categories/Python/Python/"}],"tags":[{"name":"Transformer","slug":"Transformer","permalink":"https://jmj3047.github.io/tags/Transformer/"},{"name":"Chatbot","slug":"Chatbot","permalink":"https://jmj3047.github.io/tags/Chatbot/"},{"name":"NLP","slug":"NLP","permalink":"https://jmj3047.github.io/tags/NLP/"}],"keywords":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Python","slug":"Python/Python","permalink":"https://jmj3047.github.io/categories/Python/Python/"}]},{"title":"MongoDB Update Operator","slug":"MongoDB_update","date":"2022-04-27T15:00:00.000Z","updated":"2023-03-19T07:04:36.264Z","comments":true,"path":"2022/04/28/MongoDB_update/","link":"","permalink":"https://jmj3047.github.io/2022/04/28/MongoDB_update/","excerpt":"","text":"$set: í•„ë“œê°’ì„ ì„¤ì •í•˜ê³  í•„ë“œê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒˆ í•„ë“œê°€ ìƒì„±ë¨. ìŠ¤í‚¤ë§ˆë¥¼ ê°±ì‹ í•˜ê±°ë‚˜ ì‚¬ìš©ì ì •ì˜ í‚¤ë¥¼ ì¶”ê°€ í• ë•Œ í¸ë¦¬í•¨. $unset: í‚¤ì™€ ê°’ì„ ëª¨ë‘ ì œê±°í•¨ 1234567891011121314&gt; db.users.insertOne(&#123;&quot;name&quot;:&quot;joe&quot;&#125;)&gt; db.users.updateOne(&#123;&quot;_id&quot; : ObjectId(&quot;6269e32b7b15b7097fbad433&quot;)&#125;,&#123;&quot;$set&quot;:&#123;&quot;favorite book&quot; : &quot;War and Peace&quot;&#125;&#125;)&#123; &quot;acknowledged&quot; : true, &quot;matchedCount&quot; : 1, &quot;modifiedCount&quot; : 1 &#125;&gt; db.users.findOne()&#123; &quot;_id&quot; : ObjectId(&quot;6269e32b7b15b7097fbad433&quot;), &quot;name&quot; : &quot;joe&quot;, &quot;favorite book&quot; : &quot;War and Peace&quot;&#125;&gt; db.users.updateOne(&#123;&quot;name&quot; : &quot;joe&quot;&#125;,&#123;&quot;$unset&quot;:&#123;&quot;favorite book&quot;:1&#125;&#125;)&#123; &quot;acknowledged&quot; : true, &quot;matchedCount&quot; : 1, &quot;modifiedCount&quot; : 1 &#125;&gt; db.users.find()&#123; &quot;_id&quot; : ObjectId(&quot;6269e32b7b15b7097fbad433&quot;), &quot;name&quot; : &quot;joe&quot; &#125; $inc: $setê³¼ ë¹„ìŠ·í•˜ì§€ë§Œ, ìˆ«ìë¥¼ ì¦ê°í•˜ê¸° ìœ„í•´ ì‚¬ìš©. int, long, double, decimal íƒ€ì… ê°’ì—ë§Œ ì‚¬ìš© ê°€ëŠ¥ 12345678910111213141516171819202122&gt;db.games.insertOne(&#123;&quot;game&quot; :&quot;pinball&quot;,&quot;user&quot; :&quot;joe&quot;&#125;)&#123; &quot;acknowledged&quot; : true, &quot;insertedId&quot; : ObjectId(&quot;6269e4be7b15b7097fbad436&quot;)&#125;&gt; db.games.find()&#123; &quot;_id&quot; : ObjectId(&quot;6269e4be7b15b7097fbad436&quot;), &quot;game&quot; : &quot;pinball&quot;, &quot;user&quot; : &quot;joe&quot; &#125;&gt; db.games.updateOne(&#123;&quot;game&quot; :&quot;pinball&quot;,&quot;user&quot; :&quot;joe&quot;&#125;,&#123;&quot;$inc&quot;:&#123;&quot;score&quot;:50&#125;&#125;)&#123; &quot;acknowledged&quot; : true, &quot;matchedCount&quot; : 1, &quot;modifiedCount&quot; : 1 &#125;&gt; db.games.find()&#123; &quot;_id&quot; : ObjectId(&quot;6269e4be7b15b7097fbad436&quot;), &quot;game&quot; : &quot;pinball&quot;, &quot;user&quot; : &quot;joe&quot;, &quot;score&quot; : 50 &#125;&gt; db.games.updateOne(&#123;&quot;game&quot; :&quot;pinball&quot;,&quot;user&quot; :&quot;joe&quot;&#125;,&#123;&quot;$inc&quot;:&#123;&quot;score&quot;:10000&#125;&#125;)&#123; &quot;acknowledged&quot; : true, &quot;matchedCount&quot; : 1, &quot;modifiedCount&quot; : 1 &#125;&gt; db.games.find()&#123; &quot;_id&quot; : ObjectId(&quot;6269e4be7b15b7097fbad436&quot;), &quot;game&quot; : &quot;pinball&quot;, &quot;user&quot; : &quot;joe&quot;, &quot;score&quot; : 10050 &#125;&gt; $push: ë°°ì—´ì´ ì´ë¯¸ ì¡´ì¬í•˜ì§€ë§Œ ë°°ì—´ ëì— ìš”ì†Œë¥¼ ì¶”ê°€í•˜ê³ , ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒˆë¡œìš´ ë°°ì—´ì„ ìƒì„±í•¨. $each: $pushì— $eachì œí•œìë¥¼ ì‚¬ìš©í•˜ë©´ ì‘ì—… í•œ ë²ˆìœ¼ë¡œ ê°’ì„ ì—¬ëŸ¬ê°œ ì¶”ê°€í•  ìˆ˜ ìˆìŒ. 12345678910111213141516171819202122&gt; db.blog.posts.findOne() &#123; &quot;_id&quot; : ObjectId(&quot;4b2d75476cc613d5ee930164&quot;), &quot;title&quot; : &quot;A blog post&quot;, &quot;content&quot; : &quot;...&quot;&#125; &gt; db.blog.posts.updateOne(&#123;&quot;title&quot; : &quot;A blog post&quot;&#125;, &#123;&quot;$push&quot; : &#123;&quot;comments&quot; : &#123;&quot;name&quot; : &quot;joe&quot;, &quot;email&quot; : &quot;joe@example.com&quot;, &quot;content&quot; : &quot;nice post.&quot;&#125;&#125;&#125;) &#123; &quot;acknowledged&quot; : true, &quot;matchedCount&quot; : 1, &quot;modifiedCount&quot; : 1 &#125; &gt; db.blog.posts.findOne() &#123; &quot;_id&quot; : ObjectId(&quot;4b2d75476cc613d5ee930164&quot;), &quot;title&quot; : &quot;A blog post&quot;, &quot;content&quot; : &quot;...&quot;, &quot;comments&quot; : [ &#123;&quot;name&quot; : &quot;joe&quot;, &quot;email&quot; : &quot;joe@example.com&quot;, &quot;content&quot; : &quot;nice post.&quot; &#125; ] &#125; $ne: ë°°ì—´ì´ ì¡´ì¬í•˜ì§€ ì•Šì„ ë•Œ í•´ë‹¹ ê°’ì„ ì¶”ê°€í•˜ë©´ì„œ ë°°ì—´ì„ ì§‘í•©ì²˜ëŸ¼ ì²˜ë¦¬í•  ë•Œ ì‚¬ìš©. $addToSet: ë‹¤ë¥¸ì£¼ì†Œë¥¼ ì¶”ê°€í•  ë•Œ ì¤‘ë³µì„ í”¼í•  ìˆ˜ ìˆìŒ ê³ ìœ í•œ ê°’ì„ ì—¬ëŸ¬ê°œ ì¶”ê°€í•˜ë ¤ë©´ $addToSet&#x2F;$eachì¡°í•©ì„ í™œìš©í•´ì•¼ í•¨. $ne&#x2F;$pushì¡°í•©ìœ¼ë¡œëŠ” í•  ìˆ˜ ì—†ìŒ. 123456789101112&gt; db.users.updateOne(&#123;&quot;_id&quot; : ObjectId(&quot;4b2d75476cc613d5ee930164&quot;)&#125;, &#123;&quot;$addToSet&quot; : &#123;&quot;emails&quot; : &#123;&quot;$each&quot; : [&quot;joe@php.net&quot;, &quot;joe@example.com&quot;, &quot;joe@python.org&quot;]&#125;&#125;&#125;) &#123; &quot;acknowledged&quot; : true, &quot;matchedCount&quot; : 1, &quot;modifiedCount&quot; : 1 &#125; &gt; db.users.findOne(&#123;&quot;_id&quot; : ObjectId(&quot;4b2d75476cc613d5ee930164&quot;)&#125;) &#123; &quot;_id&quot; : ObjectId(&quot;4b2d75476cc613d5ee930164&quot;), &quot;username&quot; : &quot;joe&quot;, &quot;emails&quot; : [ &quot;joe@example.com&quot;, &quot;joe@gmail.com&quot;, &quot;joe@yahoo.com&quot;, &quot;joe@hotmail.com&quot; &quot;joe@php.net&quot; &quot;joe@python.org&quot; ] &#125; Reference ëª½ê³ DB ì™„ë²½ ê°€ì´ë“œ: ì‹¤ì „ ì˜ˆì œë¡œ ë°°ìš°ëŠ” NoSQL ë°ì´í„°ë² ì´ìŠ¤ ê¸°ì´ˆë¶€í„° í™œìš©ê¹Œì§€","categories":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"MongoDB","slug":"Data-Platform-Base/MongoDB","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/MongoDB/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://jmj3047.github.io/tags/MongoDB/"}],"keywords":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"MongoDB","slug":"Data-Platform-Base/MongoDB","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/MongoDB/"}]},{"title":"MongoDB CRUD","slug":"MongoDB_CRUD","date":"2022-04-24T15:00:00.000Z","updated":"2023-03-19T07:04:10.157Z","comments":true,"path":"2022/04/25/MongoDB_CRUD/","link":"","permalink":"https://jmj3047.github.io/2022/04/25/MongoDB_CRUD/","excerpt":"","text":"Initial setting 12345678&gt;dbtest&gt;use videoswitched to db video # if video doesnt exist, created&gt;dbvideo&gt;db.movies # created movies collectionvideo.movies Create: insertOne í•¨ìˆ˜ 1234567891011121314151617181920&gt;movie = &#123;&quot;title&quot; : &quot;Star Wars: Episode IV - A New Hope&quot;, &quot;director&quot; : &quot;George Lucas&quot;, &quot;year&quot; : 1977&#125;&#123; &quot;title&quot; : &quot;Star Wars: Episode IV - A New Hope&quot;, &quot;director&quot; : &quot;George Lucas&quot;, &quot;year&quot; : 1977&#125;&gt;db.movies.insertOne(movie) #ì˜í™”ê°€ ë°ì´í„° ë² ì´ìŠ¤ì— ì €ì¥ë¨&#123; &quot;acknowledged&quot; : true, &quot;insertedId&quot; : ObjectId(&quot;6266057e4619361339f0c881&quot;)&#125;#Find í•¨ìˆ˜ë¡œ í˜¸ì¶œ&gt;db.movies.find().pretty()&#123; &quot;_id&quot; : ObjectId(&quot;6266057e4619361339f0c881&quot;), &quot;title&quot; : &quot;Star Wars: Episode IV - A New Hope&quot;, &quot;director&quot; : &quot;George Lucas&quot;, &quot;year&quot; : 1977&#125; Read: find, findOne í•¨ìˆ˜ 1234567&gt; db.movies.findOne(movie) &#123; &quot;_id&quot; : ObjectId(&quot;6266057e4619361339f0c881&quot;), &quot;title&quot; : &quot;Star Wars: Episode IV - A New Hope&quot;, &quot;director&quot; : &quot;George Lucas&quot;, &quot;year&quot; : 1977&#125; Update : updateOne í•¨ìˆ˜ 1234567891011&gt; db.movies.updateOne(&#123;title : &quot;Star Wars: Episode IV - A New Hope&quot;&#125;, &#123;$set : &#123;reviews: []&#125;&#125;) &#123; &quot;acknowledged&quot; : true, &quot;matchedCount&quot; : 1, &quot;modifiedCount&quot; : 1 &#125;&gt; db.movies.find().pretty()&#123; &quot;_id&quot; : ObjectId(&quot;6266057e4619361339f0c881&quot;), &quot;title&quot; : &quot;Star Wars: Episode IV - A New Hope&quot;, &quot;director&quot; : &quot;George Lucas&quot;, &quot;year&quot; : 1977, &quot;reviews&quot; : [ ]&#125; Delete: deleteOne, deleteMany í•¨ìˆ˜ 12&gt;db.movies.deleteOne(&#123;title : &quot;Star Wars: Episode IV - A New Hope&quot;&#125;)&#123; &quot;acknowledged&quot; : true, &quot;deletedCount&quot; : 1 &#125; Reference ëª½ê³ DB ì™„ë²½ ê°€ì´ë“œ: ì‹¤ì „ ì˜ˆì œë¡œ ë°°ìš°ëŠ” NoSQL ë°ì´í„°ë² ì´ìŠ¤ ê¸°ì´ˆë¶€í„° í™œìš©ê¹Œì§€","categories":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"MongoDB","slug":"Data-Platform-Base/MongoDB","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/MongoDB/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://jmj3047.github.io/tags/MongoDB/"}],"keywords":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"MongoDB","slug":"Data-Platform-Base/MongoDB","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/MongoDB/"}]},{"title":"MongoDB Install & Basic Command","slug":"MongoDB_Install","date":"2022-04-24T15:00:00.000Z","updated":"2023-03-19T07:04:11.321Z","comments":true,"path":"2022/04/25/MongoDB_Install/","link":"","permalink":"https://jmj3047.github.io/2022/04/25/MongoDB_Install/","excerpt":"","text":"Link: www.mongodb.com/try/download/enterprise Download proper version of Mongodb Installì´ ì™„ë£Œëœ í›„ì—ëŠ” MongoDB í™˜ê²½ë³€ìˆ˜ ì„¤ì •ì„ ìœ„í•´ ì‹œìŠ¤í…œ í™˜ê²½ ë³€ìˆ˜ í¸ì§‘ì„ ì§„í–‰í•˜ì—¬ ì¤ë‹ˆë‹¤. í™˜ê²½ë³€ìˆ˜ í¸ì§‘ì„ ìœ„í•´ í™˜ê²½ë³€ìˆ˜ &gt;ì‹œìŠ¤í…œ ë³€ìˆ˜ Path ì„¤ì •ì„ ì„ íƒí•˜ì—¬ ì¤ë‹ˆë‹¤. ì„¤ì¹˜ëœ MongoDBì˜ biní´ë” ê²½ë¡œë¥¼ ì…ë ¥í•˜ì—¬ ì¤ë‹ˆë‹¤.(C:\\Program Files\\MongoDB\\Server\\5.0\\bin) ì €ì¥ í›„ cmdì°½ì—ì„œ mongdb â€“versionì„ í†µí•´ ì •ìƒ ì„¤ì¹˜ë¥¼ í™•ì¸í•˜ì—¬ ì¤ë‹ˆë‹¤. cmd ì°½ì— mongodb ì‹¤í–‰ 1&gt;mongo ëª…ë ¹ì–´ ë‘ì¤„ë¡œ ì˜ ì‹¤í–‰ë˜ëŠ”ì§€ ê°„ë‹¨íˆ í™•ì¸ 12&gt; db.world.insert(&#123; &quot;speech&quot; : &quot;Hello World!&quot; &#125;);&gt; cur = db.world.find();x=cur.next();print(x[&quot;speech&quot;]); Basic Commandì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  ë°ì´í„°ë² ì´ìŠ¤ í‘œì‹œ : 1show dbs; ì•¡ì„¸ìŠ¤ í•  íŠ¹ì • ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì„ íƒ (Ex: mydb . ì´ë¯¸ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ mydb ê°€ ìƒì„±ë©ë‹ˆë‹¤ : 1use mydb; ë°ì´í„°ë² ì´ìŠ¤ì— ëª¨ë“  ì½œë ‰ì…˜ì„ í‘œì‹œ. ë¨¼ì € ì½œë ‰ì…˜ì„ ì„ íƒí•˜ì‹­ì‹œì˜¤ (ìœ„ ì°¸ì¡°). 1show collections; ë°ì´í„°ë² ì´ìŠ¤ì™€ í•¨ê»˜ ì‚¬ìš©í•  ìˆ˜ìˆëŠ” ëª¨ë“  ê¸°ëŠ¥ í‘œì‹œ : 1db.mydb.help(); í˜„ì¬ ì„ íƒí•œ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ í™•ì¸ 12&gt; dbmydb db.dropDatabase() ëª…ë ¹ì€ ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì‚­ì œí•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. 1db.dropDatabase() Reference https://khj93.tistory.com/entry/MongoDB-Windowì—-MongoDB-ì„¤ì¹˜í•˜ê¸° https://learntutorials.net/ko/mongodb/topic/691/mongodb-ì‹œì‘í•˜ê¸°","categories":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"MongoDB","slug":"Data-Platform-Base/MongoDB","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/MongoDB/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://jmj3047.github.io/tags/MongoDB/"}],"keywords":[{"name":"Data Platform/Base","slug":"Data-Platform-Base","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/"},{"name":"MongoDB","slug":"Data-Platform-Base/MongoDB","permalink":"https://jmj3047.github.io/categories/Data-Platform-Base/MongoDB/"}]},{"title":"BeautifulSoup Quick Start","slug":"BeautifulSoup_QuickStart","date":"2022-04-21T15:00:00.000Z","updated":"2022-04-22T02:53:21.000Z","comments":true,"path":"2022/04/22/BeautifulSoup_QuickStart/","link":"","permalink":"https://jmj3047.github.io/2022/04/22/BeautifulSoup_QuickStart/","excerpt":"","text":"Index_prac.html123456789101112131415161718192021222324252627&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;title&gt; The Dormouse&#x27;s story &lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class=&quot;title&quot;&gt;&lt;b&gt;The Dormouse&#x27;s story&lt;/b&gt;&lt;/p&gt; &lt;p class=&quot;story&quot;&gt; Once upon a time there were three little sisters; and their names were &lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;, &lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and &lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class=&quot;story&quot;&gt;To Be Continued...&lt;/p&gt; &lt;/body&gt;&lt;/html&gt; Quick Start.py123456789101112131415161718192021222324252627282930313233343536373839404142434445from bs4 import BeautifulSoupsoup = BeautifulSoup(open(&quot;index_prac.html&quot;), &#x27;html.parser&#x27;)# print allprint(soup.prettify())# navigate that data structureprint(soup.title)# &lt;title&gt;The Dormouse&#x27;s story&lt;/title&gt;print(soup.title.name)# u&#x27;title&#x27;print(soup.title.string)# u&#x27;The Dormouse&#x27;s story&#x27;print(soup.title.parent.name)# u&#x27;head&#x27;print(soup.p)# &lt;p class=&quot;title&quot;&gt;&lt;b&gt;The Dormouse&#x27;s story&lt;/b&gt;&lt;/p&gt;print(soup.p[&#x27;class&#x27;])# u&#x27;title&#x27;print(soup.a)# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;print(soup.find_all(&#x27;a&#x27;))# [&lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;,# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;,# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;]print(soup.find(id=&quot;link3&quot;))# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;## extracting all the URLsfor link in soup.find_all(&#x27;a&#x27;): print(link.get(&#x27;href&#x27;))# http://example.com/elsie# http://example.com/lacie# http://example.com/tillie## extracting all the text from a pageprint(soup.get_text()) Reference: https://www.crummy.com/software/BeautifulSoup/bs4/doc/#quick-start","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"HTML","slug":"Python/HTML","permalink":"https://jmj3047.github.io/categories/Python/HTML/"}],"tags":[{"name":"HTML","slug":"HTML","permalink":"https://jmj3047.github.io/tags/HTML/"},{"name":"Web Crawling","slug":"Web-Crawling","permalink":"https://jmj3047.github.io/tags/Web-Crawling/"},{"name":"BeautifulSoup","slug":"BeautifulSoup","permalink":"https://jmj3047.github.io/tags/BeautifulSoup/"}],"keywords":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"HTML","slug":"Python/HTML","permalink":"https://jmj3047.github.io/categories/Python/HTML/"}]},{"title":"Improved Training of Wasserstein GANs","slug":"WGAN","date":"2022-04-21T15:00:00.000Z","updated":"2022-10-15T08:05:10.371Z","comments":true,"path":"2022/04/22/WGAN/","link":"","permalink":"https://jmj3047.github.io/2022/04/22/WGAN/","excerpt":"","text":"Journal&#x2F;Conference: NIPSYear(published year): 2017Author: Ishaan Gulrajani, Faruk Ahmed, MartÃ­n Arjovsky, Vincent Dumoulin, Aaron C. CourvilleSubject: DCGAN, Generative Model Improved Training of Wasserstein GANs Summary ê¸°ì¡´ì˜ Wasserstein-GAN ëª¨ë¸ì˜ weight clipping ì„ ëŒ€ì²´í•  ìˆ˜ ìˆëŠ” gradient penalty ë°©ë²•ì„ ì œì‹œ hyperparameter tuning ì—†ì´ë„ ì•ˆì •ì ì¸ í•™ìŠµì´ ê°€ëŠ¥í•´ì¡ŒìŒì„ ì œì‹œ IntroductionGAN ëª¨ë¸ì„ ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµí•˜ê¸° ìœ„í•œ ë§ì€ ë°©ë²•ë“¤ì´ ì¡´ì¬í•´ì™”ìŠµë‹ˆë‹¤. íŠ¹íˆ, ê°€ì¹˜í•¨ìˆ˜ê°€ ìˆ˜ë ´í•˜ëŠ” ì„±ì§ˆì„ ë¶„ì„í•˜ì—¬ Discriminator(ì´í›„ Critic)ê°€ 1-Lipschitz function ê³µê°„ì— ìˆë„ë¡ í•˜ëŠ” Wasserstein GAN(WGAN) ì´ ì œì‹œëœ ë°” ìˆìŠµë‹ˆë‹¤. ë…¼ë¬¸ì€ WGAN ì˜ ë‹¨ì ì„ ê°œì„ í•œ WGAN-GP ëª¨ë¸ì„ ì œì‹œí•©ë‹ˆë‹¤. Toy datasetsì— ëŒ€í•´ criticì˜ weight clippingì´ undesired behaviorë¥¼ ìœ ë°œí•  ìˆ˜ ìˆìŒì„ ì¦ëª… â€œGradient penaltyâ€(WGAN-GP) ê¸°ë²•ìœ¼ë¡œ ì œì•ˆ ë‹¤ì–‘í•œ GAN êµ¬ì¡°ì—ëŒ€í•´ ì•ˆì •ì ì¸ í•™ìŠµì„ ìˆ˜í–‰í•  ìˆ˜ ìˆê³ , ê³ í’ˆì§ˆ ì´ë¯¸ì§€ ìƒì„±ì„ ìˆ˜í–‰í•˜ë©°, ê°œë³„ ìƒ˜í”Œë§ì´ í•„í•„ìš”í•˜ì§€ ì•ŠëŠ” ë¬¸ììˆ˜ì¤€ ì–¸ì–´ ëª¨ë¸ì„ ì œì‹œ BackgroundGenerative adversarial networksì¼ë°˜ì ì¸ GAN êµ¬ì¡°ì— ëŒ€í•´ ë‹¤ì‹œ í•œë²ˆ ê°œë…ì„ ë˜ì§šìŠµë‹ˆë‹¤. Wasserstein GANsWGAN ì€ GAN ì˜ ëª©ì í•¨ìˆ˜ì¸ JSD ê°€ parameter ì— ì—°ì†ì ì´ì§€ ì•Šì— í•™ìŠµì— ë¬¸ì œê°€ ë°œìƒí•¨ì„ ì§€ì í•˜ì˜€ìŠµë‹ˆë‹¤. ì´ì—, Earth-Mover distance ë¡œ ëª¨ë“  êµ¬ê°„ì—ì„œ ì—°ì†ì ì´ê³  ëŒ€ë¶€ë¶„ì˜ êµ¬ê°„ì—ì„œ ë¯¸ë¶„ ê°€ëŠ¥í•˜ê²Œ í•˜ì—¬ ë¬¸ì œë¥¼ í•´ê²°í•˜ì˜€ìŠµë‹ˆë‹¤. ì´ì™¸ì—ë„ WGAN ì˜ íŠ¹ì§•ì— ëŒ€í•´ ì„œìˆ í•˜ë©°, ê°€ì¥ ì¤‘ìš”í•œ íŠ¹ì§•ìœ¼ë¡œ Lipschitz ì¡°ê±´ì„ ë§Œì¡±í•˜ê¸° ìœ„í•´ ì‹œí–‰í•˜ëŠ” weight clipping ì„ ì–¸ê¸‰í•©ë‹ˆë‹¤. Properties of the optimal WGAN criticìµœì ì˜ WGAN critic ì„ ê°€ì •í–ˆì„ ë•Œ, ****weight clippingì´ WGAN criticì—ì„œ ë¬¸ì œë¥¼ ë°œìƒì‹œí‚´ì„ ì–¸ê¸‰í•˜ê³  ì¦ëª…í•œ ê²°ê³¼ë¥¼ ì œì‹œí•©ë‹ˆë‹¤. Difficulties with weight constraintsWGANì˜ weight clippingì´ ìµœì í™”ì— ë¬¸ì œë¥¼ ë°œìƒì‹œí‚¬ ìˆ˜ ìˆê³ , ìµœì í™”ê°€ ì˜ ë˜ë”ë¼ë„ criticì´ pathological value surface ì„ ê°€ì§ˆ ìˆ˜ ìˆìŒì„ ì¦ëª…í•˜ì˜€ë˜ ë‚´ìš©ì„ í™•ì¸í•˜ê¸° ìœ„í•œ ì‹¤í—˜ì„ ì§„í–‰í•©ë‹ˆë‹¤. ë…¼ë¬¸ì€ ê¸°ì¡´ WGAN ì´ ì œì‹œí•˜ì˜€ë˜ hard clipping ë°©ì‹ ì´ì™¸ì—ë„, L2 norm clipping&#x2F;weight normalization&#x2F;L1 and L2 weight decay ë“±ì˜ weight constraint ë¥¼ ê°€ì •í•˜ì˜€ì„ ë•Œ ëª¨ë‘ ë¹„ìŠ·í•œ ë¬¸ì œê°€ ë°œìƒí•˜ì˜€ìŒì„ ì–¸ê¸‰í•©ë‹ˆë‹¤. Capacity underuse &amp; Exploding and vanishing gradientsk-Lipshitz ì¡°ê±´ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•´ weight clipping ì„ ìˆ˜í–‰í•˜ì˜€ì„ ë•Œ, criticì€ ë”ìš± ë‹¨ìˆœí•œ í˜•íƒœì˜ í•¨ìˆ˜ë¥¼ ì·¨í•˜ê²Œ ë©ë‹ˆë‹¤. ë…¼ë¬¸ì€ ì´ë¥¼ ì¦ëª…í•˜ê¸° ìœ„í•´ Generator ì˜ ë¶„í¬ë¥¼ toy distribution + unit-variance ê°€ìš°ì‹œì•ˆ-ë…¸ì´ì¦ˆì— ê³ ì •í•œë’¤, weight clipping ê³¼ í•¨ê»˜ WGAN critic ì„ í•™ìŠµí•œ ê²°ê³¼ë¥¼ ì œì‹œí•©ë‹ˆë‹¤. ì™¼ìª½ ê·¸ë¦¼ì—ì„œ Weight clipping ìˆ˜í–‰í•œ ê²½ìš°ì˜ value surface ëª¨ì–‘ì´ ë‹¨ìˆœí•´ì¡ŒìŒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, ìš°ì¸¡ ê·¸ë¦¼ê³¼ ê°™ì´, Gradient penalty ë¥¼ ìˆ˜í–‰í•œ ê²½ìš°ì— gradient vanishing ì´ë‚˜ exploding ì´ ë°œìƒí•˜ì§€ ì•Šì•˜ìŒì„ ì œì‹œí•©ë‹ˆë‹¤. Gradient penaltyWeight Clipping ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  Lipschitz constraint ë¥¼ ìœ ì§€í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤. ì…ë ¥ì— ëŒ€í•œ Critic ì¶œë ¥ gradient ì˜ í¬ê¸°ë¥¼ ì§ì ‘ ì œì•½í•©ë‹ˆë‹¤. ì´ ë•Œ, tractability issue ë¥¼ í”¼í•˜ê¸° ìœ„í•´ ë¬´ì‘ìœ„ë¡œ ì¶”ì¶œí•œ ìƒ˜í”Œ $\\hat{x}$ ì˜ gradient norm ì„ ì‚¬ìš©í•´ soft í•œ ì œì•½ì„ ì¤ë‹ˆë‹¤. ì´ë ‡ê²Œ ìƒˆë¡­ê²Œ ì •ì˜ë˜ëŠ” ëª©ì í•¨ìˆ˜ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. Sampling distributionë…¼ë¬¸ì€ ë°ì´í„° ë¶„í¬ì™€ generator ë¶„í¬ì—ì„œ ìƒ˜í”Œë§í•œ ì ì˜ ìŒì„ ì´ì€ ë’¤, ì ì„ ì‡ëŠ” ì„ ë¶„ì„ ë”°ë¼ $\\hat{x}$ ë¥¼ ìƒ˜í”Œë§í•˜ì˜€ê³ , ì‹¤í—˜ì ìœ¼ë¡œ ì¢‹ì€ ì„±ëŠ¥ì„ ì–»ì—ˆìŒì„ ì–¸ê¸‰í•©ë‹ˆë‹¤. Penalty coefficientgradient penalty ë¥¼ ê°€í•˜ëŠ” ì •ë„ë¥¼ ê²½ì •í•˜ëŠ” ê³„ìˆ˜ë¡œ, ë…¼ë¬¸ì—ì„œëŠ” ëª¨ë‘ $\\lambda&#x3D;10$ ì„ ì‚¬ìš©í–ˆìŒì„ ì–¸ê¸‰í•©ë‹ˆë‹¤. No critic batch normalizationê¸°ì¡´ GAN ëª¨ë¸ì€ batch normalization ì„ ëª¨ë“  ê³³ì—ì„œ ì‚¬ìš©í–ˆì§€ë§Œ, ì´ëŠ” discriminatorì˜ ë‹¨ì¼ ì…ë ¥ì„ ë‹¨ì¼ ì¶œë ¥ìœ¼ë¡œ ë§¤í•‘í•˜ëŠ” ë¬¸ì œì—ì„œ, ì…ë ¥ì˜ ì „ì²´ ë°°ì¹˜ë¡œë¶€í„° ì¶œë ¥ì˜ ë°°ì¹˜ë¡œ ë§¤í•‘í•˜ëŠ” ë¬¸ì œë¡œ ë³€í™”ì‹œí‚µë‹ˆë‹¤. ì´ ë•Œë¬¸ì— gradient penalty ë¥¼ ìˆ˜í–‰í•˜ë©´ batch normalization ì´ ìœ íš¨í•˜ì§€ ì•Šì€ ê²°ê³¼ê°€ ë°œìƒí•œë‹¤ê³  í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ë…¼ë¬¸ì€ critic ì— batch normalization ì„ ì œê±°í•˜ì˜€ê³  ê·¸ëŸ¼ì—ë„ ì ì ˆí•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŒì„ ì–¸ê¸‰í•©ë‹ˆë‹¤. Two-sided penaltygradient penalty ëŠ” normì´ 1 ì•„ë˜ì— ë¨¸ë¬´ë¥´ì§€ ì•Šê³ (one-sided penalty), 1ë¡œ í–¥í•˜ê¸°(two-sided penalty)ëŠ” ê²ƒì„ ì´‰ì§„í•œë‹¤ëŠ” ì ì„ ì œì‹œí•©ë‹ˆë‹¤. ExperimentsTraining random architectures within a set ì¼ë°˜ì ì¸ DCGAN êµ¬ì¡°ì—ì„œ ìœ„ì˜ í‘œì˜ ì„¤ì •ì„ ëœë¤í•˜ê²Œ ì„¤ì •í•˜ì—¬ ëª¨ë¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ ë¬´ì‘ìœ„ë¡œ 200ê°œì˜ ëª¨ë¸ì„ êµ¬ì„±í•œë’¤, 32x32 ImageNet ì— ëŒ€í•´ WGAN-GP, standard GANì„ í•©ë‹ˆë‹¤. êµ¬ì„±í•œ ëª¨ë¸ì˜ inception_score ê°€ min_score ë³´ë‹¤ í° ê²½ìš° ì„±ê³µìœ¼ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤. WGAN-GP ëŠ” ë§ì€ êµ¬ì¡°ë¥¼ í•™ìŠµí•˜ëŠ”ë° ì„±ê³µí–ˆë‹¤ëŠ” ê²°ê³¼ë¥¼ ì œì‹œí•©ë‹ˆë‹¤. Training varied architectures on LSUN bedroomsì•„ë˜ì™€ ê°™ì´ 6ê°œì˜ ëª¨ë¸ì„ ê¸°ë³¸ ëª¨ë¸ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. ì—¬ê¸°ì— DCGAN, LSGAN, WGAN, WGAN-GP ë¥¼ ê°ê° ì ìš©í•˜ì˜€ì„ ë•Œì˜ ì„±ëŠ¥ì„ ë¹„êµí•©ë‹ˆë‹¤. ë‹¨, WGAN-GPëŠ” discriminator ì—ì„œ Batch normalization ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ê¸°ì— layer normalization ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. WGAN-GP ë¥¼ ì œì™¸í•œ ëª¨ë“  ëª¨ë¸ì—ì„œ ë¶ˆì•ˆì •í•˜ê±°ë‚˜ mode collapse ì— ë¹ ì§„ ëª¨ìŠµì„ ë³´ì…ë‹ˆë‹¤. Improved performance over weight clippingWGAN-GP ê°€ weight clipping ì— ë¹„í•´ ë” ë¹ ë¥¸ í•™ìŠµ ì†ë„ì™€ ìƒ˜í”Œ íš¨ìœ¨ì„ ë³´ì¸ë‹¤ëŠ” ì ì„ ì¦ëª…í•˜ê¸° ìœ„í•œ ì‹¤í—˜ ê²°ê³¼ë¥¼ ì œì‹œí•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´, WGAN ê³¼ WGAN-GP ëª¨ë¸ì„ CIFAR-10 ìœ¼ë¡œ í•™ìŠµí•˜ì—¬ Inception Score ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. ì™¼ìª½ì€ iterationì— ë”°ë¥¸ Inception Scoreì´ë©°, ì˜¤ë¥¸ìª½ì€ ì‹œê°„ì— ë”°ë¥¸ Inception Scoreì…ë‹ˆë‹¤. WGAN-GPëŠ” weight clipping ë³´ë‹¤ í•­ìƒ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. ì´ëŠ” ê°™ì€ optimizer ë¥¼ ì‚¬ìš©í–ˆì„ ë•Œë„ ë§ˆì°¬ê°€ì§€ì´ë©°, ë¹„ë¡ DCGAN ë³´ë‹¤ëŠ” ëŠë¦¬ì§€ë§Œ ìˆ˜ë ´ì— ìˆì–´ì„œ ì•ˆì •ì ì¸ ì ìˆ˜ë¥¼ ë³´ì¼ ìˆ˜ ìˆìŒì„ ì œì‹œí•©ë‹ˆë‹¤. Sample quality on CIFAR-10 and LSUN bedrooms CIFAR-10ìœ¼ë¡œ í•™ìŠµí•œ ëª¨ë¸ì˜ Inception score ë¥¼ ê³„ì‚°í•˜ì—¬ ë‹¤ì–‘í•œ êµ¬ì¡°ì˜ GANì„ ë¹„êµí•œ í‘œë¥¼ ì œì‹œí•©ë‹ˆë‹¤. WGAN-GP ëŠ” Supervised ì˜ ê²½ìš° SGAN ì„ ì œì™¸í–ˆì„ ë•Œ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. ë˜í•œ, WGAN-GP ë¡œ deep ResNet ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ 128X128 LSUN ì¹¨ëŒ€ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ì—¬ ìœ„ì™€ê°™ì€ ê²°ê³¼ë¥¼ ì œì‹œí•©ë‹ˆë‹¤. Modeling discrete data with a continuous generator Generator ëŠ” ì—°ì†ì ì¸ ë¶„í¬ì˜ í•¨ìˆ˜ë¥¼ ê°€ì •í•©ë‹ˆë‹¤. ë”°ë¼ì„œì–¸ì–´ ëª¨ë¸ì€ ë¹„ì—°ì†ì ì¸ ë¶„í¬ë¥¼ ëª¨ë¸ë§ í•´ì•¼í•˜ë¯€ë¡œ GAN ìœ¼ë¡œ í•™ìŠµí•˜ê¸°ì— ë¶€ì ì ˆí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìœ„ëŠ” Google Billion Word ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•´ ë¬¸ì ìˆ˜ì¤€ ì–¸ì–´ ëª¨ë¸ì„ WGAN-GP ë¡œ í•™ìŠµí•œ ê²°ê³¼ì…ë‹ˆë‹¤. ëª¨ë¸ì´ ë¹ˆë²ˆí•˜ê²Œ ì² ìë¥¼ í‹€ë¦¬ì§€ë§Œ, ì–¸ì–´ì˜ í†µê³„ì— ëŒ€í•´ì„œëŠ” ì–´ëŠì •ë„ í•™ìŠµì„ ìˆ˜í–‰í•˜ì˜€ìŒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Meaningful loss curves and detecting overfittingê¸°ì¡´ì˜ weight clipping ì€ loss ê°€ sample quality ì™€ ì—°ê´€ë˜ì–´ ìµœì†Œê°’ìœ¼ë¡œ ìˆ˜ë ´í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì…ë‹ˆë‹¤. WGAN-GP ê°€ í•´ë‹¹ íŠ¹ì„±ì„ ìœ ì§€í•˜ëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•œ í…ŒìŠ¤í¬ë¥¼ ì§„í–‰í•œ ê²°ê³¼ë¥¼ ì œì‹œí•©ë‹ˆë‹¤. (a)ì—ì„œ LSUN ì¹¨ëŒ€ ë°ì´í„°ì…‹ì„ í•™ìŠµí•˜ê³  critic ì˜ negative loss ë¥¼ ê·¸ë ¸ì„ ë•Œ, Gnerator ê°€ í•™ìŠµë¨ì— ë”°ë¼ ê°’ì´ ì¤„ì–´ë“œëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ê²½ìš°, WGAN-GPê°€ criticì—ì„œì˜ ê³¼ì í•©ì„ ì™„í™”í–ˆë‹¤ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, MNIST ë¬´ì‘ìœ„ ìˆ«ì 1000ê°œë¡œ í•™ìŠµí•œ ê²°ê³¼ëŠ”, ì ì€ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•œ ë§Œí¼ ê³¼ì í•©ì´ ë°œìƒí•˜ê¸° ì‰½ìŠµë‹ˆë‹¤. ë•Œë¬¸ì—, criticì´ generatorë³´ë‹¤ ë” ë¹¨ë¦¬ ê³¼ì í•©ë˜ì–´ training lossë¥¼ ì ì°¨ ì¦ê°€ì‹œí‚¤ê³  validation lossë¥¼ ê°ì†Œì‹œì¼°ìŒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ConclusionWGANì— Gradient penaltyë¥¼ ì ìš©í•˜ì—¬ ê¸°ì¡´ì˜ weight clipping ì„ ì ìš©í•¨ìœ¼ë¡œ ì¸í•´ ë°œìƒí•˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆìŒì„ ì œì‹œí•˜ì˜€ìŠµë‹ˆë‹¤. Summarize GANì˜ ê°€ì¥ í° ë¬¸ì œëŠ” í•™ìŠµí™˜ê²½ì´ ë§¤ìš° ë¶ˆì•ˆì •í•˜ë‹¤ëŠ” ê²ƒì´ë‹¤. ìƒì„±ìì™€ êµ¬ë¶„ì ë‘˜ ì¤‘ì— í•˜ë‚˜ê°€ ì‹¤ë ¥ì´ ì›”ë“±ì´ ì¢‹ì•„ì§„ë‹¤ë©´ ë°¸ëŸ°ìŠ¤ê°€ ë¶•ê´´ë˜ê³  ëª¨ë¸ì´ ì •í™•íˆ í•™ìŠµë˜ì§€ ì•Šê³  í•™ìŠµì´ ì™„ë£Œëœ í›„ì—ë„ mode dropping ì´ ìƒê¸°ëŠ”ë° ì´ëŠ” êµ¬ë¶„ìê°€ ê·¸ ì—­í• ì„ ì¶©ë¶„íˆ í•˜ì§€ ëª»í•´ ëª¨ë¸ì´ ìµœì ì ê¹Œì§€ í•™ìŠµì´ ì•ˆ ëœ ê²ƒì´ë‹¤. ë”°ë¼ì„œ ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë³¸ ë…¼ë¬¸ì—ì„œëŠ” WGAN ë°©ë²•ì„ ë„ì…í–ˆë‹¤. ê°„ë‹¨íˆ ì„¤ëª…í•˜ë©´ GANì˜ discriminatorë³´ë‹¤ ì„ ìƒë‹˜ ì—­í• ì„ ì˜ í•  ìˆ˜ ìˆëŠ” criticì„ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ gradientë¥¼ ì˜ ì „ë‹¬ì‹œí‚¤ê³  criticê³¼ generatorë¥¼ ìµœì ì ê¹Œì§€ í•™ìŠµí•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤. ê·¸ë ‡ë‹¤ë©´ ì´ë¥¼ ì ìš©í•˜ë©´ í•™ìŠµì‹œí‚¬ ë•Œ ìƒì„±ìì™€ êµ¬ë¶„ìì˜ ë°¸ëŸ°ìŠ¤ê°€ ì˜ ë§ëŠ”ì§€ ì£¼ì˜ê¹Šê²Œ ë³´ì§€ ì•Šì•„ë„ ë˜ê³  í•™ìŠµí•œ ì´í›„ì— ë°œìƒí•˜ëŠ” mode droppinì´ í•´ê²° ê°€ëŠ¥í•˜ë‹¤. ì‹ì„ í•´ì„í•´ë³´ë©´ ìƒì„±ìê°€ Lipschitz í•¨ìˆ˜ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ”ê°€ í•˜ì§€ì•ŠëŠ”ê°€ì— ëŒ€í•œ ê¸°ì¤€ì´ í•˜ë‚˜ ë” ìƒê¸°ëŠ”ê²ƒ ì´ë‹¤. Link: Improved Training of Wasserstein GANs","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Generative Model","slug":"Paper/Generative-Model","permalink":"https://jmj3047.github.io/categories/Paper/Generative-Model/"}],"tags":[{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"},{"name":"Image Classification","slug":"Image-Classification","permalink":"https://jmj3047.github.io/tags/Image-Classification/"},{"name":"WGAN-GP","slug":"WGAN-GP","permalink":"https://jmj3047.github.io/tags/WGAN-GP/"},{"name":"WGAN","slug":"WGAN","permalink":"https://jmj3047.github.io/tags/WGAN/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Generative Model","slug":"Paper/Generative-Model","permalink":"https://jmj3047.github.io/categories/Paper/Generative-Model/"}]},{"title":"Basic Web Crawling","slug":"Web_Crawling_Basic","date":"2022-04-21T15:00:00.000Z","updated":"2022-04-22T07:30:27.000Z","comments":true,"path":"2022/04/22/Web_Crawling_Basic/","link":"","permalink":"https://jmj3047.github.io/2022/04/22/Web_Crawling_Basic/","excerpt":"","text":"Crawling Tools -Beautifulsoup: íŒŒì´ì¬ì—ì„œ ê°€ì¥ ì¼ë°˜ì ì¸ ìˆ˜ì§‘ë„êµ¬(CSS í†µí•´ì„œ ìˆ˜ì§‘) -Scrapy (CSS, XPATH í˜•íƒœë¡œ ìˆ˜ì§‘) -Selenium (CSS, XPATH í†µí•´ì„œ ë°ì´í„° ìˆ˜ì§‘ + Java Script) â†’ìë°” í•„ìš” + ëª‡ê°œ ì„¤ì¹˜ ë„êµ¬ í•„ìš” ì›¹ì‚¬ì´íŠ¸ ë§Œë“œëŠ” 3ëŒ€ ì¡°ê±´ +1 :HTML, CSS, JavaScript, Ajax(ë¹„ë™ê¸°ì²˜ë¦¬) ì›¹ì‚¬ì´íŠ¸ êµ¬ë™ë°©ì‹ :GET &#x2F; POST Create virtual env(git bash)123pip install virtualenvpython -m virtualenv venvsource venv/Scripts/activate Installing Library123pip install beautifulsoup4pip install numpy pandas matplotlib seabornpip install requests Reference: https://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-beautiful-soup","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"HTML","slug":"Python/HTML","permalink":"https://jmj3047.github.io/categories/Python/HTML/"}],"tags":[{"name":"HTML","slug":"HTML","permalink":"https://jmj3047.github.io/tags/HTML/"},{"name":"Web Crawling","slug":"Web-Crawling","permalink":"https://jmj3047.github.io/tags/Web-Crawling/"},{"name":"BeautifulSoup","slug":"BeautifulSoup","permalink":"https://jmj3047.github.io/tags/BeautifulSoup/"}],"keywords":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"HTML","slug":"Python/HTML","permalink":"https://jmj3047.github.io/categories/Python/HTML/"}]},{"title":"Web Crawling Practice","slug":"Web_Crawling_Headline","date":"2022-04-21T15:00:00.000Z","updated":"2022-04-22T08:17:11.000Z","comments":true,"path":"2022/04/22/Web_Crawling_Headline/","link":"","permalink":"https://jmj3047.github.io/2022/04/22/Web_Crawling_Headline/","excerpt":"","text":"1. Crawling Headline news from Naver12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import requestsfrom bs4 import BeautifulSoupimport pandas as pddef crawling(soup): div = soup.find(&quot;div&quot;, class_=&#x27;list_issue&#x27;) # print(type(div)) print(div.find_all(&#x27;a&#x27;)) #listí˜•íƒœ result = [] urls = [] for a in div.find_all(&quot;a&quot;): # print(a.get_text()) urls.append(a[&#x27;href&#x27;]) result.append(a.get_text()) # print(result) #save as csv file df = pd.DataFrame(&#123;&#x27;news_title&#x27;: result, &quot;url&quot;: urls&#125;) print(df) df.to_csv(&quot;newscrawling.csv&quot;) print(&quot;Crawling Success!&quot;) def main(): CUSTOM_HEADER = &#123; &#x27;referer&#x27; : &#x27;https://www.naver.com/&#x27;, &#x27;user-agent&#x27; : &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36&#x27; &#125; url = &#x27;https://www.naver.com/&#x27; req = requests.get(url = url, headers=CUSTOM_HEADER) print(req.status_code) #200: ì •ìƒì ìœ¼ë¡œ ì‚¬ì´íŠ¸ ëŒì•„ê°€ê³  ìˆìŒ #404: ì£¼ì†Œ ì˜¤ë¥˜ #503: ì„œë²„ ë‚´ë ¤ì§„ ìƒíƒœ #print(req.text)ì´ê±°ë¥¼ beautifulsoupê°ì²´ë¡œ ë°”ê¿”ì¤Œ soup = BeautifulSoup(req.text, &#x27;html.parser&#x27;) print(type(soup)) # print(soup.find(&quot;strong&quot;, class_=&#x27;new&#x27;)) # print(soup.find_all(&quot;span&quot;, class_=&#x27;blind&#x27;)) #print(soup.find(&quot;a&quot;, class_=&#x27;link_newsstand&#x27;)) crawling(soup)if __name__ == &quot;__main__&quot;: main() 2. Crawling Product List from ACBF1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import requestsfrom bs4 import BeautifulSoupimport pandas as pddef crawling(soup): div = soup.find_all(&quot;div&quot;, class_=&#x27;name&#x27;) print(type(div)) #&lt;class &#x27;bs4.element.ResultSet&#x27;&gt; # print(div) product_name = [] # urls =[] for a in div: # print(a.get_text()) # urls.append(a.get(&#x27;href&#x27;)) product_name.append(a.get_text()) print(product_name) # df = pd.DataFrame(&#123;&#x27;news_title&#x27;: product_name&#125;) # print(df) # df.to_csv(&quot;suit_product.csv&quot;) print(&quot;Crawling Success!&quot;) def main(): CUSTOM_HEADER = &#123; &#x27;referer&#x27; : &#x27;https://anyonecanbeafuse.com/category/%EC%88%98%ED%8A%B8/89/&#x27;, &#x27;user-agent&#x27; : &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36&#x27; &#125; url = &#x27;https://anyonecanbeafuse.com/category/%EC%88%98%ED%8A%B8/89/&#x27; req = requests.get(url = url, headers=CUSTOM_HEADER) print(req.status_code) #200: ì •ìƒì ìœ¼ë¡œ ì‚¬ì´íŠ¸ ëŒì•„ê°€ê³  ìˆìŒ #404: ì£¼ì†Œ ì˜¤ë¥˜ #503: ì„œë²„ ë‚´ë ¤ì§„ ìƒíƒœ #print(req.text)ì´ê±°ë¥¼ beautifulsoupê°ì²´ë¡œ ë°”ê¿”ì¤Œ soup = BeautifulSoup(req.text, &#x27;html.parser&#x27;) #print(type(soup)) # print(soup.find_all(&quot;div&quot;, class_=&#x27;name&#x27;)) crawling(soup)if __name__ == &quot;__main__&quot;: main() 3. Crawling Music Title from Chart123456789101112131415161718192021222324252627282930313233343536373839404142import requestsfrom bs4 import BeautifulSoupdef crawling(soup): tbody_df = soup.find(&quot;tbody&quot;) # print(tbody_df) result = [] for a in tbody_df.find_all(&#x27;p&#x27;, class_=&#x27;title&#x27;): # print(a.get_text()) # print(type(a.get_text())) result.append(a.get_text().strip(&quot;\\n&quot;)) print(result) print(&quot;Crawling Success!&quot;) def main(): CUSTOM_HEADER = &#123; &#x27;referer&#x27; : &#x27;https://music.bugs.co.kr/chart&#x27;, #í•„ìˆ˜ ì•„ë‹˜ &#x27;user-agent&#x27; : &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36&#x27; &#125; url = &#x27;https://music.bugs.co.kr/chart&#x27; req = requests.get(url = url, headers=CUSTOM_HEADER) print(req.status_code) #200: ì •ìƒì ìœ¼ë¡œ ì‚¬ì´íŠ¸ ëŒì•„ê°€ê³  ìˆìŒ #404: ì£¼ì†Œ ì˜¤ë¥˜ #503: ì„œë²„ ë‚´ë ¤ì§„ ìƒíƒœ #print(req.text)ì´ê±°ë¥¼ beautifulsoupê°ì²´ë¡œ ë°”ê¿”ì¤Œ soup = BeautifulSoup(req.text, &#x27;html.parser&#x27;) print(type(soup)) #&lt;class &#x27;bs4.BeautifulSoup&#x27;&gt; # print(soup.find_all(&quot;p&quot;, class_=&#x27;title&#x27;)) # print(soup.find_all(&quot;span&quot;, class_=&#x27;blind&#x27;)) #print(soup.find(&quot;a&quot;, class_=&#x27;link_newsstand&#x27;)) crawling(soup)if __name__ == &quot;__main__&quot;: main()","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"HTML","slug":"Python/HTML","permalink":"https://jmj3047.github.io/categories/Python/HTML/"}],"tags":[{"name":"HTML","slug":"HTML","permalink":"https://jmj3047.github.io/tags/HTML/"},{"name":"Web Crawling","slug":"Web-Crawling","permalink":"https://jmj3047.github.io/tags/Web-Crawling/"},{"name":"BeautifulSoup","slug":"BeautifulSoup","permalink":"https://jmj3047.github.io/tags/BeautifulSoup/"}],"keywords":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"HTML","slug":"Python/HTML","permalink":"https://jmj3047.github.io/categories/Python/HTML/"}]},{"title":"Unsupervised representation learning with deep convolutional generative adversarial networks","slug":"DCGAN","date":"2022-04-20T15:00:00.000Z","updated":"2022-10-15T08:03:41.524Z","comments":true,"path":"2022/04/21/DCGAN/","link":"","permalink":"https://jmj3047.github.io/2022/04/21/DCGAN/","excerpt":"","text":"Journal&#x2F;Conference: ICLRYear(published year): 2016Author: Alec Radford, Luke Metz, Soumith ChintalaSubject: DCGAN, Generative Model Unsupervised representation learning with deep convolutional generative adversarial networks Summary CNN ê³¼ GAN framework ë¥¼ ê²°í•©í•œ DCGAN ëª¨ë¸ì„ ì œì‹œ Introductionë…¼ë¬¸ ë‹¹ì‹œ GAN ì€ ë¶ˆì•ˆì •í•œ í•™ìŠµê³¼ Generator ì˜ ì˜¤ì‘ë™ìœ¼ë¡œ ì¸í•´ ì œí•œì ìœ¼ë¡œë§Œ ì“°ì˜€ìŠµë‹ˆë‹¤. ì´ì— ë…¼ë¬¸ì€ ì´ë¯¸ì§€ ìƒì„± ëª¨ë¸ì„ ë§Œë“¤ê¸° ìœ„í•œ CNN ê¸°ë°˜ GAN frameworkì¸ DCGAN(Deep Convolutional GANs) ì„ ì œì‹œí•©ë‹ˆë‹¤. ëª¨ë¸ êµ¬ì¡°ì— ì œì•½ì„ ê°€í•˜ì—¬ ëŒ€ë¶€ë¶„ì˜ ìƒí™©ì—ì„œ ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆê²Œ í•¨ Discriminator ë¡œ image classification ì„ ìˆ˜í–‰í•˜ì˜€ì„ ë•Œ ê¸°íƒ€ ë¹„ì§€ë„ í•™ìŠµ ëª¨ë¸ê³¼ ë¹„ìŠ·í•œ ì„±ëŠ¥ì„ ë³´ì„ íŠ¹ì • í•„í„°ê°€ íŠ¹ì • objectë¥¼ ê·¸ë ¤ë‚¸ë‹¤ëŠ” ê²ƒì„ ì‹œê°í™”í•˜ì—¬ ì œì‹œí•¨ Generator ì— ì…ë ¥í•˜ëŠ” noise ë¥¼ ì œì–´í•˜ì—¬ ìƒì„±ë˜ëŠ” ìƒ˜í”Œì˜ ë‹¤ì–‘í•œ ì†ì„±ì´ ë³€í™”í•˜ëŠ” ê²ƒì„ íƒêµ¬í•¨ Approach and Model Architectureë…¼ë¬¸ ì´ì „ì—ë„ GANì— CNNì„ ì¨ì„œ ì´ë¯¸ì§€ í’ˆì§ˆì„ ë†’ì´ë ¤ëŠ” ì‹œë„ê°€ ìˆì—ˆìœ¼ë‚˜ ì¢‹ì€ ì„±ê³¼ë¥¼ ê±°ë‘ì§€ ëª»í•˜ì˜€ë‹¤ê³  ì„¤ëª…í•©ë‹ˆë‹¤. ì´í›„, ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì—ì„œ ì•ˆì •ì ì´ê³  ë†’ì€ í•´ìƒë„ì˜ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•œ DCGAN ëª¨ë¸ ì„¤ê³„ ê°€ì´ë“œë¼ì¸ì„ ì œì‹œí•©ë‹ˆë‹¤. Details of Adversarial Training3ê°€ì§€ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. Large-scale Scene Understanding(LSUN) Imagenet-1k Faces ê·¸ì™¸ì— í•™ìŠµ ë””í…Œì¼ì„ ì•„ë˜ì™€ ê°™ì´ ì œì‹œí•©ë‹ˆë‹¤. pre-processing ì œì™¸ batch size 128 ê°€ì¤‘ì¹˜ëŠ” N(0, 0.02) ë¡œ ì´ˆê¸°í™” Leaky ReLUì˜ ê¸°ìš¸ê¸°ëŠ” 0.2ë¡œ ì„¤ì •í•¨ AdamOptimizer, $\\beta_1 &#x3D;0.0002, \\beta_2&#x3D;0.9$ Generator êµ¬ì¡°ì˜ ëª¨ì‹ë„ëŠ” ìœ„ì™€ ê°™ìŠµë‹ˆë‹¤. LSUN ë°ì´í„°ì…‹ìœ¼ë¡œ 1 epoch ë¥¼ í•™ìŠµì‹œí‚¨ í›„ ì¹¨ì‹¤ì„ ìƒì„±í–ˆì„ ë•Œì˜ ê²°ê³¼ì…ë‹ˆë‹¤. ì´ë¡ ì ìœ¼ë¡œ ëª¨ë¸ì´ í›ˆë ¨ ì˜ˆì‹œë¥¼ ê¸°ì–µí–ˆì„ ìˆ˜ë„ ìˆìœ¼ë‚˜, ì‘ì€ í•™ìŠµë¥ ê³¼ ë¯¸ë‹ˆë°°ì¹˜ë¥¼ ì‚¬ìš©í–ˆìŒì„ ê°ì•ˆí•  ë•Œ ê°€ëŠ¥ì„±ì´ ë‚®ë‹¤ê³  ì„¤ëª…í•©ë‹ˆë‹¤. LSUN ë°ì´í„°ì…‹ìœ¼ë¡œ 5 epoch í•™ìŠµ í›„ ì¹¨ì‹¤ì„ ìƒì„±í•œ ê²°ê³¼ì…ë‹ˆë‹¤. ì¹¨ëŒ€ ë“±ì˜ ê·¼ì²˜ì—ì„œ ì˜¤íˆë ¤ underfitting ì´ ë°œìƒí–ˆìŒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Empirical Validation of DCGANs CapabilitiesUnsupervised representation learning ì•Œê³ ë¦¬ì¦˜ì„ í‰ê°€í•˜ëŠ” ì¼ë°˜ì ì¸ ë°©ë²•ì€ supervised ë°ì´í„°ì…‹ìœ¼ë¡œ íŠ¹ì§•ì„ ì¶”ì¶œí•œ ë’¤ performanceë¥¼ ì¸¡ì •í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. CIFAR-10 ë°ì´í„°ì…‹ì— ëŒ€í•´ ê²€ì¦í•œ ê²°ê³¼, ë‹¤ë¥¸ ë°©ë²•ë“¤(K-means, Exemplar CNN ë“±)ê³¼ ë¹„êµí•˜ì—¬ ê²°ê³¼ì— í° ì°¨ì´ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. StreetView House Numbers dataset(SVHN) ë°ì´í„°ì…‹ì—ì„œëŠ” state-of-the-art ê²°ê³¼ë¥¼ ì–»ì—ˆìŒì„ ì œì‹œí•©ë‹ˆë‹¤. Investigating and Visualizing the Internals of the Networksê°€ì¥ ê°€ê¹Œìš´ í•™ìŠµ ë°ì´í„° ì´ë¯¸ì§€ë¥¼ ì°¾ê±°ë‚˜, ìµœê·¼ì ‘ í”½ì…€&#x2F;íŠ¹ì§•ì„ í™•ì¸í•˜ê±°ë‚˜ log-likelihood metric ìœ¼ë¡œ í‰ê°€ë¥¼ í•˜ëŠ” ë°©ë²•ì€ ëª¨ë‘ ì„±ëŠ¥ì´ ë–¨ì–´ì§€ëŠ” metric ì´ê¸°ì— ì‚¬ìš©í•˜ì§€ ì•Šì•˜ìŒì„ ì–¸ê¸‰í•©ë‹ˆë‹¤. ë…¼ë¬¸ì€ ëŒ€ì‹ , 2ê°œì˜ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•  ë•Œ ì‚¬ìš©í•œ noise 2ê°œë¥¼ interpolation í•˜ê³ , interpolated z ë¡œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•œ ê²°ê³¼ë¥¼ ì œì‹œí•©ë‹ˆë‹¤. í•œ ì´ë¯¸ì§€ì—ì„œ ë‹¤ë¥¸ ì´ë¯¸ì§€ë¡œ ì ì§„ì ìœ¼ë¡œ ë³€í•´ê°€ëŠ” ëª¨ìŠµì„ ê´€ì¸¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ ë…¸ì´ì¦ˆ ë²¡í„° z ì˜ ì‚°ìˆ  ì—°ì‚°ì„ í†µí•´, vec(ì›ƒëŠ” ì—¬ì) âˆ’âˆ’ vec(ë¬´í‘œì • ì—¬ì) ++ vec(ë¬´í‘œì • ë‚¨ì) &#x3D;&#x3D; vec(ì›ƒëŠ” ë‚¨ì) ê°™ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆì—ˆìŒì„ ì œì‹œí•©ë‹ˆë‹¤. ë˜í•œ, ëœë¤í•˜ê²Œ ìƒì„±í•œ í•„í„°ì™€ í•™ìŠµëœ í•„í„°ì˜ activation ì„ ì•„ë˜ì™€ ê°™ì´ ì‹œê°í™” í•˜ì˜€ìŠµë‹ˆë‹¤. ì´í•´í•  ìˆ˜ ì—†ëŠ” feature ê°€ ì•„ë‹Œ íŠ¹ì • objectë‚˜ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ê³  ìˆìŒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Conclusions and future workë…¼ë¬¸ì€ CNN ê¸°ë°˜ì˜ ì•ˆì •ì ì¸ ì´ë¯¸ì§€ ìƒì„±ëª¨ë¸ì¸ DCGANì„ ì œì•ˆí•˜ì˜€ìœ¼ë©°, image representationì— ì í•©í•œ ì„±ëŠ¥ì„ ë³´ì„ì„ ì œì‹œí•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì—¬ì „íˆ, í•™ìŠµì´ ê¸¸ì–´ì§€ëŠ” ê²½ìš° í•„í„° ì¼ë¶€ê°€ ìš”ë™ì¹˜ëŠ” ë“±ì˜ í˜„ìƒì„ ê´€ì¸¡í•˜ê¸°ë„ í•˜ì˜€ìŒì„ ì–¸ê¸‰í•©ë‹ˆë‹¤. Link: Unsupervised representation learning with deep convolutional generative adversarial networks","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Generative Model","slug":"Paper/Generative-Model","permalink":"https://jmj3047.github.io/categories/Paper/Generative-Model/"}],"tags":[{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"},{"name":"DCGAN","slug":"DCGAN","permalink":"https://jmj3047.github.io/tags/DCGAN/"},{"name":"Convolutional Neural Networks","slug":"Convolutional-Neural-Networks","permalink":"https://jmj3047.github.io/tags/Convolutional-Neural-Networks/"},{"name":"Image Classification","slug":"Image-Classification","permalink":"https://jmj3047.github.io/tags/Image-Classification/"},{"name":"GAN","slug":"GAN","permalink":"https://jmj3047.github.io/tags/GAN/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Generative Model","slug":"Paper/Generative-Model","permalink":"https://jmj3047.github.io/categories/Paper/Generative-Model/"}]},{"title":"Generative Adversarial Nets","slug":"Generative_Adversarial_Nets","date":"2022-04-20T15:00:00.000Z","updated":"2022-10-15T08:03:52.639Z","comments":true,"path":"2022/04/21/Generative_Adversarial_Nets/","link":"","permalink":"https://jmj3047.github.io/2022/04/21/Generative_Adversarial_Nets/","excerpt":"","text":"Journal&#x2F;Conference: NIPSYear(published year): 2014Author: I. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, Yoshua BengioSubject: GAN, Generative Model Generative Adversarial Nets Summary ì ëŒ€ì ìœ¼ë¡œ ë™ì‘í•˜ëŠ” ë‘ê°œì˜ ë„¤íŠ¸ì›Œí¬ë¥¼ ì‚¬ìš©í•´ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ìƒì„±í•  ìˆ˜ ìˆëŠ” GAN(Generative Adversarial Nets) êµ¬ì¡°ë¥¼ ì œì•ˆ ìƒì„±ì(Generator) ì™€ ê°ë³„ì(Discriminator) ëª¨ë‘ ë§ˆë¥´ì½”í”„ ì²´ì¸ ë“±ì˜ êµ¬ì¡°ì—†ì´ back-propagation ìœ¼ë¡œ í•™ìŠµì´ ê°€ëŠ¥í•œ ì¸ê³µì‹ ê²½ë§ êµ¬ì¡°ë¥¼ ì‚¬ìš© ì´í›„ ë“±ì¥í•˜ëŠ” ìˆ˜ë§ì€ GAN ê¸°ë°˜ ëª¨ë¸ì˜ ê¸°ì›ì´ ë˜ëŠ” ë…¼ë¬¸ Introduction &amp; Related Worksë¶„ë¥˜ ë¬¸ì œì— ì œí•œë˜ì–´ ì‚¬ìš©ë˜ë˜ ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ìš©ë„ë¥¼ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ë¬¸ì œì—ë„ ì ìš©í•  ìˆ˜ ìˆëŠ” ì ëŒ€ì  ìƒì„± ì‹ ê²½ë§(Generative Adversarial Nets)ì„ ìµœì´ˆë¡œ ì œì‹œí•œ ë…¼ë¬¸ì…ë‹ˆë‹¤. GANì€ ì•„ë˜ì™€ ê°™ì€ ëª©í‘œë¥¼ ê°€ì§„, ì ëŒ€ì ì¸ ë‘ ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤. ê°ë³„ì(Discriminator) ëª¨ë¸ ë°ì´í„°ê°€ ì›ë³¸ ë°ì´í„°ì…‹ì—ì„œ ì˜¨ê²ƒì¸ì§€, ìƒì„±ìê°€ ë§Œë“  ê²ƒì¸ì§€ë¥¼ íŒë³„ ì˜ˆì‹œ) ê²½ì°°ì´ ì§€íê°€ ìœ„ì¡°ë˜ì—ˆëŠ”ì§€ë¥¼ íŒë³„ ìƒì„±ì(Generator) ëª¨ë¸ ê°ë³„ìê°€ êµ¬ë¶„í•  ìˆ˜ ì—†ëŠ” ê°€ì§œ ë°ì´í„°ë¥¼ ìƒì„± ì˜ˆì‹œ) ìœ„íë²”ì´ ê²½ì°°ì´ êµ¬ë¶„í•  ìˆ˜ ì—†ëŠ” ìœ„ì¡° ì§€íë¥¼ ì œì‘í•¨ ë…¼ë¬¸ì€ í•´ë‹¹ ë°©ë²•ì´ íŠ¹ë³„í•œ ëª¨ë¸ì´ë‚˜ í•™ìŠµ ë°©ë²•ì„ í•„ìš”ë¡œ í•˜ì§€ ì•ŠëŠ” ë°©ë²•ì´ë¼ê³  í•˜ë©°, MLP(multi-layer perception) êµ¬ì¡°ë¥¼ ì‚¬ìš©í•´ í•™ìŠµí•œ ê²°ê³¼ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. Adversarial netsì ëŒ€ì  ì‹ ê²½ë§ì˜ ê°€ì¥ ì§ê´€ì ì¸ ì˜ˆì‹œë¡œ MLP ëª¨ë¸ì„ ì‚¬ìš©í•œ ê²½ìš°ë¥¼ ê°€ì •í•˜ì—¬ ì„¤ëª…í•©ë‹ˆë‹¤. ì´ ë•Œ ì‚¬ìš©í•˜ëŠ” í‘œê¸°ë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. $x\\sim p_{data}$ : ì‹¤ì œ ë°ì´í„°ë¡œë¶€í„° ë½‘ì€ ìƒ˜í”Œ $p_g$ : ìƒì„±ìê°€ ìƒì„±í•˜ëŠ” ë°ì´í„°ì˜ ë¶„í¬ $p_z(z)$ : ë°ì´í„°ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•˜ëŠ” ì…ë ¥ ë…¸ì´ì¦ˆ ë¶„í¬ $G(z;\\theta_g)$ : ìƒì„±ì ëª¨ë¸ $\\theta_g$ : ìƒì„±ì ëª¨ë¸ íŒŒë¼ë¯¸í„° $D(x;\\theta_d)$ : ê°ë³„ì ëª¨ë¸ $\\theta_D$ : ê°ë³„ì ëª¨ë¸ íŒŒë¼ë¯¸í„° D ëŠ” ì‹¤ì œ ë°ì´í„°ì™€ ìƒì„±ëœ ë°ì´í„°ì— ì •í™•íˆ êµ¬ë¶„í•  ìˆ˜ ìˆëŠ” í™•ë¥ ì„ ìµœëŒ€í™” í•˜ë ¤ê³  í•©ë‹ˆë‹¤. G ëŠ” Dê°€ ì‹¤ì œ ë°ì´í„°ë¡œ ì°©ê°í•  ë§Œí•œ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì„ ëª©ì ìœ¼ë¡œ $\\log(1-D(G(z))$ ë¥¼ ìµœì†Œí™” í•˜ë ¤ê³  í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ì•„ë˜ì™€ ê°™ì´ ê°€ì¹˜í•¨ìˆ˜ $V(G,D)$ ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, G ëŠ” ìµœì†Œí™”, DëŠ” ìµœëŒ€í™”ë¥¼ ëª©ì ìœ¼ë¡œ ê²½ìŸí•©ë‹ˆë‹¤. ì‹¤ì œ ê³„ì‚°ì—ì„œ V ë¥¼ ìµœëŒ€ë¡œ í•˜ëŠ” D ë¥¼ êµ¬í•  ë•Œ ë§ì€ ê³„ì‚°ì´ í•„ìš”í•˜ê³ , ë°ì´í„°ì…‹ì´ ì œí•œëœ ìƒí™©ì—ì„œ ê³¼ì í•©ì´ ë°œìƒí•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì‹¤ì œ í›ˆë ¨ì—ì„œëŠ” D ë¥¼ k ë²ˆë§Œ í•™ìŠµí•˜ê³  G ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤. ë˜í•œ, í•™ìŠµ ì´ˆê¸°ì—ëŠ” Gê°€ ìƒì„±í•˜ëŠ” ë°ì´í„°ì˜ í’ˆì§ˆì´ ë‚®ìœ¼ë¯€ë¡œ Dê°€ íŒë³„ì„ í•˜ê¸° ì‰¬ì›Œ, $\\log(1âˆ’D(G(z)))$ í•­ì´ ì†Œì‹¤ë ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ, $\\log D(G(z))$ ë¥¼ ìµœëŒ€í™” í•˜ëŠ” ë¬¸ì œë¡œ ë³€í™˜í•˜ì—¬ ì´ˆê¸°ì— í•™ìŠµì´ ì˜ ì´ë¤„ì§ˆ ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. í•™ìŠµ ê³¼ì •ì˜ ëª¨ì‹ë„ì…ë‹ˆë‹¤. íŒŒë€ ì ì„ ì€ ê°ë³„ì Dì˜ ë¶„í¬, ê²€ì€ ì ì€ ì›ë³¸ ë°ì´í„° ë¶„í¬, ì´ˆë¡ ì‹¤ì„ ì€ ìƒì„±ì Gì˜ ë¶„í¬ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. íŒŒë€ìƒ‰ì ì„ : discriminator ê²€ì •ìƒ‰ì ì„ : real dataì—ì„œë‚˜ì˜¨sample ì´ˆë¡ìƒ‰ì‹¤ì„ : generator Zì˜†ì˜ê²€ì •ìƒ‰ì‹¤ì„ : domain from which z is sampled í™”ì‚´í‘œ: ìƒì„±ìê°€ noiseë¥¼ real dataì™€ ì–¼ë§ˆë‚˜ ë¹„ìŠ·í•˜ê²Œ ë§Œë“¤ì–´ì£¼ëŠ”ì§€ì— ëŒ€í•œ ì§€í‘œ (a) ì™€ ê°™ì´ í•™ìŠµì´ ì™„ë£Œë˜ê¸° ì „ì˜ ìƒíƒœì—ì„œ ì‹œì‘í•©ë‹ˆë‹¤.(model training ì´ˆê¸°ìƒíƒœ) (b) ì™€ ê°™ì´ D ë¥¼ ì—…ë°ì´íŠ¸ í•  ë•Œ, ìµœì ì˜ D( $D^{*}_G(x)$ ) ëŠ” $\\frac{p_{data}(x)}{p_{data}(x) + p_g(x)}$ ë¡œ ìˆ˜ë ´í•©ë‹ˆë‹¤.(ë‚´ë¶€ì˜ ì•Œê³ ë¦¬ì¦˜ì— ì˜í•´ì„œ êµ¬ë¶„ìê°€ trainë¨) (c) Gë¥¼ ì—…ë°ì´íŠ¸í•˜ë©´, D ë¥¼ êµë€í•  ìˆ˜ ìˆë„ë¡ G ê°€ ìƒì„±í•˜ëŠ” ë¶„í¬ê°€ ì‹¤ì œ ë°ì´í„° ë¶„í¬ì— ê°€ê¹Œì›Œì§‘ë‹ˆë‹¤.(êµ¬ë¶„ìê°€ í•™ìŠµí•œ ê±¸ ìƒì„±ìì—ê²Œ ì—…ë°ì´íŠ¸) (d) í•™ìŠµ ê³¼ì •ì„ ë°˜ë³µí•˜ë©´ ìƒì„±ìëŠ” ë°ì´í„° ë¶„í¬ì™€ ì¼ì¹˜í•˜ëŠ” ë°ì´í„°ë¥¼ ìƒì„±($p_g &#x3D; p_{data}$) í•˜ë©°, ê°ë³„ìëŠ” ì–´ë– í•œ ìƒ˜í”Œë„ êµ¬ë¶„í•  ìˆ˜ ì—†ê²Œ ë©ë‹ˆë‹¤. ($D(x)&#x3D;\\frac{1}{2}$) (real dataì™€f ake dataê°€ ê°™ì€ ëª¨ìŠµì´ ëœ ë‹¨ê³„. êµ¬ë¶„ìëŠ” fakeì™€ real dataë¥¼ êµ¬ë¶„í•  ìˆ˜ ì—†ê²Œ ë¨.) Fake dataê°€ì™œnoiseì¸ì§€? ëª…í™•í•œ ì´ìœ ëŠ” ëª…ì‹œë˜ì–´ ìˆì§€ ì•ŠìŒ. ëŒ€ëµì ì¸ ì´ìœ ë¥¼ ì¶”ë¡ í•´ë³´ìë©´ ìƒì„±ìì— í¸í–¥ë˜ì§€ ì•Šì€ ë°ì´í„°ê°€ ë“¤ì–´ê°€ì•¼ ì‹¤í—˜ì˜ ê²°ê³¼ê°€ ë” clearí•˜ê¸° ë•Œë¬¸. ìƒì„±ìì— ë„£ì–´ì„œ ë§Œë“¤ì–´ì§„ ë°ì´í„°ê°€ mê°œë¼ë©´, ê·¸ ë°ì´í„° mê°œê°€ ë§Œë“¤ì–´ì§€ë ¤ë©´ ê°™ì€ ìˆ«ìì˜ real dataê°€ ìˆì–´ì•¼ í•¨. ë”°ë¼ì„œ ì´ë°ì´í„°ëŠ” 2mê°œ êµ¬ë¶„ìì˜ ê²°ê³¼ê°’ì€ fake dataì¼ë•Œ 0, real data ì¼ ë•Œ 1ì¸ í•˜ë‚˜ì˜ ìŠ¤ì¹¼ë¼ ê°’ ë”°ë¼ì„œ ê°€ì¥ ì´ìƒì ì¸ êµ¬ë¶„ìê°€ ë  ë•Œì˜ ê°’ì€ 0.5 GAN ëª¨ë¸ì€ markov ëª¨ë¸ì´ í•´ì•¼í•˜ëŠ” í›ˆë ¨ ê³¼ì •ê³¼ overfittingì— ë¬¸ì œì ì„ ë³´ì™„í•˜ê¸° ìœ„í•´ ì´ë¥¼ í•œë²ˆì— í•˜ê¸° ìœ„í•´ ë§Œë“¤ì–´ì§„ ë„¤íŠ¸ì›Œí¬ Theoretical Resultsì ëŒ€ì  ì‹ ê²½ë§ ë¬¸ì œì—ì„œ ìƒì„±ìê°€ ì›ë³¸ ë°ì´í„°ì™€ ìœ ì‚¬í•œ ë¶„í¬ì˜ ë°ì´í„°ë¥¼ ìƒì„±í•  ìˆ˜ ìˆë‹¤ëŠ” ì¦ëª…ì„ ì œì‹œí•©ë‹ˆë‹¤. ë˜í•œ, ì‹¤ì œ ì ëŒ€ì  ì‹ ê²½ë§ì„ í•™ìŠµí•˜ê¸° ìœ„í•´ ì„¤ê³„í•œ ì•„ë˜ ì•Œê³ ë¦¬ì¦˜ ë˜í•œ ê°™ì€ ê²°ê³¼ì— ìˆ˜ë ´í•œë‹¤ëŠ” ì¦ëª…ì„ ì œì‹œí•©ë‹ˆë‹¤. Global Optimality of $p_g &#x3D; p_{data}$ë¨¼ì € ì„ì˜ì˜ G ê°€ ì£¼ì–´ì¡Œì„ ë•Œ ìµœì ì˜ D ë¥¼ ê³„ì‚°í•˜ëŠ” ê³¼ì •ì„ ë³´ì…ë‹ˆë‹¤. ìµœì ì˜ D ë¥¼ ì´ìš©í•˜ì—¬ Equation 1 ì„ Gì— ê´€í•œ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ë•Œ ìƒˆë¡­ê²Œ ì •ë¦¬í•œ ê°€ì¹˜í•¨ìˆ˜ê°€, Gê°€ ìƒì„±í•˜ëŠ” ë°ì´í„°ì˜ ë¶„í¬ê°€ ì‹¤ì œ ë¶„í¬ë¥¼ ë”°ë¥´ëŠ” ê²½ìš°ì—ë§Œ ìµœì†Œí™”ëœë‹¤ëŠ” ê²ƒì„ ë‹¤ìŒê³¼ ê°™ì´ ì¦ëª…í•©ë‹ˆë‹¤. Convergence of Algorithm 1G ì™€ D ê°€ $p_g$ ì¶©ë¶„í•œ í‘œí˜„ë ¥ì„ ê°–ê³  ìˆì„ ë•Œ, ì œì‹œí•œ ì•Œê³ ë¦¬ì¦˜ì´ $p_g&#x3D;p_{data}$ ë¡œ ìˆ˜ë ´í•¨ì„ ì•„ë˜ì™€ ê°™ì´ ì¦ëª…í•©ë‹ˆë‹¤. ì‹¤ì œë¡œ MLP ë¥¼ ì‚¬ìš©í•œ G ë¡œëŠ” ëª¨ë“  í˜•íƒœì˜ $p_g$ ë¥¼ í‘œí˜„í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ì´ë¡ ì ì¸ ìµœê³  ì„±ëŠ¥ì„ ë³´ì¥í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤. ë…¼ë¬¸ì€ ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³  GANì´ ì‹¤ì œ í›ˆë ¨ê²°ê³¼ì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì„ì„ ì œì‹œí•©ë‹ˆë‹¤. Experimentsì‹¤í—˜ì— ì‚¬ìš©í•œ ì¡°ê±´ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. Dataset : MNIST, Toronto Face Database(TFD), CIFAR-10 ì‚¬ìš© Generator : ReLU&#x2F;sigmoid í™œì„±í•¨ìˆ˜ë¥¼ í˜¼í•©í•˜ì—¬ ì‚¬ìš© Discriminator : maxout í™œì„±í•¨ìˆ˜ë¥¼ ì‚¬ìš© Dë¥¼ í•™ìŠµì‹œí‚¬ ë•Œë§Œ Dropoutì„ ì‚¬ìš© Gì—ì„œ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ê²½ìš°ì—ë§Œ noiseë¥¼ input ìœ¼ë¡œ ì‚¬ìš© GAN ì€ ë°ì´í„° ë¶„í¬ ìì²´ë¥¼ êµ¬í•˜ê¸° ìœ„í•œ tractable likelihood ë¥¼ ê°€ì •í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ëª¨ë¸ì„ í‰ê°€í•˜ê¸° ìœ„í•´ ê¸°ì¡´ì— ì œì•ˆëœ ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. Generator ì—ì„œ ìƒì„±í•œ ë°ì´í„°ë¥¼ Gaussian Parzen window ì— fitting fitting í•œ ë¶„í¬ê°€ ì£¼ì–´ì¡Œì„ ë•Œ log-likelihood ë¥¼ ê³„ì‚° Validation set ìœ¼ë¡œ êµì°¨ ê²€ì¦ì„ ìˆ˜í–‰í•´ í‘œì¤€ í¸ì°¨ë¥¼ ê³„ì‚° ë…¼ë¬¸ì€ í•´ë‹¹ ë°©ë²•ì˜ ë¶„ì‚°ì´ í¬ê³  ë†’ì€ ì°¨ì›ì˜ ë°ì´í„°ì—ì„œ ì˜ ì‘ë™í•˜ì§€ ì•Šì§€ë§Œ, GAN ì´ ê¸°ì¡´ ëª¨ë¸ì— ë¹„í•´ ìƒëŒ€ì ìœ¼ë¡œ ì¢‹ì€ ê²°ê³¼ë¥¼ ë³´ì´ê³  ìˆìŒì„ ì œì‹œí•©ë‹ˆë‹¤. ë‹¤ìŒìœ¼ë¡œ GAN ëª¨ë¸ë¡œ ìƒì„±í•œ ë°ì´í„°ë¥¼ ì œì‹œí•©ë‹ˆë‹¤. ê°€ì¥ ìš°ì¸¡ì—ëŠ” ì›ë³¸ ë°ì´í„°ì…‹ ì¤‘ ìƒì„±ëœ ë°ì´í„°ì— ê°€ì¥ ê°€ê¹Œìš´ ë°ì´í„°ë¥¼ ë°°ì¹˜í•˜ì˜€ìŠµë‹ˆë‹¤. ë…¼ë¬¸ì€ í•´ë‹¹ ëª¨ë¸ì´ ê¸°ì¡´ì˜ ìƒì„± ëª¨ë¸ë³´ë‹¤ ë‚«ë‹¤ê³  ì£¼ì¥í•˜ê¸°ëŠ” ì–´ë µì§€ë§Œ, ë¹„ìŠ·í•œ ì„±ê³¼ì™€ ì‘ìš© ê°€ëŠ¥ì„±ì„ ë³´ì—¬ì¤„ ìˆ˜ ìˆë‹¤ëŠ” ì˜ê²¬ì„ ì œì‹œí•©ë‹ˆë‹¤. ë˜í•œ ìœ„ ê·¸ë¦¼ê³¼ ê°™ì´, Generator ì˜ Input noise ë¥¼ ì ì§„ì ìœ¼ë¡œ ë³€í˜•ì‹œí‚¬ ë•Œ, ì ì  interploation ë˜ì–´ê°€ëŠ” ìƒì„± ë°ì´í„°ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Advantages and disadvantagesGAN ì˜ ë‹¨ì ì„ ì•„ë˜ì™€ ê°™ì´ ì •ë¦¬í•©ë‹ˆë‹¤. Generator ê°€ ìƒì„±í•˜ëŠ” ë°ì´í„°ì˜ ë¶„í¬ê°€ ëª…ì‹œì ìœ¼ë¡œ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. Generator ì™€ Discriminator ì˜ ê· í˜•ì´ ê¹¨ì§€ëŠ” ê²½ìš° í•™ìŠµì´ ì›í™œì´ ì´ë£¨ì–´ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤. ë˜í•œ, GAN ì˜ ì¥ì ì„ ì•„ë˜ì™€ ê°™ì´ ì •ë¦¬í•©ë‹ˆë‹¤. ë§ˆë¥´ì½”í”„ ì²´ì¸ ê°™ì€ êµ¬ì¡° ì—†ì´ ì—­ì „íŒŒ ë§Œìœ¼ë¡œë„ í•™ìŠµì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. Generator ì˜ ë¶„í¬ë¡œ íŠ¹ë³„í•œ ëª¨ë¸ì„ ê°€ì •í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë”ìš± ë³µì¡í•œ ë°ì´í„° ë¶„í¬ë¥¼ ëª¨ì‚¬í•  ìˆ˜ ìˆì–´ ì„ ëª…í•œ ë°ì´í„°ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Conclusions and future workGAN í”„ë ˆì„ì›Œí¬ë¥¼ í™•ì¥í•˜ê³  ê°œì„ í•  ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤. ì£¼ì–´ì§„ ì¡°ê±´ì— ë”°ë¼ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ëª¨ë¸ë¡œ ë°œì „ ê°€ëŠ¥ xê°€ ì£¼ì–´ì¡Œì„ ë•Œ zë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë³´ì¡° ë„¤íŠ¸ì›Œí¬ë¥¼ í•™ìŠµí•œë‹¤ë©´ ìƒì„±ìì˜ ë°ì´í„° ë¶„í¬ë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆìŒ parametersë¥¼ ê³µìœ í•˜ëŠ” conditionals modelë¥¼ í•™ìŠµí•¨ìœ¼ë¡œì¨ ë‹¤ë¥¸ conditionals modelsì„ ê·¼ì‚¬ì ìœ¼ë¡œ ëª¨ë¸ë§í•  ìˆ˜ ìˆìŒ Semi-supervised learningì—ë„ í™œìš© ê°€ëŠ¥ : ë°ì´í„°ê°€ ì œí•œëœ ê²½ìš° Discriminator ë¥¼ í™œìš©í•˜ì—¬ classifierì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŒ íš¨ìœ¨ì„± ê°œì„ : Gì™€ Dë¥¼ ê· í˜•ìˆê²Œ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ë°©ë²•ì´ë‚˜ ìƒˆë¡œìš´ z ë¶„í¬ë¥¼ ì œì‹œí•˜ì—¬ í•™ìŠµ ì†ë„ ê°œì„  ê°€ëŠ¥ Summarize GAN ëª¨ë¸ì€ ìƒì„±ì(Generator)ì™€ êµ¬ë¶„ì(Discriminator) ë‘˜ì˜ ì ëŒ€ì ì¸ ê²½ìŸì„ í†µí•´ì„œ í•™ìŠµí•˜ëŠ” ë”¥ëŸ¬ë‹ ë„¤íŠ¸ì›Œí¬ ì‹¤ì œ ìš°ë¦¬ê°€ í•™ìŠµì‹œí‚¤ë ¤ëŠ” ë°ì´í„°ì™€ ìƒì„±ìê°€ ë§Œë“  Fake ë°ì´í„°ë¥¼ êµ¬ë¶„ìì— ëª¨ë‘ í•™ìŠµì‹œì¼œì„œ êµ¬ë¶„ì„ ë” ì˜ ì§“ê²Œ í•˜ëŠ” ë°©ì‹ìœ¼ë¡œì´ë£¨ì–´ì§„ë„¤íŠ¸ì›Œí¬ì´ë©°, ìƒì„±ìëŠ”ëœë¤ë…¸ì´ì¦ˆë¥¼í•™ìŠµë°ì´í„°ì™€ìœ ì‚¬í•œíŒ¨í„´ìœ¼ë¡œë§Œë“¤ì–´ì£¼ëŠ”ë„¤íŠ¸ì›Œí¬êµ¬ì¡°ë¥¼ê°€ì§„ë‹¤. ì´ë¥¼ í…ŒìŠ¤íŠ¸í•˜ê¸° ìœ„í•´ì„œ í™•ì¸í•  ì§€í‘œëŠ” ë°”ì´ë„ˆë¦¬í¬ë¡œìŠ¤ì—”íŠ¸ë¡œí”¼ì™€ ì†ì‹¤í•¨ìˆ˜ì˜ ê°’ì´ êµ¬ë¶„ìê°€ ì¶œë ¥í•œ í™•ë¥ ê°’ì´ ì •ë‹µì— ê°€ê¹Œìš°ë©´ ë‚®ì•„ì§€ê¸° ë•Œë¬¸ì— ì´ê²ƒì´ ëª¨ë¸ í•™ìŠµì˜ ëª©í‘œê°€ ëœë‹¤. êµ¬ë¶„ìì˜ ì†ì‹¤í•¨ìˆ˜ëŠ” ê·¸ë˜ì„œ ë‘ê°€ì§€ í•©ì¸ë° í•˜ë‚˜ëŠ” ê°€ì§œì´ë¯¸ì§€ë¥¼ ì…ë ¥í–ˆì„ ë•Œì˜ ì¶œë ¥ê°’ê³¼ 1ì˜ì°¨ì´, ê·¸ë¦¬ê³  ê°€ì§œ ì´ë¯¸ì§€ë¥¼ ì…ë ¥í–ˆì„ ë•Œì˜ ì¶œë ¥ê°’ê³¼ 0ì˜ ì°¨ì´. ì´ ë‘˜ì˜ í•©ì´ êµ¬ë¶„ìì˜ ì†ì‹¤í•¨ìˆ˜ì´ë©° ì´ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ êµ¬ë¶„ìì˜ íŒŒë¼ë¯¸í„°ê°€ ì—…ë°ì´íŠ¸ ëœë‹¤. ì´ ì—…ë°ì´íŠ¸ëŠ” ìµœì í™” í•¨ìˆ˜ë¥¼ í†µí•´ ì´ë£¨ì–´ì§„ë‹¤. ë°ì´í„°ê°€ ì–´ë–¤ ìœ í˜•ì¸ì§€ì— ë”°ë¼ì„œ fake dataë¥¼ ì–´ë–¤ ê²ƒì„ ì‚¬ìš©í• ì§€ë„ ë‹¬ë¼ì§€ëŠ”ë° ì´ ë…¼ë¬¸ì—ì„œëŠ” fake dataë¥¼ ë°ì´í„° ë¶„í¬ë¥¼ í†µí•´ì„œ ìƒ˜í”Œì„ ì‚¬ìš©í•˜ë©° ì´ëŠ” ëŒ€ì²´ì ìœ¼ë¡œ ì°¨ì›ì´ ë‚®ì€ ëœë¤ë…¸ì´ì¦ˆì´ë‹¤. ìµœì•…ì˜ ê²½ìš°(max)ë¥¼ ê°€ì •í–ˆì„ ë•Œ ì†ì‹¤ì„ ìµœì†Œí™”(min)í•˜ëŠ” ê²ƒì„ minimaxê²Œì„ì´ë¼ê³  í•˜ë©° ì´ê²ƒì´ GAN ê¸°ì €ì— ê¹”ë ¤ìˆëŠ” ì´ë¡ ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. GANì˜ ê°€ì¥ í°ë¬¸ì œëŠ” í•™ìŠµí™˜ê²½ì´ ë§¤ìš° ë¶ˆì•ˆì •í•˜ë‹¤ëŠ” ê²ƒì´ë‹¤. ìƒì„±ìì™€ êµ¬ë¶„ì ë‘˜ ì¤‘ì— í•˜ë‚˜ê°€ ì‹¤ë ¥ì´ ì›”ë“±ì´ ì¢‹ì•„ì§„ë‹¤ë©´ ë°¸ëŸ°ìŠ¤ê°€ ë¶•ê´´ë˜ê³  ëª¨ë¸ì´ ì •í™•íˆ í•™ìŠµë˜ì§€ ì•Šê³  í•™ìŠµì´ ì™„ë£Œëœ í›„ì—ë„ mode dropping ì´ ìƒê¸°ëŠ”ë° ì´ëŠ” êµ¬ë¶„ìê°€ ê·¸ ì—­í• ì„ ì¶©ë¶„íˆ í•˜ì§€ ëª»í•´ ëª¨ë¸ì´ ìµœì ì ê¹Œì§€ í•™ìŠµì´ ì•ˆ ëœ ê²ƒì´ë‹¤. ì´ëŸ° ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì´í›„ ë…¼ë¬¸ì—ì„œ ë‹¤ì–‘í•œ í•´ê²° ë°©ë²•ì´ ì œì‹œëœë‹¤. Link: Generative Adversarial Nets","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Generative Model","slug":"Paper/Generative-Model","permalink":"https://jmj3047.github.io/categories/Paper/Generative-Model/"}],"tags":[{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"},{"name":"GAN","slug":"GAN","permalink":"https://jmj3047.github.io/tags/GAN/"},{"name":"Generative Model","slug":"Generative-Model","permalink":"https://jmj3047.github.io/tags/Generative-Model/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Generative Model","slug":"Paper/Generative-Model","permalink":"https://jmj3047.github.io/categories/Paper/Generative-Model/"}]},{"title":"Hexo Hueman Tutorial","slug":"Hexo_Hueman_tutorial","date":"2022-04-20T15:00:00.000Z","updated":"2023-02-28T07:50:13.607Z","comments":true,"path":"2022/04/21/Hexo_Hueman_tutorial/","link":"","permalink":"https://jmj3047.github.io/2022/04/21/Hexo_Hueman_tutorial/","excerpt":"","text":"1.Starting Hexo Blog1234567891011121314151617181920212223242526username@LAPTOP-D1EUIRLS MINGW64 ~/Desktop$ hexo init your_blog_folderusername@LAPTOP-D1EUIRLS MINGW64 ~/Desktop$ cd your_blog_folder/username@LAPTOP-D1EUIRLS MINGW64 ~/Desktop/your_blog_folder$ echo &quot;# your_blog_folder&quot; &gt;&gt; README.mdgit initgit add README.mdgit commit -m &quot;first commit&quot;git branch -M mastergit remote add origin https://github.com/your_id/your_blog_folder.gitgit push -u origin masterusername@LAPTOP-D1EUIRLS MINGW64 ~/Desktop/your_blog_folder (master)$ git add .username@LAPTOP-D1EUIRLS MINGW64 ~/Desktop/your_blog_folder (master)$ git commit -m &quot;updated&quot;username@LAPTOP-D1EUIRLS MINGW64 ~/Desktop/your_blog_folder (master)$ git pushusername@LAPTOP-D1EUIRLS MINGW64 ~/Desktop/your_blog_folder (master)$ code . 2.Applying Hueman Theme3.Basic Hexo Tutorial4.Hexo Tag Plugins5.Add Math Formula(without changing from Notion) Creat File name mathjax.ejs on themes/hueman/layout folder 123456789101112MathJax.Hub.Config(&#123; jax: [&quot;input/TeX&quot;, &quot;output/HTML-CSS&quot;], # mathjax tex2jax: &#123; inlineMath: [ [&#x27;$&#x27;, &#x27;$&#x27;] ], displayMath: [ [&#x27;$$&#x27;, &#x27;$$&#x27;]], processEscapes: true, skipTags: [&#x27;script&#x27;, &#x27;noscript&#x27;, &#x27;style&#x27;, &#x27;textarea&#x27;, &#x27;pre&#x27;, &#x27;code&#x27;] &#125;, messageStyle: &quot;none&quot;, &quot;HTML-CSS&quot;: &#123; preferredFont: &quot;TeX&quot;, availableFonts: [&quot;STIX&quot;,&quot;TeX&quot;] &#125;&#125;); Check #Plugins in themes/hueman/_config.yml file and change mathjax: false to true Add mathjax:true at the header when you post Reference: Math Formula 6.Font Change7.Deleting Posts8.Error in Hueman ThemeTo Be Continued..","categories":[{"name":"Setting","slug":"Setting","permalink":"https://jmj3047.github.io/categories/Setting/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://jmj3047.github.io/tags/Hexo/"},{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Hueman","slug":"Hueman","permalink":"https://jmj3047.github.io/tags/Hueman/"}],"keywords":[{"name":"Setting","slug":"Setting","permalink":"https://jmj3047.github.io/categories/Setting/"}]},{"title":"ImageNet Classification with Deep Convolutional Neural Networks","slug":"ImageNet_Classification","date":"2022-04-20T15:00:00.000Z","updated":"2022-10-15T08:04:25.032Z","comments":true,"path":"2022/04/21/ImageNet_Classification/","link":"","permalink":"https://jmj3047.github.io/2022/04/21/ImageNet_Classification/","excerpt":"","text":"Journal&#x2F;Conference: NIPSYear(published year): 2012Author: Alex Krizhevsky, Ilya Sutskever, Geoffrey E. HintonSubject: AlexNet, Computer Vision ImageNet Classification with Deep Convolutional Neural Networks Summary ê¸°ì¡´ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ ì œì¹˜ê³  ë”¥ëŸ¬ë‹ ëª¨ë¸ì´ ë” ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì¼ ìˆ˜ ìˆìŒì„ ì¦ëª…í•œ ìµœì´ˆì˜ ëª¨ë¸ ReLU í™œì„±í™” í•¨ìˆ˜ì™€ Dropout ì˜ ìœ ìš©í•¨, Data Augmentation ê¸°ë²•ì„ ì œì‹œ 2012ë…„ ImageNet ëŒ€íšŒ ILSVRC ì—ì„œ ìš°ìŠ¹ì„ ì°¨ì§€í•œ ëª¨ë¸ IntroductionAlexNet ì´ì „ì˜ ê°ì²´ ì¸ì‹ ëª¨ë¸ì€ ëŒ€ë¶€ë¶„ ê³ ì „ì ì¸ ML ëª¨ë¸ë¡œ, ìˆ˜ë§Œê°œ ì •ë„ì˜ ì‘ì€ ë°ì´í„°ì…‹(NORB, Caltech-101&#x2F;256, CIFAR-10&#x2F;100)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´í›„, ìˆ˜ì‹­ë§Œ ê°œì˜ ì™„ì „ ë¶„í•  ëœ ì´ë¯¸ì§€ë¡œ êµ¬ì„±ëœ LabelMe ì™€ 1500 ë§Œ ê°œ ì´ìƒì˜ ê³ í•´ìƒë„ ì´ë¯¸ì§€ë¡œ êµ¬ì„±ëœ ImageNet ì´ ë“±ì¥í•©ë‹ˆë‹¤. ì´ëŸ° ë°ì´í„°ì…‹ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ì„œëŠ” ë†’ì€ í•™ìŠµ ì—­ëŸ‰ì„ ê°€ì§„ ëª¨ë¸ì´ í•„ìš”í•©ë‹ˆë‹¤. ë˜í•œ, í•™ìŠµê³¼ì •ì— ì‚¬ìš©ë˜ì§€ ì•Šì€ ìˆ˜ë§ì€ ë°ì´í„°ì— ëŒ€í•´ì„œë„ ì¶”ë¡ ì„ í•  ìˆ˜ ìˆëŠ” ë°©ëŒ€í•œ ì‚¬ì „ ì§€ì‹ì„ ë‹´ì•„ë‚´ì•¼í•©ë‹ˆë‹¤. ì´ì— ë…¼ë¬¸ì€ ì»¨ë³¼ë£¨ì…˜ ì‹ ê²½ë§(CNN) ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” AlexNet ì„ ì œì‹œí•©ë‹ˆë‹¤. CNN ì€ FFNN(feed-forward NN)ì— ë¹„í•´ ë” ì ì€ ë§¤ê°œ ë³€ìˆ˜ë¥¼ ê°€ì§€ë¯€ë¡œ í›ˆë ¨ì´ ìš©ì´í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ILSVRC-2010, ILSVRC-2012 ëŒ€íšŒì— ì‚¬ìš©ëœ ImageNet subsetì—ì„œ ìµœê³ ì˜ ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, ë„¤íŠ¸ì›Œí¬ ì„±ëŠ¥ í–¥ìƒê³¼ í›ˆë ¨ì‹œê°„ ê°ì†Œë¥¼ ìœ„í•œ ì—¬ëŸ¬ê°€ì§€ ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤. ALexNet ì€ 2ê°œì˜ GTX 580 3GB GPUì—ì„œ 5-6 ì¼ë™ì•ˆ í›ˆë ¨ì„ ìˆ˜í–‰í•˜ì˜€ìŠµë‹ˆë‹¤. The Datasetì§€ê¸ˆì€ ëŒ€ë¶€ë¶„ì˜ ë”¥ëŸ¬ë‹ ëª¨ë¸ì—ì„œ ê¸°ë³¸ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ImageNet ì— ëŒ€í•œ ì†Œê°œì…ë‹ˆë‹¤. 22,000 ê°œ ë²”ì£¼ë¡œ êµ¬ë¶„ë˜ëŠ” 1,500 ë§Œê°œ ê³ í•´ìƒë„ ì´ë¯¸ì§€ ì›¹ì—ì„œ ìˆ˜ì§‘í•œ ì´ë¯¸ì§€ë¥¼ Amazon ì˜ Mechanical Turk í¬ë¼ìš°ë“œ ì†Œì‹± ë„êµ¬ë¡œ ë¼ë²¨ë§ 2010 ë…„ë¶€í„° Pascal Visual Object Challengeì˜ ì¼í™˜ìœ¼ë¡œ ImageNet ëŒ€ê·œëª¨ ì‹œê° ì¸ì‹ ë„ì „ (ILSVRC)ì´ë¼ëŠ” ì—°ë¡€ ëŒ€íšŒê°€ ì—´ë ¸ìŠµë‹ˆë‹¤. ILSVRCëŠ” 1000 ê°œì˜ ì¹´í…Œê³ ë¦¬ ê°ê°ì— ì•½ 1000 ê°œì˜ ì´ë¯¸ì§€ê°€ìˆëŠ” ImageNetì˜ í•˜ìœ„ ì§‘í•©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ëŠ” ì•½ 120 ë§Œ ê°œì˜ í›ˆë ¨ ì´ë¯¸ì§€, 50,000 ê°œì˜ ê²€ì¦ ì´ë¯¸ì§€, 150,000 ê°œì˜ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. ëŒ€ë¶€ë¶„ì˜ ì‹¤í—˜ ê²°ê³¼ëŠ” í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ê°€ ê³µê°œëœ ILSVRC-2010 ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ë³„ë„ë¡œ, AlexNet ì´ ì°¸ê°€í–ˆë˜ ILSVRC-2012 ì‹¤í—˜ ê²°ê³¼ ë˜í•œ ì œì‹œí•©ë‹ˆë‹¤. ImageNet ë°ì´í„°ì…‹ ì„±ëŠ¥ ì§€í‘œë¡œëŠ” Top-1&#x2F;Top-5 Accuracy ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ê°€ë³€ í•´ìƒë„ë¡œ êµ¬ì„±ëœ ImageNet ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ 256 Ã— 256ì˜ ê³ ì • í•´ìƒë„ë¡œ ë‹¤ìš´ ìƒ˜í”Œë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì§ì‚¬ê°í˜• ì´ë¯¸ì§€ëŠ” scaling í›„ ì¤‘ì•™ 256x256 íŒ¨ì¹˜ë¥¼ ì˜ë¼ëƒ…ë‹ˆë‹¤. ì´ì™¸ì˜ ì „ì²˜ë¦¬ëŠ” ìˆ˜í–‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. The ArchitectureReLU Nonlinearityë…¼ë¬¸ ë°œí‘œ ë‹¹ì‹œ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ëœ perceptron ì˜ activation í•¨ìˆ˜ëŠ” tanh í˜¹ì€ sigmoid ì…ë‹ˆë‹¤. ì´ë“¤ì€ ì¶œë ¥ê°’ì´ ë¬´í•œëŒ€ë¡œ ë°œì‚°í•˜ì§€ ì•Šê³  íŠ¹ì •í•œ ì˜ì—­ìœ¼ë¡œ ì œí•œë˜ëŠ” saturating í•¨ìˆ˜ì…ë‹ˆë‹¤. ë°˜ë©´ ReLU(Recitified Linear Unit) activation ì€ ì¶œë ¥ê°’ì´ 0 ì—ì„œ ë¬´í•œëŒ€ê¹Œì§€ ë°œì‚°í•  ìˆ˜ ìˆëŠ” non-saturating í•¨ìˆ˜ì…ë‹ˆë‹¤. ë…¼ë¬¸ì€ 4 layer CNN ìœ¼ë¡œ CIFAR-10 ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµí•˜ì˜€ì„ ë•Œ, ReLU ê°€ 6ë°° ë¹ ë¥¸ í•™ìŠµ ì†ë„ë¥¼ ë³´ì—¬ì£¼ì—ˆìŒì„ ì œì‹œí•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´, non-saturating í•œ í•¨ìˆ˜ê°€ gradient ë¥¼ ë” ë¹ ë¥´ê²Œ update í•  ìˆ˜ ìˆìŒì„ ì œì‹œí•©ë‹ˆë‹¤. Training on Multiple GPUsGPU ë©”ëª¨ë¦¬ ì œí•œê³¼ ëŠë¦° í•™ìŠµ ì†ë„ë¥¼ ê°œì„ í•  ìˆ˜ ìˆëŠ” ë³‘ë ¬í•™ìŠµ ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤. ê¸°ë³¸ ê³¨ìëŠ” ë„¤íŠ¸ì›Œí¬ë¥¼ ë¶„í• (ì»¤ë„, ë‰´ëŸ° ë“±)í•˜ì—¬ ì„œë¡œ ë‹¤ë¥¸ GPU ì—ì„œ ë³‘ë ¬ì ìœ¼ë¡œ ì—°ì‚°ì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ ë•Œ, ë©”ëª¨ë¦¬ì˜ í•œê³„ ë° ë³‘ëª© í˜„ìƒì„ ê³ ë ¤í•˜ì—¬, íŠ¹ì •í•œ ë ˆì´ì–´ì—ì„œë§Œ ì—°ì‚° ê²°ê³¼ë¥¼ êµí™˜í•©ë‹ˆë‹¤. ë…¼ë¬¸ì€ ì´ë¥¼ í†µí•´ half-size kernel ì„ ì‚¬ìš©í•œ ë‹¨ì¼ GPU ëª¨ë¸ë³´ë‹¤ Top-1&#x2F;Top-5 accuracy ë¥¼ 1.7% &#x2F; 1.2% ê°ì†Œì‹œì¼°ìŒì„ ì œì‹œí•©ë‹ˆë‹¤. Local Response NormalizationReLU í™œì„± í•¨ìˆ˜ëŠ” ì…ë ¥ì„ normalization í•˜ì§€ ì•Šì•„ë„ saturation ì´ ë°œìƒí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ positive value ë¥¼ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ëŠ” ReLU í•¨ìˆ˜ì˜ íŠ¹ì„±ìœ¼ë¡œ ì¸í•´ CNN ì˜ ì¼ë¶€ êµ¬ì—­ì—ì„œ ê°•í•œ ì‹ í˜¸ê°€ ìƒì„±ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ì— ë…¼ë¬¸ì€ ì•„ë˜ì™€ ê°™ì€ local response normalization ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤. ìš”ì•½í•˜ë©´, CNN ì—ì„œ ì¸ì ‘í•œ í•„í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ normalization ì„ ì§„í–‰í•œ ê²ƒìœ¼ë¡œ, ë…¼ë¬¸ì—ì„œëŠ” Top-1&#x2F;Top-5 Accuracy ë¥¼ 1.4%, 1.2% ê°œì„ í•  ìˆ˜ ìˆì—ˆìŒì„ ì œì‹œí•©ë‹ˆë‹¤. ë˜í•œ, CIFAR-10 ìœ¼ë¡œ í•™ìŠµì„ ìˆ˜í–‰í•˜ì˜€ì„ ë•Œë„ 2% ì˜ ì˜¤ì°¨ìœ¨ ê°ì†Œë¥¼ ë³´ì˜€ìŒì„ ì œì‹œí•©ë‹ˆë‹¤.(ë…¼ë¬¸ ë‹¹ì‹œì—ëŠ” Batch Normalization ì´ ì†Œê°œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.) Overlapping PoolingCNNì˜ í’€ë§ ë ˆì´ì–´ëŠ” ê°™ì€ ì±„ë„ì— ì¡´ì¬í•˜ëŠ” ì¸ì ‘í•œ ë‰´ëŸ°ì˜ ì¶œë ¥ì„ ìš”ì•½í•´ì¤ë‹ˆë‹¤. ë…¼ë¬¸ ì´ì „ì—ëŠ” pooling ì„ ìˆ˜í–‰í•˜ëŠ” ì˜ì—­ì´ ê²¹ì¹˜ì§€ ì•Šë„ë¡ êµ¬ì„±í•˜ì—¬ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ì—ˆìŠµë‹ˆë‹¤. ë…¼ë¬¸ì€ í’€ë§ì„ ìˆ˜í–‰í•˜ëŠ” ì˜ì—­ì´ ì´ë™í•˜ëŠ” ê±°ë¦¬ë¥¼ ì¡°ì ˆí•˜ì—¬ í’€ë§ ì˜ì—­ì´ ê²¹ì¹˜ë„ë¡ í•œ ê²°ê³¼, Top-1&#x2F;Top-5 Accuracy ë¥¼ 0.4 %&#x2F;0.3 % ê°ì†Œí–ˆë‹¤ê³  í•©ë‹ˆë‹¤. ë˜í•œ ì´ë¥¼ í†µí•´ ëª¨ë¸ì˜ ê³¼ì í•© ê°€ëŠ¥ì„±ì„ ì¤„ì¼ ìˆ˜ ìˆì—ˆë‹¤ê³  í•©ë‹ˆë‹¤. Overall Architecture AlexNet ì˜ ì „ì²´ êµ¬ì¡°ë„ ì…ë‹ˆë‹¤. 2GPU ë¡œ ë³‘ë ¬í•™ìŠµì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ ë‘ ê°ˆë˜ë¡œ ë‚˜ë‰˜ì–´ í‘œí˜„ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ 5ê°œì˜ convolution layer ì™€ 3ê°œì˜ max pooling layer, 3ê°œì˜ dense layer ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, í•„ìš”í•œ ê²½ìš°ì—ë§Œ GPU ì—°ì‚° ê²°ê³¼ë¥¼ ê³µìœ í•©ë‹ˆë‹¤. ë˜í•œ convolution&#x2F;dense layer ì˜ í™œì„±í•¨ìˆ˜ëŠ” ReLU ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. Input : 224 x 224 x 3 &#x3D; 150,528 Convolution 1 : 11x11 kernel, 4 stride : 54x54x96 Max pooling 1 : 3x3 kernel, 2 stride : 26x26x96 Convolution 2 : 5x5 kernel, 2 padding : 26x26x256 Max pooling 2 : 3x3 kernel, 2 stride : 12x12x256 Convolution 3 : 3x3 kernel, 1 padding : 12x12x384 Convolution 4 : 3x3 kernel, 1 padding : 12x12x384 Convolution 5 : 3x3 kernel, 1 padding : 12x12x384 Max pooling 3 : 3x3 kernel, 2 stride : 5x5x256 Dense 1 : 4096 Dense 2 : 4096 Dense 3 : 1000 Reducing Overfitting6ì²œë§Œê°œì˜ íŒŒë¼ë¯¸í„°ë¡œ êµ¬ì„±ëœ ëª¨ë¸ì˜ ê³¼ì í•©ì„ ë§‰ê¸° ìœ„í•´ ì‚¬ìš©í•œ ë°©ë²•ì„ ì†Œê°œí•©ë‹ˆë‹¤. Data Augmentationí•™ìŠµ ë°ì´í„°ë¥¼ ì¸ìœ„ì ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ í›ˆë ¨ ë°ì´í„°ë¥¼ ì¦ê°€ì‹œí‚¤ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ë³€í™˜ëœ ì´ë¯¸ì§€ë¥¼ ì €ì¥í•˜ì§€ ì•Šê³  GPU í•™ìŠµì‹œì— CPUì—ì„œ ê³„ì‚°í•˜ë„ë¡ í•˜ì—¬, ì¶”ê°€ì ì¸ ê³„ì‚° ë¹„ìš©ì„ ì†Œëª¨í•˜ì§€ ì•Šì•˜ë‹¤ê³  í•©ë‹ˆë‹¤. ì£¼ìš” ë°©ë²•ì€ ë‘ê°€ì§€ë¡œ ìš”ì•½ë©ë‹ˆë‹¤. 256 Ã— 256 ì´ë¯¸ì§€ì—ì„œ 224 Ã— 224 íŒ¨ì¹˜ë¥¼ ì¶”ì¶œí•˜ê³ , ìˆ˜í‰ ë°©í–¥ìœ¼ë¡œ ë’¤ì§šê¸° ê¸°ì¡´ ë°ì´í„° ì…‹ì˜ 2048 ë°° í™•ì¥ ê°€ëŠ¥ ì‹¤ì œ : 5 ê°œì˜ 224 Ã— 224 íŒ¨ì¹˜ (4 ê°œì˜ ì½”ë„ˆ íŒ¨ì¹˜ ë° ì¤‘ì•™ íŒ¨ì¹˜)ì™€ ìˆ˜í‰ ë°˜ì‚¬ë¥¼ ìˆ˜í–‰í•œ 10ê°œì˜ íŒ¨ì¹˜ ì‚¬ìš© RGB ì±„ë„ ê°•ë„ ì¡°ì • í•™ìŠµ ë°ì´í„°ì…‹ì˜ í”½ì…€ê°’ìœ¼ë¡œ PCA ë¥¼ ìˆ˜í–‰ PCA eigenvector ì— N(0,0.1) ì¸ ì •ê·œë¶„í¬ì— ì¶”ì¶œí•œ ëœë¤ê°’ì„ ê³±í•´ ìƒ‰ìƒì„ ì¡°ì • Top-1 ì˜¤ì°¨ìœ¨ì„ 1% ê°ì†Œí•  ìˆ˜ ìˆì—ˆìŒ DropoutDense Layer ì˜ Output ì— Dropout rate 0.5 ë¥¼ ì‚¬ìš©í•œ Dropout layer ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤. í•™ìŠµì— í•„ìš”í•œ Epoch ë¥¼ 2ë°° ì •ë„ ëŠ˜ë ¸ìœ¼ë‚˜, ê³¼ì í•©ì„ ì„±ê³µì ìœ¼ë¡œ ë°©ì§€í–ˆìŒì„ ì œì‹œí•©ë‹ˆë‹¤. Details of learningëª¨ë¸ í•™ìŠµì˜ ì„¸ë¶€ë‚´ìš©ì…ë‹ˆë‹¤. Batch size : 128 SGD (momentum 0.9, weight decay 0.0005) weight decay ê°€ ëª¨ë¸ì„ ì •ê·œí™” í•  ë¿ë§Œ ì•„ë‹ˆë¼ ì§ì ‘ì ìœ¼ë¡œ ëª¨ë¸ì˜ í•™ìŠµ ì˜¤ì°¨ë¥¼ ì¤„ì˜€ìŒì„ ì œì‹œí•©ë‹ˆë‹¤. ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ê³¼ì •ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. ê°€ì¤‘ì¹˜ëŠ” í‰ê· ì´ 0, í‘œì¤€ í¸ì°¨ê°€ 0.01ì¸ ì •ê·œ ë¶„í¬ë¥¼ ë”°ë¥´ë„ë¡ ì´ˆê¸°í™”í•©ë‹ˆë‹¤. 2&#x2F;4&#x2F;5 ë²ˆì§¸ convolution ê³¼ dense layerì˜ bias ëŠ” 1ë¡œ ì´ˆê¸°í™”í•˜ì—¬, í•™ìŠµì„ ê°€ì†í•  ìˆ˜ ìˆì—ˆìŒì„ ì œì‹œí•©ë‹ˆë‹¤. í•™ìŠµë¥ ì€ ëª¨ë“  layer ì— ëŒ€í•´ì„œ ë™ì¼í•˜ë˜, í›ˆë ¨ì„ ìˆ˜í–‰í•˜ë©´ì„œ ë©”ë‰´ì–¼í•˜ê²Œ ì¡°ì •í•©ë‹ˆë‹¤. í•™ìŠµë¥  0.01 ì—ì„œ ì‹œì‘í•˜ì—¬, í•™ìŠµì´ ê°œì„ ë˜ì§€ ì•Šì„ ë•Œ í•™ìŠµë¥ ì„ 10ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ë°©ì‹ìœ¼ë¡œ ìˆ˜í–‰í•©ë‹ˆë‹¤. RESULT ILSVRC-2010 ë°ì´í„°ì— ëŒ€í•´ì„œ ê¸°ì¡´ ëª¨ë¸ì˜ ê²°ê³¼ë¥¼ ì••ë„ì ìœ¼ë¡œ ìƒíšŒí•˜ëŠ” ê²°ê³¼ë¥¼ ì œì‹œí•˜ì˜€ìŠµë‹ˆë‹¤. AlexNet ì´ ì§ì ‘ ì°¸ê°€í–ˆë˜ ILSVRC-2012 ì—ì„œë„ ë‹¤ë¥¸ ìµœê³  ì„±ëŠ¥ì˜ ëª¨ë¸ì— ë¹„í•´ ì••ë„ì ì¸ ê²°ê³¼ë¥¼ ë³´ì˜€ìŒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, CNN Layer ê°¯ìˆ˜ë¥¼ ì¶”ê°€í•  ë•Œë§ˆë‹¤ ì„±ëŠ¥ì´ ìƒìŠ¹í•¨ì„ ì œì‹œí•©ë‹ˆë‹¤. Qualitative Evaluations CNN kernel ì„ ì‹œê°í™”í•œ ê·¸ë¦¼ì„ ì œì‹œí•˜ë©´ì„œ, ê° ì»¤ë„ì´ ì´ë¯¸ì§€ì˜ ë‹¤ì–‘í•œ Feature ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì¶”ì¶œí•´ëƒˆìŒì„ ì œì‹œí•©ë‹ˆë‹¤. AlexNet ì€ ì¤‘ì•™ì„ ë²—ì–´ë‚˜ëŠ” ë°ì´í„°ë„ íš¨ê³¼ì ìœ¼ë¡œ ë¶„ë¥˜í•´ëƒˆìŠµë‹ˆë‹¤. ë˜í•œ, Top-5 ì˜ˆì¸¡ì´ ëŒ€ë¶€ë¶„ ìœ ì‚¬í•œ ë²”ì£¼ì¸ ê²ƒìœ¼ë¡œ ë³´ì•„ í•©ë¦¬ì ì¸ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ê³  ìˆìŒì„ ì œì‹œí•©ë‹ˆë‹¤. ë˜í•œ, ìì„¸ê°€ ì„œë¡œ ë‹¤ë¥¸ ì½”ë¼ë¦¬ì˜ ì‚¬ë¡€ì™€ ê°™ì´, Pixel ì°¨ì›ì—ì„œ ì™„ì „íˆ ë‹¤ë¥¸ ë°ì´í„°ì„ì—ë„ ìœ ì‚¬í•œ ë²”ì£¼ë¡œ ë¶„ë¥˜í•  ìˆ˜ ìˆëŠ” ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. Discussionâ€œê¹Šì€â€ CNN ì´ íš¨ê³¼ì ìœ¼ë¡œ ì‘ë™í•˜ì˜€ìŒì„ ì œì‹œí•©ë‹ˆë‹¤. Convolution layer ë¥¼ ì œê±°í•  ë•Œë§ˆë‹¤ Top-1 Accuracy ê°€ 2%ì”© ê°ì†Œí•˜ëŠ” ì ì— ë¯¸ë£¨ì–´, â€œê¹Šì´â€ì˜ ì¤‘ìš”ì„±ì„ ë‹¤ì‹œ í•œë²ˆ ì œì‹œí•©ë‹ˆë‹¤. Link: ImageNet Classification with Deep Convolutional Neural Networks","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Computer Vision","slug":"Paper/Computer-Vision","permalink":"https://jmj3047.github.io/categories/Paper/Computer-Vision/"}],"tags":[{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"},{"name":"Convolutional Neural Networks","slug":"Convolutional-Neural-Networks","permalink":"https://jmj3047.github.io/tags/Convolutional-Neural-Networks/"},{"name":"Image Classification","slug":"Image-Classification","permalink":"https://jmj3047.github.io/tags/Image-Classification/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Computer Vision","slug":"Paper/Computer-Vision","permalink":"https://jmj3047.github.io/categories/Paper/Computer-Vision/"}]},{"title":"K-Nearest Neighbor","slug":"KNN","date":"2022-04-20T15:00:00.000Z","updated":"2023-02-23T01:34:13.754Z","comments":true,"path":"2022/04/21/KNN/","link":"","permalink":"https://jmj3047.github.io/2022/04/21/KNN/","excerpt":"","text":"1. Classification ë¶„ë¥˜ë‚˜ ì˜ˆì¸¡ì„ ì§„í–‰í• ë•Œ ë‚˜ë‘ ê°€ì¥ ê°€ê¹Œìš´ ì´ì›ƒ kê°œë¥¼ ê³ ë ¤í•˜ê² ë‹¤. ë‚˜ë‘ ê°€ê¹Œìš´ ì´ì›ƒ í•œëª…ì´ ê²€ì •ìƒ‰ì´ë©´ ê²€ì •ìƒ‰ìœ¼ë¡œ íŒë‹¨ íŒŒë€ìƒ‰ì˜ ê°€ì¥ ê°€ê¹Œìš´ ì´ì›ƒì„ í™•ì¸í•´ë³¸ ê²°ê³¼ ê²€ì •ìƒ‰ ì´ë¯€ë¡œ íŒŒë€ìƒ‰ë„ ê²€ì •ìƒ‰ìœ¼ë¡œ ë¶„ë¥˜ë˜ì—ˆë‹¤ K&#x3D;3 ì¼ ê²½ìš° í˜•ê´‘ìƒ‰ ì¹œêµ¬ë¥¼ ë¶„ë¥˜í•œë‹¤ê³  í•˜ì˜€ì„ë•Œ ì´ì›ƒì¤‘ íŒŒë€ìƒ‰ì´ 2ê°œ ê²€ì •ìƒ‰ì´ í•œê°œì´ê¸° ë•Œë¬¸ì— íŒŒë€ìƒ‰ìœ¼ë¡œ ë¶„ë¥˜ëœë‹¤. ë¶„ë¥˜ë¥¼ ì›í•˜ëŠ” ê´€ì¸¡ì¹˜ì˜ ì£¼ë³€ Nê°œì˜ ë°ì´í„°(ê·¼ì ‘ ì´ì›ƒ)ì„ ê³¨ë¼ì„œ, ì£¼ë³€ëŒ€ì„¸ë¥¼ í™•ì¸ (ë‹¤ìˆ˜ê²°ì˜ ì›ì¹™ìœ¼ë¡œ) 2. Prediction ì¸ì ‘ Kê°œì˜ ë°ì´í„°ì˜ ìˆ˜ì¹˜ë¥¼ í™•ì¸í•´ì¤˜ì„œ ê·¸ ë°ì´í„°ì˜ í‰ê· ì„ ê²€ì€ì ì˜ ì˜ˆì¸¡ì¹˜ë¡œ ì„¤ì •í•´ì¤€ë‹¤. 3. How to find optimal k?kì˜ ê²°ì • kê°€ ë„ˆë¬´ í° ê²½ìš°, KNNëª¨ë¸ì´ ì§€ë‚˜ì¹˜ê²Œ ì¼ë°˜í™”ë¨ Kê°€ ë„ˆë¬´ ì‘ì€ ê²½ìš°,KNN ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ì˜ ë¶„ì‚°ì´ í¼ ì£¼ë¡œ ì´ê²ƒì €ê²ƒ í•´ë³´ê³  errorì´ ê°€ì¥ ì‘ì€ kë¥¼ ì„¤ì •í•˜ì—¬ì¤€ë‹¤. ê±°ë¦¬ ì²™ë„ì˜ ê²°ì • ìƒí™©ì— ë§ëŠ” ê±°ë¦¬ì²™ë„ë¥¼ ì‚¬ìš©í•˜ì—¬ì•¼ í•œë‹¤. ê±°ë¦¬ì²™ë„ì˜ ì¢…ë¥˜:Minkowski distance , Euclidean distance, Citi block distance, Mahalanobis distance, Correlation distance ë“± Reference: í•œêµ­ê³µí•™ëŒ€í•™êµ ê°•ì§€í›ˆêµìˆ˜ë‹˜ ê°•ì˜","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}],"tags":[{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}]},{"title":"Pyspark Tutorial(1)","slug":"Pyspark_Tutorial_1","date":"2022-04-20T15:00:00.000Z","updated":"2022-04-22T07:33:10.000Z","comments":true,"path":"2022/04/21/Pyspark_Tutorial_1/","link":"","permalink":"https://jmj3047.github.io/2022/04/21/Pyspark_Tutorial_1/","excerpt":"","text":"Reference: https://spark.apache.org/docs/latest/quick-start.html Get Started01.basic.py1234567891011121314# -*- coding: utf-8 -*-import pysparkprint(pyspark.__version__)from pyspark.sql import SparkSession#ìŠ¤íŒŒí¬ ì„¸ì…˜ ì´ˆê¸°í™” :spark sessionì´ í•˜ë‚˜ ë§Œë“¤ì–´ì§„ê²ƒspark = SparkSession.builder.master(&#x27;local[1]&#x27;).appName(&#x27;SampleTutorial&#x27;).getOrCreate()rdd = spark.sparkContext.parallelize([1,2,3,4,5])print(&quot;rdd Count&quot;, rdd.count())spark.stop() 02.rating.py1234567891011121314151617181920212223242526272829303132333435#SparkContext#RDDfrom pyspark import SparkConf, SparkContextimport collectionsprint(&quot;Hello&quot;)def main(): # MasterNode = local # MapReduce conf = SparkConf().setMaster(&quot;local&quot;).setAppName(&#x27;RatingHistogram&#x27;) sc = SparkContext(conf = conf) lines = sc.textFile(&quot;ml-100k/u.logs&quot;) #print(lines) ratings = lines.map(lambda x: x.split()[2]) #print(&quot;ratings:&quot;,ratings) #rddë¼ëŠ” ê°ì²´ê°€ ë§Œë“¤ì–´ì§„ê²ƒ result = ratings.countByValue() #print(&quot;result:&quot;,result) #ì •ë ¬í•˜ê¸° sortedResults = collections.OrderedDict(sorted(result.items())) for key, value in sortedResults.items(): print(&quot;%s %i&quot; % (key, value)) if __name__ == &quot;__main__&quot;: main() ##sparkë¥¼ ì“°ëŠ” ì´ìœ :ë¡œê·¸ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ ê·œì¹™ì„ ëŒ€ì…í•´ì„œ ì •ë ¬í•œë‹¤ìŒì— ì •í˜•ë°ì´í„°ë¡œ ì¹˜í™˜í•˜ê¸°ìœ„í•´#ì‹¤ì œë¡œ ì˜ë¯¸ìˆëŠ” ë¡œê·¸ë¼ë©´ ë¶„ì„ë„ ì˜ë¯¸ê°€ ìˆë‹¤#ë¶„ì„ê³¼ ë¡œê·¸ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” í™˜ê²½ì„ ì§€ì›í•´ì¤Œ#ê³¼ê±°ì—ëŠ” ë‘ê°œê°€ ë”°ë¡œ ìˆì—ˆìŒ 03.dataloading.py1234567891011121314151617181920212223242526272829303132333435363738394041424344#Spark SQL ì ìš©#Spark Sessionfrom pyspark.sql import SparkSession# #ìŠ¤íŒŒí¬ ì„¸ì…˜ ìƒì„±# my_spark = SparkSession.builder.getOrCreate()# print(my_spark)# #í…Œì´ë¸”ì„ í™•ì¸í•˜ëŠ” ì½”ë“œ# print(my_spark.catalog.listDatabases())# #show database# my_spark.sql(&quot;show databases&quot;).show()# #check current DB# my_spark.catalog.currentDatabase()# my_spark.stop()#loading csv filespark = SparkSession.builder.master(&#x27;local[1]&#x27;).appName(&quot;DBTutorial&quot;).getOrCreate()flights = spark.read.option(&#x27;header&#x27;,&#x27;true&#x27;).csv(&#x27;data/flight_small.csv&#x27;)#flights.show(4)#spark.catalog.currentDatabase()#flights í…Œì´ë¸”ì„ default DBì— ì¶”ê°€í•¨flights.createOrReplaceTempView(&#x27;flights&#x27;)#print(spark.catalog.listTables(&#x27;default&#x27;))#spark.sql(&#x27;show tables from default&#x27;).show()#ì¿¼ë¦¬ í†µí•´ì„œ ë°ì´í„° ì €ì¥query = &quot;FROM flights SELECT * LIMIT 10&quot;query2 = &quot;SELECT * FROM flights LIMIT 10&quot;# ìŠ¤íŒŒí¬ ì„¸ì…˜ í• ë‹¹flights10 = spark.sql(query2)#flights10.show()#spark ë°ì´í„° í”„ë ˆì„ì„ pandas data frameìœ¼ë¡œ ë³€í™˜import pandas as pdpd_flights10 = flights10.toPandas()print(pd_flights10.head()) 04.struct_type.py Reference: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.types.StructType.html 1234567891011121314151617181920212223242526272829303132from pyspark.sql import SparkSessionfrom pyspark.sql import functions as func #aliasfrom pyspark.sql.types import StructType, StructField, IntegerType, LongTypeprint(&quot;Hello&quot;)#ì„¸ì…˜ í• ë‹¹spark = SparkSession.builder.appName(&quot;PopularMovies&quot;).getOrCreate()#ìŠ¤í‚¤ë§ˆ ì‘ì„±(u.logs ë°ì´í„°)schema = StructType( [ StructField(&quot;userID&quot;,IntegerType(), True) , StructField(&quot;movieID&quot;,IntegerType(), True) , StructField(&quot;rating&quot;, IntegerType(), True) , StructField(&quot;timestamp&quot;, LongType(), True) ])print(&quot;Schema is done&quot;)#ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°movies_df = spark.read.option(&quot;sep&quot;,&quot;\\t&quot;).schema(schema).csv(&quot;ml-100k/u.logs&quot;)#ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì¸ê¸° ìˆëŠ” ì˜í™” ì •ë ¬#movieIDë¡œ ê·¸ë£¹ë°”ì´, count() ì§„í–‰, orderbytopMovieIds = movies_df.groupby(&quot;movieID&quot;).count().orderBy(func.desc(&#x27;count&#x27;))print(topMovieIds.show(10))#ì„¸ì…˜ ì¢…ë£Œspark.stop() 05.advance_structtype.py1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162from pyspark.sql import SparkSessionfrom pyspark.sql import functions as func #aliasfrom pyspark.sql.types import StructType, StructField, IntegerType, LongTypeimport codecsprint(&quot;Starting Session&quot;)def loadMovieNames(): #u.itemì—ì„œ ì˜í™” ì´ë¦„ ê°€ì ¸ì˜´ movieNames = &#123;&#125; with codecs.open(&quot;ml-100k/u.item&quot;,&quot;r&quot;, encoding=&quot;ISO-8859-1&quot;, errors =&quot;ignore&quot;) as f: for line in f: fields = line.split(&quot;|&quot;) movieNames[int(fields[0])] = fields[1] #ë°ì´í„° ì¶”ê°€í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ ë¬¸ë²• return movieNames #ì„¸ì…˜ í• ë‹¹spark = SparkSession.builder.appName(&quot;PopularMovies&quot;).getOrCreate()#íŒŒì´ì¬ ë”•ì…”ë„ˆë¦¬ ê°ì²´ë¥¼ spark ê°ì²´ë¡œ ë³€í™˜nameDict = spark.sparkContext.broadcast(loadMovieNames())#ìŠ¤í‚¤ë§ˆ ì‘ì„±(u.logs ë°ì´í„°)schema = StructType( [ StructField(&quot;userID&quot;,IntegerType(), True) , StructField(&quot;movieID&quot;,IntegerType(), True) , StructField(&quot;rating&quot;, IntegerType(), True) , StructField(&quot;timestamp&quot;, LongType(), True) ])print(&quot;Schema is done&quot;)#ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°movies_df = spark.read.option(&quot;sep&quot;,&quot;\\t&quot;).schema(schema).csv(&quot;ml-100k/u.logs&quot;)#ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì¸ê¸° ìˆëŠ” ì˜í™” ì •ë ¬í•  í•„ìš” ì—†ìŒ#movieIDë¡œ ê·¸ë£¹ë°”ì´, count() ì§„í–‰, orderby#topMovieIds = movies_df.groupby(&quot;movieID&quot;).count().orderBy(func.desc(&#x27;count&#x27;))topMovieIds = movies_df.groupby(&quot;movieID&quot;).count()# ë”•ì…”ë„ˆë¦¬ # key-value# í‚¤ê°’ì„ ì•Œë©´ valueìë™ìœ¼ë¡œ ê°€ì ¸ì˜´(movietitle)def lookupName(movieID): return nameDict.value[movieID]# ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜ ì‚¬ìš©í•  ë•Œ ì“°ëŠ” spark ë¬¸ë²•lookupNameUDF = func.udf(lookupName)# MovieTitleì„ ê¸°ì¡´ topMovieIds ë°ì´í„°ì— ì¶”ê°€#ì»¬ëŸ¼ ì¶”ê°€moviesWithNames = topMovieIds.withColumn(&quot;movietitle&quot;,lookupNameUDF(func.col(&quot;movieID&quot;)))#ì •ë ¬final_df = moviesWithNames.orderBy(func.desc(&quot;count&quot;))print(final_df.show(10))#ì„¸ì…˜ ì¢…ë£Œspark.stop()","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Pyspark","slug":"Python/Pyspark","permalink":"https://jmj3047.github.io/categories/Python/Pyspark/"}],"tags":[{"name":"pyspark","slug":"pyspark","permalink":"https://jmj3047.github.io/tags/pyspark/"}],"keywords":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Pyspark","slug":"Python/Pyspark","permalink":"https://jmj3047.github.io/categories/Python/Pyspark/"}]},{"title":"Pyspark Tutorial(2)","slug":"Pyspark_Tutorial_2","date":"2022-04-20T15:00:00.000Z","updated":"2022-04-22T07:33:14.000Z","comments":true,"path":"2022/04/21/Pyspark_Tutorial_2/","link":"","permalink":"https://jmj3047.github.io/2022/04/21/Pyspark_Tutorial_2/","excerpt":"","text":"Data cleansing01.pipeline.py123456789101112131415161718192021222324from pyspark.sql import SparkSessionfrom pyspark.sql import *from pyspark.sql import functions as F#Create Spark Sessionspark = SparkSession.builder.master(&quot;local[1]&quot;).appName(&quot;MLSampleTutorial&quot;).getOrCreate()#Load Datadf = spark.read.csv(&quot;data/AA_DFW_2015_Departures_Short.csv.gz&quot;, header = True)print(&quot;file loaded&quot;)print(df.show())#remove duration = 0df = df.filter(df[3] &gt; 0) #Actual elapsed time (Minutes) ì—¬ê¸° ì»¬ëŸ¼ ê°’ì´ 0ë³´ë‹¤ ì‘ì€ê±´ ë³´ì—¬ì£¼ì§€ ì•ŠìŒ# df.show()#ADD ID columndf = df.withColumn(&#x27;id&#x27;,F.monotonically_increasing_id()) #idê°’ì„ ìë™ìœ¼ë¡œ ë„£ì–´ì¤Œdf.write.csv(&quot;data/output.csv&quot;, mode = &#x27;overwrite&#x27;)spark.stop() 02.total_spent.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# #ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°# from pyspark import SparkConf, SparkContext# #ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜# #mainí•¨ìˆ˜# def main():# conf = SparkConf.setMaster(&quot;local&quot;).setAppName(&#x27;SpentbyCustomer&#x27;)# sc = SparkContext(conf= conf)# # íŒŒì´ì¬ ì½”ë“œ# # ì‹¤í–‰ì½”ë“œ ì‘ì„±# if __name__ == &quot;__main__&quot;:# main()########## ì´ê²Œ spark ê¸°ë³¸ ì„¸íŒ… ##########ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°from pyspark import SparkConf, SparkContext#ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜def extractCusPrice(line): fields = line.split(&quot;,&quot;) return(int(fields[0]), float(fields[2]))#mainí•¨ìˆ˜def main(): #ìŠ¤íŒŒí¬ ì„¤ì • conf = SparkConf().setMaster(&quot;local&quot;).setAppName(&#x27;SpentbyCustomer&#x27;) sc = SparkContext(conf= conf) #ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° input = sc.textFile(&#x27;data/customer-orders.csv&#x27;) #print(&#x27;is data?&#x27;) mappedInput = input.map(extractCusPrice) #íŠœí”Œ í˜•íƒœë¡œ ë‚˜ì˜´ totalByCustomer = mappedInput.reduceByKey(lambda x, y : x + y) #ì •ë ¬ flipped = totalByCustomer.map(lambda x: (x[1], x[0])) totalByCustomerSorted = flipped.sortByKey() results = totalByCustomerSorted.collect() for result in results: print(result) #íŒŒì´ì¬ ì½”ë“œ # ì‹¤í–‰ì½”ë“œ ì‘ì„±if __name__ == &quot;__main__&quot;: main() 03.friends_by_age.py123456789101112131415161718from pyspark import SparkConf, SparkContextconf = SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;FriendsByAge&quot;)sc = SparkContext(conf = conf)def parseLine(line): fields = line.split(&#x27;,&#x27;) age = int(fields[2]) numFriends = int(fields[3]) return (age, numFriends)lines = sc.textFile(&quot;data/fakefriends.csv&quot;)rdd = lines.map(parseLine)totalsByAge = rdd.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))averagesByAge = totalsByAge.mapValues(lambda x: x[0] / x[1])results = averagesByAge.collect()for result in results: print(result) 04.min_temp.py123456789101112131415161718192021222324252627282930313233#ì˜¨ë„ë¥¼ ì¸¡ì •í•˜ëŠ” í”„ë¡œê·¸ë¨ ë§Œë“¤ê¸°from dataclasses import fieldfrom pyspark import SparkConf, SparkContextconf = SparkConf().setMaster(&#x27;local&#x27;).setAppName(&#x27;MinTemperatures&#x27;) #ë§ˆìŠ¤í„° ë…¸ë“œì—ë‹¤ê°€ ì˜¬ë¦°ë‹¤sc = SparkContext(conf = conf)print(&quot;Start&quot;)def parseLine(line): fields = line.split(&quot;,&quot;) #ì‰¼í‘œë¡œ ë‹¤ ëŠì–´ì¤Œ -&gt; ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜ë¨ stationID = fields[0] entryType = fields[2] temperature = float(fields[3]) * 0.1 * (9.0/5.0) + 32.0 return (stationID, entryType, temperature)lines = sc.textFile(&#x27;data/1800.csv&#x27;)#print(lines)parseLines = lines.map(parseLine)#print(parseLine)minTemps = parseLines.filter(lambda x: &quot;TMIN&quot; in x[1])stationTemps = minTemps.map(lambda x: (x[0],x[2]))minTemps = stationTemps.map(lambda x, y: min(x, y))results = minTemps.collect()#print(results)for result in results: print(result[0]+ &quot;\\t&#123;:.2f&#125;F&quot;.format(result[1]))","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Pyspark","slug":"Python/Pyspark","permalink":"https://jmj3047.github.io/categories/Python/Pyspark/"}],"tags":[{"name":"pyspark","slug":"pyspark","permalink":"https://jmj3047.github.io/tags/pyspark/"}],"keywords":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Pyspark","slug":"Python/Pyspark","permalink":"https://jmj3047.github.io/categories/Python/Pyspark/"}]},{"title":"Pyspark Tutorial(3)","slug":"Pyspark_Tutorial_3","date":"2022-04-20T15:00:00.000Z","updated":"2022-04-22T07:33:20.000Z","comments":true,"path":"2022/04/21/Pyspark_Tutorial_3/","link":"","permalink":"https://jmj3047.github.io/2022/04/21/Pyspark_Tutorial_3/","excerpt":"","text":"Machine Learning01.regression.py1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950from pyspark.ml.regression import DecisionTreeRegressorfrom pyspark.sql import SparkSessionfrom pyspark.ml.feature import VectorAssemblerprint(&quot;Starting Session&quot;)#ì„¸ì…˜ í• ë‹¹spark = SparkSession.builder.appName(&quot;DecisionTree&quot;).getOrCreate()#ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°#StructType ê³¼ì • ìƒëµ ê°€ëŠ¥data = spark.read.option(&quot;header&quot;, &quot;true&quot;).option(&quot;inferSchema&quot;, &quot;true&quot;).csv(&quot;data/realestate.csv&quot;)#print(data.show())#ë°ì´í„° í”„ë ˆì„ì„ í–‰ë ¬ë¡œ ë³€í™˜assembler = VectorAssembler().setInputCols([&quot;HouseAge&quot;, &quot;DistanceToMRT&quot;,&quot;NumberConvenienceStores&quot;]).setOutputCol(&quot;features&quot;) #ë°ì´í„° ì»¬ëŸ¼ ê°’ ì•„ë¬´ê±°ë‚˜ ë„£ì–´ë„ ë¨#íƒ€ê²Ÿ ë°ì´í„° ì„¤ì •df = assembler.transform(data).select(&quot;PriceofUnitArea&quot;,&quot;features&quot;)#ë°ì´í„° ë¶„ë¦¬trainTest = df.randomSplit([0.5,0.5])trainingDF = trainTest[0]testDF = trainTest[1]#Decision Tree í´ë˜ìŠ¤ ì •ì˜dtr = DecisionTreeRegressor().setFeaturesCol(&quot;features&quot;).setLabelCol(&quot;PriceofUnitArea&quot;)#ëª¨ë¸ í•™ìŠµmodel = dtr.fit(trainingDF)#print(model)#ëª¨ë¸ ì˜ˆì¸¡fullPredictions = model.transform(testDF).cache()#ì˜ˆì¸¡ê°’ê³¼ labelí™•ì¸predictions = fullPredictions.select(&quot;prediction&quot;).rdd.map(lambda x: x[0])#ì‹¤ì œë°ì´í„°labels = fullPredictions.select(&quot;PriceofUnitArea&quot;).rdd.map(lambda x: x[0])#ì˜ˆì¸¡ê°’ê³¼ labelì„ zipìœ¼ë¡œ ë¬¶ì–´ì¤Œpreds_label = predictions.zip(labels).collect()for prediction in preds_label: print(prediction)#ì„¸ì…˜ ì¢…ë£Œspark.stop() 02.logistic_regression.py1234567891011121314151617181920212223from pyspark.sql import SparkSessionfrom pyspark.ml.classification import LogisticRegression #Important# ì„¸ì…˜ í• ë‹¹spark = SparkSession.builder.appName(&quot;AppName&quot;).getOrCreate()# load Datatraining = spark.read.format(&quot;libsvm&quot;).load(&quot;data/mllib/sample_libsvm_data.txt&quot;)print(&quot;Data loaded&quot;)# model# Scikit-Learn ë¬¸ë²•ê³¼ ë¹„ìŠ·mlr = LogisticRegression() # Importantmlr_model = mlr.fit(training) # Important# ë¡œì§€ìŠ¤í… íšŒê·€, ì„ í˜• ëª¨ë¸ .. ê³„ìˆ˜ì™€ ìƒìˆ˜ë¥¼ ë½‘ì•„ë‚¼ ìˆ˜ ìˆìŒprint(&quot;Coefficients :&quot; + str(mlr_model.coefficients))print(&quot;Intercept :&quot; + str(mlr_model.intercept))spark.stop() 03.pipeline.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657from tokenize import Tokenfrom pyspark.ml import Pipelinefrom pyspark.ml.classification import LogisticRegressionfrom pyspark.ml.feature import HashingTF, Tokenizerfrom pyspark.sql import SparkSession# ì„¸ì…˜ í• ë‹¹ spark = SparkSession.builder.appName(&quot;MLPipeline&quot;).getOrCreate()# ê°€ìƒì˜ ë°ì´í„° ë§Œë“¤ê¸°training = spark.createDataFrame([ (0, &quot;a b c d e spark&quot;, 1.0), (1, &quot;b d&quot;, 0.0), (2, &quot;spark f g h&quot;, 1.0), (3, &quot;hadoop mapreduce&quot;, 0.0)], [&quot;id&quot;, &quot;text&quot;, &quot;label&quot;])# Feature Engineering# 1. Prparation# step01. í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì–´ë¡œ ë¶„ë¦¬tokenizer = Tokenizer(inputCol=&#x27;text&#x27;, outputCol=&#x27;words&#x27;)# step02. ë³€í™˜ëœ í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ ë³€í™˜hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=&quot;features&quot;)# step03. ëª¨ë¸ì„ ê°€ì ¸ì˜´lr = LogisticRegression(maxIter=5, regParam=0.01)# 2. Starting pipeplinepipeline = Pipeline(stages=[tokenizer, hashingTF, lr])# 3. Model Trainingmodel = pipeline.fit(training)# 4. Prepare test documents, which are unlabeled (id, text) tuples.test = spark.createDataFrame([ (4, &quot;spark i j k&quot;), (5, &quot;l m n&quot;), (6, &quot;spark hadoop spark&quot;), (7, &quot;apache hadoop&quot;)], [&quot;id&quot;, &quot;text&quot;])# 5. Predictionprediction = model.transform(test)selected = prediction.select(&quot;id&quot;, &quot;text&quot;, &quot;probability&quot;, &quot;prediction&quot;)for row in selected.collect(): row_id, text, prob, prediction = row #íŠœí”Œ í˜•íƒœë¡œ ë°˜í™˜ print( # ë¬¸ìì—´ í¬ë§·íŒ… &quot;(%d, %s) -------&gt; probability=%s, prediction=%f&quot; % (row_id, text, str(prob), prediction) )# training.show()# ì„¸ì…˜ ì¢…ë£Œspark.stop() 04.randomforest.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960from cProfile import labelfrom pyspark.sql import SparkSession# ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬from pyspark.ml import Pipelinefrom pyspark.ml.classification import RandomForestClassifierfrom pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexerfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° spark = SparkSession.builder.appName(&quot;RandomForest&quot;).getOrCreate()data = spark.read.format(&quot;libsvm&quot;).load(&quot;data/mllib/sample_libsvm_data.txt&quot;)print(type(data))# Feature Engineering# label column labelIndexer = StringIndexer(inputCol=&#x27;label&#x27;, outputCol=&#x27;indexedLabel&#x27;).fit(data)# ë²”ì£¼í˜• ë°ì´í„° ì²´í¬, ì¸ë±ìŠ¤í™”featureIndexer = VectorIndexer(inputCol=&#x27;features&#x27;, outputCol=&#x27;IndexedFeatures&#x27;, maxCategories=4).fit(data)# ë°ì´í„° ë¶„ë¦¬(trainingData, testData) = data.randomSplit([0.7, 0.3])# ëª¨ë¸ rf = RandomForestClassifier(labelCol=&#x27;indexedLabel&#x27;, # ì¢…ì†ë³€ìˆ˜ featuresCol=&#x27;IndexedFeatures&#x27;, # ë…ë¦½ë³€ìˆ˜ numTrees=10)# outputCol=&#x27;indexedLabel&#x27; --&gt; original labelë¡œ ë³€í™˜labelConvereter = IndexToString(inputCol=&#x27;prediction&#x27;, outputCol=&#x27;predictedLabel&#x27;, labels=labelIndexer.labels)# íŒŒì´í”„ë¼ì¸ êµ¬ì¶•pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConvereter])# ëª¨ë¸ í•™ìŠµmodel = pipeline.fit(trainingData)# ëª¨ë¸ ì˜ˆì¸¡predictions = model.transform(testData)# í–‰ì— í‘œì‹œí•  ê²ƒ ì¶”ì¶œ predictions.select(&quot;predictedLabel&quot;, &#x27;label&#x27;, &#x27;features&#x27;).show(5)# ëª¨í˜• í‰ê°€evaluator = MulticlassClassificationEvaluator( labelCol=&quot;indexedLabel&quot;, predictionCol=&quot;prediction&quot;, metricName=&quot;accuracy&quot;)accuracy = evaluator.evaluate(predictions)print(&quot;Test Error = %f &quot; % (1.0 - accuracy))spark.stop()","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Pyspark","slug":"Python/Pyspark","permalink":"https://jmj3047.github.io/categories/Python/Pyspark/"}],"tags":[{"name":"pyspark","slug":"pyspark","permalink":"https://jmj3047.github.io/tags/pyspark/"}],"keywords":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Pyspark","slug":"Python/Pyspark","permalink":"https://jmj3047.github.io/categories/Python/Pyspark/"}]},{"title":"How to install PySpark","slug":"install_PySpark","date":"2022-04-18T15:00:00.000Z","updated":"2022-04-22T02:53:55.000Z","comments":true,"path":"2022/04/19/install_PySpark/","link":"","permalink":"https://jmj3047.github.io/2022/04/19/install_PySpark/","excerpt":"","text":"Preparation installing spark need python3 if you are first using python, install anaconda Installing JAVA Installing file: Java SE 8 Archive Downloads (JDK 8u211 and later) Need to login Oracle Run the download file as admin â†’ Click Next button â†’ Changing the path on file (Space between words like Program Files can be problem during installation) Changing Path Same changes to folders in the JAVA runtime environment folder (Click â€˜Changeâ€™ and modify) Create and save jre folder in the path right after the C dirve Installing Spark Installing site: https://spark.apache.org/downloads.html Download installation file After clicking Download Spark: [spark-3.2.0-bin-hadoop3.2.tgz](https://www.apache.org/dyn/closer.lua/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz), you can download it by clicking the HTTP í•˜ë‹¨ page like picture below Installation URL: https://www.apache.org/dyn/closer.lua/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz (2022.01) Download WinRAR Program You need to install WinRAR, to unzip .tgz file. Installation file: https://www.rarlab.com/download.htm Install what fits your computer Create Spark folder and move files Moving files Copy all the file in spark-3.2.0-bin-hadoop3.2 folder After that, create spark folder below C drive and move all of them to it. Modify log4j.properties file â€¢ Open the fileconf - [log4j.properties](http://log4j.properties) Open the log file as notebook and change INFO â†’ ERROR just like example below. During the process, all the output values can be removed. 1234567# Set everything to be logged to the console# log4j.rootCategory=INFO, consolelog4j.rootCategory=ERROR, consolelog4j.appender.console=org.apache.log4j.ConsoleAppenderlog4j.appender.console.target=System.errlog4j.appender.console.layout=org.apache.log4j.PatternLayoutlog4j.appender.console.layout.ConversionPattern=%d&#123;yy/MM/dd HH:mm:ss&#125; %p %c&#123;1&#125;: %m%n Installing winutils This time, we need program that makes local computer mistakes Sparks for Hadoop. Installing file: https://github.com/cdarlint/winutils Download winutils programs that fit installation version. I downloaded version 3.2.0 Create winutils&#x2F;bin folder on C drive and save the downloaded file. Ensure this file is authorized to be used so that it can be executed without errors whne running Spark This time, open CMD as admin and run the file If ChangeFileModeByMask error (3) occurs, create tmp\\hive folder below C drive. 12C:\\Windows\\system32&gt;cd c:\\winutils\\binc:\\winutils\\bin&gt; winutils.exe chmod 777 \\tmp\\hive Setting environment variables Set the system environment variable Click the ì‚¬ìš©ì ë³€ìˆ˜ - ìƒˆë¡œ ë§Œë“¤ê¸° button on each user account Set SPARK_HOME variable Set JAVA_HOME variable Set HADOOP_HOME variable Edit PATH variable. Add the code below. Add code below %SPARK_HOME%\\bin %JAVA_HOME%\\bin Testing Spark Open CMD file, set the path as c:\\spark folder if the logo appears when input â€˜sparkâ€™, success Check whether the code below works 1234&gt;&gt;&gt; rd = sc.textFile(&quot;README.md&quot;)&gt;&gt;&gt; rd.count()109&gt;&gt;&gt;","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Pyspark","slug":"Python/Pyspark","permalink":"https://jmj3047.github.io/categories/Python/Pyspark/"}],"tags":[{"name":"pyspark","slug":"pyspark","permalink":"https://jmj3047.github.io/tags/pyspark/"}],"keywords":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Pyspark","slug":"Python/Pyspark","permalink":"https://jmj3047.github.io/categories/Python/Pyspark/"}]},{"title":"Adversarial Speaker Verification","slug":"Adversaria 2d6a8","date":"2022-04-16T15:00:00.000Z","updated":"2022-10-15T08:03:42.984Z","comments":true,"path":"2022/04/17/Adversaria 2d6a8/","link":"","permalink":"https://jmj3047.github.io/2022/04/17/Adversaria%202d6a8/","excerpt":"","text":"Journal&#x2F;Conference: ICASSP IEEEYear(published year): 2019Author: Zhong Meng, Yong Zhao, Jinyu Li, Yifan GongSubject: Speaker Verification Adversarial Speaker Verification GoalWith ASV, our goal is to learn a condition-invariant and speaker-discriminative deep hidden feature in the background DNN through adversarial multi-task learning such that a noise-robust deep embedding can be obtained from these deep features for an enrolled speaker or a test utterance. Dataâ€œHey Cortanaâ€ from the Windows 10 desktop Cortana service logs.CHiME-3: buses (BUS), in cafes (CAF), in pedestrian areas (PED), at street junctions (STR))From the clean Cortana data, we select 6 utterances from each of the 3k speakers as the enrollment data (called â€œEnroll Aâ€). We select 60k utterances from 3k target speakers and 3k impostors in Cortana dataset and mix them with CHiME-3 real noise to generate the noisy evaluation set. Result Why? In ASV, a speaker classification network and a condition identification network are jointly trained to minimize the speaker classification loss and to mini-maximize the condition loss through adversarial multitask learning.The target labels of the condition network can be categorical (environment types) and continuous (SNR values). With ASV, speaker-discriminative and condition-invariant deep embeddings can be extracted for both enrollment and test speech. ì ëŒ€ì  í•™ìŠµì€ [22] ë…¼ë¬¸ì—ì„œ ë¨¼ì € ì ìš©ë˜ì—ˆëŠ”ë° ì´ ë…¼ë¬¸ê³¼ì˜ ì°¨ì´ì ì€, ë‘ê°€ì§€ ì†ŒìŒ ì»¨ë””ì…˜ì„ ì„œë¡œ ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ ë§‰ì€ ê²ƒ(22 ë…¼ë¬¸ì—ì„œëŠ” í™˜ê²½ ê°œì„  ë³´ë‹¤ëŠ” unlabeled íƒ€ê²Ÿ ë„ë©”ì¸ ë°ì´í„°ë¥¼ í›ˆë ¨í•˜ì—¬ ì ì‘ ì‹œí‚¤ëŠ” ê±¸ ëª©í‘œë¡œ í•¨), ê·¸ë¦¬ê³  ë³¸ ë…¼ë¬¸ì€ ë„¤íŠ¸ì›Œí¬ì— ì§ì ‘ì ìœ¼ë¡œ ìŒì„± í”¼ì²˜ë¥¼ ì¸í’‹ìœ¼ë¡œ ë„£ì–´ í›ˆë ¨í•˜ëŠ” ë°˜ë©´, 22 ë…¼ë¬¸ì€ i-ë²¡í„°ë¥¼ ì¸í’‹ìœ¼ë¡œ ë„£ì—ˆê³  ì´ëŠ” computationalí•œ ì‹œê°„ê³¼ ìì›ì´ ë” ë“¤ì–´ê°. ** Train ASV model to adapt to noise ** Experiment Embeddings : ì¸ê³µì‹ ê²½ë§ì—ì„œ ì›ë˜ ì°¨ì›ë³´ë‹¤ ì €ì°¨ì›ì˜ ë²¡í„°ë¡œ ë§Œë“œëŠ” ê²ƒì„ ì˜ë¯¸ì›ë˜ ì°¨ì›ì€ ë§¤ìš° ë§ì€ ë²”ì£¼í˜• ë³€ìˆ˜ë“¤ë¡œ êµ¬ì„±ë˜ì–´ ìˆê³  ì´ê²ƒë“¤ì´ í•™ìŠµë°©ì‹ì„ í†µí•´ ì €ì°¨ì›ìœ¼ë¡œ ëŒ€ì‘ë¨. ìˆ˜ì²œ ìˆ˜ë§Œê°œì˜ ê³ ì°¨ì› ë³€ìˆ˜ë“¤ì„ ëª‡ë°±ê°œì˜ ì €ì°¨ì› ë³€ìˆ˜ë¡œ ë§Œë“¤ì–´ ì£¼ê³ , ë˜í•œ ë³€í˜•ëœ ì €ì°¨ì› ê³µê°„ì—ì„œë„ ì¶©ë¶„íˆ ì¹´í…Œê³ ë¦¬í˜• ì˜ë¯¸ë¥¼ ë‚´ì¬í•¨.ì¶œì²˜: ì¸ê³µì‹ ê²½ë§(ë”¥ëŸ¬ë‹)ì˜ Embedding ì´ë€ ë¬´ì—‡ì¼ê¹Œ? - ì„ë² ë”©ì˜ ì˜ë¯¸(1&#x2F;3) í›ˆë ¨ë‹¨ê³„ì—ì„œ background DNNì„ í™”ìë“¤ì„ êµ¬ë³„í•˜ê¸° ìœ„í•´ í›ˆë ¨ ì‹œí‚´. F &#x3D; {f1 ,â€¦, fT }, ft âˆˆ Rrf : deep hidden featuresX &#x3D; {x1 ,â€¦, xT}, xt âˆˆ Rrx , t &#x3D; {1 ,â€¦, T} : input speech frames from training set to intermediate deep hidden featuresÎ˜f: parameters maps input speech framesMf: the hidden layers of the background DNN as a feature extractor network with parameters Î˜f P(a|ft;Î˜y), a âˆˆ A : Speaker posteriors, where A is the set of all speakers in the training setÎ˜y: maps the deep features F to the speaker posteriors.My: the upper layers of the background DNN as a speaker classifier network with parameters Î˜y Î˜f and Î˜y are optimized by Minimizing cross entropy loss of speaker classification. Y &#x3D; {y1 ,â€¦, yT }, yt âˆˆA : sequence of speaker labels aligned with X1[.]: indicator function equals to 1 if the condition in the bracket is satisfied and 0 other wise. Categorical Condition Classification Loss: to address the conditions that are characterized as a categorical variable additional condition classification network Mc: which predicts the condition posteriors p(b| ft;Î˜f ); b âˆˆ B given the deep features F from the training setB : the set of all conditions in the training set With a sequence of condition labels C &#x3D; {c1 ,â€¦, cT} that is aligned with X, compute the condition classification loss through cross-entropy Continuous Condition Regression Loss: an additional condition regression network Mc to predict the frame-level condition value (SNR value) compute the condition regression loss through mean-square error Deep feature F ë¥¼ condition invariant í•˜ê²Œ ë§Œë“¤ë ¤ë©´, ì†ŒìŒë“¤ ê°ê°ì˜ í™˜ê²½ì—ì„œ ë‚˜ì˜¤ëŠ” í”¼ì²˜ë“¤ì˜ ì°¨ì´ê°€ ìµœëŒ€í•œ ì ì–´ì•¼ í•¨.ë”°ë¼ì„œ Mf ì™€ Mc ëŠ” ê°™ì´ ì ëŒ€ì ìœ¼ë¡œ train í•˜ê²Œ ë˜ê³ , Î˜f ê°€ frame-level condition loss, Lcondition ì„ ìµœëŒ€í™” ì‹œí‚¤ê³  Î˜cê°€ Lconditionì„ ìµœì†Œí™” ì‹œí‚¤ëŠ” ë°©í–¥ìœ¼ë¡œ ê°.ì´ ë‘˜ì˜ ê²½ìŸì€ ì²˜ìŒì— Mcì— ëŒ€í•œ ì°¨ë³„ì„±ì„ ë†’ì—¬ì£¼ê³ , speaker invariance ì˜ deep feature ê°€ Mfì— ì˜í•´ ë§Œë“¤ì–´ì§.ê²°êµ­ Mfê°€ ê·¹ë‹¨ì ìœ¼ë¡œ Mcê°€ êµ¬ë³„í•˜ì§€ ëª»í•˜ëŠ” í”¼ì²˜ë¥¼ ë§Œë“œëŠ” ì§€ì ì— ìˆ˜ë ´.ê·¸ì™€ ë™ì‹œì— ë…¼ë¬¸ì—ì„œëŠ” í™”ì ì°¨ë³„ì ì¸ deep feature ë“¤ì„ Lspeaker(Eq3)ì˜ speaker classification ì†ì‹¤í•¨ìˆ˜ë¥¼ ìµœì†Œí™” í•˜ë©´ì„œ ë§Œë“¦. ìµœì ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ëŠ” ì‹: ì—¬ê¸°ì„œ Î»ê°€ speaker classification ì†ì‹¤í•¨ìˆ˜ì™€ condition í•¨ìˆ˜ ì‚¬ì´ì˜ ê· í˜•ì„ í†µì œ.GRLì€ forward propagation ì—ì„œ identity transform ì—­í• ì„ í•˜ë©° back propagation ì—ì„œ ê²½ì‚¬ë„ë¥¼ â€“ Î»ë¡œ ê³±í•¨. Link: ADVERSARIAL SPEAKER VERIFICATION","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speaker Verification","slug":"Paper/Speaker-Verification","permalink":"https://jmj3047.github.io/categories/Paper/Speaker-Verification/"}],"tags":[{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"},{"name":"Adversarial Speaker Verification","slug":"Adversarial-Speaker-Verification","permalink":"https://jmj3047.github.io/tags/Adversarial-Speaker-Verification/"}],"keywords":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speaker Verification","slug":"Paper/Speaker-Verification","permalink":"https://jmj3047.github.io/categories/Paper/Speaker-Verification/"}]},{"title":"Definition of Distance","slug":"Definition_of_Distance","date":"2022-04-16T15:00:00.000Z","updated":"2023-02-23T01:33:50.993Z","comments":true,"path":"2022/04/17/Definition_of_Distance/","link":"","permalink":"https://jmj3047.github.io/2022/04/17/Definition_of_Distance/","excerpt":"","text":"1. Euclidean distance ê°€ì¥ í”íˆ ì‚¬ìš©í•˜ëŠ” ê±°ë¦¬ì¸¡ë„ ëŒ€ì‘ë˜ëŠ” x,yê°’ ê°„ ì°¨ì´ ì œê³±í•©ì˜ ì œê³±ê·¼ìœ¼ë¡œì¨, ë‘ ê´€ì¸¡ì¹˜ ì‚¬ì´ì˜ ì§ì„  ê±°ë¦¬ë¥¼ ì˜ë¯¸í•¨. ë‹¤ì°¨ì› ë°ì´í„°ì—ì„œë„ ë§ˆì°¬ê°€ì§€ ì´ë‹¤. 2. Manhattan Distance ë§¨í•˜íƒ„ì€ ë¸”ëŸ­ì´ ë‚˜ëˆ„ì–´ì ¸ ìˆì–´ ì§ì„ ìœ¼ë¡œ ê°ˆ ìˆ˜ê°€ ì—†ë‹¤. ì§ì„ ê±°ë¦¬ê°€ ì•„ë‹Œ ê²©ìê±°ë¦¬. ê²©ì:ë°”ë‘‘íŒì²˜ëŸ¼ ê°€ë¡œì„¸ë¡œë¥¼ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ì§ê°ì´ ë˜ê²Œ ì§  êµ¬ì¡°ë‚˜ ë¬¼ê±´. ê° ì¢Œí‘œì˜ ì°¨ì´ì˜ ì ˆëŒ“ê°’ì˜ í•© 3. Mahalanobis Distance ë³€ìˆ˜ ë‚´ ë¶„ì‚°,ë³€ìˆ˜ ê°„ ê³µë¶„ì‚°ì„ ëª¨ë‘ ë°˜ì˜í•˜ì—¬ x,y,ê°„ ê±°ë¦¬ë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ì‹â‡’ë³€ìˆ˜ê°„ ìƒê´€ê´€ê³„ë¥¼ ê³ ë ¤í•œ ê±°ë¦¬ì§€í‘œì´ë‹¤. ë°ì´í„°ì˜ ê³µë¶„ì‚° í–‰ë ¬ì´ ë‹¨ìœ„í–‰ë ¬ì¸ ê²½ìš°ëŠ” ìœ í´ë¦¬ë””ì•ˆ ê±°ë¦¬ì™€ ë™ì¼í•¨ ê³µë¶„ì‚° í–‰ë ¬ì˜ ì—­í–‰ë ¬ì„ ì·¨í–ˆë‹¤ëŠ” ê²ƒ â†’ ë¶„ì‚°ì´ ë¶„ëª¨ì— ë“¤ì–´ê°„ë‹¤ëŠ” ëœ» â†’ ë¶„ì‚°ì´ ì»¤ì§€ë©´ ê±°ë¦¬ê°€ ì‘ì•„ì§€ê³  , ë¶„ì‚°ì´ ì‘ì•„ì§€ë©´ ê±°ë¦¬ê°€ ê¸¸ì–´ì§ ë§ˆí• ë¼ë…¸ë¹„ìŠ¤ê±°ë¦¬ê°€ ì œê³±ê·¼ì´ ì·¨í•´ì ¸ ìˆê¸° ë•Œë¬¸ì— ì œê³±ê·¼ì„ ì—†ì•´ë‹¤. 2ì°¨ì› í–‰ë ¬ë¡œ ë¹„ìœ ë¥¼ í–ˆì„ì‹œ , ì­ˆìš± ëŒ€ì…í•˜ë©´ ì•„ë˜ì˜ ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚œë‹¤ yê°’ì— 0,0 ì„ì£¼ê³  ëŒ€ì…í•˜ë©´ íƒ€ì›ì˜ ë°©ì •ì‹ì´ ë‚˜ì˜¨ë‹¤. ìœ í´ë¦¬ë””ì•ˆ ê´€ì ì—ì„œëŠ” ì¤‘ì•™ì ê³¼ ë¹„êµí–ˆì„ë•Œ, Aê°€ ë” ë©€ë‹¤. ìƒê´€ê´€ê³„ë¥¼ ê³ ë ¤í•œ ë§ˆí• ë¼ë…¸ë¹„ìŠ¤ ê±°ë¦¬ë¡œ ë³´ë©´ Bê°€ ë” ë©€ë‹¤ Reference: í•œêµ­ê³µí•™ëŒ€í•™êµ ê°•ì§€í›ˆêµìˆ˜ë‹˜ ê°•ì˜","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"}],"keywords":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}]}]}