{"meta":{"title":"Jang Minjee","subtitle":"","description":"","author":"Jang Minjee","url":"https://jmj3047.github.io","root":"/"},"pages":[],"posts":[{"title":"Datastream, Dataflow, BigQuery ML, Looker Studio를 사용하여 수요 예측 빌드 및 시각화","slug":"Dataflow_BQML_Looker","date":"2023-03-12T15:00:00.000Z","updated":"2023-03-13T10:30:06.705Z","comments":true,"path":"2023/03/13/Dataflow_BQML_Looker/","link":"","permalink":"https://jmj3047.github.io/2023/03/13/Dataflow_BQML_Looker/","excerpt":"","text":"개요 이 노트북의 예제로 실습 진행 데이터 흐름: oracle → pub&#x2F;sub → datastream → dataflow → bigquery → looker 전체적인 process compute engine에서 vm 인스턴스 생성 저장장소 Cloud Storage Bucket 생성 객체 변경사항에 대한 알림을 Pub&#x2F;sub으로 전송하도록 bucket 구성 Data stream을 만들어 google cloud storage에 오라클 데이터 복제 DataFlow로 복제된 데이터를 json파일로 빅쿼리에 적재 빅쿼리로 데이터 분석 및 ML 루커로 시각화 본 포스트는 위 전체 프로세스에서 5~6번에 해당하는 내용(이전 내용 확인) DataFlow로 복제된 데이터를 json파일로 변환하여 Bigquery 적재 하는 방법을 알아보자 BQML의 ARIMA_PLUS 대해서 알아보자 루커를 사용하지 않고 루커 스튜디오로 시각화 사례에서 활용된 DataFlow Dataflow Datastream to BigQuery 스트리밍 템플릿에 배포하여 Datastream에서 캡처한 변경사항을 BigQuery에 복제 UDF를 만들고 사용하여 이 템플릿의 기능을 확장 수신 데이터를 처리할 수 있는 UDF 만들기 UDF를 만들어 backfill된 데이터와 모든 새 수신 데이터에서 다음 작업을 수행 backfill: 데이터 파이프라인을 운용할때 이미 지난 날짜를 기준으로 재처리하는 작업, 메우는 작업, 버그가 있거나 어떤 이유로 로직이 변경됐을때 전체 데이터를 새로 말아주어야 할때 컬럼 등의 메타 데이터가 변경되었을 때 이를 반영하기 위한 append 성의 작업이 필요할때 고객 결제 수단과 같은 민감한 정보를 수정 데이터 계보와 검색을 위해 Oracle 소스 테이블을 BigQuery에 추가 이 로직은 Datastream에서 생성된 JSON 파일을 입력 매개변수로 사용하는 자바스크립트 파일에서 캡처됨 Cloud Shell 세션에서 다음 코드를 복사하여 retail_transform.js 파일에 저장 Oracle에서 추출한 json 파일을 암호화 해서 새로운 json 파일을 생성함. 123456789101112function process(inJson) &#123; var obj = JSON.parse(inJson), includePubsubMessage = obj.data &amp;&amp; obj.attributes, data = includePubsubMessage ? obj.data : obj; data.PAYMENT_METHOD = data.PAYMENT_METHOD.split(&#x27;:&#x27;)[0].concat(&quot;XXX&quot;); data.ORACLE_SOURCE = data._metadata_schema.concat(&#x27;.&#x27;, data._metadata_table); return JSON.stringify(obj);&#125; retail_transform.js file을 저장할 Cloud Storage 버킷을 만든 후 자바스크립트 파일을 새로 만든 버킷에 업로드합니다. 위에서 생성된 json 파일을 새로운 버켓에 저장함 1234gsutil mb gs://js-$&#123;BUCKET_NAME&#125;gsutil cp retail_transform.js \\gs://js-$&#123;BUCKET_NAME&#125;/utils/retail_transform.js Dataflow 작업 만들기 Cloud Shell에서 데드 레터 큐(DLQ) 버킷을 만든다: 이 버킷은 DataFlow에서 사용됨 dead-letter-queue: 하나 이상의 Source Queue가 성공적으로 컨슘되지 못한 메세지들을 재전송하기 위해 사용하는 별도의 큐. DLQ에 쌓인 메세지들을 보면 왜 이 메세지들이 컨슈머에 의해 처리되지 못했는지를 알 수 있다. 1gsutil mb gs://dlq-$&#123;BUCKET_NAME&#125; Dataflow 실행에 필요한 서비스 계정을 만들고 계정을 Dataflow Worker, Dataflow Admin, Pub/Sub Admin, BigQuery Data Editor, BigQuery Job User, Datastream Admin 역할에 할당 1234567891011121314151617181920212223242526272829gcloud iam service-accounts create df-tutorialgcloud projects add-iam-policy-binding $&#123;PROJECT_ID&#125; \\--member=&quot;serviceAccount:df-tutorial@$&#123;PROJECT_ID&#125;.iam.gserviceaccount.com&quot; \\--role=&quot;roles/dataflow.admin&quot;gcloud projects add-iam-policy-binding $&#123;PROJECT_ID&#125; \\--member=&quot;serviceAccount:df-tutorial@$&#123;PROJECT_ID&#125;.iam.gserviceaccount.com&quot; \\--role=&quot;roles/dataflow.worker&quot;gcloud projects add-iam-policy-binding $&#123;PROJECT_ID&#125; \\--member=&quot;serviceAccount:df-tutorial@$&#123;PROJECT_ID&#125;.iam.gserviceaccount.com&quot; \\--role=&quot;roles/pubsub.admin&quot;gcloud projects add-iam-policy-binding $&#123;PROJECT_ID&#125; \\--member=&quot;serviceAccount:df-tutorial@$&#123;PROJECT_ID&#125;.iam.gserviceaccount.com&quot; \\--role=&quot;roles/bigquery.dataEditor&quot;gcloud projects add-iam-policy-binding $&#123;PROJECT_ID&#125; \\--member=&quot;serviceAccount:df-tutorial@$&#123;PROJECT_ID&#125;.iam.gserviceaccount.com&quot; \\--role=&quot;roles/bigquery.jobUser&quot;gcloud projects add-iam-policy-binding $&#123;PROJECT_ID&#125; \\--member=&quot;serviceAccount:df-tutorial@$&#123;PROJECT_ID&#125;.iam.gserviceaccount.com&quot; \\--role=&quot;roles/datastream.admin&quot;gcloud projects add-iam-policy-binding $&#123;PROJECT_ID&#125; \\--member=&quot;serviceAccount:df-tutorial@$&#123;PROJECT_ID&#125;.iam.gserviceaccount.com&quot; \\--role=&quot;roles/storage.admin&quot; 자동 확장이 사용 설정되면 Dataflow VM이 TCP 포트 12345 및 12346에서 네트워크 트래픽과 통신하고 전송 및 수신할 수 있도록 방화벽 이그레스 규칙을 만든다: VM끼리 통신이 가능하게끔 하는 작업 12345678gcloud compute firewall-rules create fw-allow-inter-dataflow-comm \\--action=allow \\--direction=ingress \\--network=GCP_NETWORK_NAME \\--target-tags=dataflow \\--source-tags=dataflow \\--priority=0 \\--rules tcp:12345-12346 Dataflow 작업을 만들고 실행 → Dataflow 콘솔을 확인하여 새 스트리밍 작업이 시작되었는지 확인 123456789101112131415export REGION=us-central1gcloud dataflow flex-template run orders-cdc-template --region $&#123;REGION&#125; \\--template-file-gcs-location &quot;gs://dataflow-templates/latest/flex/Cloud_Datastream_to_BigQuery&quot; \\--service-account-email &quot;df-tutorial@$&#123;PROJECT_ID&#125;.iam.gserviceaccount.com&quot; \\--parameters \\inputFilePattern=&quot;gs://$&#123;BUCKET_NAME&#125;/&quot;,\\gcsPubSubSubscription=&quot;projects/$&#123;PROJECT_ID&#125;/subscriptions/oracle_retail_sub&quot;,\\inputFileFormat=&quot;json&quot;,\\outputStagingDatasetTemplate=&quot;retail&quot;,\\outputDatasetTemplate=&quot;retail&quot;,\\deadLetterQueueDirectory=&quot;gs://dlq-$&#123;BUCKET_NAME&#125;&quot;,\\autoscalingAlgorithm=&quot;THROUGHPUT_BASED&quot;,\\mergeFrequencyMinutes=1,\\javascriptTextTransformGcsPath=&quot;gs://js-$&#123;BUCKET_NAME&#125;/utils/retail_transform.js&quot;,\\javascriptTextTransformFunctionName=&quot;process&quot; Cloud Shell에서 다음 명령어를 실행하여 Datastream 스트림을 시작 12gcloud datastream streams update oracle-cdc \\--location=us-central1 --state=RUNNING --update-mask=state DataStream 스트림 상태를 확인 12gcloud datastream streams list \\--location=us-central1 상태가 Running으로 표시되는지 확인. 새 상태값이 반영되기까지 몇 초 정도 걸릴 수 있음. Datastream 콘솔을 확인하여 ORDERS 테이블 백필 진행 상황을 확인 이 task는 초기 로드 이므로 Datastream은 ORDERS 객체에서 읽음. 스트림 생성 중에 지정한 Cloud Storage 버킷에 있는 JSON 파일에 모든 레코드를 쓴다. 백필 태스크가 완료되는데 약 10분 정도 걸림. BigQuery에서 데이터 분석 데이터 세트에서 다음 새 테이블 두 개가 Dataflow 작업으로 생성 ORDERS: 이 출력 테이블은 Oracle 테이블 복제본이며 Dataflow 템플릿의 일부로 데이터에 적용된 변환을 포함 ORDERS_log: 이 스테이징 테이블은 Oracle 소스의 모든 변경사항을 기록. 테이블은 파티션으로 나눠지고 변경사항이 업데이트, 삽입 또는 삭제인지 여부와 같은 일부 메타데이터 변경 정보와 함께 업데이트된 레코드를 저장 BigQuery ML에서 수요 예측 모델 빌드 BigQuery ML은 ARIMA_PLUS 알고리즘을 사용하여 수요 예측 모델을 빌드하고 배포하는 데 사용될 수 있음. 이 섹션에서는 BigQuery ML을 사용하여 매장 내 제품 수요를 예측하는 모델을 빌드. 학습 데이터 준비 백필한 데이터의 샘플을 사용하여 모델을 학습 이 경우 1년 동안의 데이터를 사용합니다. 학습 데이터에서는 다음을 보여줍니다. 제품 이름(product_name) 판매된 각 제품의 단위 수량(total_sold) 시간당 판매된 제품 수(hourly_timestamp) BigQuery에서 다음 SQL을 실행하여 학습 데이터를 만들고 training_data라는 새 테이블에 저장 1234567891011CREATE OR REPLACE TABLE `retail.training_data`AS SELECT TIMESTAMP_TRUNC(time_of_sale, HOUR) as hourly_timestamp, product_name, SUM(quantity) AS total_sold FROM `retail.ORDERS` GROUP BY hourly_timestamp, product_name HAVING hourly_timestamp BETWEEN TIMESTAMP_TRUNC(&#x27;2021-11-22&#x27;, HOUR) ANDTIMESTAMP_TRUNC(&#x27;2021-11-28&#x27;, HOUR)ORDER BY hourly_timestamp 예측 수요 BigQuery에서 다음 SQL을 실행하여 ARIMA_PLUS 알고리즘을 사용하는 시계열 모델을 생성 12345678910111213CREATE OR REPLACE MODEL `retail.arima_plus_model` OPTIONS( MODEL_TYPE=&#x27;ARIMA_PLUS&#x27;, TIME_SERIES_TIMESTAMP_COL=&#x27;hourly_timestamp&#x27;, TIME_SERIES_DATA_COL=&#x27;total_sold&#x27;, TIME_SERIES_ID_COL=&#x27;product_name&#x27; ) ASSELECT hourly_timestamp, product_name, total_soldFROM `retail.training_data` ML.FORECAST함수는 n시간 범위에 걸쳐 예상되는 수요를 예측하는 데 사용됨 다음 SQL을 실행하여 향후 30일 동안의 유기농 바나나 수요를 예측 1SELECT * FROM ML.FORECAST(MODEL `retail.arima_plus_model`, STRUCT(720 AS horizon)) 학습 데이터는 시간 단위이므로 범위 값은 예측 시 동일한 시간 단위(시간)를 사용. 720시간 범위 값은 다음 30일 동안의 예측 결과를 반환 이 튜토리얼에서는 소량의 샘플 데이터 세트를 사용하므로 모델의 정확성에 대한 자세한 조사는 이 튜토리얼에서 다루지 않음. 시각화하기 BigQuery에서 다음 SQL 쿼리를 실행하여 유기농 바나나의 실제 판매량과 예상 판매량을 통합하는 뷰를 생성 12345678910111213141516171819202122232425262728CREATE OR REPLACE VIEW `retail.orders_forecast` AS (SELECTtimestamp,product_name,SUM(forecast_value) AS forecast,SUM(actual_value) AS actualfrom(SELECT TIMESTAMP_TRUNC(TIME_OF_SALE, HOUR) AS timestamp, product_name, SUM(QUANTITY) as actual_value, NULL AS forecast_value FROM `retail.ORDERS` GROUP BY timestamp, product_nameUNION ALLSELECT forecast_timestamp AS timestamp, product_name, NULL AS actual_value, forecast_value, FROM ML.FORECAST(MODEL `retail.arima_plus_model`, STRUCT(720 AS horizon)) ORDER BY timestamp)GROUP BY timestamp, product_nameORDER BY timestamp) 이 뷰를 사용하면 실제 데이터와 예측 데이터를 탐색할 때 Looker에서 관련 데이터를 쿼리할 수 있음 다음 SQL을 실행하여 뷰를 검증 1234SELECT * FROM `retail.orders_forecast`WHERE PRODUCT_NAME=&#x27;Bag of Organic Bananas&#x27;AND TIMESTAMP_TRUNC(timestamp, HOUR) BETWEEN TIMESTAMP_TRUNC(&#x27;2021-11-28&#x27;, HOUR) AND TIMESTAMP_TRUNC(&#x27;2021-11-30&#x27;, HOUR)LIMIT 100; 이전 포스팅의 ‘Big Query를 사용하여 SQL로 데이터를 정제한 후 Looker Studio에 SQL로 차트 만들기’ 처럼 sql쿼리로 데이터 셋을 생성 12SELECT * FROM `retail.orders_forecast`WHERE actual IS NOT NULL 결과 페이지 드롭다운 목록의 날짜 부분은 함수를 걸어 date형태로 조회할수 있게 하였음. Reference https://cloud.google.com/architecture/build-visualize-demand-forecast-prediction-datastream-dataflow-bigqueryml-looker?hl=ko#create-a-dataflow-job https://cloud.google.com/dataflow?hl=ko https://cloud.google.com/dataflow/pricing?hl=ko#shuffle-pricing-details https:&#x2F;&#x2F;velog.io&#x2F;@usaindream&#x2F;Dead-Letter-QueueDLQ https://wookiist.dev/175","categories":[{"name":"Data Base","slug":"Data-Base","permalink":"https://jmj3047.github.io/categories/Data-Base/"},{"name":"GCP","slug":"Data-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Base/GCP/"}],"tags":[{"name":"Big Query","slug":"Big-Query","permalink":"https://jmj3047.github.io/tags/Big-Query/"},{"name":"BigQueryML","slug":"BigQueryML","permalink":"https://jmj3047.github.io/tags/BigQueryML/"},{"name":"Looker Studio","slug":"Looker-Studio","permalink":"https://jmj3047.github.io/tags/Looker-Studio/"},{"name":"DataFlow","slug":"DataFlow","permalink":"https://jmj3047.github.io/tags/DataFlow/"}]},{"title":"BQML을 이용한 고객 분류","slug":"BQML_Classification","date":"2023-03-09T15:00:00.000Z","updated":"2023-03-13T06:13:21.542Z","comments":true,"path":"2023/03/10/BQML_Classification/","link":"","permalink":"https://jmj3047.github.io/2023/03/10/BQML_Classification/","excerpt":"","text":"개요 K means clustering을 빅쿼리 ML(BQML)을 사용하여 고객을 세분화 하기 GA360의 데이터를 빅쿼리에 적재해 ML학습하기 파이썬을 사용하여 빅쿼리와 연동하고 관련 그래프 시각화하기 목표 구글 브랜드 상품을 판매하는 실제 이커머스 스토어인 구글 머천다이스 스토어의 난독화된 GA360 12개월(2016년 8월~2017년 8월)의 데이터를 가지고 고객을 분류 환경 구축 GCP와 주피터 노트북을 연결 config파일이 필요(이전포스트 설명)PIP install packages and dependencies 123456!pip install google-cloud-bigquery!pip install google-cloud-bigquery-storage!pip install pandas-gbq# Reservation package needed to setup flex slots for flat-rate pricing!pip install google-cloud-bigquery-reservation 123456789101112131415161718192021222324252627282930Requirement already satisfied: google-cloud-bigquery in /opt/homebrew/lib/python3.10/site-packages (3.6.0)Requirement already satisfied: google-cloud-core&lt;3.0.0dev,&gt;=1.6.0 in /opt/homebrew/lib/python3.10/site-packages (from google-cloud-bigquery) (2.3.2)Requirement already satisfied: requests&lt;3.0.0dev,&gt;=2.21.0 in /opt/homebrew/lib/python3.10/site-packages (from google-cloud-bigquery) (2.28.2)Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,&lt;5.0.0dev,&gt;=3.19.5 in /opt/homebrew/lib/python3.10/site-packages (from google-cloud-bigquery) (4.22.1)Requirement already satisfied: grpcio&lt;2.0dev,&gt;=1.47.0 in /opt/homebrew/lib/python3.10/site-packages (from google-cloud-bigquery) (1.51.3)Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,&lt;3.0.0dev,&gt;=1.31.5 in /opt/homebrew/lib/python3.10/site-packages (from google-cloud-bigquery) (2.11.0)Requirement already satisfied: google-resumable-media&lt;3.0dev,&gt;=0.6.0 in /opt/homebrew/lib/python3.10/site-packages (from google-cloud-bigquery) (2.4.1)Requirement already satisfied: packaging&gt;=20.0.0 in /opt/homebrew/lib/python3.10/site-packages (from google-cloud-bigquery) (23.0)Requirement already satisfied: python-dateutil&lt;3.0dev,&gt;=2.7.2 in /opt/homebrew/lib/python3.10/site-packages (from google-cloud-bigquery) (2.8.2)Requirement already satisfied: proto-plus&lt;2.0.0dev,&gt;=1.15.0 in /opt/homebrew/lib/python3.10/site-packages (from google-cloud-bigquery) (1.22.2)Requirement already satisfied: google-auth&lt;3.0dev,&gt;=2.14.1 in /opt/homebrew/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,&lt;3.0.0dev,&gt;=1.31.5-&gt;google-cloud-bigquery) (2.16.1)Requirement already satisfied: googleapis-common-protos&lt;2.0dev,&gt;=1.56.2 in /opt/homebrew/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,&lt;3.0.0dev,&gt;=1.31.5-&gt;google-cloud-bigquery) (1.58.0)Requirement already satisfied: grpcio-status&lt;2.0dev,&gt;=1.33.2 in /opt/homebrew/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,&lt;3.0.0dev,&gt;=1.31.5-&gt;google-cloud-bigquery) (1.51.3)Requirement already satisfied: google-crc32c&lt;2.0dev,&gt;=1.0 in /opt/homebrew/lib/python3.10/site-packages (from google-resumable-media&lt;3.0dev,&gt;=0.6.0-&gt;google-cloud-bigquery) (1.5.0)Requirement already satisfied: six&gt;=1.5 in /opt/homebrew/lib/python3.10/site-packages (from python-dateutil&lt;3.0dev,&gt;=2.7.2-&gt;google-cloud-bigquery) (1.16.0)Requirement already satisfied: idna&lt;4,&gt;=2.5 in /opt/homebrew/lib/python3.10/site-packages (from requests&lt;3.0.0dev,&gt;=2.21.0-&gt;google-cloud-bigquery) (3.4)Requirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /opt/homebrew/lib/python3.10/site-packages (from requests&lt;3.0.0dev,&gt;=2.21.0-&gt;google-cloud-bigquery) (3.0.1)Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /opt/homebrew/lib/python3.10/site-packages (from requests&lt;3.0.0dev,&gt;=2.21.0-&gt;google-cloud-bigquery) (1.26.14)Requirement already satisfied: certifi&gt;=2017.4.17 in /opt/homebrew/lib/python3.10/site-packages (from requests&lt;3.0.0dev,&gt;=2.21.0-&gt;google-cloud-bigquery) (2022.12.7)Requirement already satisfied: pyasn1-modules&gt;=0.2.1 in /opt/homebrew/lib/python3.10/site-packages (from google-auth&lt;3.0dev,&gt;=2.14.1-&gt;google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,&lt;3.0.0dev,&gt;=1.31.5-&gt;google-cloud-bigquery) (0.2.8)Requirement already satisfied: rsa&lt;5,&gt;=3.1.4 in /opt/homebrew/lib/python3.10/site-packages (from google-auth&lt;3.0dev,&gt;=2.14.1-&gt;google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,&lt;3.0.0dev,&gt;=1.31.5-&gt;google-cloud-bigquery) (4.9)Requirement already satisfied: cachetools&lt;6.0,&gt;=2.0.0 in /opt/homebrew/lib/python3.10/site-packages (from google-auth&lt;3.0dev,&gt;=2.14.1-&gt;google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,&lt;3.0.0dev,&gt;=1.31.5-&gt;google-cloud-bigquery) (5.3.0)Requirement already satisfied: pyasn1&lt;0.5.0,&gt;=0.4.6 in /opt/homebrew/lib/python3.10/site-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;3.0dev,&gt;=2.14.1-&gt;google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,&lt;3.0.0dev,&gt;=1.31.5-&gt;google-cloud-bigquery) (0.4.8)Requirement already satisfied: google-cloud-bigquery-storage in /opt/homebrew/lib/python3.10/site-packages (2.19.0)Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,&lt;5.0.0dev,&gt;=3.19.5 in /opt/homebrew/lib/python3.10/site-packages (from google-cloud-bigquery-storage) (4.22.1)...Requirement already satisfied: certifi&gt;=2017.4.17 in /opt/homebrew/lib/python3.10/site-packages (from requests&lt;3.0.0dev,&gt;=2.18.0-&gt;google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,&lt;3.0.0dev,&gt;=1.34.0-&gt;google-cloud-bigquery-reservation) (2022.12.7)Requirement already satisfied: pyasn1&lt;0.5.0,&gt;=0.4.6 in /opt/homebrew/lib/python3.10/site-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;3.0dev,&gt;=2.14.1-&gt;google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,&lt;3.0.0dev,&gt;=1.34.0-&gt;google-cloud-bigquery-reservation) (0.4.8)Installing collected packages: google-cloud-bigquery-reservationSuccessfully installed google-cloud-bigquery-reservation-1.10.0 설치 후 커널 다시 시작 1234# Automatically restart kernel after installsimport IPythonapp = IPython.Application.instance()app.kernel.do_shutdown(True) Project ID와 인증 12345678PROJECT_ID = &quot;your-project-id&quot; REGION = &#x27;US&#x27;DATA_SET_ID = &#x27;bqml_kmeans&#x27; # Ensure you first create a data set in BigQuery# If you have not built the Data Set, the following command will build it for you!bq mk --location=$REGION --dataset $PROJECT_ID:$DATA_SET_ID !gcloud config set project $PROJECT_ID Import libraries and define constants 1234567891011121314151617181920import globfrom google.cloud import bigqueryfrom google.oauth2 import service_accountfrom google.cloud import bigqueryimport numpy as npimport pandas as pdimport pandas_gbqimport matplotlib.pyplot as plt# 서비스 계정 키 JSON 파일 경로key_path = glob.glob(&quot;./config/*.json&quot;)[0]# Credentials 객체 생성credentials = service_account.Credentials.from_service_account_file(key_path)project_id=&quot;your-project-id&quot;# GCP 클라이언트 객체 생성pd.set_option(&#x27;display.float_format&#x27;, lambda x: &#x27;%.3f&#x27; % x) # used to display float formatclient = bigquery.Client(credentials = credentials, project = credentials.project_id) 데이터모델을 구축하기 전에 일반적으로 모델링을 위해 의미 있는 방식으로 데이터 집합을 정리, 탐색 및 집계하는 데 상당한 시간을 투자해야 함. 이 포스트 목적상 이 단계는 BigQuery ML에서 k-평균을 사용한 클러스터링을 우선적으로 보여주기 위해 표시하지 않음. GA360, GA4차이 GA360 GA의 유료 버전으로 무료 버전과 가장 큰 차이점은 ‘데이터 소유권’ 데이터 소유권이 구글인 무료 버전에 비해 GA360은 데이터 소유권이 사용자 때문에 데이터 샘플링이 없고 빅쿼리를 통해 Raw Data를 이용할 수 있음. 다만 유료인만큼 연간 1.5억의 사용료를 지불해야 함. GA4 2019년에 새로 생긴 구글 애널리틱스로 WEB과 APP을 심리스하게 보기 위한 GA 360처럼 유료 버전을 쓰지 않아도 빅쿼리로 데이터를 보내주기 때문에 Raw Data를 쿼리 비용만 내고 사용할수 있음. GA4 UI에선 속도가 매우 빠르지만 GAUI에 비해 GA4 UI에서는 많은 것으르 보옂쥐 않고 데이터 분석을 위해 제대로 사용하기 위해서는 빅쿼리에 익숙애햐 함. 합성 데이터 구축 최종 목표는 온라인(GA360) 및 오프라인(CRM) 데이터를 모두 사용하는 것. 자체 CRM 데이터를 사용할 수도 있지만, 이 경우에는 보여줄 CRM 데이터가 없으므로 대신 합성 데이터를 생성: 예상 가구 소득(House Hold income, hhi)과 성별 이를 위해 전체 방문자 ID를 해시하고 해시의 마지막 숫자를 기반으로 간단한 규칙을 구축합니다. hash: 임의의 길이를 갖는 임의의 데이터를 고정된 길이의 데이터로 매핑하는 것 자체 데이터로 이 프로세스를 실행하면 여러 차원으로 CRM 데이터를 조인할 수 있음. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960# We start with GA360 data, and will eventually build synthetic CRM as an example. # This block is the first step, just working with GA360ga360_only_view = &#x27;GA360_View&#x27;shared_dataset_ref = client.dataset(DATA_SET_ID)ga360_view_ref = shared_dataset_ref.table(ga360_only_view)ga360_view = bigquery.Table(ga360_view_ref)ga360_query = &#x27;&#x27;&#x27;SELECT fullVisitorID, ABS(farm_fingerprint(fullVisitorID)) AS Hashed_fullVisitorID, # This will be used to generate random data. MAX(device.operatingSystem) AS OS, # We can aggregate this because an OS is tied to a fullVisitorID. SUM (CASE WHEN REGEXP_EXTRACT (v2ProductCategory, r&#x27;^(?:(?:.*?)Home/)(.*?)/&#x27;) = &#x27;Apparel&#x27; THEN 1 ELSE 0 END) AS Apparel, SUM (CASE WHEN REGEXP_EXTRACT (v2ProductCategory, r&#x27;^(?:(?:.*?)Home/)(.*?)/&#x27;) = &#x27;Office&#x27; THEN 1 ELSE 0 END) AS Office, SUM (CASE WHEN REGEXP_EXTRACT (v2ProductCategory, r&#x27;^(?:(?:.*?)Home/)(.*?)/&#x27;) = &#x27;Electronics&#x27; THEN 1 ELSE 0 END) AS Electronics, SUM (CASE WHEN REGEXP_EXTRACT (v2ProductCategory, r&#x27;^(?:(?:.*?)Home/)(.*?)/&#x27;) = &#x27;Limited Supply&#x27; THEN 1 ELSE 0 END) AS LimitedSupply, SUM (CASE WHEN REGEXP_EXTRACT (v2ProductCategory, r&#x27;^(?:(?:.*?)Home/)(.*?)/&#x27;) = &#x27;Accessories&#x27; THEN 1 ELSE 0 END) AS Accessories, SUM (CASE WHEN REGEXP_EXTRACT (v2ProductCategory, r&#x27;^(?:(?:.*?)Home/)(.*?)/&#x27;) = &#x27;Shop by Brand&#x27; THEN 1 ELSE 0 END) AS ShopByBrand, SUM (CASE WHEN REGEXP_EXTRACT (v2ProductCategory, r&#x27;^(?:(?:.*?)Home/)(.*?)/&#x27;) = &#x27;Bags&#x27; THEN 1 ELSE 0 END) AS Bags, ROUND (SUM (productPrice/1000000),2) AS productPrice_USDFROM `bigquery-public-data.google_analytics_sample.ga_sessions_*`, UNNEST(hits) AS hits, UNNEST(hits.product) AS hits_productWHERE _TABLE_SUFFIX BETWEEN &#x27;20160801&#x27; AND &#x27;20160831&#x27; AND geoNetwork.country = &#x27;United States&#x27; AND type = &#x27;EVENT&#x27;GROUP BY 1, 2&#x27;&#x27;&#x27;ga360_view.view_query = ga360_query.format(PROJECT_ID)ga360_view = client.create_table(ga360_view) # API requestprint(f&quot;Successfully created view at &#123;ga360_view.full_table_id&#125;&quot;) 데이터 확인 1234567891011121314# Show a sample of GA360 dataga360_query_df = f&#x27;&#x27;&#x27;SELECT * FROM &#123;ga360_view.full_table_id.replace(&quot;:&quot;, &quot;.&quot;)&#125; LIMIT 5&#x27;&#x27;&#x27;job_config = bigquery.QueryJobConfig()# Start the queryquery_job = client.query(ga360_query_df, job_config=job_config) #API Requestdf_ga360 = query_job.result()df_ga360 = df_ga360.to_dataframe()df_ga360 CRM data 추출하여 합성데이터 만들기 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# Create synthetic CRM data in SQLCRM_only_view = &#x27;CRM_View&#x27;shared_dataset_ref = client.dataset(DATA_SET_ID)CRM_view_ref = shared_dataset_ref.table(CRM_only_view)CRM_view = bigquery.Table(CRM_view_ref)# Query below works by hashing the fullVisitorID, which creates a random distribution. # We use modulo to artificially split gender and hhi distribution.CRM_query = &#x27;&#x27;&#x27;SELECT fullVisitorID,IF (MOD(Hashed_fullVisitorID,2) = 0, &quot;M&quot;, &quot;F&quot;) AS gender, CASE WHEN MOD(Hashed_fullVisitorID,10) = 0 THEN 55000 WHEN MOD(Hashed_fullVisitorID,10) &lt; 3 THEN 65000 WHEN MOD(Hashed_fullVisitorID,10) &lt; 7 THEN 75000 WHEN MOD(Hashed_fullVisitorID,10) &lt; 9 THEN 85000 WHEN MOD(Hashed_fullVisitorID,10) = 9 THEN 95000 ELSE Hashed_fullVisitorIDEND AS hhiFROM ( SELECT fullVisitorID, ABS(farm_fingerprint(fullVisitorID)) AS Hashed_fullVisitorID, FROM `bigquery-public-data.google_analytics_sample.ga_sessions_*`, UNNEST(hits) AS hits, UNNEST(hits.product) AS hits_product WHERE _TABLE_SUFFIX BETWEEN &#x27;20160801&#x27; AND &#x27;20160831&#x27; AND geoNetwork.country = &#x27;United States&#x27; AND type = &#x27;EVENT&#x27; GROUP BY 1, 2)&#x27;&#x27;&#x27;CRM_view.view_query = CRM_query.format(PROJECT_ID)CRM_view = client.create_table(CRM_view) # API requestprint(f&quot;Successfully created view at &#123;CRM_view.full_table_id&#125;&quot;) 데이터 확인 1234567891011121314# See an output of the synthetic CRM dataCRM_query_df = f&#x27;&#x27;&#x27;SELECT * FROM &#123;CRM_view.full_table_id.replace(&quot;:&quot;, &quot;.&quot;)&#125; LIMIT 5&#x27;&#x27;&#x27;job_config = bigquery.QueryJobConfig()# Start the queryquery_job = client.query(CRM_query_df, job_config=job_config) #API Requestdf_CRM = query_job.result()df_CRM = df_CRM.to_dataframe()df_CRM 클러스터링을 위한 학습 데이터로 사용할 최종뷰 작성1234567891011121314151617181920# Build a final view, which joins GA360 data with CRM datafinal_data_view = &#x27;Final_View&#x27;shared_dataset_ref = client.dataset(DATA_SET_ID)final_view_ref = shared_dataset_ref.table(final_data_view)final_view = bigquery.Table(final_view_ref)final_data_query = f&#x27;&#x27;&#x27;SELECT g.*, c.* EXCEPT(fullVisitorId)FROM &#123;ga360_view.full_table_id.replace(&quot;:&quot;, &quot;.&quot;)&#125; gJOIN &#123;CRM_view.full_table_id.replace(&quot;:&quot;, &quot;.&quot;)&#125; cON g.fullVisitorId = c.fullVisitorId&#x27;&#x27;&#x27;final_view.view_query = final_data_query.format(PROJECT_ID)final_view = client.create_table(final_view) # API requestprint(f&quot;Successfully created view at &#123;final_view.full_table_id&#125;&quot;) 데이터 시각화 1234567891011121314# Show final data used prior to modelingsql_demo = f&#x27;&#x27;&#x27;SELECT * FROM &#123;final_view.full_table_id.replace(&quot;:&quot;, &quot;.&quot;)&#125; LIMIT 5&#x27;&#x27;&#x27;job_config = bigquery.QueryJobConfig()# Start the queryquery_job = client.query(sql_demo, job_config=job_config) #API Requestdf_demo = query_job.result()df_demo = df_demo.to_dataframe()df_demo K-Means Model초기 모델 만들기 초기 k-means model을 구축 아직 최적의 k 또는 다른 하이퍼 파라미터에는 초점을 맞추지 않겠습니다. 몇 가지 추가 사항 클러스터링을 위한 피처로 fullVisitorID가 필요하지 않기 때문에 해당 수준에서 그룹화되어 있더라도 fullVisitorID를 입력에서 제거. 전체 방문자 ID를 피처로 사용해서는 안됨. 범주형 피처와 숫자 피처가 모두 존재 숫자 피처를 정규화할 필요가 없는데, 이는 BigQuery ML이 자동으로 수행하기 때문 12345678910111213141516171819202122def makeModel (n_Clusters, Model_Name): sql =f&#x27;&#x27;&#x27; CREATE OR REPLACE MODEL `&#123;PROJECT_ID&#125;.&#123;DATA_SET_ID&#125;.&#123;Model_Name&#125;` OPTIONS(model_type=&#x27;kmeans&#x27;, kmeans_init_method = &#x27;KMEANS++&#x27;, num_clusters=&#123;n_Clusters&#125;) AS SELECT * except(fullVisitorID, Hashed_fullVisitorID) FROM `&#123;final_view.full_table_id.replace(&quot;:&quot;, &quot;.&quot;)&#125;` &#x27;&#x27;&#x27; job_config = bigquery.QueryJobConfig() client.query(sql, job_config=job_config) # Make an API request.# Let&#x27;s start with a simple test to ensure everything works. # After running makeModel(), allow a few minutes for training to complete.model_test_name = &quot;test&quot;makeModel(3, model_test_name)# After training is completed, you can either check in the UI, or you can interact with it using list_models(). for model in client.list_models(DATA_SET_ID): print(model) 더 나은 모델을 만들기 위한 작업 올바른 k 값을 결정하는 것은 전적으로 사용 사례에 따라 달라짐.ex) 손으로 쓴 숫자를 사전 처리 → k &#x3D; 10 비즈니스 이해관계자가 세 개의 서로 다른 마케팅 캠페인만 제공하고자 하고 세 개의 고객 클러스터를 식별해야 하는 경우 → k&#x3D;3 그러나 현업에서 위의 예시처럼 딱 떨어지는 사용 사례는 거의 없기 때문에 보통 k의 범위를 지정하고 그 안에서 결과값이 좋은 k를 선택하기도 함. k 값을 결정하기 위한 수단으로 엘보우 방법을 수행한후 데이비스-볼딘 점수로 평가함. DBI가 작을수록 cluster를 자세히 구분했다고 말할수 있음 아래에서는 엘보 방법을 모두 수행하고 데이비스-볼딘 점수를 얻기 위한 몇 가지 모델을 생성함. low_k, high_k: 하이퍼 파라미터, 두 값 사이의 모델을 생성. 12345678910111213# Define upper and lower bound for k, then build individual models for each. # After running this loop, look at the UI to see several model objects that exist. low_k = 3high_k = 15model_prefix_name = &#x27;kmeans_clusters_&#x27;lst = list(range (low_k, high_k+1)) #build list to iterate through k valuesfor k in lst: model_name = model_prefix_name + str(k) makeModel(k, model_name) print(f&quot;Model started: &#123;model_name&#125;&quot;) 123456# list all current modelsmodels = client.list_models(DATA_SET_ID) # Make an API request.print(&quot;Listing current models:&quot;)for model in models: full_model_id = f&quot;&#123;model.dataset_id&#125;.&#123;model.model_id&#125;&quot; print(full_model_id) 12345# Remove our sample model from BigQuery, so we only have remaining models from our previous loopmodel_id = DATA_SET_ID+&quot;.&quot;+model_test_nameclient.delete_model(model_id) # Make an API request.print(f&quot;Deleted model &#x27;&#123;model_id&#125;&#x27;&quot;) 123456789101112131415161718192021# This will create a dataframe with each model name, the Davies Bouldin Index, and Loss. # It will be used for the elbow method and to help determine optimal Kdf = pd.DataFrame(columns=[&#x27;davies_bouldin_index&#x27;, &#x27;mean_squared_distance&#x27;])models = client.list_models(DATA_SET_ID) # Make an API request.for model in models: full_model_id = f&quot;&#123;model.dataset_id&#125;.&#123;model.model_id&#125;&quot; sql =f&#x27;&#x27;&#x27; SELECT davies_bouldin_index, mean_squared_distance FROM ML.EVALUATE(MODEL `&#123;full_model_id&#125;`) &#x27;&#x27;&#x27; job_config = bigquery.QueryJobConfig() # Start the query, passing in the extra configuration. query_job = client.query(sql, job_config=job_config) # Make an API request. df_temp = query_job.to_dataframe() # Wait for the job to complete. df_temp[&#x27;model_name&#x27;] = model.model_id df = pd.concat([df, df_temp], axis=0) 아래 코드는 원래 이 노트북에서 만든 명명 규칙을 사용했으며, 두 번째 밑줄 뒤에 k 값이 있다고 가정. model_prefix_name 변수를 변경한 경우, 이 코드가 깨질 수 있음. 123456# This will modify the dataframe above, produce a new field with &#x27;n_clusters&#x27;, and will sort for graphingdf[&#x27;n_clusters&#x27;] = df[&#x27;model_name&#x27;].str.split(&#x27;_&#x27;).map(lambda x: x[2])df[&#x27;n_clusters&#x27;] = df[&#x27;n_clusters&#x27;].apply(pd.to_numeric)df = df.sort_values(by=&#x27;n_clusters&#x27;, ascending=True)df 1df.plot.line(x=&#x27;n_clusters&#x27;, y=[&#x27;davies_bouldin_index&#x27;, &#x27;mean_squared_distance&#x27;]) 참고 - 이 노트북을 실행하면 무작위 클러스터 초기화로 인해 다른 결과를 얻을 수 있음. 도달 범위 실행에 대해 일관되게 동일한 클러스터를 반환하려면 하이퍼파라미터 선택을 통해 초기화를 명시적으로 선택가능. k 선택하기: 최적의 k 값을 결정할 때 완벽한 접근 방식이나 프로세스는 정해져 있지 않음. 비즈니스 규칙이나 요구 사항에 따라 결정되는 경우가 많음. 이 예에서는 간단한 요구 사항이 없으므로 다음과 같은 고려 사항을 따를 수도 있음: 항상 그런 것은 아니지만, 증분 클러스터가 손실을 크게 줄이지 못하는 자연스러운 ‘엘보 방법’이 있는 경우가 있음. 이 특정 예에서는, 그리고 종종 발견할 수 있듯이, 안타깝게도 자연스러운 ‘엘보’가 존재하지 않으므로 프로세스를 계속 진행해야 함. 다음으로 데이비스-볼딘과 k를 차트로 표. 이 점수는 각 클러스터가 얼마나 ‘다른지’를 알려주며, 최적 점수는 0. 클러스터가 5개인 경우 점수는 약 1.4이며, k가 9를 초과하는 경우에만 더 나은 값을 볼 수 있음. 마지막으로 각 모델의 차이를 해석하기 시작. 다양한 모델에 대한 평가 모듈을 검토하여 기능의 분포를 이해할 수 있음. 데이터를 통해 성별, 가구 소득, 쇼핑 습관에 따른 패턴을 찾을 수 있음. 최종 클러스터 분석 모델의 특성을 이해하는 데는 두 가지 옵션이 있습니다. BigQuery UI를 살펴보거나 모델 개체와 프로그래밍 방식 아래에서 후자의 옵션에 대한 간단한 예제를 찾을 수 있습니다. 123456789101112131415161718192021model_to_use = &#x27;kmeans_clusters_5&#x27; # User can edit thisfinal_model = DATA_SET_ID+&#x27;.&#x27;+model_to_usesql_get_attributes = f&#x27;&#x27;&#x27;SELECT centroid_id, feature, categorical_valueFROM ML.CENTROIDS(MODEL &#123;final_model&#125;)WHERE feature IN (&#x27;OS&#x27;,&#x27;gender&#x27;)&#x27;&#x27;&#x27;job_config = bigquery.QueryJobConfig()# Start the queryquery_job = client.query(sql_get_attributes, job_config=job_config) #API Requestdf_attributes = query_job.result()df_attributes = df_attributes.to_dataframe()df_attributes.head() 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556# get numerical information about clusterssql_get_numerical_attributes = f&#x27;&#x27;&#x27;WITH T AS (SELECT centroid_id, ARRAY_AGG(STRUCT(feature AS name, ROUND(numerical_value,1) AS value) ORDER BY centroid_id) AS clusterFROM ML.CENTROIDS(MODEL &#123;final_model&#125;)GROUP BY centroid_id),Users AS(SELECT centroid_id, COUNT(*) AS Total_UsersFROM(SELECT * EXCEPT(nearest_centroids_distance)FROM ML.PREDICT(MODEL &#123;final_model&#125;, ( SELECT * FROM &#123;final_view.full_table_id.replace(&quot;:&quot;, &quot;.&quot;)&#125; )))GROUP BY centroid_id)SELECT centroid_id, Total_Users, (SELECT value from unnest(cluster) WHERE name = &#x27;Apparel&#x27;) AS Apparel, (SELECT value from unnest(cluster) WHERE name = &#x27;Office&#x27;) AS Office, (SELECT value from unnest(cluster) WHERE name = &#x27;Electronics&#x27;) AS Electronics, (SELECT value from unnest(cluster) WHERE name = &#x27;LimitedSupply&#x27;) AS LimitedSupply, (SELECT value from unnest(cluster) WHERE name = &#x27;Accessories&#x27;) AS Accessories, (SELECT value from unnest(cluster) WHERE name = &#x27;ShopByBrand&#x27;) AS ShopByBrand, (SELECT value from unnest(cluster) WHERE name = &#x27;Bags&#x27;) AS Bags, (SELECT value from unnest(cluster) WHERE name = &#x27;productPrice_USD&#x27;) AS productPrice_USD, (SELECT value from unnest(cluster) WHERE name = &#x27;hhi&#x27;) AS hhiFROM T LEFT JOIN Users USING(centroid_id)ORDER BY centroid_id ASC&#x27;&#x27;&#x27;job_config = bigquery.QueryJobConfig()# Start the queryquery_job = client.query(sql_get_numerical_attributes, job_config=job_config) #API Requestdf_numerical_attributes = query_job.result()df_numerical_attributes = df_numerical_attributes.to_dataframe()df_numerical_attributes.head() 위의 결과를 분석해보면 1번 클러스터는 유저수가 두번째로 많고, 제일 유저수가 많은 클러스터 2번보다 구매율이 높은걸 알수 있음 2번 클러스터는 가장 인구가 많지만 구매 횟수가 적고 평균 지출액이 적음. 브랜드 충성도가 높다기 보다는 일회성 구매자 3번 클러스터는 의류에 관심이 많고 평균 구매 가격이 제일 높음. 브랜드별로 소비하진 않지만 가치가 가장 높은 고객 4번 클러스터는 브랜드 별로 소비를 많이 하는 고객들이 몰려 있음. 5번 클러스터는 사무용품에 가장 돈을 많이 사용하는 고객 Export to GA360 모델을 완성한 후에는 이를 추론에 사용 아래 코드는 사용자를 점수화하거나 클러스터에 할당하는 방법을 간략하게 설명 이 코드에는 CENTROID_ID라는 레이블이 붙습니다. 이 코드 자체도 도움이 되지만, 이 점수를 다시 GA360으로 수집하는 프로세스를 권장 BigQuery 테이블에서 Google 애널리틱스 360으로 BigQuery ML 예측을 내보내는 가장 쉬운 방법은 MoDeM(마케팅을 위한 모델 배포) 참조 구현을 사용하는 것 MoDeM은 Google 광고, 디스플레이 및 동영상 360, 검색 광고 360에서 최종적으로 활성화할 수 있도록 데이터를 Google 애널리틱스에 로드하는 데 도움이 됨 1234567891011121314151617181920sql_score = f&#x27;&#x27;&#x27;SELECT * EXCEPT(nearest_centroids_distance)FROM ML.PREDICT(MODEL &#123;final_model&#125;, ( SELECT * FROM &#123;final_view.full_table_id.replace(&quot;:&quot;, &quot;.&quot;)&#125; -- LIMIT 1 ))&#x27;&#x27;&#x27;job_config = bigquery.QueryJobConfig()# Start the queryquery_job = client.query(sql_score, job_config=job_config) #API Requestdf_score = query_job.result()df_score = df_score.to_dataframe()df_score Reference http://googleanalytics360.com/board/view.php?bo_table&#x3D;googleanalytics&amp;wr_id&#x3D;34 https://dev-kani.tistory.com/2","categories":[{"name":"Data Base","slug":"Data-Base","permalink":"https://jmj3047.github.io/categories/Data-Base/"},{"name":"GCP","slug":"Data-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Base/GCP/"}],"tags":[{"name":"Big Query","slug":"Big-Query","permalink":"https://jmj3047.github.io/tags/Big-Query/"},{"name":"BigQueryML","slug":"BigQueryML","permalink":"https://jmj3047.github.io/tags/BigQueryML/"}]},{"title":"Mac VScode GCP 인증 관련 오류","slug":"Mac_GCP_Error","date":"2023-03-08T15:00:00.000Z","updated":"2023-03-10T12:01:39.264Z","comments":true,"path":"2023/03/09/Mac_GCP_Error/","link":"","permalink":"https://jmj3047.github.io/2023/03/09/Mac_GCP_Error/","excerpt":"","text":"gcp내에 있는 예제들을 실행 시킬때면 주피터 노트북으로 gcp를 사용할때 사용자를 인증해야 하는 이슈가 생김 1Error google.auth.exceptions.DefaultCredentialsError: Could not automatically determine credentials. Please set GOOGLE_APPLICATION_CREDENTIALS or explicitly create credentials and re-run the application. mac에서 방법을 찾다가 오류를 해결함 이 노트북의 아래 코드(!gcloud~)를 실행하다가 오류가 남 이 게시물에서 ‘GCP에 데이터 세트 만들고 서비스 계정 생성하기’ 항목의 서비스 계정을 생성하고 json파일을 다운 받아 주었다. 그리고 코드를 수정하여 이렇게 작성했더니 주피터 노트북에서도 bq명령어나 gcloud명령어가 잘 돌아갔다..! 문제는 m1 때문인거 같은데 이거 때문에 vscode까지 지우고 다시 깔았다. 아마 구글이 사용자 인증하는 서비스를 macOS에서는 제공하지 않아서 그런거 같은데.. 물리적으로 json 파일을 저장하지 않고도 credential 사용해서 바로 연결 할수 있는 방법을 찾았다면 추후에 포스팅 하겠다.","categories":[{"name":"Data Base","slug":"Data-Base","permalink":"https://jmj3047.github.io/categories/Data-Base/"},{"name":"GCP","slug":"Data-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Base/GCP/"}],"tags":[{"name":"GCP","slug":"GCP","permalink":"https://jmj3047.github.io/tags/GCP/"},{"name":"Auth","slug":"Auth","permalink":"https://jmj3047.github.io/tags/Auth/"},{"name":"Error","slug":"Error","permalink":"https://jmj3047.github.io/tags/Error/"}]},{"title":"BQML을 이용한 게임유저 경향 모델링","slug":"Game_Modeling","date":"2023-03-07T15:00:00.000Z","updated":"2023-03-09T04:15:32.415Z","comments":true,"path":"2023/03/08/Game_Modeling/","link":"","permalink":"https://jmj3047.github.io/2023/03/08/Game_Modeling/","excerpt":"","text":"개요 빅쿼리 ML을 사용하여 다양한 머신러닝 모델을 돌리기 GA4와 빅쿼리 연동 시 추출되는 데이터들을 정제해서 머신러닝 훈련데이터로 만들기 각 모델의 평가, 파라미터들을 알아보고 조정해보기 목표 앱 설치 후 첫 24시간 동안의 사용자 활동을 기반으로 하는 “Flood It!” 데이터 세트를 사용하여 다양한 분류 모델을 시도하여 이탈 성향(1) 또는 이탈하지 않을 성향(0)을 예측 비용 BigQuery 가격 분석 가격 책정: SQL 쿼리, 사용자 정의 함수, 스크립트, 테이블을 스캔하는 DML(데이터 조작 언어) 및 DDL(데이터 정의 언어)문을 포함한 쿼리를 처리할 때 발생하는 비용 주문분석형 가격책정 (주문형)쿼리: 5$ per TB, 매월 1TB까지는 무료 정액제 월간 정액제: 슬롯 100개당 2000$ 연간 정액제: 슬롯 100개당 1700$(달마다) 스토리지 가격 책정: BigQuery에 로드한 데이터를 저장하는 데 드는 비용 BigQueyrML 가격 무료사용량 한도 스토리지: 매월 10GB 쿼리(분석): 매월 처리되는 쿼리 데이터중 최초 1TB는 무료 BigQuery Storage Write API: 매월 처음 2TB는 무료 BigQuery ML CREATE MODEL쿼리: 매월 10GB까지는 CREATE MODEL 문이 포함된 쿼리 데이터가 무료로 처리 주문형 가격 데이터 학습 데이터로 정제 하기 앱으로 복귀할 가능성이 없는 사용자를 필터링합니다. 사용자 인구통계 데이터에 대한 특성을 만듭니다. 사용자 행동 데이터에 대한 특성을 만듭니다. 인구통계 데이터 및 행동 데이터를 결합하면 더 효과적인 예측 모델을 만드는 데 도움이 된다. 처리 후 학습 데이터의 각 행은 user_pseudo_id 열로 식별된 순 사용자의 데이터를 나타낸다. 전체 데이터 조회→ GA4에서 넘어온 데이터 형식(스키마 및 각 열에 대한 세부 정보) 12345SELECT *FROM `firebase-public-project.analytics_153293282.events_*` TABLESAMPLE SYSTEM (1 PERCENT) 총 15000명의 유저와 5.7만개의 이벤트가 있는걸 볼수 있음 12345SELECT COUNT(DISTINCT user_pseudo_id) as count_distinct_users, COUNT(event_timestamp) as count_eventsFROM `firebase-public-project.analytics_153293282.events_*` 학습 데이터로 정제하기 User ID: 고유한 사용자 ID User demographic data: 인구 통계 학적 데이터 User behavioral data: 행동 데이터 Churned: 예측하고자 하는 실제 라벨(1&#x3D;이탈, 0&#x3D;귀환) STEP 1: 각 유저에 대한 라벨 식별 사용자 이탈을 정의하는 방법에는 여러 가지가 있지만, 이 노트북에서는 사용자가 앱을 처음 사용한 후 24시간이 지나도 다시 앱을 사용하지 않는 사용자를 1일 이탈로 예측 → 사용자가 앱에 처음 참여한 후 24시간이 지난 후를 기준으로 합니다: 사용자가 그 이후 이벤트 데이터를 표시하지 않는 경우, 해당 사용자는 탈퇴한 것으로 간주됩니다. 사용자가 그 이후 이벤트 데이터 포인트가 하나 이상 있으면, 해당 사용자는 귀환한 것으로 간주됩니다. 앱에서 단 몇 분만 사용한 후 다시 돌아올 가능성이 낮은 사용자를 제거하고자 할 수도 있는데, 이를 “bouncing”이라고도 합니다. 예를 들어, 앱을 최소 10분 이상 사용한 사용자(이탈하지 않은 사용자)를 대상으로만 모델을 구축하고자 한다고 가정해 보겠습니다. 따라서 이 노트북의 이탈한 사용자에 대한 업데이트된 정의 “앱에서 10분 이상 시간을 보냈지만 앱을 처음 사용한 후 24시간이 지난 후 다시는 앱을 사용하지 않은 모든 사용자” 1234567891011121314151617181920212223242526272829303132333435363738394041424344CREATE OR REPLACE VIEW `bqmlga4.returningusers` AS ( WITH firstlasttouch AS ( SELECT user_pseudo_id, MIN(event_timestamp) AS user_first_engagement, MAX(event_timestamp) AS user_last_engagement FROM `firebase-public-project.analytics_153293282.events_*` WHERE event_name=&quot;user_engagement&quot; GROUP BY user_pseudo_id ) SELECT user_pseudo_id, user_first_engagement, user_last_engagement, EXTRACT(MONTH from TIMESTAMP_MICROS(user_first_engagement)) as month, EXTRACT(DAYOFYEAR from TIMESTAMP_MICROS(user_first_engagement)) as julianday, EXTRACT(DAYOFWEEK from TIMESTAMP_MICROS(user_first_engagement)) as dayofweek, #add 24 hr to user&#x27;s first touch (user_first_engagement + 86400000000) AS ts_24hr_after_first_engagement,#churned = 1 if last_touch within 24 hr of app installation, else 0IF (user_last_engagement &lt; (user_first_engagement + 86400000000), 1, 0 ) AS churned,#bounced = 1 if last_touch within 10 min, else 0IF (user_last_engagement &lt;= (user_first_engagement + 600000000), 1, 0 ) AS bounced, FROM firstlasttouch GROUP BY 1,2,3 );SELECT * FROM `bqmlga4.returningusers` Churned 열의 경우 첫 24시간이 지난 후에 액션을 수행하면 Churned &#x3D;0이 되고 그렇지 않으면 마지막 액션이 첫 24시간 이내에만 이루어진 경우 Churned&#x3D;1이 된다. Bounced 열의 경우 사용자의 마지막 동작이 앱을 처음 터치한 후 10분 이내인 경우 bounced &#x3D; 1, 그렇지 않으면 bounced&#x3D;0 이 15000명의 사용자 중 몇명이 이탈했다가 다시 돌아왔는지 확인 12345678SELECT bounced, churned, COUNT(churned) as count_usersFROM bqmlga4.returningusersGROUP BY 1,2ORDER BY bounced 15000명의 사용자를 기준으로 5,557명(41%)의 사용자가 앱을 처음 사용한 후 10분 이내에 이탈했지만 나머지 8,031명의 사용자 중 1,883(23%)이 24시간 후에 이탈한 것을 확인 학습 데이터의 경우 bounce&#x3D;0인 데이터만 사용함 → 이미 귀환한 유저에 대해서는 할 필요가 없기 때문 12345SELECT COUNTIF(churned=1)/COUNT(churned) as churn_rateFROM bqmlga4.returningusersWHERE bounced = 0 STEP 2: 각 사용자에 대한 인구 통계 데이터 추출 앱정보, 기기, 이커머스, 이벤트 파라미터, 지역 등 사용자에 대한 인구통계학적 데이터 추출 자체 데이터 세트를 사용하고 있고 조인 가능한 first-party data가 있는 경우 이 섹션은 GA4에서 쉽게 사용할 수 없는 각 사용자에 대한 추가 속성을 추가할 수 있는 좋은 기회 인구통계는 변경가능성이 있음. 간단하게 설명하기 위해 사용자가 앱에 처음접속 했을때 구글 애널리틱스 4가 제공하는 인구통계학적 정보를 MIN(event_timestamp)로 표시된대로만 사용 123456789101112131415161718192021CREATE OR REPLACE VIEW bqmlga4.user_demographics AS ( WITH first_values AS ( SELECT user_pseudo_id, geo.country as country, device.operating_system as operating_system, device.language as language, ROW_NUMBER() OVER (PARTITION BY user_pseudo_id ORDER BY event_timestamp DESC) AS row_num FROM `firebase-public-project.analytics_153293282.events_*` WHERE event_name=&quot;user_engagement&quot; ) SELECT * EXCEPT (row_num) FROM first_values WHERE row_num = 1 );SELECT *FROM bqmlga4.user_demographics STEP 3: 각 사용자에 대한 행동 데이터 추출 첫 24시간 동안의 사용자 활동을 기반으로 해당 사용자의 이탈 또는 재방문 여부를 예측 → 첫 24시간 행동 데이터만 학습 시켜야 함 ‘user_first_engagement’에서 첫 인게이지먼트의 월 또는 일과 같은 추가 시간 관련 기능을 추출할 수도 있음 우선 event_name을 기준으로 데이터 세트에 존재하는 모든 고유 이벤트를 탐색 → 총 37개의 이벤트 12345678SELECT event_name, COUNT(event_name) as event_countFROM `firebase-public-project.analytics_153293282.events_*`GROUP BY 1ORDER BY event_count DESC 여기서 user_engagement, level_start_quickplay, level_end_quickplay, level_complete_quickplay, level_reset_quickplay, post_score, spend_virtual_currency, ad_reward, challenge_a_friend, completed_5_levels, use_extra_steps → 이 피처들만 가지고 각각 유저가 몇번이나 이 이벤트를 발생시켰는지 확인 자체 데이터 세트를 사용하는 경우 집계 및 추출할 수 있는 이벤트 유형이 다를 수 있음. 앱이 GA에 매우 다른 event_names를 전송할수 있으므로 시나리오 가장 적합한 이벤트를 사용해야 함 1234567891011121314151617181920212223242526272829303132333435363738CREATE OR REPLACE VIEW bqmlga4.user_aggregate_behavior AS (WITH events_first24hr AS ( #select user data only from first 24 hr of using the app SELECT e.* FROM `firebase-public-project.analytics_153293282.events_*` e JOIN bqmlga4.returningusers r ON e.user_pseudo_id = r.user_pseudo_id WHERE e.event_timestamp &lt;= r.ts_24hr_after_first_engagement )SELECT user_pseudo_id, SUM(IF(event_name = &#x27;user_engagement&#x27;, 1, 0)) AS cnt_user_engagement, SUM(IF(event_name = &#x27;level_start_quickplay&#x27;, 1, 0)) AS cnt_level_start_quickplay, SUM(IF(event_name = &#x27;level_end_quickplay&#x27;, 1, 0)) AS cnt_level_end_quickplay, SUM(IF(event_name = &#x27;level_complete_quickplay&#x27;, 1, 0)) AS cnt_level_complete_quickplay, SUM(IF(event_name = &#x27;level_reset_quickplay&#x27;, 1, 0)) AS cnt_level_reset_quickplay, SUM(IF(event_name = &#x27;post_score&#x27;, 1, 0)) AS cnt_post_score, SUM(IF(event_name = &#x27;spend_virtual_currency&#x27;, 1, 0)) AS cnt_spend_virtual_currency, SUM(IF(event_name = &#x27;ad_reward&#x27;, 1, 0)) AS cnt_ad_reward, SUM(IF(event_name = &#x27;challenge_a_friend&#x27;, 1, 0)) AS cnt_challenge_a_friend, SUM(IF(event_name = &#x27;completed_5_levels&#x27;, 1, 0)) AS cnt_completed_5_levels, SUM(IF(event_name = &#x27;use_extra_steps&#x27;, 1, 0)) AS cnt_use_extra_steps,FROM events_first24hrGROUP BY 1 );SELECT *FROM bqmlga4.user_aggregate_behavior 이 단계는 동작 수행 빈도 외에도 사용자가 사용한 게임 내 화폐의 총액이나 앱과 더 관련이 있을수 있는 특정 앱별 마일스톤(예: 특정 임계값의 경험치 획득 또는 5회 이상 레벨업)에 도달했는지 여부와 같은 다른 행동 특징을 포함할수 있다는 점에 유의. STEP 4: 세 데이터 결합하여 학습데이터 구축 최종 학습 데이터 베이스 구축: 여기어세 bounce&#x3D;0을 지정하여 앱 사용후 처음 10분 이내에 bounce하지 않은 사용자로만 학습 데이터를 제할할 수도 있음. 12345678910111213141516171819202122232425262728293031323334353637CREATE OR REPLACE VIEW bqmlga4.train AS ( SELECT dem.*, IFNULL(beh.cnt_user_engagement, 0) AS cnt_user_engagement, IFNULL(beh.cnt_level_start_quickplay, 0) AS cnt_level_start_quickplay, IFNULL(beh.cnt_level_end_quickplay, 0) AS cnt_level_end_quickplay, IFNULL(beh.cnt_level_complete_quickplay, 0) AS cnt_level_complete_quickplay, IFNULL(beh.cnt_level_reset_quickplay, 0) AS cnt_level_reset_quickplay, IFNULL(beh.cnt_post_score, 0) AS cnt_post_score, IFNULL(beh.cnt_spend_virtual_currency, 0) AS cnt_spend_virtual_currency, IFNULL(beh.cnt_ad_reward, 0) AS cnt_ad_reward, IFNULL(beh.cnt_challenge_a_friend, 0) AS cnt_challenge_a_friend, IFNULL(beh.cnt_completed_5_levels, 0) AS cnt_completed_5_levels, IFNULL(beh.cnt_use_extra_steps, 0) AS cnt_use_extra_steps, ret.user_first_engagement, ret.month, ret.julianday, ret.dayofweek, ret.churned FROM bqmlga4.returningusers ret LEFT OUTER JOIN bqmlga4.user_demographics dem ON ret.user_pseudo_id = dem.user_pseudo_id LEFT OUTER JOIN bqmlga4.user_aggregate_behavior beh ON ret.user_pseudo_id = beh.user_pseudo_id WHERE ret.bounced = 0 );SELECT *FROM bqmlga4.train 학습데이터로 빅쿼리 ML 학습하기 이진분류 작업이므로 간단하게 logistic regression으로 시작할수 있지만 다른 모델도 사용 가능하다. M o d e l Advantage Disadvantage Logistic Regression LOGISTIC_REG 다른 타입에 비해 학습 시간이 빠름 모델 성능이 조금 떨어짐 XGBoost BOOSTED_TREE_CLASSIFIER 높은 모델 수행 능력 feature importance를 검사할수 있다, LOGISTIC_REG 에 비해 학습 시간이 느림 Deep Neural Networks DNN_CLASSIFIER 높은 모델 수행 능력 LOGISTIC_REG 에 비해 학습 시간이 느림 AutoML AUTOML_CLASSIFIER 매우 높은 모델 수행 능력 적어도 몇시간 정도 훈련시간이 걸리고 모델이 어떻게 작동하는지 설명하기 쉽지 않음 훈련, 테스트 셋으로 분할하지 않아도 CREATE MODEL문을 실행하면 BigQuery ML이 자동적으로 학습하기 때문에 학습 후 바로 모델을 평가 할수 있음 하이퍼 파라미터 튜닝: 각 모델에 대한 하이퍼파라미터를 튜닝할수 도 있음(link) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354CREATE OR REPLACE MODEL bqmlga4.churn_logregOPTIONS( MODEL_TYPE=&quot;LOGISTIC_REG&quot;, INPUT_LABEL_COLS=[&quot;churned&quot;]) ASSELECT *FROM bqmlga4.train---CREATE OR REPLACE MODEL bqmlga4.churn_xgbOPTIONS( MODEL_TYPE=&quot;BOOSTED_TREE_CLASSIFIER&quot;, INPUT_LABEL_COLS=[&quot;churned&quot;]) ASSELECT * EXCEPT(user_pseudo_id)FROM bqmlga4.train---CREATE OR REPLACE MODEL bqmlga4.churn_dnnOPTIONS( MODEL_TYPE=&quot;DNN_CLASSIFIER&quot;, INPUT_LABEL_COLS=[&quot;churned&quot;]) ASSELECT * EXCEPT(user_pseudo_id)FROM bqmlga4.train---CREATE OR REPLACE MODEL bqmlga4.churn_automlOPTIONS( MODEL_TYPE=&quot;AUTOML_CLASSIFIER&quot;, INPUT_LABEL_COLS=[&quot;churned&quot;], BUDGET_HOURS=1.0) ASSELECT * EXCEPT(user_pseudo_id)FROM bqmlga4.train AutoML AutoML Tables를 사용하면 구조화된 데이터에 대한 최신 머신러닝 모델을 속도와 규모를 대폭 향상 시켜 자동으로 구축할 수 있음. AutoML Tables는 구글의 모델 집단에서 구조화된 데이터를 자동으로 검색하여 간단한 데이터 집합을 위한 선형&#x2F;로지스틱 휘귀모델부터 더 크고 복잡한 데이터 집합을 위한 고급 심층, 앙상블 및 아키텍쳐 검색 방법에 이르기까지 사용자의 요구에 가장 적합한 모델을 찾아줌. BUDGET_HOURS 매개변수는 AutoML 테이블 학습을 위한 것으로 시간 단위로 지정됨. 기본값은 1.0이며 1.0에서 72.0사이여야 함. 모델 평가Logistic Regression1234SELECT *FROM ML.EVALUATE(MODEL bqmlga4.churn_logreg) XGBoost1234SELECT *FROM ML.EVALUATE(MODEL bqmlga4.churn_xgb) DNN1234SELECT *FROM ML.EVALUATE(MODEL bqmlga4.churn_dnn) AutoML1234SELECT *FROM ML.EVALUATE(MODEL bqmlga4.churn_automl) 모델 예측 이탈 성향에 대한 예측을 할 수 있음 12345SELECT *FROM ML.PREDICT(MODEL bqmlga4.churn_logreg, (SELECT * FROM bqmlga4.train)) #can be replaced with a test dataset 성향 모델링에서 가장 중요한 출력은 행동이 발생할 확률. 밑의 코드는 사용자가 24시간 후에 재방문할 확률을 반환 → 확률이 높고 1에 가까울수록 사용자가 이탈할 가능성이 높고, 0에 가까울수록 사용자가 재방문할 가능성이 높음 123456789SELECT user_pseudo_id, churned, predicted_churned, predicted_churned_probs[OFFSET(0)].prob as probability_churned FROM ML.PREDICT(MODEL bqmlga4.churn_logreg, (SELECT * FROM bqmlga4.train)) #can be replaced with a proper test dataset 빅쿼리 밖으로 예측 결과 export Bigquery Storage API를 사용하여 데이터를 Pandas 데이터 프레임으로 내보낼수 있음(문서 및 코드 샘플) 다른 BigQuery 클라이언트 라이브러리를 사용할수도 있음. 별도의 서비스에서 사용할 수 있도록 예측 테이블을 Google 클라우드 스토리지(GCS)로 직접 보낼수도 있음 → 가장 쉬운 방법은 SQL을 사용하여 GCS로 직접 보내는 것 123456789101112EXPORT DATA OPTIONS (uri=&quot;gs://mybucket/myfile/churnpredictions.csv&quot;, format=CSV) AS SELECT user_pseudo_id, churned, predicted_churned, predicted_churned_probs[OFFSET(0)].prob as probability_churnedFROM ML.PREDICT(MODEL bqmlga4.churn_logreg, (SELECT * FROM bqmlga4.train)) #can be replaced with a proper test dataset Reference https://cloud.google.com/architecture/propensity-modeling-gaming?hl=ko https://github.com/GoogleCloudPlatform/analytics-componentized-patterns/tree/master/gaming/propensity-model/bqml","categories":[{"name":"Data Base","slug":"Data-Base","permalink":"https://jmj3047.github.io/categories/Data-Base/"},{"name":"GCP","slug":"Data-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Base/GCP/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"Big Query","slug":"Big-Query","permalink":"https://jmj3047.github.io/tags/Big-Query/"},{"name":"ML Process","slug":"ML-Process","permalink":"https://jmj3047.github.io/tags/ML-Process/"},{"name":"BigQueryML","slug":"BigQueryML","permalink":"https://jmj3047.github.io/tags/BigQueryML/"}]},{"title":"로지스틱 회귀 모델 평가시 나오는 용어들 정리","slug":"Logisitic_Eval_words","date":"2023-03-05T15:00:00.000Z","updated":"2023-03-08T08:19:08.766Z","comments":true,"path":"2023/03/06/Logisitic_Eval_words/","link":"","permalink":"https://jmj3047.github.io/2023/03/06/Logisitic_Eval_words/","excerpt":"","text":"개요 이전 포스트의 5단계에서 ML 모델을 평가할때 나왔던 지표들에 대한 소개 바이너리 로지스틱 회귀 모델을 사용했을 때 모델의 성능을 평가하는 지표들을 소개 알아야 할 개념True&#x2F;False &amp; Positive&#x2F;Negative 임계값(Threshold) 로지스틱 회귀 값을 이진 카테고리에 매핑하려면 분류 임계값(결정 임계값)을 정의 해야 함 일반적으로 0.5로 잡지만, 데이터나 상황에 따라서 달라질수 있으므로 조정해야 하는 값. 정밀도(Precision) &#x3D; $\\frac{TP}{TP+FP}$ 거짓 양성(FP)이 생성되지 않는 모델의 정밀도는 1.0 재현율(Recall) &#x3D; $\\frac{TP}{TP+FN}$ 거짓 음성(FN)을 생성하지 않는 모델의 재현율은 1.0 실제 양성 중 정확히 양성이라고 식별된 사례의 비율 정확도(Accuracy) &#x3D; $\\frac{올바른 예측값}{전체 예측 값}$ 이진분류에서 정확도 &#x3D; $\\frac{TP+TN}{TP+TN+FP+FN}$ 비 공식적으로 모델이 올바르게 예측한 비율 클래스간 데이터 차이가 있다면 정확하지 않을 가능성이 높음 ROC curve 모든 분류 임계값에서 분류 모델의 성능을 보여주는 그래프 참 양성률(TPR)과 거짓양성률(FPR)을 매개변수로 표시한다. 분류 임계값을 낮추면 더 많은 항목이 양수로 분류 되므로 거짓양성과 참 양성이 모두 증가한다. ROC 곡선의 포인트를 계사나기 위해 분류 입곗값이 다른 여러차례의 로지스틱 회귀모델을 평가했지만 이는 비효율적 → 이러한 정보제공을 할수 있는 효율적인 정렬기반 알고리즘인 AUC를 사용하는 이유 AUC: ROC 곡선 아래의 영역 ROC 곡선의 적분값 0부터 1사의 값 절댓값이 아닌 예측의 순위를 측정하므로 규모 불변 선택한 분유 임곗값과 관계 없이 모델의 예측 품질을 측정하기에 분류 기준 불변 주의 사항 확장 불변이 항상 바람직한것은 아님 → 잘 보정된 확률 출력이 필요한 경우가 있는데 AUC는 이를 알려주지 않음 분류 임계값 불변이 항상 바람직한 것은 아님 → 거짓음성 대비 거짓양성의 비용이 큰 경우 한 자기 유형의 분류 오류를 최소화 하는 것이 중요함. 예를 들어 스팸 감지를 실행할때 거짓 양성을 최소화하는 것이 중요할 수 있음.(이렇게 하면 거짓 음성이 크게 증가하더라도 상관 없음) AUC는 이 유형의 최적화에 유용한 측정항목이 아님. GCP 공식 설명 Aggregate Metrics For binary classification models, all metrics reflect the values computed when threshold is set to 0.5. 이진 분류 모델의 경우 모든 지표는 임계값을 0.5로 설정했을 때 계산된 값을 반영합니다. Threshold : For binary classification models, this is the positive class threshold. 이진 분류 모델의 경우 이는 양성 클래스의 임계값을 뜻합니다. Precision : Fraction of predicted positives that were actual positives. 예측된 양성 반응 중 실제 양성 반응으로 판명된 비율입니다. → 학습 모델이 판단한 양성 비율 Recall : Fraction of actual positives that were predicted positives. 예측된 양성 반응 중 실제 양성 반응의 비율입니다. → 실제 양성이라고 나온 비율 Accuracy : Fraction of predictions given the correct label. 올바른 레이블이 주어진 예측의 비율입니다. → precision과 recall 을 비교했을 때 학습 모델이 맞춘 비율 F1 score : Harmonic mean of precision and recall. 정확도 및 회수율의 조화 평균입니다. Precision 과 Recall은 서로 Trade-off 관계를 가지면서 접근하는 방식도 Precision은 모델의 예측, Recall은 정답 데이터 기준이므로 서로 상이 합니다. 하지만 두 지표 모두 모델의 성능을 확인하는 데 중요하므로 둘 다 사용되어야 합니다. 따라서 두 지표를 평균값으로 통해 하나의 값으로 나타내는 방법으로 F1 score를 사용합니다. 극단적으로 Precision과 Recall 중에 한쪽이 1에 가깝고 한쪽이 0에 가까운 경우 산술 평균과 같이 0.5가 아니라 0에 가깝도록 만들어 줍니다. 따라서 F1 score를 높이려면 Precision, Recall이 균일한 값이 필요 하기 때문에 두 지표 성능을 모두 높일 수 있도록 해야 합니다. $F_1 &#x3D; 2*\\frac{precision X recall}{precision + recall}$ Log loss : A measure for model performance between 0 (perfect) and 1. The greater the log-loss, the greater the predicted probability diverges from the actual label. 0(완벽)에서 1 사이의 모델 성능 측정값입니다. 로그 손실이 클수록 예측 확률이 실제 레이블과 차이가 커집니다. ROC AUC : Area under the receiver operating characteristic curve. Score threshold Positive class threshold: Predictions above the threshold are classified as positive. 임계값을 초과하는 예측은 양수로 분류됩니다. Positive class &gt;50K Negative class &lt;&#x3D;50K Precision : Fraction of predicted positives that were actual positives. 예측된 양성 반응 중 실제 양성 반응으로 판명된 비율입니다. Recall: Fraction of actual positives that were predicted positives. 예측된 양성 반응 중 실제 양성 반응의 비율입니다. Accuracy : Fraction of actual positives that were predicted positives. 올바른 레이블이 주어진 예측의 비율입니다. F1 score : Harmonic mean of precision and recall. 정밀도 및 회수율의 조화 평균입니다. Precision-recall by threshold: Shows how your model performs on the top-scored label along the full range of confidence threshold values. A higher confidence threshold produces fewer false positives, which increases precision. A lower confidence threshold produces fewer false negatives, which increases recall. 전체 신뢰도 임계값 범위에서 가장 높은 점수를 받은 레이블에 대한 모델의 성능을 표시합니다. 신뢰도 임계값이 높을수록 false positive 값이 줄어들어 정밀도가 높아집니다. 신뢰도 임계값이 낮을수록 false negative 값이 줄어들어 회수율이 높아집니다. Precision-recall curve: Shows the trade-off between precision and recall at different confidence thresholds. A lower threshold results in higher recall but typically lower precision, while a higher threshold results in lower recall but typically with higher precision. 서로 다른 신뢰 임계값에서 정밀도와 리콜 간의 균형을 표시합니다. 임계값이 낮을수록 회수율은 높지만 일반적으로 정밀도는 낮으며 임계값이 높을수록 회수율은 낮아지지만 일반적으로 정밀도가 높습니다. ROC curve: The receiver operating characteristic (ROC) curve shows the trade-off between true positive rate and false positive rate. A lower threshold results in a higher true positive rate (and a higher false positive rate), while a higher threshold results in a lower true positive rate (and a lower false positive rate) receiver operating characteristic(ROC) 곡선은 true positive의 비율과 false positive의 비율 사이의 균형을 보여줍니다. 임계값이 낮을수록 true positive의 비율(및 false positive의 비율)이 높아지고, 임계값이 높을수록 true positive의 비율(및 false positive의 비율)이 낮아집니다 Confusion matrix This table shows how often the model classified each label correctly (in blue), and which labels were most often confused for that label (in gray). 이 표는 모델이 각 레이블을 올바르게 분류한 빈도(파란색)와 해당 레이블과 가장 자주 혼동되는 레이블(회색)을 보여줍니다. Reference https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall?hl=ko https://developers.google.com/machine-learning/crash-course/classification/true-false-positive-negative?hl=ko https://gaussian37.github.io/ml-concept-ml-evaluation/#f1-score-1","categories":[{"name":"Data Base","slug":"Data-Base","permalink":"https://jmj3047.github.io/categories/Data-Base/"},{"name":"GCP","slug":"Data-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Base/GCP/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"Big Query","slug":"Big-Query","permalink":"https://jmj3047.github.io/tags/Big-Query/"}]},{"title":"Python으로 kaggle 데이터 GCP에 적재","slug":"Kaggle_GCP","date":"2023-03-02T15:00:00.000Z","updated":"2023-03-02T09:16:22.309Z","comments":true,"path":"2023/03/03/Kaggle_GCP/","link":"","permalink":"https://jmj3047.github.io/2023/03/03/Kaggle_GCP/","excerpt":"","text":"요약 Kaggle 데이터 다운로드 GCP에 데이터 세트 만들고 서비스 계정 생성하기 Python-BigQuery 연결 후 데이터 조회 데이터 적재 하기 Kaggle 데이터 다운로드 kaggle을 설치한다 1!pip install kaggle kaggle의 key를 받아온다 123!mkdir ~/.kaggle!echo &#x27;&#123;&quot;username&quot;:&quot;your_id&quot;,&quot;key&quot;:&quot;your_key&quot;&#125;&#x27; &gt; ~/.kaggle/kaggle.json!chmod 600 ~/.kaggle/kaggle.json kaggel TOKEN 받아오기 kaggle에 접속한다음 프로필을 선택하고 Account를 누른다 화면을 내리면 API 탭을 찾을수 있다. Create New API Token을 누르고 다운 kaggle.json 파일을 받아줍니다. 한번 발급 받으면 이전의 것은 알려주지 않기 때문에 잊어버렸다면 Expire API TOKEN 으로 모두 지고 새로운 토큰을 받는다. kaggle에서 원하는 데이터를 다운 받아준다 → competitions에서 원하는 competition을 선택하고 data 탭으로 가면 다운 받을 수 있는 api가 있다. GCP에 데이터 세트 만들고 서비스 계정 생성하기 GCP에 프로젝트를 만들고 새로운 데이터 세트를 만들어 준다 중간에 Default table expiration 을 클릭하면 테이블이 며칠 후에 만료되는지도 설정 가능하다 서비스 계정 생성 GCP 좌측 상단 ‘탐색 메뉴’ 클릭후 ‘IAM 및 관리자’의 ‘서비스 계정’으로 이동 ‘+ 서비스 계정 만들기’ 클릭 후 서비스 계정 만들기 진행 서비스 계정 키 생성 후 JSON 파일 추출 Email에 생성된 서비스 계정 클릭 → 페이지 상단에 KEYS → ADD KEY → Create new key config 폴더 생성 후 해당 경로에 json 파일 다운로드 서비스 계정에 빅쿼리 관련 역할 추가 GCP 좌측 상단 ‘탐색 메뉴’ 클릭후 ‘IAM 및 관리자’의 ‘IAM’으로 이동 ‘추가’ 클릭 - 생성된 서비스 계정 이메일 추가 및 ‘BigQuery 관리자’ 역할 선택 후 저장 Python-BigQuery 연결 후 데이터 조회 구글 클라우드 빅쿼리 클라이언트 설치 1!pip install google-cloud-bigquery 서비스 계정 키 설정 → 빅쿼리 클라이언트 정의 → 데이터 조회 쿼리 실행 데이터 적재 하기 데이터를 불러온다 1234567import pandas as pdBASE_DIR = &quot;./&quot;train = pd.read_csv(BASE_DIR + &#x27;train.csv&#x27;)test = pd.read_csv(BASE_DIR + &#x27;test.csv&#x27;)train.shape, test.shape 1((3000888, 6), (28512, 5)) 데이터를 적재 한다 12345678910from google.oauth2 import service_accountimport pandas_gbqcd = service_account.Credentials.from_service_account_file(&quot;./config/my-project-230227-c3691227da1d.json&quot;)project_id = &#x27;my-project-230227&#x27;train_table = &#x27;kaggle_data.train&#x27;test_table = &#x27;kaggle_data.test&#x27;train.to_gbq(train_table,project_id,if_exists=&#x27;replace&#x27;,credentials=cd) test.to_gbq(test_table,project_id,if_exists=&#x27;replace&#x27;,credentials=cd) print(&#x27;migration complete&#x27;) GCP에 들어가서 확인해 본다 이렇게 테이블을 gcp에 적재하면 데이터들을 다양한 방면으로 사용이 가능하다: Looker Studio를 이용한 시각화 Reference https://dschloe.github.io/gcp/bigquery/01_settings&#x2F;python_bigquery&#x2F; https:&#x2F;&#x2F;velog.io&#x2F;@skyepodium&#x2F;Kaggle-API-사용법 https://wooiljeong.github.io/python/python-bigquery/","categories":[{"name":"Data Base","slug":"Data-Base","permalink":"https://jmj3047.github.io/categories/Data-Base/"},{"name":"GCP","slug":"Data-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Base/GCP/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/tags/Python/"},{"name":"Big Query","slug":"Big-Query","permalink":"https://jmj3047.github.io/tags/Big-Query/"},{"name":"kaggle","slug":"kaggle","permalink":"https://jmj3047.github.io/tags/kaggle/"}]},{"title":"GCP - Looker Studio 연결하여 대시보드 작성","slug":"GCP_LookerStudio","date":"2023-03-01T15:00:00.000Z","updated":"2023-03-02T04:06:51.338Z","comments":true,"path":"2023/03/02/GCP_LookerStudio/","link":"","permalink":"https://jmj3047.github.io/2023/03/02/GCP_LookerStudio/","excerpt":"","text":"개요 GCP - Looker Studio 연결해서 대시보드 작성하기 bigquery-public-data.ml_datasets.census_adult_income 데이터 사용 목표 대시보드로 데이터를 시각화 하여 인사이트를 도출해본다. 개인의 연간 소등이 50,000달러 이상인지 예측하기 를 위해 지표들의 상관관계를 확인해본다. public-dataset 가져오기 이전 포스트의 [0단계]https://jmj3047.github.io/2023/02/28/BQML_Pred/)와 동일하다. 데이터 확인 및 EDA Looker Studio를 사용하여 데이터를 확인하고 EDA를 하는 방법 전체 데이터 가져와서 차트 만들기 Big Query를 사용하여 SQL로 데이터를 정제한 후 Looker Studio에 SQL로 차트 만들기 전체 데이터를 가져와서 차트 만들기 Big Query를 사용할 필요가 없다. 버튼으로 간단하게 조작이 가능하다. 지표당 간단한 합계, 평균, 집계, 최솟값, 최대값, 중앙값, 표준편차, 분산 까지의 연산은 버튼식으로 수정할수 있으며, 함수 또한 필드추가의 항목으로 만드는 것이 가능하다. GCP에 데이터 세트가 업로드 되어 있다면, Looker Studio를 바로 열어서 데이터 추가 버튼을 누른다. Big Query를 누르고 GCP 데이터 세트를 찾는다. public data set을 사용하는 경우라면 공개 데이터 집합으로 들어가야 한다. 추가 버튼을 누르면 화면 왼쪽에 데이터 세트의 이름과 그 안의 스키마 정보들이 뜬다. 원하는 대시보드를 만들기 위해 차트나 표를 추가하여 자유롭게 대시보드를 구성하면 된다. 예시: https://lookerstudio.google.com/reporting/10e2c716-6289-4e07-a2de-da6cdac415b6 Big Query를 사용하여 SQL로 데이터를 정제한 후 Looker Studio에 SQL로 차트 만들기 표에 있는 지표를 가공하여 보고 싶을때 유용하다. 버튼으로 조작하는 것보다 더 많은 지표 표현이 가능하다. 유지 보수의 어려움이 있다. 사용할 데이터의 지표들을 확인해보면 아래처럼 나타낼 수 있다. age(나이): 개인의 나이를 연단위로 나타냅니다 workclass(노동 계급): 개인의 고용형태 Private, ?, Local-gov , Self-emp-inc, Federal-gov, State-gov, Self-emp-not-inc, Never-worked, Witout-pay functional_weight: 일련의 관측결과를 바탕으로 인구조사국이 부여하는 개인의 가중치 education: 개인의 최종학력 education_num: 교육수준을 숫자로 범주화 하여 열거 합니다. 숫자가 높을수록 개인의 교육수준이 높습니다. 11: Assoc_voc: 전문학교 준학사 13: Bachelors: 학사 9: HS-grad: 고등학교 졸업 marital_status: 개인의 결혼 여부 입니다. Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse occupation: 개인의 직업입니다. relationship: 가정 내 각 개인의 관계입니다. Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried race: 인종을 나타냅니다 White, Asian-Pac-Islander, Amer-Indian-Eskimo, Black, Other sex: 개인의 성별입니다. Female, Male capital_gain: 개인의 자본 이익을 미국 달러로 표기 합니다. capital_loss: 개인의 자본 손실을 미국 달러로 표기 합니다. hours_per_week: 주당 근무시간입니다. native_country: 개인의 출신 국가 입니다. ?,Cambodia,Canada,China,Columbia,Cuba,Dominican-Republic,Ecuador,El-Salvador,England,France,Germany,Greece,Guatemala,Haiti,Holand-Netherlands,Honduras,Hong,Hungary,India,Iran,Ireland,Italy,Jamaica,Japan,Laos,Mexico,Nicaragua,Outlying-US(Guam-USVI-etc),Peru,Philippines,Poland,Portugal,Puerto-Rico,Scotland,South,Taiwan,Thailand,Trinadad&amp;Tobago,United-States,Vietnam,Yugoslavia income_bracket: 개인의 연간 소득이 미화 50,000달러가 넘는지 여부를 나타냅니다 예측을 위한 가설들을 세운다 예시: native_country를 기준으로 평균 주당 근무시간과 개인의 자본 현황의 평균을 보고 싶다. 이러한 데이터는 Looker studio의 기능으로 조회가 불가능하다. → GCP Big Query로 조회하기 실행 쿼리 123456SELECT DISTINCT native_country, AVG(hours_per_week) as avg_hours_per_week, AVG(capital_gain - capital_loss) as avg_capitalFROM `bigquery-public-data.ml_datasets.census_adult_income`GROUP BY 1ORDER BY 1 위 데이터를 Looker Studio로 옮겨서 그래프를 작성해 보자 데이터 추가 → 빅쿼리 → 맞춤 검색어 → 프로젝트를 선택한 후 SQL을 입력해 준다. 시간 데이터가 있을 때는 기간 매개변수 사용 설정을 체크 하면 기간 컨트롤에 대한 부분을 조작할 수 있다. → 하지만 이 데이터에는 기간에 대한 내용은 없으니 추후 작성하는 것으로 한다. 데이터를 추가 하고 차트를 추가하면 대시보드가 완성 된다. Reference https://notebook.community/google/eng-edu/ml/cc/exercises/estimators/ko/intro_to_fairness","categories":[{"name":"Data Base","slug":"Data-Base","permalink":"https://jmj3047.github.io/categories/Data-Base/"},{"name":"GCP","slug":"Data-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Base/GCP/"}],"tags":[{"name":"Big Query","slug":"Big-Query","permalink":"https://jmj3047.github.io/tags/Big-Query/"},{"name":"Looker Studio","slug":"Looker-Studio","permalink":"https://jmj3047.github.io/tags/Looker-Studio/"}]},{"title":"BQML을 이용한 개인 소득 예측","slug":"BQML_Pred","date":"2023-02-27T15:00:00.000Z","updated":"2023-03-09T04:15:53.260Z","comments":true,"path":"2023/02/28/BQML_Pred/","link":"","permalink":"https://jmj3047.github.io/2023/02/28/BQML_Pred/","excerpt":"","text":"개인 연간 소득이 5만 달러 이상인지 예측하기개요 GCP에서 BQML 사용하기 BQML의 로지스틱 회귀 모델 유형으로 supervised learning을 지원하는 기능 사용 바이너리&#x2F;멀티 로지스틱 회귀 모형을 사용하면 값이 두&#x2F;여러 범주 중 하나에 속할지 예측할 수 있다. 데이터를 둘 이상의 범주로 분류하려는 문제 bigquery-public-data.ml_datasets.census_adult_income 데이터 사용 목표 로지스틱 회귀 모델을 만들고 평가한다. 로지스틱 회귀 모델을 사용하여 예측한다. 비용 BigQuery 가격정책 BigQuery ML 가격정책 0단계: public-dataset 가져오기 GCP BigQuery로 이동하여 프로젝트를 만들고, 프로젝트 밑에 +ADD DATA 를 클릭한다 Public Datasets 를 선택한다 데이터 셋중 아무거나 선택 한 후 VIEW DATASET 을 선택하면 새 창이 뜨면서 구글 퍼블릭데이터 셋이 연결 된다. 이번 포스팅에서 다룰 데이터는 bigquery-public-data.ml_datasets.census_adult_income 이다. 아래처럼 창이 뜨면 빅쿼리SQL을 사용할 준비가 다 되었다. 1단계: 데이터 세트 만들기 내 프로젝트를 선택하고 옆에 점세개를 누르고 Create dataset 을 눌러서 데이터 세트를 만든다. 데이터 세트 ID에 census 를 입력한다 리전을 미국(US)로 선택한다 → public dataset이 US멀티 리전에 있기 때문. 같은 리전에 두어야 혼선을 막을수 있다. 만들어진 데이터 셋을 확인할 수 있다. 2단계: 데이터 확인 및 EDA 데이터 확인 age(나이): 개인의 나이를 연단위로 나타냅니다 workclass(노동 계급): 개인의 고용형태 functional_weight: 일련의 관측결과를 바탕으로 인구조사국이 부여하는 개인의 가중치 education: 개인의 최종학력 education_num: 교육수준을 숫자로 범주화 하여 열거 합니다. 숫자가 높을수록 개인의 교육수준이 높습니다. marital_status: 개인의 결혼 여부 입니다. occupation: 개인의 직업입니다. relationship: 가정 내 각 개인의 관계입니다. race: 인종을 나타냅니다 sex: 개인의 성별입니다. capital_gain: 개인의 자본 이익을 미국 달러로 표기 합니다. capital_loss: 개인의 자본 손실을 미국 달러로 표기 합니다. hours_per_week: 주당 근무시간입니다. native_country: 개인의 출신 국가 입니다. income_bracket: 개인의 연간 소득이 미화 50,000달러가 넘는지 여부를 나타냅니다 예측 작업: 개인의 연간 소득이 50,000달러 이상인지 확인해보기 쿼리 실행 123456789101112SELECT age, workclass, marital_status, education_num, occupation, hours_per_week, income_bracketFROM `bigquery-public-data.ml_datasets.census_adult_income`LIMIT 100; 쿼리 실행 결과 분석 income_bracket : &lt;=50K 또는 &gt;50K 값중 하나만 있음 education , education_num → 동일한 데이터가 서로 다른 형식으로 표기되어 있음 functional_weight : 인구 조사 기관에서 특정행이 대표한다고 판단하는 개인의 수 → 우리가 원하는 income 예측과는 관련이 없음. 3단계: 학습 데이터 선택 학습에 필요한 데이터 선택 age: 연령 workclass: 고용형태 marital_status: 결혼여부 education_num: 교육수준 occupation: 직업 hours_per_week: 주당 근무시간 학습 데이터를 컴파일 하는 뷰를 만든다. 1234567891011121314151617CREATE OR REPLACE VIEW `census.input_view` ASSELECT age, workclass, marital_status, education_num, occupation, hours_per_week, income_bracket, CASE WHEN MOD(functional_weight, 10) &lt; 8 THEN &#x27;training&#x27; WHEN MOD(functional_weight, 10) = 8 THEN &#x27;evaluation&#x27; WHEN MOD(functional_weight, 10) = 9 THEN &#x27;prediction&#x27; END AS dataframeFROM `bigquery-public-data.ml_datasets.census_adult_income` education, education_num 등과 같은 중복되는 카테고리를 제외한다. functional_weight 는 income_bracket과 상관이 없는 컬럼이므로 라벨링으로 사용한다. 80%training, 10% 평가, 10% 예측 MOD(X,Y): X를 Y로 나눴을때의 나머지 위 쿼리를 수행시키면 1단계에서 만든 census에 input_view가 만들어졌음을 알수 있다. 4단계: 로지스틱 회귀모델 만들기 Create Model 문을 LOGISTIC_REG 옵션과 함께 사용하면 로지스틱 회귀 모델을 만들고 학습 시킬수 있다. 12345678910111213CREATE OR REPLACE MODEL `census.census_model`OPTIONS ( model_type=&#x27;LOGISTIC_REG&#x27;, auto_class_weights=TRUE, input_label_cols=[&#x27;income_bracket&#x27;] ) ASSELECT * EXCEPT(dataframe)FROM `census.input_view`WHERE dataframe = &#x27;training&#x27; CREATE MODEL문은 SELECT 문의 학습 데이터를 사용하여 모델을 학습 시킨다. OPTIONS 절은 모델 유형과 학습 옵션을 지정한다. 여기서 LOGISTIC_REG 옵션은 로지스틱 회귀 모델 유형을 지정한다. 바이너리 로지스틱 회귀모델과 멀티클래스 로지스틱 회귀모델을 구분하여 지정할 필요는 없다. BigQuery ML은 라벨열 고유 값 수 기반으로 학습 대상을 결정할 수 있다. → 고유 값 수에 따라서 자동으로 선택하여 학습함 input_label_cols 옵션은 SELECT 문에서 라벨 열로 사용할 열을 지정한다. 여기서 라벨 열은 income_bracket 이므로 모델은 각 행에 있는 다른 값을 기반으로 income_bracket의 두 값 중 가장 가능성이 높은 값을 학습 한다. SELECT 문은 2단계의 뷰를 사용한다. 이 뷰에는 모델 학습용 특성 데이터가 포함된 열만 포함된다. WHERE 절은 학습데이터 프레임에 속하는 행만 학습 데이터에 포함되도록 input_view의 행을 필터링 한다. CREATE MODEL 쿼리 실행1234567891011121314CREATE OR REPLACE MODEL `census.census_model`OPTIONS ( model_type=&#x27;LOGISTIC_REG&#x27;, auto_class_weights=TRUE, data_split_method=&#x27;NO_SPLIT&#x27;, input_label_cols=[&#x27;income_bracket&#x27;], max_iterations=15) ASSELECT * EXCEPT(dataframe)FROM `census.input_view`WHERE dataframe = &#x27;training&#x27; SCHEMA 탭은 BigQuery ML이 로지스틱 회귀를 수행하는데 사용한 속성을 나열한다. 5단계: ML.EVALUATE 함수를 사용하여 모델 평가 4단계의 CREATE MODEL 문을 수행했을때 모델의 결과 창에 EVALUATION 탭으로 확인할 수 있다. ML.EVALUATE 함수는 실제 데이터를 기준으로 예측 값을 평가한다. 12345678910111213SELECT *FROM ML.EVALUATE (MODEL `census.census_model`, ( SELECT * FROM `census.input_view` WHERE dataframe = &#x27;evaluation&#x27; ) ) 앞에서 학습시킨 모델과 SELECT 서브 쿼리에서 반환된 평가 데이터를 받아들인다. 이 함수는 모델에 대한 단일행의 통계를 반환한다. input_view 의 데이터를 평가 데이터로 사용한다. 실행 결과 로지스틱 회귀를 수행했으므로 결과에 precision, recall, accuracy, f1_score, log_loss, roc_auc 열이 포함된다. 1번과 2번의 차이 ML.EVALUATE는 학습 과정에서 계산된 평가 측정값을 가져오는데, 이를 위해 자동으로 예약된 평가 데이터셋을 사용합니다. data_split_method 학습 옵션에 NO_SPLIT 가 지정된 1번 방법의 경우 전체 입력 데이터 세트가 학습과 평가에 모두 사용된다. 평가 데이터와 훈련 데이터를 구분하지 않고 ML.EVALUATE 를 호출 하면 학습 데이터 세트에서 임의의 평가 데이터 세트가 측정되고 이러한 평가 효과는 모델 학습 데이터와 별도로 유지된 데이터 세트에 대한 평가 실행보다 적다. → 따라서 구분해주는게 좋다. 6단계: ML.PREDICT 함수를 사용하여 소득 계층 예측 특정 응답자가 속한 소득 계층을 식별하려면 ML.PREDICT 함수를 사용한다. 12345678910111213SELECT *FROM ML.PREDICT (MODEL `census.census_model`, ( SELECT * FROM `census.input_view` WHERE dataframe = &#x27;prediction&#x27; ) ) prediction 데이터 프레임에 있는 모든 응답자의 소득 계층을 예측한다. 실행 결과: predicted_income_bracket은 income_bracket의 예측값 7단계: Explainable AI 메서드로 예측 결과 설명 모델에서 이러한 예측 결과를 생성하는 이유를 알아 보려면 ML.EXPLAIN_PREDICT 함수를 사용하면 된다. ML.EXPLAIN_PREDICT 는 ML.PREDICT 의 확장된 버전 예측 결과 뿐만 아니라 예측결과를 설명하는 추가 열을 출력한다. BigQueryML의 Shapley 값과 Explainable AI서비스에 대한 자세한 내용 12345678910111213#standardSQLSELECT*FROMML.EXPLAIN_PREDICT(MODEL `census.census_model`, ( SELECT * FROM `census.input_view` WHERE dataframe = &#x27;evaluation&#x27;), STRUCT(3 as top_k_features)) 실행 결과 로지스틱 회귀모델에서 Shapley 값은 머신 러닝 모델의 예측 결과에 각 특성이 기여하는 정도를 평가하기 위해 사용된다. 이 값을 통해 모델의 예측 결과를 해석하고, 모델의 예측 결과를 개선하기 위해 어떤 특성을 수정해야 하는지를 결정하는 데 도움을 준다. top_k_features 가 3으로 설정되었기 때문에 제공된 테이블의 행당 특성 기여 학목 3개를 출력함. 기여항목은 절댓값 기준으로 내림차순으로 정렬된다. 44번행의 결과 값은 education_num이 가장 큰 기여를 하였다면, 47번행의 결과는 age가 가장 큰 기여를 했음을 알수 있다. 8단계: 모델을 전연적으로 설명 일반적으로 소득계층을 셜정하는데 가장 중요한 특성이 무엇인지 확인하려면 ML.GLOABL_EXPLAIN 함수를 사용하면 된다. 이를 사용하려면 모델을 학습 시킬때 ENABLE_GLOBAL_EXPLAIN &#x3D; TRUE 옵션을 사용하여야 한다. sklearn의 feature importance와 같은 기능을 하는 함수 학습 쿼리 1234567891011121314CREATE OR REPLACE MODEL census.census_modelOPTIONS ( model_type=&#x27;LOGISTIC_REG&#x27;, auto_class_weights=TRUE, enable_global_explain=TRUE, input_label_cols=[&#x27;income_bracket&#x27;] ) ASSELECT * EXCEPT(dataframe)FROM census.input_viewWHERE dataframe = &#x27;training&#x27; 전역 설명에 Access하는 쿼리 12345#standardSQLSELECT *FROM ML.GLOBAL_EXPLAIN(MODEL `census.census_model`) 실행 결과 Reference https://notebook.community/google/eng-edu/ml/cc/exercises/estimators/ko/intro_to_fairness https://cloud.google.com/bigquery-ml/docs/logistic-regression-prediction?hl=ko","categories":[{"name":"Data Base","slug":"Data-Base","permalink":"https://jmj3047.github.io/categories/Data-Base/"},{"name":"GCP","slug":"Data-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Base/GCP/"}],"tags":[{"name":"Big Query","slug":"Big-Query","permalink":"https://jmj3047.github.io/tags/Big-Query/"},{"name":"BigQueryML","slug":"BigQueryML","permalink":"https://jmj3047.github.io/tags/BigQueryML/"}]},{"title":"Mac bash 파일로 hexo, git 명령어 자동화","slug":"Bash_Automation","date":"2023-02-26T15:00:00.000Z","updated":"2023-02-28T07:50:29.954Z","comments":true,"path":"2023/02/27/Bash_Automation/","link":"","permalink":"https://jmj3047.github.io/2023/02/27/Bash_Automation/","excerpt":"","text":"bash 파일로 hexo, git 명령어 자동화 필자가 블로그글을 작성하는데 hexo, git 명령어 자동화의 필요성을 느껴 이 글을 작성한다. mac에서 자동화 하는 경우 윈도우와 달리 batch 파일이 아니라 bash 파일로 실행해야 한다. 우선 메모장에 자동화를 원하는 코드를 작성한다. bash 파일 작성시에는 #!/bin/bash 를 꼭 작성해주어야 실행이 가능하다. 실행파일 이름을 정하고(필자는 submit 으로 설정하였음) 저장한 뒤 확장자를 없애주어야 한다. 저장한 submit 파일을 블로그 로컬 폴더 안에 넣는다 터미널에 chmod +x submit 를 입력해서 권한을 부여해준다 실행시키려면 sh submit 을 입력해주면 잘 돌아가는 것을 볼 수 있다.","categories":[{"name":"Setting","slug":"Setting","permalink":"https://jmj3047.github.io/categories/Setting/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://jmj3047.github.io/tags/Hexo/"},{"name":"Git","slug":"Git","permalink":"https://jmj3047.github.io/tags/Git/"},{"name":"Automation","slug":"Automation","permalink":"https://jmj3047.github.io/tags/Automation/"},{"name":"Bash","slug":"Bash","permalink":"https://jmj3047.github.io/tags/Bash/"}]},{"title":"Basic ML Process","slug":"ML_Process","date":"2023-02-23T15:00:00.000Z","updated":"2023-02-24T04:50:59.756Z","comments":true,"path":"2023/02/24/ML_Process/","link":"","permalink":"https://jmj3047.github.io/2023/02/24/ML_Process/","excerpt":"","text":"Basic ML Process 이 포스트에서는 필자가 생각한 기본 프로세스를 소개 한다. 머신러닝을 접해보지 않은 사람들에게 대략적인 개념을 보여주는 포스트 이다. 자세한 내용은 추후 추가 예정 가설 수립 → 데이터 확인 및 전처리 → 모델 학습&#x2F; 모델 검증 → 예측하기 → 결과 확인 가설 수립(회귀&#x2F;분류 여부 확인) → 잠재 고객 분류, 매출 예측, 리텐션 예측 등등.. 잠재 고객 분류: 특정 고객군은 추후 유료 서비스를 사용할 것이다. 매출 예측: 다음달 매출 예상액은 전월 대비 ?? 일것이다. 리텐션 예측: 다음주 리텐션 비율이 ?? 일 것이다, ??일 동안 들어온 고객은 장기 고객이 될 것이다 예시 가설: 이틀 이상 접속 &amp; 1 경기 이상 플레이 경험 유저 대상으로 귀환 유저 비율 예측 → 회귀문제 데이터 확인 및 전처리 데이터 feature 선택 예시 raw data(Big Query에서 추출한 부분) A그룹은 2021-12-07 ~ 2021-12-21 B그룹은 2022-02-14 ~ 2022-02-28 전체: 접속시간, 레벨, 게임횟수, 로그인횟수, 평균순위, 최대순위, 총킬횟수, 유저킬횟수, 헤드샷킬수, 신고 횟수, 솔로&#x2F;듀오&#x2F;스쿼드 참여 횟수, 오직 솔로모드만 이용 가입첫날: 총 게임수, 평균순위, 최대순위, 총킬횟수, 유저킬횟수, 헤드샷킬수, 신고 횟수, 솔로&#x2F;듀오&#x2F;스쿼드 참여 횟수, 오직 솔로모드만 이용 가입 다음날: 총 게임수, 평균순위, 최대순위, 총킬횟수, 유저킬횟수, 헤드샷킬수, 신고 횟수, 솔로&#x2F;듀오&#x2F;스쿼드 참여 횟수, 오직 솔로모드만 이용 EDA 유저 킬수, 최대순위, 로그인횟수, 헤드샷킬수, 유저레벨, 플레이 시간, 로그아웃 날짜, 총 게임 횟수, 총 킬수, 평균 순위 데이터 샘플링 데이터를 일부 정리해서 최적의 입력 데이터로 만드는 과정 확률적 샘플링 단순 랜덤 샘플링 2단계 샘플링: 전체 n개의 데이터를 m개의 모집단으로 나누고 m개의 모집단 중에 N개의 데이터를 단순랜덤 샘플링 층별 샘플링: 모집단을 여러개 층으로 구분함으로써 각 층에서 n개씩 랜덤하게 데이터 추출 군집&#x2F;집락 샘플링: 모집단이 여러개의 군집으로 구성되어 있는 경우 군집 중 하나 or 여러개의 군집을 선정해서 선정된 군집의 전체 데이터를 사용하는 방법. Ex) 한국의 시,도 데이터 계통 샘플링: 1에서 n까지 모든 데이터에 번호를 매겨서 일정 간격마다 하나씩 데이터를 추출하는 방법. 대표적으로 시계열에서 사용됨 비확률적 샘플링 편의 샘플링: 데이터르르 수집하기 좋은 시점이나 위치를 선정하여 샘플링 판단 샘플링: 목적에 가장 적합한 대상이라고 생각하느느 대상을 선택 할당 샘플링 참고: https://jmj3047.github.io/2022/09/07/Data_Sampling&#x2F; feature 축소 or 확대 ‘유저 킬수, 최대순위, 로그인횟수, 헤드샷킬수, 유저레벨, 플레이 시간, 로그아웃 날짜, 총 게임 횟수, 총 킬수, 평균 순위’ 의 지표를 EDA 과정을 거쳐 ‘접속시간, 레벨, 게임횟수, 로그인횟수, 평균순위, 최대순위, 총킬횟수, 유저킬횟수, 헤드샷킬수, 신고 횟수, 솔로&#x2F;듀오&#x2F;스쿼드 참여 횟수, 오직 솔로모드만 이용’ 으로 지표를 확대 시킴 → 약 64만행의 데이터 확보 모델 학습&#x2F; 모델 검증 모델 선택 및 파라미터 조정 → 모델 학습 → 모델 검증 대부분 한줄의 코드로 모델 선택이 가능함 파라미터 조정 같은 경우 GridSearchCV로 조정 할수 있음 n_estimators&#x3D;500, learning_rate&#x3D;0.05, gamma&#x3D;0, subsample&#x3D;0.75, → 이 네가지의 파라미터를 최적값으로 조정하고 싶다면, 각각의 범위를 선택해주면 범위 내의 모든 확률을 다 확인한다: 시간도 오래 걸리고 굉장히 무겁게 돌아감. feature importance로 어떤 파라미터가 모델 학습에 기여를 많이 했는지 알수 있음 학습 시 train 데이터를 validation set을 떼어두고 학습 해야 모델 검증이 가능 함. 모델 검증 → 모델과 데이터 간의 적합도 확인 하는 과정 f1 score, $r^2$ score, confusion matrix 등등 예측하기 및 결과 확인 모델 저장 후 test 데이터로 실행해봄 결과의 신뢰성이 어느정도 인지 확인하는 방법 → 미래의 사건이라서 100프로 확인은 안되지만 오류에 대한 부분을 확인하면 유추 가능","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"ML Process","slug":"ML-Process","permalink":"https://jmj3047.github.io/tags/ML-Process/"}]},{"title":"Auto-correlation Function, Partial Auto-correlation Function","slug":"ACF_PACF","date":"2023-02-22T15:00:00.000Z","updated":"2023-02-23T01:50:54.354Z","comments":true,"path":"2023/02/23/ACF_PACF/","link":"","permalink":"https://jmj3047.github.io/2023/02/23/ACF_PACF/","excerpt":"","text":"자기 상관 함수와 부분 자기 상관 함수Autocorrelation Function, 자기 상관 함수 자기 상관 함수(Auto-correlation Function) 어떤 신호의 시간이동 된 자기 자신과의 ‘상관성(Correlation)’ 척도 주요 특징 결정 신호(주기 신호&#x2F;비주기 신호)이든, 랜덤 신호 이든 모든 신호에 대해 적용 가능 특히 랜덤 과정인 경우에 자기상관함수를 이용하여 굳이 시간신호에 대한 푸리에 변환을 구할 필요 없이 주파수상에 분포된 전력(전력밀도스펙트럼)을 취급할 수 있으므로 이를 사용하게 됨 [참고] 서로 다른 신호 간의 상관성 척도에 대해서는 상호상관 참조 상관데 대한 보다 정확한 이해를 위해서는 상관성 참조 상관성 개념의 종합화&#x2F;일반화는 비교(같음&#x2F;닮음&#x2F;다름) 참조 확정적 신호(Deterministic signal)에서, 자기 상관 함수 에너지신호의 자기상관함수: 컨볼루션(*)에 의해 정의돔 x(t)가 실수 신호이면, $Rx(τ)&#x3D;∫^∞ _{−∞} x(t)x(t+τ)dt&#x3D;x(τ)∗x(−τ)$ x(t)가 복소수 신호이면, $Rx(τ)&#x3D;∫^∞_{−∞}x(t)x^∗(t+τ)dt&#x3D;x(τ)∗x^∗(−τ)$ 전력신호의 자기 상관 함수: 시간평균(&lt;&gt;)에 의해 정의됨 실수 신호 복소수 신호 랜덤 과정(Random Prodcess)에서, 자기상관 함수 정의 통계적 평균에 의한 자기상관함수 정의: $R_x(t_1,t_2) &#x3D; E[X(t_1)X(t_2)]$ 결합 PDF(결합 확률밀도함수)에 의한 자기 상관함수 정의: $R_x(t_1,t_2) &#x3D; ∫^∞_{−∞}∫^∞_{−∞}x_1x_2fx_1x_2(x_1,t_1,x_2,t_2)dx_1dx_2$ 만일 랜덤 과정이 광의의 정상과정이면, 시간 t의 함수가 아니라 시간천이 $t-t&#x3D;τ$의 함수가 됨 $R_x(t_1,t_2) &#x3D; R_x(t,t+τ)&#x3D;R_x(τ) &#x3D; E[X(t)X(t+τ)]$ 이때 시간 영역 자기상관과 주파수영역 스펙트럼밀도 간에 푸리에 변환 쌍 관계가 있음. $R(τ)$ ← 푸리에변환 쌍 관계 → $S(f)$ 만일, 랜덤과정이 에르고딕과정이라면, 통계적 평균 및 시간 평균이 상호 호환이 가능함 $R_x(τ) &#x3D; E[X(t)X(t+τ)] &#x3D;&lt; X(t)X(t+τ)&gt;$ 따라서 이 경우에는 R(τ)는 시간 평균이나 통계적 평균 어느 것으로도 구할 수 있음. 자기상관 함수의 성질&#x2F;특징 신호의 ‘시변(time-variant)’ 특성이 어떤가를 보여줌 그 신호가 갖는 스펙트럼의 특성 정보를 나타냄 시간적인(시변) 상관성 척도임 분산이 확률변수가 통계적으로 불규칙하게 분포되는 정도를 나타내는 척도라면 자기 상관은 분산과 유사하게 확률과정이 시간적으로 상관 또는 분산되는 척도를 나타냄 직관적으로 자기 자신과의 시간천이(τ)가 작을수록 상관성이 커짐 따라서 τ &#x3D; 0 에서 최대 상관성 값을 갖음 $|R_x(τ)|≤R_x(0)$식 τ &#x3D; 0 일때 물리적 의미로는 에너지 신호: τ &#x3D; 0 에서의 최대값이 전체 신호에너지와 같음 $R_x(0) &#x3D; ∫^∞_{−∞}|x(t)|^2dt&#x3D; E_x$ 전력신호: τ &#x3D; 0 에서의 최대값이 평균 전력과 같음 $R_x(0) &#x3D;&lt; x^2(t) &gt;&#x3D; ∫^∞_{−∞}S_x(f)df &#x3D; P_{av}$ WSS 랜덤과정: τ &#x3D; 0에서의 최대값이 평균전력과 같음 $R_x(0) &#x3D; E[X^2(t)] &#x3D; ∫^∞_{−∞}S_x(f)df &#x3D; P_{av}$ 시간 영역 자기 상관과 주파수 영역 스펙트럼 밀도 간에 푸리에 변환 쌍 관계가 있음 자기 상관 ← 푸리에변환 쌍 관계 → 스펙트럼 밀도 : 위너킨친정리 참조 Partial Autocorrelation Function, 부분 자기 상관 함수 편 자기 상관 함수(부분자기상관함수)는 다른 모든 짧은 시차 항에 따라 조정한 후 k 시간 단위로 구분된 시계열($y_{t-1},y_{t-2},…,y_{t-k-1}$)의 관측치 ($y_t 및 y_{t-k}$) 간의 상관의 측도임 해석 ARIMA 모형에서 자기 회귀 차수를 식별하는 용도로 사용 됨. 편 자기 상관 함수에서 다음과 같은 패턴을 찾음. 각 시차에서 큰 값을 조사하여 유의한지 확인함. 유의한 큰 값은 유의 한계를 벗어나면, 이는 해당 시차에 대한 상관이 0이 아니라는 것을 나타냄 이 그림에서는 시차 1에 유의한 상관이 있고 그 뒤에는 유의하지 않은 상관이 있음. 이 패턴은 1차 자기회귀 항을 나타냄. Reference http://www.ktword.co.kr/test/view/view.php?m_temp1&#x3D;3547 https://support.minitab.com/ko-kr/minitab/20/help-and-how-to/statistical-modeling/time-series/how-to/partial-autocorrelation/interpret-the-results/partial-autocorrelation-function-pacf/ https://zephyrus1111.tistory.com/135","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"ACF","slug":"ACF","permalink":"https://jmj3047.github.io/tags/ACF/"},{"name":"PACF","slug":"PACF","permalink":"https://jmj3047.github.io/tags/PACF/"},{"name":"Time Series","slug":"Time-Series","permalink":"https://jmj3047.github.io/tags/Time-Series/"}]},{"title":"Probability Distribution Function & Probability Density Function","slug":"Probability_Distribution_Function","date":"2022-11-19T15:00:00.000Z","updated":"2023-02-23T01:38:48.226Z","comments":true,"path":"2022/11/20/Probability_Distribution_Function/","link":"","permalink":"https://jmj3047.github.io/2022/11/20/Probability_Distribution_Function/","excerpt":"","text":"확률 분포 함수와 확률 밀도 함수확률 분포 함수(probability distribution function)와 확률 밀도 함수(probability density function)는 확률 변수의 분포 즉, 확률 분포를 수학적으로 정의하기 위한 수식이다. 연속 확률 분포우선 확률 밀도 함수에 대해 먼저 알아보자. 확률 밀도 함수를 이해하면 확률 분포 함수를 이해하는 것은 쉽다. 확률 밀도 함수는 연속 확률 변수(continuous random variable)를 정의하는데 필요하다. 연속 확률 변수의 값은 실수(real number) 집합처럼 연속적이고 무한개의 경우의 수를 가진다. 연속 확률 변수의 분포를 연속 확률 분포라고 한다. 시계 바늘을 예로 들어보자. 다음과 같은 아날로그 시계의 시계 바늘을 눈을 감고 임의로 돌렸다고 하면 시계 바늘이 정각 12시(각도 0도)를 가리킬 확률은 얼마일까? 만약 이 확률 변수의 확률 분포가 0 이상 360 미만의 구간내에서 균일 분포(uniform distribution) 모형을 가진다고 가정하면 답은 0(zero)이다.시계 바늘이 가리키는 각도의 값은 0도 이상 360도 미만의 모든 실수 값을 가질 수 있는데, 이 경우 수가 무한대이므로 각각의 경우에 대한 확률은 0이 되어야 하기 때문이다.사실 각도가 0도가 아니라 어떤 특정한 각도를 지정하더라도 같은 이유로 그 각도를 가리킬 확률은 0이다. 그럼 도대체 어떤 방법으로 확률 분포를 설명해야 할까? 이렇게 경우의 수가 무한대인 연속 확률 변수의 분포를 설명하려면 특정한 값이 아니라 구간을 지정하여 확률을 설명해야 한다. 예를 들어 위와 같은 시계바늘의 예에서는 다음과 같은 분포의 묘사가 가능하다. 시계 바늘이 12시와 1시 사이에 있을 확률은 1&#x2F;12 시계 바늘이 1시와 3시 사이에 있을 확률은 2&#x2F;12 &#x3D; 1&#x2F;6 시계 바늘이 6시와 9시 사이에 있을 확률은 3&#x2F;12 &#x3D; 1&#x2F;4 이 방법의 단점 중 하나는 분포를 설명하는데 범위를 지정하는 두 개의 숫자가 필요하다는 점이다. 예를 들어 ‘1시와 3시 사이’ 라는 범위를 지정하는데는 1과 3이라는 숫자가 필요하다.그럼 하나의 숫자로 확률 변수의 범위를 지정하는 방법은 없을까? 가능한 방법 중의 하나는 범위를 지정하는 두 개의 숫자 중 작은 숫자 즉, 범위가 시작하는 숫자를 미리 가장 작은 숫자로 고정하는 방법이다. 이 방법을 쓰면 다음과 같이 하나의 숫자로 랜덤 변수의 범위와 해당 확률을 서술할 수 있다. 숫자&#x3D;1 -&gt; 범위&#x3D;12시부터 1시까지 -&gt; 확률 1&#x2F;12 숫자&#x3D;2 -&gt; 범위&#x3D;12시부터 2시까지 -&gt; 확률 2&#x2F;12 숫자&#x3D;5 -&gt; 범위&#x3D;12시부터 5시까지 -&gt; 확률 5&#x2F;12 누적 확률 분포위와 같은 방법으로 서술된 확률 분포를 누적 확률 밀도 함수 (cumulative probability density function) 또는 누적 확률 분포라고 하고 약자로 cdf라고 쓴다. 일반적으로 cdf는 대문자를 사용하여 F(x)와 같은 기호로 표시하며 이 때 독립 변수 x는 범위의 끝을 뜻한다. 범위의 시작은 일반적으로 음의 무한대(negative infinity, -∞)값을 사용한다.몇가지 누적 확률 분포 표시의 예를 들면 다음과 같다. F(-1) : 확률 변수가 -∞ 이상 -1 미만인 구간 내에 존재할 확률 F(10) : 확률 변수가 -∞ 이상 10 미만인 구간 내에 존재할 확률 확률 변수 X에 대한 누적 확률 분포 F(x)의 수학적 정의는 다음과 같다. $$F(x) &#x3D; P(X &lt; x) &#x3D; P(X&lt;x)$$ 일례로 표준 정규 분포의 누적 확률을 그리면 아래와 같다. 123x = np.linspace(-4,4)y = sp.stats.norm.cdf(x)plt.plot(x,y) 누적 밀도 함수 즉, cdf는 다음과 같은 특징을 가진다. F(-∞) &#x3D; 0 F(∞) &#x3D; 1 F(x) ≥ F(y) if x &gt; y 확률 밀도 함수누적 밀도 함수의 단점 중의 하나는 어떤 값이 더 자주 나오든가 혹은 더 가능성이 높은지에 대한 정보를 알기 힘들다는 점이다. 이를 알기 위해서는 확률 변수가 나올 수 있는 전체 구간 (-∞ ~ ∞)을 아주 작은 폭을 가지는 구간들로 나눈 다음 각 구간의 확률을 살펴보는 것이 편리하다. 다만 이렇게 되면 구간의 폭을 얼마로 정해야 하는지에 대한 의문이 생긴다. 123x = np.linspace(-4,4,20)y = sp.stats.norm.cdf(x)z = np.insert(np.diff(y), 0, None) 12w = (4-(-4))/20plt.bar(x-w, z/w, width = w) 이 때 사용할 수 있는 수학적 방법이 바로 미분(differentiation)이다. 미분은 함수의 구간을 무한대 갯수로 쪼개어 각 구간 변화의 정도 즉, 기울기를 계산하는 방법이다. 누적 밀도 함수를 미분하여 나온 도함수(derivative)를 확률 밀도 함수(probability density function)라고 한다. 누적 밀도 함수는 보통 f(x)와 같이 소문자 함수 기호를 사용하여 표기한다. $$f(x)&#x3D;\\frac{dF(x)}{dx} \\text 또는 F(x) &#x3D; \\int_{-\\infty}^{x}f(u)du$$ 확률 밀도 함수는 다음과 같은 특징을 가진다. -∞ 부터 ∞까지 적분하면 그 값은 1이 된다. Reference https:&#x2F;&#x2F;velog.io&#x2F;@groovallstar&#x2F;확률-분포-함수와-확률-밀도-함수의-의미","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"Probability Distribution Function","slug":"Probability-Distribution-Function","permalink":"https://jmj3047.github.io/tags/Probability-Distribution-Function/"},{"name":"Probability Density Function","slug":"Probability-Density-Function","permalink":"https://jmj3047.github.io/tags/Probability-Density-Function/"}]},{"title":"Difference between Normal Distribution & Standard Normal Distribution","slug":"Normal_Distribution","date":"2022-11-10T15:00:00.000Z","updated":"2023-02-23T01:38:44.907Z","comments":true,"path":"2022/11/11/Normal_Distribution/","link":"","permalink":"https://jmj3047.github.io/2022/11/11/Normal_Distribution/","excerpt":"","text":"정규분표와 표준정규분포함수의 차이본 포스팅에서는 정규분포(Normal distribution)와 표준 정규 분포(Standard normal distribution)에 대해 다루도록 한다. 정규 분포의 확률밀도 함수와 예상치(평균), 분산 그리고 증명에 대해 다루며 표준정규분포에 대해서는 확률밀도함수, 누적분포함수, 그리고 표준정규분포를 이용한 정규분포의 확률계산 등의 내용이 다뤄진다. 1. 정규분포(Normal distribution)**정규 분포(Normal distribution)**는 연속확률분포 중 하나이며 광범위하게 사용된다. 확률 분포 중 가장 유명하며 가장 중요하게 다루는 확률 분포이다. **오류 분포(Error distribution)**와 다른 많은 자연현상을 직접 모델링 하기 위한 확률 분포이다. 중심극한정리(Central Limit Theorm)로 인해 매우 유용하며 단순하고 정확한 근사가 가능하다. 정규 분포는 가우스 분포(Gaussian distribution)라고도 불린다. 비율과 개별적인 확률을 모델링하는데도 유용하다. 정규분포는 일반적으로 다음과 같이 표현된다. : X~N(μ, σ2) 아래 그림은 정규 분포의 확률밀도 함수를 보여준다 확률밀도함수(PDF)정규분포의 확률 밀도 함수는 다음과 같다 예상치(Expectation)와 분산(Variance) 정규분포의 적률생성함수(MGF) 예상치와 분산의 증명 본 증명에서는 적률생성함수(MGF)를 이용하여 증명을 수행해보도록 하겠다. 적률생성함수는 다음과 같다 적률생성함수의 미분값과 그 미분값에서 t&#x3D;0인 경우는 다음과 같다 적률생성함수의 2계 미분값과 그 미분값에서 t&#x3D;0인 경우는 다음과 같다 따라서 분산은 다음과 같이 계산된다 표준정규분포(Standard Normal distribution)정규 분포에서는 μ는 0으로 σ2은 1로 설정하여 표준화를 수행한 정규분포를 표준정규분포라고 한다. 즉, 예상치(평균)는 0, 분산은 1이다. 아래 그림은 표준 정규 분포의 확률밀도함수와 누적분포함수를 보여준다. 확률밀도함수(PDF) 표준정규분포의 확률밀도함수는 다음과 같다. 누적분포함수(CDF) 표준정규분포의 누적분포함수는 다음과 같다. 정규분포의 적률생성함수(MGF) 표준정규분포를 이용한 정규분포의 확률계산확률분포가 정규분포인 경우 표준정규분포로 치환하여 특정범위의 확률을 계산할수 있다. 치환은 다음과 같이 수행 하면 된다. 표준정규분포의 누적확률분포(CDF) 값은 비교적 쉽게 계산이 가능하며 다음과 같은 방식으로 계산할 수 있다. 표준 정규 분포의 누적확률 분포(CDF) 표 Excel, R, Python 모듈 등을 활용한 데이터 취득 다음은 정규분포의 특정 범위에서에 대한 확률을 표준정규분포로 치환하는 과정을 보여준다. 정규분포는 중앙값에서 좌우 대칭이다. 따라서 중앙값에서 다음과 같이 특정 범위의 확률을 표현하는 것이 가능하다. 또한 표준 편차를 이용하여 표준화 시켜 특정 범위를 표현할 수 있다. Reference https://color-change.tistory.com/m/61 https://kongdols-room.tistory.com/145","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"Normal Distribution","slug":"Normal-Distribution","permalink":"https://jmj3047.github.io/tags/Normal-Distribution/"},{"name":"Standard Normal Distribution","slug":"Standard-Normal-Distribution","permalink":"https://jmj3047.github.io/tags/Standard-Normal-Distribution/"}]},{"title":"SpeakerGAN, Speaker identification with conditional generative adversarial network","slug":"SpeakerGAN","date":"2022-11-04T15:00:00.000Z","updated":"2022-11-10T15:27:17.845Z","comments":true,"path":"2022/11/05/SpeakerGAN/","link":"","permalink":"https://jmj3047.github.io/2022/11/05/SpeakerGAN/","excerpt":"","text":"Journal&#x2F;Conference : NeurocomputingYear(published year): 2020Author: Liyang Chen, Yifeng Liu , Wendong Xiao, Yingxue Wang, Haiyong XieSubject: Speaker GAN, Generative Adversarial Network SpeakerGAN: Speaker identification with conditional generative adversarial network Summary This paper proposes a novel approach, SpeakerGAN, for speaker identification with the conditional generative adversarial network (CGAN). We configure the generator and the discriminator in SpeakerGAN with the gated convolutional neural network (CNN) and the modified residual network (ResNet) to obtain generated samples of high diversity as well as increase the network capacity. Under the scenario of limited training data, SpeakerGAN obtains significant improvement over the baselines. IntroductionThe x-vector in [13,14] is proposed as a strong contender for the speaker representation and is considered to supplant the i-vector system by many researchers. We evaluate our approach on the dataset of Librispeech-100 for the text-independent SI task. The baselines include i-vector, xvector, CNN and LSTM. Generative Adversarial NetworksConditional GANThe CGAN is a variant of GAN, which aims to let the generator $G$ produce $G(c,z)$ from the condition $c$ and random noise $z$. In this paper, the real samples x are directly utilized as the condition c and the random noise z is abandoned for showing no effectiveness in the experiments. SpeakerGAN for speaker identificationBasic principle of SpeakerGANWe investigate the CGAN as a classifier by enabling the network to learn on additional unlabeled examples. The SpeakerGAN combines the discriminator and classifier by letting the classifier take samples from the generator as inputs and have $N+1$ output units, where $l_{N+1}$corresponds to the probability $P_{model}(y&#x3D;N+1|x)$ that the inputs are real. To stabilize the training and overcome the problem of vanishing gradients, the least square GAN (LSGAN) [40] is adopted. $L_{adv}$ is then split into the losses for the discriminator and generator. Network architecture of SpeakerGAN Fig. 1. Framework of SpeakerGAN. The front extraction part extracts FBanks $x$ from the real speech samples. The generator takes the real FBanks x as inputs and produces fake samples $G(x)$. The discriminator is then fed with the real and generated FBanks to predict class labels and distinguish between the real and fake samples. The adversarial loss in Multiple loss actually denotes the formulation of LSGAN [40]. The dashed lines with arrows denote calculating loss between two objects, and the solid lines denote the flow of information. The generator takes real sequences as the condition for inputs, and produces the fake samples of the same size after passing through a series of convolutional and shuffler layers that progressively downsample and upsample. The discriminator takes the generated samples and real acoustic features from the corpus as inputs, and outputs the discrimination of real&#x2F;fake along with the $N$ classes. Generator design These generators only capture relationships among feature dimension and the generated samples are in lack of consistency. An effective way to solve this problem would be to introduce the RNN, but it is timeconsuming due to the difficulty of parallel computing. For these reasons, we configure the generator using gated CNNs [43], which allow for both sequential structure and faster convergence. This idea was previously explored in [25], and achieved competitive performance. Discriminator design As for the discriminator, we prefer deeper networks to classify speakers. However, training deep neural networks is computationally expensive and difficult. This paper modifies the ResNet [45] toaccelerate the training. ResNets have been applied in many SI systems [16,17] and are known for good classification performance on image data. To reduce parameters and improve calculation efficiency, the variant ResBlock is adopted, which comprises a stack of three convolutional layers. Four ResBlocks are stacked and a softmax classifier is used to predict the speaker identity in the final layer. ExperimentsDataset and basic setupTo evaluate the performance of the proposed approach, we conduct experiments on the Librispeech [46] dataset. We use the train-clean-100 subset in it that contains 251 speakers of 125 females and 126males. Each speaker is provided with an average of 113 utterances lasting 1–15 s. The same acoustic features are used for the baselines of CNN, LSTM and GAN classifier. Speech samples are shuffled before training. Sixty percentage of all utterances are randomly selected as training data and the rest are used as test data. Conclustions and future workIt directly utilizes the discriminator as a classifier using the fake samples produced by the generator as the additional class. The Hybrid loss function includes the adversarial loss of the regular GAN, thecross-entropy loss of the labeled data and the Huber loss between the real samples and generation. Experimental results demonstrate that SpeakerGAN can achieve higher identification accuracy than other state-of-the-art DL based methods and the traditional i-vector and x-vector systems. Link: SpeakerGAN, Speaker identification with conditional generative adversarial network","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Generative Model","slug":"Paper/Generative-Model","permalink":"https://jmj3047.github.io/categories/Paper/Generative-Model/"}],"tags":[{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Speaker GAN","slug":"Speaker-GAN","permalink":"https://jmj3047.github.io/tags/Speaker-GAN/"},{"name":"Speaker Identification","slug":"Speaker-Identification","permalink":"https://jmj3047.github.io/tags/Speaker-Identification/"},{"name":"Generative Adversarial Network","slug":"Generative-Adversarial-Network","permalink":"https://jmj3047.github.io/tags/Generative-Adversarial-Network/"}]},{"title":"Multi-Task Learning for Voice Trigger Detection","slug":"MTL_for_VTD","date":"2022-11-01T15:00:00.000Z","updated":"2022-11-10T15:18:26.813Z","comments":true,"path":"2022/11/02/MTL_for_VTD/","link":"","permalink":"https://jmj3047.github.io/2022/11/02/MTL_for_VTD/","excerpt":"","text":"Journal&#x2F;Conference : ICASSP IEEEYear(published year): 2020Author: Siddharth Sigtia, Pascal Clark, Rob Haynes, Hywel Richards, John BridleSubject: Multi-Task Learning Multi-Task Learning for Voice Trigger Detection Summary We start by training a general acoustic model that produces phonetic transcriptions given a large labelled training dataset. 우리는 레이블이 지정된 대규모 훈련 데이터 세트가 주어지면 phonetic transcriptions를 생성하는 일반적인 음향 모델을 훈련하는 것으로 시작한다. Next, we collect a much smaller dataset of examples that are challenging for the baseline system. 다음으로, 우리는 기준 시스템에 도전하는 훨씬 더 작은 예제의 데이터 세트를 수집한다. We then use multi-task learning to train a model to simultaneously produce accurate phonetic transcriptions on the larger dataset and discriminate between true and easily confusable examples using the smaller dataset. 그런 다음 다중 작업 학습을 사용하여 모델을 훈련시켜 더 큰 데이터 세트에서 정확한 phonetic transcriptions를 생성하고 더 작은 데이터 세트를 사용하여 실제 예제와 쉽게 혼동할 수 있는 예제를 구별한다. IntroductionSignificant challenge is that unlike automatic speech recognition (ASR) systems, collecting training examples for a specific keyword or phrase in a variety of conditions is a difficult problem. 중요한 과제는 자동 음성 인식(ASR) 시스템과 달리 다양한 조건에서 특정 키워드 또는 구문에 대한 훈련 예제를 수집하는 것은 어려운 문제라는 것이다. In the literature, the problem of detecting a speech trigger phrase is interchangeably referred to as voice trigger detection [3], keyword spotting [4], wake-up word detection [5] or hotword detection [6]. In the rest of this paper, we refer to this problem as voice trigger detection. 문헌에서 음성 트리거 구문을 검출하는 문제는 voice trigger detection [3], keyword spotting [4], wake-up word detection [5] 또는 hotword detection [6]으로 상호 교환적으로 언급된다. 이 논문의 나머지 부분에서는 이 문제를 voice trigger detection라고 합니다. In the multi-stage approach (Figure 1), the first stage comprises a low-power DNN-HMM system that is always on [3]. 그림1에 보면, 첫 번째 단계는 항상 [3]에 있는 저전력 DNN-HMM 시스템을 포함한다. In this design, it is the second stage that determines the final accuracy of the system and the models used in this stage are the subject of this paper. 이 설계에서 시스템의 최종 정확도를 결정하는 것은 두 번째 단계이며, 이 단계에서 사용되는 모델이 이 논문의 주제이다. Our main contribution is to propose a multi-task learning strategy where a single model is trained to optimise 2 objectives simultaneously. 우리의 주요 기여는 단일 모델이 두 가지 목표를 동시에 최적화하도록 훈련되는 다중 작업 학습 전략을 제안하는 것이다. The first objective is to assign the highest score to the correct sequence of phonetic labels given a speech recording. 첫 번째 목표는 주어진 음성 녹음의 음성 레이블이 올바른 순서로 되어 있다면 가장 높은 점수를 할당하는 것이다. This objective is optimised on a large labelled training dataset which is also used for training the main speech recogniser and is therefore easy to obtain. 이 목표는 주요 음성 인식기를 훈련시키는 데 사용되므로 쉽게 얻을 수 있는 대규모 레이블링된 훈련 데이터 세트에 최적화된다. The second objective is to discriminate between utterances that contain the trigger phrase and those that are phonetically similar and easily confusable. 두 번째 목표는 trigger phrase를 포함하는 발화와 음성학적으로 유사하고 쉽게 혼동되는 발화를 구별하는 것이다. BaselineThe baseline model architecture comprises an acoustic model (AM) with four bidirectional LSTM layers with 256 units each, followed by an output affine transformation + softmax layer over context independent (CI) phonemes, word and sentence boundaries, resulting in 53 output symbols (Figure 2). Firstly, the fact that the second-pass model is used for re-scoring and not in a continuous streaming setting allows us to use bidirectional LSTM layers. 첫째, 2차 통과 모델이 연속 스트리밍 설정이 아닌 재 득점에 사용된다는 사실은 양방향 LSTM 레이어를 사용할 수 있게 합니다. Secondly, using context-independent phones as targets allows us to share training data with the main ASR. 둘째, context-independent phones 를 target으로 사용하면 주요 ASR과 training 데이터를 공유할수 있습니다. This is particularly important since in many cases it is not possible to obtain a large number of training utterances with the trigger phrase, for example when developing a trigger detector for a new language. 이는 특히 중요하다. 많은 경우에 가령 새로운 언어로 트리거 감지를 개발할때 training 발화를 trigger phrase로 다량의 데이터를 얻는것이 불가능하기 때문이다. Furthermore, having CI phones as targets results in a flexible model that can be used for detecting any keyword. 더 나아가서 유연한 모델에서 CI 음소들을 타겟 결과로 갖는 것은 어느 키워드를 감지하는데 사용될수 있다. Given an audio segment x from the first pass, we are interested in calculating the probability of the phone sequence in the trigger phrase, $P(TriggerPhrasePhoneSeq|x)$. segment x 음성이 1차에서 주어졌을때, 우리는 trigger phrase의 음성 시퀀스, $P(TriggerPhrasePhoneSeq|x)$,의 확률을 계산하는 것에 관심이 있다. Multi-Task LearningThe question we really want to answer is, “given an audio segment from the first pass, does it contain the trigger phrase or not?” 우리가 실제로 답하고 싶은 질문은 “1차에서 통과된 주어진 음성 segment가 trigger phrase를 포함하고 있는가 아닌가?” 이다. We would like the second-pass model to be a binary classifier which determines the presence or absence of the trigger phrase. 우리는 2차 모델이 trigger phrase를 포함하는지 하지 않는지를 결정하는 이진 분류기였으면 한다. However the issue with this design is that collecting a large number of training examples that result in false detections by the baseline system is a difficult problem (c.f. Section 4). 그러나, 이 디자인의 이슈는 baseline 시스템에 의해 다량의 training 예시를 수집하는 것이 어려운 문제라는 것이다. Furthermore, the second pass models have millions of parameters, so they can easily overfit a small training set resulting in poor generalisation. 더 나아가 2차 모델은 수백만개의 파라미터가 있다. 그래서 그들은 작은 training set에도 쉽게 과적합 되며 안좋은 일반화를 결과로 낸다. Therefore, we are faced with the choice between a more general phonetic AM that can be trained on a large, readily available dataset but is optimised for the wrong criterion or a trigger phrase specific detector that is trained on the correct criterion but with a significantly smaller training dataset. 따라서, 우리는 크고 손쉽게 사용 가능한 데이터셋으로 train된 하지만 잘못된 기준으로 최적화된 일반화된 음성 AM과 올바른 기준으로 train됐지만 매우 적은 training 데이터로 학습된 특정 trigger phrase 감지기, 둘 중 하나를 선택해야 했다. One solution to this problem is to use multi-task learning (MTL) [19] 우리의 해결책은 multi-task learning을 사용하는 것이었다. Note that predicting the sequence of phonetic labels in an utterance and deciding whether an utterance contains a specific trigger phrase or not, are related tasks. 발화 속에서 phonetic label의 시퀀스를 예측하는 것 그리고 발화가 trigger phrase를 포함하고 있는지 아닌지 결정하는 것은 서로 연관성이 있는 일이다. We train a single network with a stack of shared&#x2F;tied biLSTM layers with two seperate output layers (one for each task) and train the network jointly on both sets of training data (Figure 2). 우리는 두개의 출력 레이어(각각 하나의 task씩)를 갖고 있고 공유하는&#x2F;묶인 biLSTM 레이어들의 묶음으로 이루어진 하나의 네트워크를 훈련했고 두 세트의 training data(Figure 2)에 합동으로 엮인 네트워크를 훈련했다. We hypothesise that the joint network is able to learn useful features from both tasks: a) the network can be trained to predict phone labels on a large labelled dataset of general speech which covers a wide distribution of complex acoustic conditions, b) the same network can also learn to discriminate between examples of true triggers and confusable examples on a relatively smaller dataset. 우리는 합동 네트워크가 두 task의 유용한 feature들을 학습이 가능하다고 가정했다: a) 네트워크는 복잡한 음향 조건의 광범위한 분포를 다루는 일반적인 음성의 큰 labelled된 데이터 셋을 기반으로 phone label들을 예측하게끔 훈련될수 있다. b) 같은 네트워크는 또한 상대적으로 적은 데이터 양으로 실제의 trigger phrase와 헷갈리는 예시들을 구분하는 법을 배울수 있다. An alternative view of this process is that the phonetic transcription task with a significantly larger training set acts as a regulariser for the trigger phrase discrimination task with a much smaller dataset. 이 process에 대한 다른 시각은 엄청나게 많은 training set를 사용하는 phonetic transcription task가 regulariser로 훨씬 적은 데이터를 가지고 trigger pharse를 구분할때 사용된다. The objective function for the phrase specific&#x2F;discriminative output layer is defined as follows: the softmax output layer contains two output units, one for the trigger phrase and the other one for the blank symbol used by the CTC loss function [8, 16] phrase를 특정화&#x2F;차별화 하는 output layer의 목적함수는 다음과 같이 정의된다: softmax output layer은 두개의 output unit을 포함하는데 하나는 trigger phrase를 위함이고 또 다른 하나는 CTC loss function에서 사용되는 blanck symbol을 위함이다. EvaluationThere were 100 subjects, approximately balanced between male and female adults. Distances from the device were controlled, ranging from 8 to 15 feet away. 대략적으로 반반의 성비로 100명의 참가자들이 참여했다. 기기와의 거리도 8-15피트 정도로 통제되었다. There are over 13K utterances overall, evenly divided between four acoustic conditions: (a) quiet room, (b) external noise from a TV or kitchen appliance in the room, (c) music playback from the recording device at medium volume, and (d) music playback from the recording device at loud volume. 전반적으로 만 3천개의 발화들이 있는데 네가지 컨디션으로 동일하게 나누어졌다: (a) 조용한 방, (b) 방의 TV 또는 주방기기의 외부 소음. (c) 중간 볼륨의 녹음 장치에서 음악 재생, (d) 큰 볼륨의 녹음 장치에서 음악 재생 These examples are used to measure the proportion of false rejections (FRs). 이 예시들은 FR 비율을 측정하기 위해 사용되었다. In addition to these recordings, this test set also consists of almost 2,000 hours of continuous audio recordings from TV, radio, and podcasts. This allows the measurement of the false-alarm (FA) rate in terms of FAs per hour of active external audio. 그 녹음들에 관해 덧붙이자면, 이 test set 또한 TV, 라디오, 팟캐스트로부터 2천 시간의 연속적인 음성 녹음을 포함하고 있다. 이를 통해 active한 외부 음성의 시간당 FA, FA비율을 측정할 수 있다. The second test set is an unstructured data collection at home by our employees, designed to be more representative of realistic, spontaneous usage of the smart speaker. 두번째 test set은 사원들의 집에서 나는 정제되지 않은 데이터 모음이다. 이는 조금더 현실적이고 즉흥적인 스마트폰 스피커의 사용을 대표한다. With this data, it is possible to measure nearly unbiased false-reject and false-alarm rates for realistic in-home scenarios similar to customer usage. 이 데이터로 현실적인 집 내부의 시나리오들(소비자의 사용과 비슷한)을 위한 편향되지 않은 FA와 FA 비율 측정이 가능하다. We use detection error trade-off (DET) curves to compare the accuracy between models. Each curve displays the FA rate and the proportion of FRs associated with sweeping the trigger threshold for aparticular model. 우리는 DET 곡선을 두 모델의 정확성을 비교하기 위해 사용한다. 각 곡선은 특정 모델에 대한 트리거 임계값을 스윕하는 것과 연관된 FA 비율 및 FR 비율을 표시한다. In practice, we compare the shapes of the DET curves for different models in the vicinity of viable operating points. 실제로 우리는 실행가능한 작동지점 근처에서 서로 다른 모델들의 DET 곡선의 모양을 비교했다. We compare five models: the baseline phonetic CTC model trained on the ASR dataset (blue), the baseline phrase specific model trained on the much smaller training set with randomly initialised weights (red), the same phrase specific model but with weights initialised with the learned weights from the baseline phonetic CTC model (yellow), the phonetic (purple) and phrase specific (green) branches of the proposed MTL model. 우리는 다섯개의 모델을 비교했다 파랑: ASR 데이터로 훈련한 baseline phonetic CTC 모델 빨강: 무작위로 초기화된 가중치를 갖는 훨씬 더 작은 training 세트 상에서 훈련된 baseline phrase 특정 모델 노랑: 동일한 phrase의 특정 모델이지만 기본 음성 CTC 모델의 학습된 가중치를 초기화된 가중치를 갖는 모델 보라: 제안된 MTL모델 중 phonetic branches 초록: 제안된 MTL 모델 중 phrase specific 모델 Note that the phrase specific model with weight initialisation from the baseline phonetic model (yellow) is effectively trained using both datasets. baseline phonetic model의 초기화된 가중치를 가지는 phrase specific 모델은 효과적으로 두 dataset을 사용하면서 훈련되었다. In both test sets, the MTL phonetic (purple) and phrase-specific (green) models outperform the baseline phonetic CTC (blue), reducing the FR rate by almost half at many points along the curve. 양쪽 test set에서 보라색 그리고 초록색 모델을은 파란색보다 FR비율을 다른 곡선에 비해 반절이나 감소시키면서 결과가 좋았다. The non-MTL phrase specific models (red and yellow) yield significantly worse accuracies in comparison, which is unsurprising given that the training dataset is two orders of magnitude smaller compared to the phonetic baseline (blue). MTL을 사용하지 않은 specific 모델(빨강, 노랑) 부분은 심각하게 안좋은 정확성을 상대적으로 보였으며 이것은 training dataset이 phonetic baseline(파랑)에 비해 두자릿수 더 작다는 점을 감안했을때 당연하다. Comparing the structured data evaluation (left) and the take-home data evaluation (right), it is also striking how the error rates are generally much higher for the latter. 구조화된 데이터 evaluation(왼쪽)과 take-home data evaluation(오른쪽)을 비교해보면 일반적으로 후자의 경우 오류율이 더 높은것도 눈에 띈다. ConclusionsWe trained the model to simultaneously produce phonetic transcriptions on a large ASR dataset and to discriminate between difficult examples on a much smaller trigger phrase specific training set. 우리는 큰 ASR dataset에서 phonetic transcription을 제공하고 동시에 훨씬 더 작은 trigger phrase training set에서 어려운 샘플들을 구별하는 모델을 훈련했다. We evaluate the proposed model on two challenging test sets and find the proposed method is able to almost halve errors and does not require any extra model parameters. 우리는 제안된 모델의 두 test set을 평가했고 제안된 방법이 오류를 반감하기 위해 더 많은 파라미터를 필요로 하지 않은다는 것을 발견했다. Link: Multi-Task Learning for Voice Trigger Detection","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Multi-Task Learning","slug":"Paper/Multi-Task-Learning","permalink":"https://jmj3047.github.io/categories/Paper/Multi-Task-Learning/"}],"tags":[{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Multi-Task Learning","slug":"Multi-Task-Learning","permalink":"https://jmj3047.github.io/tags/Multi-Task-Learning/"},{"name":"Voice Trigger Detection","slug":"Voice-Trigger-Detection","permalink":"https://jmj3047.github.io/tags/Voice-Trigger-Detection/"}]},{"title":"Multi-Task Learning for Speaker Verification and Voice Trigger Detection","slug":"MTL_for_SV&VTD","date":"2022-10-30T15:00:00.000Z","updated":"2022-11-07T14:48:58.652Z","comments":true,"path":"2022/10/31/MTL_for_SV&VTD/","link":"","permalink":"https://jmj3047.github.io/2022/10/31/MTL_for_SV&VTD/","excerpt":"","text":"Journal&#x2F;Conference : ICASSP IEEEYear(published year): 2020Author: Siddharth Sigtia, Erik Marchi, Sachin Kajarekar, Devang Naik, John BridleSubject: Multi-Task Learning Multi-Task Learning for Speaker Verification and Voice Trigger Detection Summary In this study, we investigate training a single network to perform automatic speech transcription and speaker recognition, both tasks jointly. 본 연구에서는 단일 네트워크를 훈련하여 automatic speech transcription와 speaker recognition의 두 가지 작업을 공동으로 수행하는 방법을 연구합니다. We train the network in a supervised multi-task learning setup, where the speech transcription branch of the network is trained to minimise a phonetic connectionist temporal classification (CTC) loss while the speaker recognition branch of the network is trained to label the input sequence with the correct label for the speaker. 우리는 네트워크의 speech transcription 브랜치가 음성 CTC 손실을 최소화하도록 훈련되는 감독 된 멀티 태스킹 학습 설정에서 네트워크를 훈련시키는 반면, 네트워크의 화자 인식 브랜치는 입력 시퀀스를 화자에 대한 올바른 라벨로 라벨링하도록 훈련된다. Results demonstrate that the network is able to encode both phonetic and speaker information in its learnt representations while yielding accuracies at least as good as the baseline models for each task, with the same number of parameters as the independent models. 결과는 네트워크가 학습된 표현에서 음성 및 화자 정보를 모두 인코딩할 수 있으며 독립 모델과 동일한 수의 매개 변수를 사용하여 각 작업의 기본 모델만큼 정확도를 산출할 수 있음을 보여줍니다. IntroductionVoice trigger detection, which is interchangeably known as keyword spotting [4], wake-up word detection [5], or hotword detection [6], is treated as an acoustic modeling problem. keyword spotting [4], wake-up word detection [5] 또는 hotword detection [6]로 상호 교환 가능하게 알려진 음성 트리거 검출은 음향 모델링 문제에 속한다 Their primary aim is to recognise the phonetic content (or the trigger phrase directly) in the input audio, with no regard for the identity of the speaker. 그들의 주된 목표는 화자의 신원을 고려하지 않고 입력 오디오의 음성 내용(또는 트리거 문구)을 인식하는 것이다. On the other hand, speaker verification systems aim to confirm the identity of the speaker by comparing an input utterance with a set of enrolment utterances which are collected when a user sets up their device. 한편, 스피커 검증 시스템은 사용자가 장치를 설정할 때 수집되는 등록 발화 집합과 입력 발화를 비교하여 화자의 신원을 확인하는 것을 목표로 한다. Speaker verification algorithms can be characterised based on whether the phonetic content in the inputs is limited, which is known as text-dependent speaker verification [9]. 화자 검증 알고리즘은 입력의 음성 콘텐츠가 제한되어 있는지 여부에 따라 특성화할 수 있는데, 이를 text-dependent speaker verification[9]이라고 한다. We believe that knowledge of the speaker would help determine the phonetic content in the acoustic signal and vice versa, therefore estimating both properties is similar to solving simultaneous equations. 우리는 화자에 대한 지식이 음향 신호의 음성 내용을 결정하는 데 도움이 되고 그 반대의 경우도 마찬가지라고 생각합니다. 따라서 두 속성을 모두 추정하는 것은 연립 방정식을 푸는 것과 유사합니다. In this study, the main research question we try to answer is “can a single network efficiently represent both phonetic and speaker specific information?”. 이 연구에서 우리가 대답하려고하는 주요 연구 질문은 “하나의 네트워크가 음성 및 화자 특정 정보를 효율적으로 표현할 수 있습니까?”입니다. From a practical standpoint, being able to share computation between the two tasks can save on-device memory, computation time or latency and the amount of power&#x2F;battery consumed. 실용적인 관점에서 두 작업간에 계산을 공유할 수 있으면 장치 메모리, 계산 시간 또는 대기 시간 및 소비되는 전력 &#x2F; 배터리 양을 절약할 수 있습니다. More generally, we are interested in studying whether a single model can perform multiple speech understanding tasks rather than designing a separate model for each task. 보다 일반적으로, 우리는 각 작업에 대해 별도의 모델을 설계하기보다는 단일 모델이 여러 개의 음성 이해 작업을 수행할 수 있는지 연구하는 데 관심이 있습니다. We train a joint network to perform a phonetic labelling task and a speaker recognition task. 우리는 음성 라벨링 작업과 화자 인식 작업을 수행하기 위해 공동 네트워크를 훈련시켰습니다. We evaluate the 2 branches of the model on a voice trigger detection task and a speaker verification task, respectively. 우리는 음성 트리거 검출 작업과 화자 검증 작업에서 모델의 두 가지를 각각 평가합니다. It is possible for a single network to encode both speaker and phonetic information and yield similar accuracies as the baseline models without requiring any additional parameters. 단일 네트워크가 스피커 및 음성 정보를 모두 인코딩하고 추가 매개 변수를 필요로하지 않고 기준 모델과 유사한 정확도를 산출할 수 있습니다. Voice Trigger Detection BaselineWe extract 40-dimensional log-filterbanks from the audio at 100 frame-per-second (FPS). At every step, 7 frames are spliced together to form symmetric windows and finally this sequence of windows is sub-sampled by a factor of 3, yielding a 280-dimensional input vector to the model at a rate of 33 FPS. The features are input to a stack of 4 bidirectional LSTM layers with 256 units in each layer (Figure 1). This is followed by a fully connected layer and an output softmax layer over context-independent phonemes and additional sentence and word boundary symbols, resulting in a total of 53 output symbols and 6 million model parameters. This model is then trained by minimising the CTC loss function [16]. The training data for this model is 5000 hours of anonymised audio data that is manually transcribed, where all of the recordings are sampled from intentional voice assistant invocations and are assumed to be near-field. Fig. 1. The left branch of the model represents the voice trigger detector, the right branch is the speaker verification model. Solid horizontal arrows represent layers with tied weights, dashed arrows represent layers with weights that may or may not be tied. 모델의 왼쪽 분기는 음성 트리거 검출기를 나타내고, 오른쪽 분기는 화자 검증 모델이다. 실선 화살표는 묶인 가중치가 있는 레이어를 나타내고 점선 화살표는 묶일 수도 있고 묶이지 않을 수도 있는 가중치가 있는 레이어를 나타냅니다. Speaker Verification BaselineWe use a simple location-based attention mechanism [18] to summarise the encoder activations as a fixed-dimensional vector. 우리는 인코더 활성화를 고정 차원 벡터로 요약하기 위해 간단한 위치 기반주의 메커니즘 [18]을 사용합니다. We found the attention mechanism to be particularly effective in the text-independent setting. 우리는 attention 메커니즘이 텍스트 독립적인 환경에서 특히 효과적이라는 것을 발견했다. During inference, given a test utterance x, the speaker embedding is obtained by removing the final softmax layer and using the 128-dimensional activations of the previous layer. 추론 중에 테스트 발화 x가 주어지면 스피커 삽입은 최종 소프트 맥스 레이어를 제거하고 이전 레이어의 128 차원 활성화를 사용하여 얻어집니다. Each training utterance is of the form “Trigger phrase, payload” for e.g.“Hey Siri (HS), play me something I’d like”. For every training example, we generate 3 segments: the trigger phrase, the payload and the whole utterance. We found that breaking the utterances up this way results in models that generalise significantly better. Evaluation ConclustionsOur results demonstrate that sharing the first two layers of the model between the speaker and phonetic tasks gives accuracies that are as good as the individual baselines. 우리의 결과는 화자와 음성 작업 사이에 모델의 처음 두 레이어를 공유하면 개별 기준선만큼 정확도가 높다는 것을 보여줍니다. This result indicates that it is possible to share some of the lowlevel computation between speech processing tasks without hurting accuracies. 이 결과는 정확도를 해치지 않으면 서 음성 처리 작업간에 저수준 계산의 일부를 공유할 수 있음을 나타냅니다. Link: Multi-Task Learning for Speaker Verification and Voice Trigger Detection","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Multi-Task Learning","slug":"Paper/Multi-Task-Learning","permalink":"https://jmj3047.github.io/categories/Paper/Multi-Task-Learning/"}],"tags":[{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Multi-Task Learning","slug":"Multi-Task-Learning","permalink":"https://jmj3047.github.io/tags/Multi-Task-Learning/"},{"name":"Speaker Verification","slug":"Speaker-Verification","permalink":"https://jmj3047.github.io/tags/Speaker-Verification/"},{"name":"Voice Trigger Detection","slug":"Voice-Trigger-Detection","permalink":"https://jmj3047.github.io/tags/Voice-Trigger-Detection/"}]},{"title":"Hexo Blog 생성 및 재연결","slug":"Hexo_Create","date":"2022-10-05T15:00:00.000Z","updated":"2023-02-28T07:50:08.127Z","comments":true,"path":"2022/10/06/Hexo_Create/","link":"","permalink":"https://jmj3047.github.io/2022/10/06/Hexo_Create/","excerpt":"","text":"Hexo Blog 생성 간단하게 Hexo 블로그를 만들어 본다. I. 필수 파일 설치 1단계: nodejs.org 다운로드 설치가 완료 되었다면 간단하게 확인해본다. 1$ node -v 2단계: git-scm.com 다운로드 설치가 완료 되었다면 간단하게 확인해본다. 1$ git --version 3단계: hexo 설치 hexo는 npm을 통해서 설치가 가능하다. 1$ npm install -g hexo-cli II. 깃허브 설정 두개의 깃허브 Repo를 생성한다. 포스트 버전관리 (name: myblog) 포스트 배포용 관리 (name: rain0430.github.io) rain0430 대신에 각자의 username을 입력하면 된다. 이 때, myblog repo를 git clone을 통해 적당한 경로로 내려 받는다. $ git clone your_git_repo_address.git III. 블로그 만들기 (옵션) 적당한 곳에 경로를 지정한 다음 다음과 같이 폴더를 만든다. 12$ mkdir makeBlog # 만약 Powershell 이라면 mkdir 대신에 md를 쓴다. $ cd makeBlog 임의의 블로그 파일명을 만든다. 12345$ hexo init myblog$ cd myblog$ npm install$ npm install hexo-server --save$ npm install hexo-deployer-git --save ERROR Deployer not found: git hexo-deployer-git을 설치 하지 않으면 deploy시 위와 같은 ERROR가 발생합니다. _config.yml 파일 설정 싸이트 정보 수정 1234title: 제목을 지어주세요subtitle: 부제목을 지어주세요description: description을 지어주세요author: YourName 블로그 URL 정보 설정 1234url: https://rain0430.github.ioroot: /permalink: :year/:month/:day/:title/permalink_defaults: 깃허브 연동 12345# Deploymentdeploy: type: git repo: https://github.com/rain0430/rain0430.github.io.git branch: main IV. 깃허브에 배포하기 배포 전, 터미널에서 localhost:4000 접속을 통해 화면이 뜨는지 확인해본다. 1234$ hexo generate$ hexo serverINFO Start processingINFO Hexo is running at http://localhost:4000 . Press Ctrl+C to stop. 화면 확인이 된 이후에는 깃허브에 배포한다. 사전에, gitignore 파일에서 아래와 같이 설정을 진행한다. 1234567.DS_StoreThumbs.dbdb.json*.lognode_modules/public/.deploy*/ 최종적으로 배포를 진행한다. 1$ hexo deploy 배포가 완료가 되면 브라우저에서 USERNAME.github.io로 접속해 정상적으로 배포가 되었는지 확인한다. Hexo Blog 재연결 기존 블로그 폴더 파일 압축해서 백업한 후 진행해야 한다. → theme 같은 경우 받아오는거부터 다시 해야 하기 때문 재연결보다는 재생성이라고 말하는게 더 적합하다. 다른 로컬에서 블로그를 재연결해서 사용할 경우 아래와 같이 순차적으로 진행하면 된다. 1234$ hexo init your_blog_repo # 여기는 각자 소스 레포 확인$ cd myblog$ git init $ git remote add origin https://github.com/your_name/your_blog_repo.git # 각자 소스 레포 주소 아래 명령어에서 에러가 발생이 있다. 1$ git pull --set-upstream origin main # 에러 발생 그런 경우, 아래 명령어를 추가한다. 기존의 디렉토리와 파일을 모두 삭제한다는 뜻이다. 1$ git clean -d -f 그리고 에러가 발생했던 명령어를 다시 실행한다. 이 때에는 이제 정상적으로 실행되는 것을 확인할 수 있다. 1$ git pull --set-upstream origin main # 에러 발생 안함 / 소스 확인 이제 정상적으로 환경 세팅은 된 것이다. 순차적으로 아래와 같이 진행하도록 한다. 이 때, theme 폴더에 본인의 테마 소스코드가 잘 있는지 확인을 하도록 한다. 1234$ npm install $ hexo clean$ hexo generate$ hexo server reference 생성 재연결","categories":[{"name":"Setting","slug":"Setting","permalink":"https://jmj3047.github.io/categories/Setting/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://jmj3047.github.io/tags/Hexo/"}]},{"title":"Big Query","slug":"Big_Query","date":"2022-09-28T15:00:00.000Z","updated":"2023-02-28T07:41:28.495Z","comments":true,"path":"2022/09/29/Big_Query/","link":"","permalink":"https://jmj3047.github.io/2022/09/29/Big_Query/","excerpt":"","text":"1. 쿼리 실행순서FROM → WHERE → GROUP BY, Aggregation → HAVING → WINDOW → QUALIFY → DISTINCT → ORDER BY → LIMIT 2. JOIN 3. WINDOW 함수 4. DECLARE 변수를 선언 혹은 초기화할 때 사용 DECLARE variable_name[, ...] [variable_type] [ DEFAULT expression]; 1234567-- 아래 쿼리를 실행하면 값이 모두 1로 나오는걸 확인 할 수 있음-- DEFAULT를 안주면 NULL로 지정 됨DECLARE x, y, z INT64 DEFAULT 1;SELECT x,y,z-- 현재 날짜로 d할당DECLARE d DATE DEFAULT CURRENT_DATE(); 위 예시 말고 쿼리의 결과를 사용해 변수를 초기화 할 수도 있음 5. SET DECLARE와 같이 사용 되어짐. DECLARE에서 변수 타입을 지정하고 SET으로 값 할당이 가능, DECLARE에서 두 과정 모두 할 수 있지만 SET은 쿼리 내 어느 위치에서나 사용 가능 함 6. UDF - User Define Function 영구 UDF는 여러 쿼리에서 재사용 할 수 있음CREATE TEMP FUNCTION → 임시 UDF 생성CREATE FUNCTION → 영구 UDF 생성CREATE OR REPLACE FUNCTION → 영구 UDF 생성 및 수정 12345678910111213141516CREATE OR REPLACE FUNCTION ps-datateam.cbt_global_ceo.함수명(변수명 변수타입)RETURNS INT64 --리턴 타입LANGUAGE js -- 작성 언어(임시는 SQL로도 가능한 듯)AS &quot;&quot;&quot;if (mode == &#x27;solo&#x27;) &#123;return 1;&#125; else if (mode == &#x27;duo&#x27;) &#123;return 2;&#125; else if (mode == &#x27;trio&#x27;)&#123;return 3;&#125; else if (mode == &#x27;squad&#x27;)&#123;return 4;&#125; else &#123;return 0;&#125;&quot;&quot;&quot;; -- 함수 7. 파이썬에서 빅쿼리 데이터 사용12345678910111213141516171819from google.cloud import bigqueryfrom google.oauth2 import service_accountkey_path = &quot;키파일 경로&quot;credentials = service_account.Credentials.from_service_account_file(key_path, scopes=[&quot;https://www.googleapis.com/auth/cloud-platform&quot;])client = bigquery.Client(credentials=credentials, project=&#x27;프로젝트명&#x27;)# 쿼리 결과를 CSV파일로 -----------------------------------------------------------------query = client.query(&quot;&quot;&quot;QUERY&quot;&quot;&quot;)# 쿼리 -&gt; DataFramedf_result = query.to_dataframe ()# DataFrame -&gt; csvdf_result.to_csv(&quot;filename.csv&quot;, index=False, encoding=&#x27;euc-kr&#x27;)# 프로젝트&gt;데이터세트 내의 테이블 목록 조회------------------------------------------------dataset_ref = client.dataset(&quot;kr_dict&quot;) # kr_dict라는 데이터 세트의 정보를 조회dataset = client.get_dataset(dataset_ref)list(client.list_tables(dataset)) # 데이터 세트의 테이블 목록을 리스트로 가져옴 Reference1 Reference2","categories":[{"name":"Data Base","slug":"Data-Base","permalink":"https://jmj3047.github.io/categories/Data-Base/"},{"name":"GCP","slug":"Data-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Base/GCP/"}],"tags":[{"name":"Big Query","slug":"Big-Query","permalink":"https://jmj3047.github.io/tags/Big-Query/"}]},{"title":"Pandas Dataframe 사용법 정리","slug":"Pandas","date":"2022-09-22T15:00:00.000Z","updated":"2022-10-18T15:05:07.992Z","comments":true,"path":"2022/09/23/Pandas/","link":"","permalink":"https://jmj3047.github.io/2022/09/23/Pandas/","excerpt":"","text":"데이터 합치기 https://yganalyst.github.io/data_handling&#x2F;Pd_12&#x2F; https://seong6496.tistory.com/122 https://datascienceschool.net/01 python/04.06 데이터프레임 합성.html https://hyunmin1906.tistory.com/132 on에 대한 설명: https://wikidocs.net/153875 특정 칼럼의 데이터 종류별로 평균&#x2F;합 구하는 방법 https://www.dinolabs.ai/72 행&#x2F;열 순서 및 이름 변경 https://k-glory.tistory.com/21 시리즈, 데이터 프레임으로 만들기 https://hyefungi.tistory.com/68 리스트, 딕셔너리를 데이터프레임, 시리즈로 바꾸는 법 https://jimmy-ai.tistory.com/89 행열 전환 https://computer-science-student.tistory.com/158 데이터 프레임 index 변경하기 https://cosmosproject.tistory.com/337 데이터 프레임 index 조작 https://datascienceschool.net/01 python/04.05 데이터프레임 인덱스 조작.html 데이터 프레임 필요한 열 추출 https://zephyrus1111.tistory.com/43 데이터 프레임 sum 함수 https://www.delftstack.com/ko/api/python-pandas/pandas-dataframe-dataframe.sum-function/ 행추가, 열 삭제&#x2F;추가 방법 https://jimmy-ai.tistory.com/210 https://ldgeao99.tistory.com/8 https://blog.naver.com/rising_n_falling&#x2F;221631637822 데이터프레임에 리스트를 행 추가 https://emilkwak.github.io/dataframe-list-row-append-ignore-index 랜덤 샘플링 https://rfriend.tistory.com/602 특정 조건에 맞는 데이터 추출 https://computer-science-student.tistory.com/375","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Python","slug":"Python/Python","permalink":"https://jmj3047.github.io/categories/Python/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/tags/Python/"},{"name":"Pandas Dataframe","slug":"Pandas-Dataframe","permalink":"https://jmj3047.github.io/tags/Pandas-Dataframe/"}]},{"title":"임계치 조절","slug":"Threshold","date":"2022-09-19T15:00:00.000Z","updated":"2023-02-23T01:38:08.678Z","comments":true,"path":"2022/09/20/Threshold/","link":"","permalink":"https://jmj3047.github.io/2022/09/20/Threshold/","excerpt":"","text":"&lt; 분류에서 사용하는 성능지표 &gt; 1. Confusion Matrix 분류에서 가장 많이 사용되는 오분류표이다. 행렬의 배치는 그리는 사람에 따라 달라질 수 있으며, Scikit learn에 기반한 confusion matrix는 다음과 같다. FP: 예측은 참이나 실제는 거짓, 제 1종 오류FN: 실제는 참이나 예측은 거짓, 제 2종 오류 정밀도에서는 FP를 줄이는 것, 재현율에서는 FN을 줄이는 것이 중요하다.즉 FP, FN이 커지면 정밀도, 재현율 각각 작아진다. 정확도(Accuracy): 전체 데이터 중에서 예측한 값이 실제 값과 부합하는 비율 정밀도, 재현율, 특이도 정밀도(precision): 예측이 참인 값 중 실제 참인 값재현율(recall,Sensitivity,TPR): 실제 참인 값 중 예측도 참인 값특이도(specificity): 예측이 거짓인 값 중 실제 거짓인 값 2. Precision-Recall Trade-off로지스틱 회귀 모형에서 특정 이메일이 스팸일 확률이 0.9이면 스팸일 가능성이 매우 높다고 예측할 수 있다. 이와 반대로 0.03이라면 스팸이 아닐 가능성이 높다. 그렇다면 0.6인 이메일의 경우 어떻게 분류해야 할까? 이때 분류할 기준이 필요한데, 이 기준을 임계값(Threshold)이라 한다. 어떤 분류기의 임계값에 따른 정밀도와 재현율을 그래프로 나타내면 위과 같으며, Precision과 Recall이 만나는 점이 최적의 임계값이다. 임계값을 높이면(Positive로 판별하는 기준을 빡빡하게 잡으면) 정밀도는 올라가고 재현율은 낮아진다. 반대로 임계값을 낮추면(기준을 널널하게 잡으면) 정밀도는 낮아지고 재현율은 높아진다. 이를 정밀도-재현율 트레이드 오프(Precision-Recall Trade-off)라 한다. 어떤 것을 분류하느냐에 따라 정밀도가 더 중요할 때가 있고, 재현율이 더 중요할 때도 있다. 스팸 메일 분류기는 FP를 줄이는 즉 정밀도, 암 환자 분류기는 FN을 줄이는 즉 재현율을 각각 더 중요하게 생각해야 한다. 3. G-mean, F1 measure G-mean, F1 measure 실제 데이터의 특성상 정확도보다는 제1종 오류와 제2종 오류 중 성능이 나쁜 쪽에 더 가중치를 주는 G-mean지표나 정밀도와 재현율만 고려하는 F1 measure가 더 고려해볼 수 있는 지표이다. 둘 다 높으면 높을 수록 좋은 지표이다. (F1 measure가 더 자주 쓰인다.) 4. ROC curve, AUC ROC curve : 양성에 대한 오답&#x2F;정답 비율 시각화 가로축을 1-특이도(FPR), 세로축을 재현율**(TPR)**로 하여 시각화한 그래프이다.FPR과 TPR은 오차 행렬 내 4개의 요소를 사용한 수치이며 다음과 같다. FPR: 실제 Negative 클래스인 인스턴스를 얼마나 잘못 분류했는지를 나타내는 수치.TPR: 실제 Positive인 클래스인 인스턴스를 얼마나 제대로 분류했는 지를 나타내는 수치. 임계값(Threshold)을 변화시키면 FPR이 변하게 된다. 임계값이 높으면(1) 정밀도(Precision)가 높아지며 FP가 낮아지므로(FN이 높아지므로) FPR은 0이다. 반대로, 임계값이 낮으면(0) FP가 높아지고(FN 낮아지므로)TN은 0이므로 FPR은 1이다. 즉 임계값이 낮추면 더 많은 항목이 양성으로 분류되므로 FPR과 TPR이 모두 증가한다. 이렇게 임계값에 따라 FPR을 0~1까지 변화시켜가며 그에 따라 TPR이 어떻게 변화하는지 기록한 것이 ROC curve이다. AUC(area under the curve): ROC곡선 아랫부분 면적: 0~1사이의 값을 가지며, AUC값은 클수록 좋다. 123456from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, precision_score,recall_score,f1_scorefrom sklearn.metrics import precision_recall_curvefrom sklearn.metrics import roc_auc_score,roc_curve#오분류표confusion_matrix(y_test,y_vld) 24는 FN으로 실제는 참이나, 예측은 거짓이다. 즉 생존자를 사망자로 잘못 예측한 경우이다. 반대로 15는 FP이며 사망자를 생존자로 잘못 예측했다. 이 때문에 정확도가 떨어진다. 12# 간략하게 한 번에 보고 싶을 때 사용classification_report(y_test,y_vld) 123456789101112131415161718# 위에(cr)보다 아래 코드를 더 자주 사용한다.accuracy_score(y_test,y_vld)precision_score(y_test,y_vld)recall_score(y_test,y_vld)f1_score(y_test,y_vld)# 예측 확률proba= model.predict_proba(X_test)# precision,recall은 trade off관계, precision_recall_curve( )precision,recall,th = precision_recall_curve(y_test,proba[:,1])plt.xlabel(&#x27;threadhold&#x27;) #임계값plt.ylabel(&#x27;score&#x27;)plt.plot(th,precision[:len(th)],&#x27;red&#x27;,linestyle = &#x27;--&#x27;,label = &#x27;precision&#x27;)plt.plot(th,recall[:len(th)],&#x27;blue&#x27;,label = &#x27;recall&#x27;)plt.legend()plt.show() 12345678# roc_curve( )fpr,tpr,th = roc_curve(y_test,proba[:,1])plt.xlabel(&#x27;FPR-False Positive&#x27;)plt.ylabel(&#x27;TPR-True Positive &#x27;)plt.plot(fpr,tpr,&#x27;red&#x27;,label = &#x27;rate&#x27;)plt.plot([0,1],[0,1],&#x27;blue&#x27;,linestyle = &#x27;--&#x27;,label = &#x27;th 0.5&#x27;)plt.legend()plt.show() 임계값 조정임계값(Threshold)을 조정해 정밀도 또는 재현율의 수치를 높일 수 있다. 하지만 정밀도와 재현율은 trade off관계이기 때문에 한쪽이 올라가면 다른 한쪽이 떨어지기 쉽다. 일반적으로 임계값을 0.5로 정하고 이보다 크면 positive, 작으면 negative이다. predict_proba()를 통해 레이블별 예측확률을 반환한다. Binarizer: threshold 기준값보다 같거나 작으면 0을, 크면 1을 반환한다. &lt;임계값을 0.5로 설정한 경우&gt; 123456789101112131415from sklearn.preprocessing import Binarizer# Binarizer의 threshold 값을 0.5로 설정custom_threshold = 0.5#즉 Positive 클래스의 컬럼 하나만 추출하여 Binarizer를 적용proba_1 = proba[:,1].reshape(-1,1)binarizer=Binarizer(threshold=custom_threshold).fit(proba_1)custom_pred = binarizer.transform(proba_1)recall=recall_score(y_test, custom_pred)acc=accuracy_score(y_test, custom_pred)print(f&quot;재현율:&#123;recall&#125;, 정확도:&#123;acc:.4f&#125;&quot;) &lt;임계값을 0.4로 설정한 경우&gt; 12345678910111213141516from sklearn.preprocessing import Binarizer# Binarizer의 threshold 값을 0.4로 설정custom_threshold = 0.4# predict_proba() 결과 값의 두 번째 컬럼,#즉 Positive 클래스의 컬럼 하나만 추출하여 Binarizer를 적용proba_1 = proba[:,1].reshape(-1,1)binarizer=Binarizer(threshold=custom_threshold).fit(proba_1)custom_pred = binarizer.transform(proba_1)recall=recall_score(y_test, custom_pred)acc=accuracy_score(y_test, custom_pred)print(f&quot;재현율:&#123;recall:.4f&#125;, 정확도:&#123;acc:.4f&#125;&quot;) 임계값을 낮추면 recall이 상승하고, precision이 떨어진다. Reference","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/tags/Python/"},{"name":"Threshold Adjustment","slug":"Threshold-Adjustment","permalink":"https://jmj3047.github.io/tags/Threshold-Adjustment/"},{"name":"Confusion Matrix","slug":"Confusion-Matrix","permalink":"https://jmj3047.github.io/tags/Confusion-Matrix/"},{"name":"Precision-Recall","slug":"Precision-Recall","permalink":"https://jmj3047.github.io/tags/Precision-Recall/"},{"name":"F1 measure","slug":"F1-measure","permalink":"https://jmj3047.github.io/tags/F1-measure/"},{"name":"ROC curve","slug":"ROC-curve","permalink":"https://jmj3047.github.io/tags/ROC-curve/"}]},{"title":"Grid Search CV","slug":"Grid_Search","date":"2022-09-15T15:00:00.000Z","updated":"2023-02-23T01:37:20.820Z","comments":true,"path":"2022/09/16/Grid_Search/","link":"","permalink":"https://jmj3047.github.io/2022/09/16/Grid_Search/","excerpt":"","text":"Grid search finds the optimal parameters; each model has its own parameters, and it compares which combination yields the best score. This time, we will see a combination of two parameters and use decision tree. 123456789101112from sklearn.datasets import load_irisfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.model_selection import GridSearchCV, train_test_splitimport pandas as pd#훈련데이터 검증데이터 분류iris = load_iris()data = iris.datatarget = iris.targetX_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=999) Bring the iris data Data and target are classified into training data and validation data, and the ratio of validation data is set to 20%. Random_state &#x3D; 999 means that you will continue to use random data that you have picked once. It does not matter if it is not 999 or any other number. 123456789101112#그리드서치dtree = DecisionTreeClassifier()grid_parameters = &#123;&quot;max_depth&quot;: [1, 2, 3], &quot;min_samples_split&quot;: [2, 3] &#125;grid_dtree = GridSearchCV(dtree, param_grid=grid_parameters, cv=3, refit=True)grid_dtree.fit(X_train, y_train) Create a decision tree model, save the value of the desired parameter in dictionary form. Cv is the number of sets of training data divided for cross-validation. Refit &#x3D; True means finding the optimal parameter and then learning based on it. It is added another step before fitting 12345678910111213#결과를 데이터 프레임으로 변환scores_df = pd.DataFrame(grid_dtree.cv_results_)print(scores_df)출력mean_fit_time std_fit_time mean_score_time std_score_time param_max_depth param_min_samples_split params split0_test_score split1_test_score split2_test_score mean_test_score std_test_score rank_test_score0 0.001321 4.587829e-04 0.000997 3.893359e-07 1 2 &#123;&#x27;max_depth&#x27;: 1, &#x27;min_samples_split&#x27;: 2&#125; 0.675 0.675 0.700 0.683333 0.011785 51 0.001676 4.799690e-04 0.001196 2.498432e-04 1 3 &#123;&#x27;max_depth&#x27;: 1, &#x27;min_samples_split&#x27;: 3&#125; 0.675 0.675 0.700 0.683333 0.011785 52 0.002052 8.137361e-05 0.000613 4.378794e-04 2 2 &#123;&#x27;max_depth&#x27;: 2, &#x27;min_samples_split&#x27;: 2&#125; 0.925 0.925 0.975 0.941667 0.023570 33 0.000998 8.104673e-07 0.000332 4.694597e-04 2 3 &#123;&#x27;max_depth&#x27;: 2, &#x27;min_samples_split&#x27;: 3&#125; 0.925 0.925 0.975 0.941667 0.023570 34 0.001271 2.113806e-04 0.000563 4.170978e-04 3 2 &#123;&#x27;max_depth&#x27;: 3, &#x27;min_samples_split&#x27;: 2&#125; 0.975 0.950 0.975 0.966667 0.011785 15 0.001044 4.221438e-05 0.000665 4.699172e-04 3 3 &#123;&#x27;max_depth&#x27;: 3, &#x27;min_samples_split&#x27;: 3&#125; 0.975 0.950 0.975 0.966667 0.011785 1 To see the results fitted above, use cv_results_. Then there are a number of indicators. 123456789101112131415161718#최적의 파라미터 출력scores_df = scores_df[[&quot;params&quot;, &quot;mean_test_score&quot;, &quot;rank_test_score&quot;, &quot;split0_test_score&quot;, &quot;split1_test_score&quot;, &quot;split2_test_score&quot;]]print(scores_df)# 최고의 파라미터 저장해줌print(f&quot;최적의 파라미터: &#123;grid_dtree.best_params_&#125;&quot;)print(f&quot;최고 정확도: &#123;grid_dtree.best_score_&#125;&quot;)출력 params mean_test_score rank_test_score split0_test_score split1_test_score split2_test_score0 &#123;&#x27;max_depth&#x27;: 1, &#x27;min_samples_split&#x27;: 2&#125; 0.683333 5 0.675 0.675 0.7001 &#123;&#x27;max_depth&#x27;: 1, &#x27;min_samples_split&#x27;: 3&#125; 0.683333 5 0.675 0.675 0.7002 &#123;&#x27;max_depth&#x27;: 2, &#x27;min_samples_split&#x27;: 2&#125; 0.941667 3 0.925 0.925 0.9753 &#123;&#x27;max_depth&#x27;: 2, &#x27;min_samples_split&#x27;: 3&#125; 0.941667 3 0.925 0.925 0.9754 &#123;&#x27;max_depth&#x27;: 3, &#x27;min_samples_split&#x27;: 2&#125; 0.966667 1 0.975 0.950 0.9755 &#123;&#x27;max_depth&#x27;: 3, &#x27;min_samples_split&#x27;: 3&#125; 0.966667 1 0.975 0.950 0.975최적의 파라미터: &#123;&#x27;max_depth&#x27;: 3, &#x27;min_samples_split&#x27;: 2&#125;최고 정확도: 0.9666666666666667 Print out the optimal parameters which is max_depth:3, min_samples_split:2 and the score is 0.96 In this case, the function that gives the best parameter and the highest score is best_params_, best_score_. Korean reference","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/tags/Python/"},{"name":"Grid Search CV","slug":"Grid-Search-CV","permalink":"https://jmj3047.github.io/tags/Grid-Search-CV/"}]},{"title":"Ensemble Model","slug":"Ensemble","date":"2022-09-14T15:00:00.000Z","updated":"2023-02-23T01:36:55.728Z","comments":true,"path":"2022/09/15/Ensemble/","link":"","permalink":"https://jmj3047.github.io/2022/09/15/Ensemble/","excerpt":"","text":"1. Ensemble Model어떠한 한 현상에 대한 답을 얻는다고 가정해보자, 많은 경우에 한 명의 전문가보다 여려 명의 일반인들의 의견이 더 나은 경우가 있다. 위 예제와 비슷하게, 하나의 좋은 모형(회귀,분류)으로부터 예측을 하는 것보다 여러 개의 모형으로부터 예측을 수집하는 것이 더 좋은 예측을 할 수 있다. 이러한 여러 개의 모형을 앙상블이라고 부르고, 여러 개의 모형을 조화롭게 학습시키는 것을 앙상블 학습이라고 한다. 그리고 6주차에서 배운 결정 트리 모형이 하나가 아니라, 훈련 세트를 무작위로 다른 서브셋으로 만들어서 결정 트리 분류기를 만들고, 많은 모형들 중에서 가장 많은 선택을 받은 클래스를 예측하는 앙상블 모형을 랜덤포레스트라고 한다. 오늘날의 랜덤포레스트 모델은 가장 강력한 머신러닝 알고리즘 하나이다. 그리고 머신러닝 대회에서 우승하는 솔루션들은 대부분 앙상블 방법을 사용하여서 최고 성능을 낸다. 뒤에서 앙상블 방법들 중 배깅을 설명할 것이다. 투표 기반 분류기 하나의 데이터셋을 여러종류의 분류기들로 훈련시켰다고 가정해보자. 위에서 언급한대로 하나의 좋은 모델을 사용하는 것보다, 여러 종류의 분류기들이 가장 많이 예측한 클래스를 예측하는 것이 더 좋은 분류기를 만드는 매우 간단한 방법이다. 이렇게 다수결의 투표로 정해지는 분류기를 hard voting(집접 투표) 분류기라고 한다. 놀랍게도 위 모델 중 가장 성능이 좋은 모델의 정확도보다 다수결을 통해 예측한 앙상블 모델의 성능이 높은 경우가 많다. 이렇게 랜덤 추측보다 조금 더 높은 성능을 내는 weak learner(약한 학습기) 가 충분히 많고 다양하다면 strong learner(강한 학습기)가 될 수 있다. 어떻게 약한 학습기가 강한 학습기가 되어서 더 좋은 성능을 낼 수 있을까?, 이 질문은 &quot;큰 수의 법칙&quot;으로 설명될 수 있다. 먼저, 50:50의 동전이 아니라, 51:49의 불균형하게 앞면과 뒷면이 나오는 동전이 있다고 가정을 해보자. 이 동전을 1,000번을 던진다면 거의 앞면 510번과 뒷면 490번이 나올 것이다. 수학적으로 1,000번을 던졌을 때 앞면이 더 많게 나오는 확률은 거의 75% 정도 된다. 수학적으로 10,000번을 던졌을 때 앞면이 더 많게 나오는 확률은 거의 97% 정도 된다. 위 수학적 계산은 이항분포의 확률 질량 함수로 계산 가능하다. ex) 1-scipy.stats.binom.cdf(499,1000,0.51) &#x3D; 0.747 위의 내용을 기반으로 우리의 약한 분류기(51%) 1,000개로 앙상블 모형을 구축하고, 가장 많은 클래스를 예측으로 삼는다면 75%, 10,000개로 모형을 만들면 97% 정도의 성능을 낼 수 있다. 하지만..! 위의 과정은 모든 분류기가 완벽하게 독립이고, 모델의 예측 오차에 대해서 상관관계가 없을때만 가능하다. 🌞 TIP : 앙상블에서 예측기가 가능한 서로 독립일 때 최고 성능을 발휘한다. 그래서 가능한 다양한 알고리즘을 사용해서 학습을 하면 다양한 종류의 오차를 만들기 때문에 앙상블 모델의 성능을 높일 수 있다. 여러 종류의 알고리즘을 사용해서 투표기반 분류기를 만드는 예제를 해보자. 123456789101112131415161718192021222324252627282930313233343536373839404142from sklearn.datasets import load_irisfrom sklearn.ensemble import RandomForestClassifier,VotingClassifierfrom sklearn.linear_model import LogisticRegressionfrom sklearn.svm import SVCfrom sklearn.metrics import accuracy_scorefrom sklearn.model_selection import train_test_split# 데이터셋 로드iris = load_iris()X = iris.data[:,2:] # 꽃잎의 길이, 너비Y = iris.targetx_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.3,random_state=2021,shuffle=True)# 약한 학습기 구축log_model = LogisticRegression()rnd_model = RandomForestClassifier()svm_model = SVC()# 앙상블 모델 구축# 만약에 모든 모델이 predict_proba() 메서드가 있으면, 예측의 평균을 내어 soft voting(간접 투표)도 할수 있다.# 간접 투표 방식은 확률이 높은 투표에 비중을 두기 때문에 성능이 더 높다. (voting=&#x27;soft&#x27; 사용)# svc는 기본적으로 predict_proba를 제공하지 않아, probability = True 지정 해야 사용 가능# 대신 svc에서 probability = True를 지정하면 교차 검증을 사용해서 확률을 추정하기 때문에 훈련 속도 느려짐# 대신 성능을 올라감voting_model = VotingClassifier( estimators=[(&#x27;lr&#x27;,log_model),(&#x27;rf&#x27;,rnd_model),(&#x27;svc&#x27;,svm_model)], # 3개의 약한 학습기 voting=&#x27;hard&#x27; # 직접 투표(hard voting))# 앙상블 모델 학습voting_model.fit(x_train,y_train)# 모델 비교for model in (log_model,rnd_model,svm_model,voting_model): model.fit(x_train,y_train) y_pred = model.predict(x_test) print(model.__class__.__name__,&quot; : &quot;,accuracy_score(y_test,y_pred))&gt; LogisticRegression : 1.0 RandomForestClassifier : 0.9555555555555556 SVC : 1.0 VotingClassifier : 1.0 2.배깅과 페이스팅 앙상블 모형의 좋은 성능을 내기 위해서는 다양한 종류의 오차를 만들어야 하고, 그러기 위해서는 다양한 알고리즘을 사용해야 한다고 배웠다. 다양한 오차를 만들기위한 다른 하나의 방법으로는 훈련 세트의 서브셋을 무작위로 구성하여 모델을 학습시키는 것이 있다. 이를 배깅과 페이스팅이라고 부른다. 배깅 : 훈련 세트의 중복을 허용하여 샘플링을 하는 방식 (통계학에서는 “부트스트래핑”이라고도 부름) 페이스팅 : 훈련 세트의 중복을 허용하지 않고 샘플링 하는 방식 배깅은 각 예측기가 학습하는 서브셋에 다양성을 증가시키므로 페이스팅보다 편향이 조금 더 높다. 하지만 배깅은 예측기들의 상관관계를 줄이므로 앙상블의 분산을 감소 시킨다. 전반적으로 배깅이 더 나은 모델을 만들지만, 시간과 장비가 좋다면 교차검증으로 배깅과 페이스팅을 둘다 해보면 좋다. 1. 사이킷런의 배깅과 페이스팅 12345678910111213141516171819202122from sklearn.ensemble import BaggingClassifierfrom sklearn.tree import DecisionTreeClassifier# 모델 구축# BaggingClassifier에서 사용한 분류기가 클래스 확률추정(predict_proba)이 가능하면 자동으로 간접 투표 사용 bag_model = BaggingClassifier( DecisionTreeClassifier(), # 약한 학습기(결정 트리) n_estimators=500, # 약한 학습기(결정 트리) 500개 생성 max_samples=0.05, # 0.0~1.0 사이 실수 선택(실수 x 샘플 수) 혹은 샘플수 지정 bootstrap=True, # True : 배깅, False : 페이스팅 n_jobs=-1 # 훈련과 예측에 사용할 CPU 코어 수 (-1 : 가용한 모든 코어 사용))# 모델 학습bag_model.fit(x_train,y_train)# 모델 예측y_pred = bag_model.predict(x_test)# 모델 평가print(bag_model.__class__.__name__,&quot; : &quot;,accuracy_score(y_test,y_pred))&gt; BaggingClassifier : 0.9777777777777777 단일 결정 트리와 배깅을 사용한 결정트리 앙상블의 결정경계를 비교해보면 트리 앙상블이 더욱 일반화가 잘 된것을 확인할 수 있다. 2. oob 평가 배깅(중복 허용 샘플링)을 하다보면 평균적으로 훈련 샘플의 약 63%정도만 추출되고 나머지 약 37%는 추출되지 않고, 이렇게 추출되지 않은 샘플들을 oob(out-of-bag)샘플이라고 부른다. 예측기가 훈련되는 동안에는 oob샘플을 사용하지 않으므로, 검증 세트나 교차 검증을 사용하지 않고 oob샘플만을 가지고 모델 최적화를 위한 평가를 할 수 있다. 앙상블의 평가는 각 예측기의 oob평가의 평균으로 확인한다. 1234567891011121314151617181920# 모델 구축bag_model = BaggingClassifier( base_estimator = DecisionTreeClassifier(), n_estimators = 500, bootstrap = True, n_jobs = -1, oob_score = True # oob평가를 위해 True를 지정한다.)# 모델 학습bag_model.fit(x_train,y_train)# 모델 평가(oob_score_)print(&#x27;oob_score : &#x27;,bag_model.oob_score_)# 모델 평가y_pred = bag_model.predict(x_test)print(&#x27;test_score : &#x27;,accuracy_score(y_test,y_pred))&gt;oob_score : 0.9523809523809523 test_score : 0.9333333333333333 3. 랜덤 포레스트 랜덤포레스트는 일반적으로 배깅방법을 사용한 결정트리 앙상블 모델이다. 그래서 BaggingClassifier에 DecisionTreeClassifier를 넣는 대신, RandomForestClassifier를 사용할 수 있다. 그래서 RandomForestClassifier는 DecisionTreeClassifier와 BaggingClassifier 매개변수 모두 가지고 있다. 랜덤포레스트 모델은 트리의 노드를 분할할 때 전체 특성 중에서 최선의 특성을 찾는 것이 아니라, 무작위로 선택한 특성들 중에서 최선의 특성을 찾는 방식을 채택하여 무작위성을 더 가지게 된다. 이를 통해 약간의 편향은 손해보지만, 더욱 다양한 트리를 만들므로 분산을 전체적으로 낮추어서 더 훌륭한 모델을 만들 수 있다. 123456789101112131415161718from sklearn.ensemble import RandomForestClassifier# 랜덤포레스트 모델 구축rnd_model = RandomForestClassifier( n_estimators = 500, # 예측기 500개 max_leaf_nodes = 16, # 자식노드의 최대 개수 n_jobs = -1 # CPU 코어 구동 개수)# 모델 학습rnd_model.fit(x_train,y_train)# 모델 예측y_pred_rf = rnd_model.predict(x_test)# 모델 평가print(&quot;rnd_model : &quot;,accuracy_score(y_pred_rf,y_test))&gt; rnd_model : 0.9333333333333333 Reference https:&#x2F;&#x2F;velog.io&#x2F;@changhtun1&#x2F;ensemble#-랜덤-포레스트","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/tags/Python/"},{"name":"Ensemble Model","slug":"Ensemble-Model","permalink":"https://jmj3047.github.io/tags/Ensemble-Model/"}]},{"title":"Decision Tree Classifier","slug":"Decision_Tree","date":"2022-09-12T15:00:00.000Z","updated":"2023-02-23T01:40:32.401Z","comments":true,"path":"2022/09/13/Decision_Tree/","link":"","permalink":"https://jmj3047.github.io/2022/09/13/Decision_Tree/","excerpt":"","text":"1. 의사결정트리 데이터에 있는 규칙을 학습을 통해 자동으로 찾아내 트리 기반의 분류 규칙을 만드는 알고리즘입니다. 조금 더 쉽게 하자면 if else를 자동으로 찾아내 예측을 위한 규칙을 만드는 알고리즘입니다. 하지만 Decision Tree에서 많은 규칙이 있다는 것은 분류 방식이 복잡해진다는 것이고이는 과적합(Overfitting)으로 이어지기 쉽습니다. 트리의 깊이(depth)가 깊어질수록 결정트리는 과적합되기 쉬워 예측 성능이 저하될 수 있습니다. 가능한 적은 규칙노드로 높은 성능을 가지려면 데이터 분류를 할 때 최대한 많은 데이터 세트가 해당 분류에 속할 수 있도록 규칙 노드의 규칙이 정해져야 합니다. 이를 위해 최대한 균일한 데이터 세트가 구성되도록 분할(Split)하는 것이 필요합니다.(분할된 데이터가 특정 속성을 잘 나타내야 한다는 것입니다.) 규칙 노드는 정보균일도가 높은 데이터 세트로 쪼개지도록 조건을 찾아 서브 데이터 세트를 만들고, 이 서브 데이터에서 이런 작업을 반복하며 최종 클래스를 예측하게 됩니다. 사이킷런에서는 기본적으로 지니계수를 이용하여 데이터를 분할합니다. 지니계수 : 경제학에서 불평등지수를 나타낼 때 사용하는 것으로 0일 때 완전 평등, 1일 때 완전 불평등을 의미합니다. 머신러닝에서는 데이터가 다양한 값을 가질수록 평등하며 특정 값으로 쏠릴 때 불평등한 값이 됩니다. 즉, 다양성이 낮을수록 균일도가 높다는 의미로 1로 갈수록 균일도가 높아 지니계수가 높은 속성을 기준으로 분할된다. 2. 파라미터정보 균일도 측정 방법 정보 이득 방식: 엔트로피는 데이터의 혼잡도를 의미한다. 엔트로피가 놓다는 것은 혼잡도가 높다는 것 지니계수: 불평등지수&#x2F; 이 값이 0이면 평등하다는 것을(분류가 잘됐다) 뜻한다. 이 값이 리프노드가 된다 min_samples_split 노드를 분할하기 위한 최소한의 샘플 데이터수 → 과적합을 제어하는데 사용 Default &#x3D; 2 → 작게 설정할 수록 분할 노드가 많아져 과적합 가능성 증가 min_samples_leaf 리프노드가 되기 위해 필요한 최소한의 샘플 데이터수 min_samples_split과 함께 과적합 제어 용도 불균형 데이터의 경우 특정 클래스의 데이터가 극도로 작을 수 있으므로 작게 설정 필요 max_features 주요 파라미터 최적의 분할을 위해 고려할 최대 feature 개수 Default &#x3D; None → 데이터 세트의 모든 피처를 사용 int형으로 지정 →피처 갯수 &#x2F; float형으로 지정 →비중 sqrt 또는 auto : 전체 피처 중 √(피처개수) 만큼 선정 log : 전체 피처 중 log2(전체 피처 개수) 만큼 선정 max_depth 트리의 최대 깊이 default &#x3D; None → 완벽하게 클래스 값이 결정될 때 까지 분할 또는 데이터 개수가 min_samples_split보다 작아질 때까지 분할 깊이가 깊어지면 과적합될 수 있으므로 적절히 제어 필요 max_leaf_nodes 리프노드의 최대 개수 random_state 주로 검색해서 나오는 소스코드에 random_state &#x3D; 42 라고 되어있어서 엄청난 의미를 가진 것 같지만 사실 42라는 random_state에 할당된 숫자 자체에 큰 의미는 없습니다. 중요한건 이 random_state를 None으로 두냐 정수를 넣느냐더라구요. random_status가 None인 경우 한번 Decision tree를 생성할 때 1,3,47,5…번 데이터를 이용했다고 해서 다시 이 Decision tree를 생성할 때 1,3,47…번째 데이터를 이용하지는 않습니다. 또 다른 어떤 난수번째의 데이터를 이용하게 되는거죠. 만약 random_state가 None이면 정말 규칙없는 어떤 데이터를 뽑아서 Decision tree를 생성하게 되지만 random_state에 어떤 값이 있다면 난수 생성에 어떠한 규칙(이건 만든사람 말고는 알 수 없음)을 넣어서 동일한 결과가 나오게 합니다. 예를 들어 1이라는 값을 넣어서 1,3,47,5…번째 데이터를 이용했다면 또다시 1을 넣으면 1,3,47,5…번째 데이터를 사용하게 되는거죠. 이걸 일반적으로는 random seed라고 합니다. random seed 일반적으로 시스템이 난수를 만들 때 말이 난수지 일정한 패턴의 수를 생성합니다. 여기서 인자로 random seed라는걸 넣어서 어떠한 규칙을 만들어주는건데 C에서는 기본적으로 random seed가 정해져있어서 일반적으로 시간을 random seed로 쓰는 반면 파이썬에서는 기본적으로 이 random seed가 없는 경우 완전 랜덤이 되더라구요. 그래서 이 Random Seed라는건 불규칙속에 규칙을 만들어주는 매개변수라고 생각해주시면 됩니다. random_state를 사용하는 이유 만약 Random_state를 None으로 두는 경우 Decisiontreeclassifier 함수를 이용해 Decision tree를 생성하면 그때그때 다른 데이터를 이용하기 때문에 결과가 바뀝니다. 그러나 Random_state에 변수를 입력할 경우 특정한 규칙을 갖게 되고 A라는 사람이 random_state&#x3D;1로 Decision tree를 생성할 때와 B라는 사람이 random_state&#x3D;1로 Decision tree를 생성할 때의 결과가 동일해지도록 하는거죠. 그러니까 random_state가 0인지 1인지 42인지보다는 같은 변수를 이용해 같은 결과를 도출해내는 데에 큰 의미가 있습니다. 3. 장,단점장점 쉽고 직관적입니다. 각 피처의 스케일링과 정규화 같은 전처리 작업의 영향도가 크지 않습니다. 단점 규칙을 추가하며 서브트리를 만들어 나갈수록 모델이 복잡해지고, 과적합에 빠지기 쉽습니다 → 트리의 크기를 사전에 제한하는 튜닝이 필요합니다. 4. 쿼리 구현123456789101112131415161718192021222324252627282930313233343536373839404142from sklearn.tree import DecisionTreeClassifierfrom sklearn.linear_model import LogisticRegressionfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.preprocession import LabelEncoder, OneHotEncoder, StandardScaler, MinMaxScaler, Binarizerfrom sklearn.model_selection import train_test_split, GridSearchCVfrom skelearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_scorefrom sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curveimport pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as snsimport warningswarnings.filterwarnings(&#x27;ignore&#x27;)from sklearn.datasets import load_iris#분류기 생성dtc_iris = DecisionTreeClassifier(random_state=100)#데이터 로드 및 전처리#학습, 테스트 데이터로 분리iris_data = load_iris()X_train, X_test, y_train, y_test, = train_test_split(iris_data.data, iris_data.target, test_size=0.2, random_state=100)#학습dtc_iris.fit(X_train, y_train)from sklearn.tree import export_graphviz#export_graphiz()의 호출 결과로 out_file로 지정된 tree.dot파일을 생성함export_graphiz(dtc_iris, out_file=&quot;tree.dot&quot;, class_names = iris_data.target_names, feature_names = iris_data.feature_names, impurity=True, filled=True)print(&#x27;===============max_depth의 제약이 없는 경우의 Decision Tree 시각화==================&#x27;)import graphviz# 위에서 생성된 tree.dot 파일을 Graphiviz 가 읽어서 시각화with open(&quot;tree.dot&quot;) as f: dot_graph = f.read()graphviz.Source(dot_graph) 12345678910111213import seaborn as snsimport numpy as np%matplotlib inline# feature importance 추출print(&quot;Feature Importances:\\n&#123;0&#125;\\n&quot;.format(np.round(dt_clf.feature_importances_, 3)))# feature 별 feature importance 매핑for name, value in zip(iris_data.feature_names, dt_clf.feature_importances_): print(&#x27;&#123;0&#125;: &#123;1:.3f&#125;&#x27;.format(name, value)) # feature importance 시각화sns.barplot(x=dt_clf.feature_importances_, y=iris_data.feature_names) Reference https://injo.tistory.com/15 https://continuous-development.tistory.com/173 https://jerry-style.tistory.com/entry/Decisiontreeclassifier-함수의-파라미터-randomstate란","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/tags/Python/"},{"name":"Decision Tree Classifier","slug":"Decision-Tree-Classifier","permalink":"https://jmj3047.github.io/tags/Decision-Tree-Classifier/"}]},{"title":"Density-Based Spatial Clustering of Applications with Noise","slug":"DBSCAN","date":"2022-09-11T15:00:00.000Z","updated":"2023-02-23T01:36:29.696Z","comments":true,"path":"2022/09/12/DBSCAN/","link":"","permalink":"https://jmj3047.github.io/2022/09/12/DBSCAN/","excerpt":"","text":"1. What is Density-Based Spatial Clustering of Applications with Noise DBSCAN (Density-based spatial clustering of applications with noise) uses density-based clustering among clustering algorithms. In the case of K-Means or Hierarchical clustering, clustering is performed using the distance between clusters. Density-based clustering is a method of clustering high-density parts because the dots are densely clustered. To put it simply, if there are ‘n’(or more) points within a radius ‘x’ of a certain point, it is recognized as a single cluster. For example, suppose that there is a point ‘p’, and if there are ‘m’(minPts) points within a distance ‘e’(epsilon) from the point ‘p’, it is recognized as a cluster. In this condition, that is, a point ‘p’ having ‘m’ points within a distance ‘e’ is called a core point. To use the DBSCAN algorithm, the distance epsilon value from the reference point and the number of points(minPts) within this radius, should be passed as a factor. In the figure below, if minPts &#x3D; 4, if there are more than four points in the radius of epsilon around the blue point ‘P’, it can be judged as one cluster, and in the figure below ‘P’ becomes a core point because there are five points. In the figure below, since the gray point P2 has three points within the epsilon radius based on the point P2, it does not reach minPts&#x3D;4, so it does not become the core point of the cluster, but it is called a border point because it belongs to the cluster with the previous point P as the core point. In the figure below, P3 becomes a core point because it has four points within the epsilon radius. However, another core point P is included in the radius around P3, and in this case, core point P and P3 are considered to be connected and are grouped into one cluster. Finally, P4 in the figure below is not included in the range that satisfies minPts&#x3D;4 no matter what point is centered. In other words, it becomes an outlier that does not belong to any cluster, which is called noise point. Putting it all together, we get the following picture: In summary, if there are more points than the number of minPts in the epsilon radius around a point, it becomes a cluster around that point, and that point is called a core point. When a core point becomes part of a cluster of different core points, it became one big cluster. A point belonging to a cluster but not a core point by itself is called a border point, and is mainly a point forming the outer edge of the cluster. A point that does not belong to any cluster becomes a noise point. 2. Key Points The advantage of the DBSCAN algorithm is that it does not have to set the number of clusters like K Means, and because clusters are connected to each other according to the density of clusters, clusters with geometric shapes can be found well, and outlier detection is possible through noise point. It is oftenly used for learning but not in the field → If it is small data, you can use it, but because there is a lot of data in the field, use efficiency is low. Example of clustering geometry Sample Code Korean reference","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}],"tags":[{"name":"Clustering","slug":"Clustering","permalink":"https://jmj3047.github.io/tags/Clustering/"},{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"DBSCAN","slug":"DBSCAN","permalink":"https://jmj3047.github.io/tags/DBSCAN/"},{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/tags/Python/"}]},{"title":"Comparison K means & GMM","slug":"Kmeans_VS_GMM","date":"2022-09-10T15:00:00.000Z","updated":"2023-02-23T01:36:04.330Z","comments":true,"path":"2022/09/11/Kmeans_VS_GMM/","link":"","permalink":"https://jmj3047.github.io/2022/09/11/Kmeans_VS_GMM/","excerpt":"","text":"1. K-Means It can be used for easy, concise, and large data. If the number of features becomes too large with distance-based algorithms, the performance of clustering is degraded. Therefore, in some cases, it is applied with reducing dimensions using PCA In addition, since it is an iterative algorithm, it is a model that is quite sensitive to outliers and learning execution time could slow down when the number of iterations increases rapidly. The K-means API in Scikit-learn provides the following key parameters. n_clusters : As the number of clusters defined in advance, it defines how many clusters K-means will cluster into, that is, the “K” value. init : It is a method of initially setting the coordinates of the cluster center point and is usually set in the ‘k-means++’ method. It is used because setting it in a random way can lead to out-of-the-way results. max_iter : It is to set the number of iterations. If clustering is completed before the set number of times is reached, it ends in the middle even if the number of iterations is not filled. The following are attribute values returned by the K-means API provided by Skikit-learn. In other words, these are values returned by K-means after performing all clustering. labels_ : Returns the cluster label to which each individual data point belongs. (However, keep in mind that this label value is not mapped to the same value as the label value of the actual original data!) cluster_centers_ : After clustering into K clusters, the coordinates of the center points of each K cluster are returned. Using this, the center point coordinates can be visualized. 1234567891011121314151617181920212223242526from sklearn.datasets import make_blobs# X에는 데이터, y에는 클러스터링 된 label값 반환X, y = make_blobs(n_samples=200, n_features=2, centers=3, cluster_std=0.8, random_state=1)print(X.shape, y.shape)# y Target값 분포 확인# return_counts=True 추가하면 array요소마다 value_counts()해줌unique, counts = np.unique(y, return_counts=True)# 클러스터링용으로 생성한 데이터 데이터프레임으로 만들기cluster_df = pd.DataFrame(data=X, columns=[&#x27;ftr1&#x27;,&#x27;ftr2&#x27;])cluster_df[&#x27;target&#x27;] = ycluster_df.head()# 생성 데이터포인트들 시각화해보기target_lst = np.unique(y)markers = [&#x27;o&#x27;,&#x27;s&#x27;,&#x27;^&#x27;,&#x27;P&#x27;,&#x27;D&#x27;]for target in target_lst: target_cluster = cluster_df[cluster_df[&#x27;target&#x27;]==target] plt.scatter(x=target_cluster[&#x27;ftr1&#x27;], y=target_cluster[&#x27;ftr2&#x27;], edgecolor=&#x27;k&#x27;, marker=markers[target])plt.show() Let’s look at the distribution of individual data made with the make_blobs function that creates a clustering dataset. Now, let’s apply K-means to the above data to visualize the coordinates of the center points of each cluster. 123456789101112131415161718192021222324252627282930313233# K-means 클러스터링 수행하고 개별 클러스터 중심을 시각화# 1.K-means 할당kmeans = KMeans(n_clusters=3, init=&#x27;k-means++&#x27;, max_iter=200, random_state=12) # X는 cluster_df의 feature array임.cluster_labels = kmeans.fit_predict(X)cluster_df[&#x27;kmeans_label&#x27;] = cluster_labels# 2.K-means속성의 cluster_centers_는 개별 클러스터의 중심 위치 좌표를 반환centers = kmeans.cluster_centers_unique_labels = np.unique(cluster_labels)markers = [&#x27;o&#x27;,&#x27;s&#x27;,&#x27;^&#x27;,&#x27;P&#x27;]# 3. label별로 루프돌아서 개별 클러스터링의 중심 시각화for label in unique_labels: label_cluster = cluster_df[cluster_df[&#x27;kmeans_label&#x27;] == label] # 각 클러스터의 중심 좌표 할당 center_x_y = centers[label] # 각 클러스터 데이터들 시각화 plt.scatter(x=label_cluster[&#x27;ftr1&#x27;], y=label_cluster[&#x27;ftr2&#x27;], marker=markers[label]) # 각 클러스터의 중심 시각화 # 중심 표현하는 모형 설정 plt.scatter(x=center_x_y[0], y=center_x_y[1], s=200, color=&#x27;white&#x27;, alpha=0.9, edgecolor=&#x27;k&#x27;, marker=markers[label]) # 중심 표현하는 글자 설정 plt.scatter(x=center_x_y[0], y=center_x_y[1], s=70, color=&#x27;k&#x27;, edgecolor=&#x27;k&#x27;, marker=&#x27;$%d$&#x27; % label)# label값에 따라 숫자로 표현한다는 의미plt.show() 2. GMM It is a parametric model and is a representative clustering model using EM algorithms. It is to estimate the probability that individual data belongs to a specific normal distribution under the assumption that they belong to a Gaussian distribution. n_components is main parameter of the API provided by Scikit-learn which refers to the number of clustering predefined GMM has a particularly well-applied data distribution, which is mainly easy to apply to elliptical elongated data distributions 1234567891011121314151617181920212223242526from sklearn.datasets import load_irisfrom sklearn.cluster import KMeansimport matplotlib.pyplot as pltimport numpy as npimport pandas as pd%matplotlib inlineiris = load_iris()feature_names = [&#x27;sepal_length&#x27;,&#x27;sepal_width&#x27;,&#x27;petal_length&#x27;,&#x27;petal_width&#x27;]# 보다 편리한 데이타 Handling을 위해 DataFrame으로 변환irisDF = pd.DataFrame(data=iris.data, columns=feature_names)irisDF[&#x27;target&#x27;] = iris.target# GMM 적용from sklearn.mixture import GaussianMixture# n_components로 미리 군집 개수 설정gmm = GaussianMixture(n_components=3, random_state=42)gmm_labels = gmm.fit_predict(iris.data)# GMM 후 클러스터링 레이블을 따로 설정irisDF[&#x27;gmm_cluster&#x27;] = gmm_labels# 실제 레이블과 GMM 클러스터링 후 레이블과 비교해보기(두 레이블 수치가 동일해야 똑같은 레이블 의미 아님!)print(irisDF.groupby(&#x27;target&#x27;)[&#x27;gmm_cluster&#x27;].value_counts()) The result values are as follows, where target is the label of the actual original data, and gmm_cluster is the label of clustering after clustering. To interpret the above results, all of the labels with a target of 0 were clustered into a clustering label of 2. It is 100% well clustered. On the other hand, among the labels with a target of 1, there are 45 clusters with 0 and 5 clusters with 1, so there are 5 incorrectly clustered data. Korean reference","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}],"tags":[{"name":"Clustering","slug":"Clustering","permalink":"https://jmj3047.github.io/tags/Clustering/"},{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/tags/Python/"},{"name":"Gaussian Mixture Model","slug":"Gaussian-Mixture-Model","permalink":"https://jmj3047.github.io/tags/Gaussian-Mixture-Model/"},{"name":"K-Means Clustering","slug":"K-Means-Clustering","permalink":"https://jmj3047.github.io/tags/K-Means-Clustering/"}]},{"title":"Gaussian Mixture Model","slug":"GMM","date":"2022-09-09T15:00:00.000Z","updated":"2023-02-23T01:36:14.673Z","comments":true,"path":"2022/09/10/GMM/","link":"","permalink":"https://jmj3047.github.io/2022/09/10/GMM/","excerpt":"","text":"1. What is GMM It is one of several models applying the Expectation Maximum (EM) algorithm. What is EM algorithm? EM algorithm is basically an algorithm mainly used for Unsupervised learning. It is also used in clustering. The EM algorithm can be largely divided into two stages: E-step and M-step. In conclusion, it is a flow that finds the optimal parameter value by repeating E-step and M-step. E-step: Calculate the likelihood value that is as close as possible to the likelihood of the initial value of a given arbitrary parameter. M-step: Obtained a new parameter value that maximizes the likelihood calculated in the E-step The above two steps are continuously repeated until the parameter value does not change significantly. The purpose of the EM algorithm is to find parameters that maximize the likelihood. Maximum Likelihood Estimation (MLE) is a Convex or Convex function whose objective function can be differentiated (which can obtain a global optimum) because the optimal parameter must be obtained directly by partial differentiation of the parameter variable. However, EM is a process of finding a value close to the optimal parameter while repeating the E-M step. Similar to the ANN (artificial neural network) model, it does not guarantee to find the Global Optimum, and even if it does, it does not recognize that the point is the real Global Optimum. Therefore, it is very likely that the local minimum is found and the objective function is not composed of Convex or Concave functions. The EM algorithm defines the function of the parameters to be obtained (ex. probability distribution) as a probability variable and optimizes it. It is also used in clustering models and is mainly used in speech recognition modeling. This is a post that Kakao, which develops voice recognition technology using GMM algorithms. Clustering is performed on the assumption that data are generated by datasets with multiple normal distributions. Several normal distribution curves are extracted and individual data are determined to which normal distribution belongs. This process is called parameter estimation in GMM, and two typical estimates are made and the EM method is applied for this parameter estimation. Means and variance of individual normal distributions Probability of which normal distribution each data corresponds 2. Code123456789101112131415161718192021222324252627282930import numpy as npimport pandas as pdimport matplotlib as mplimport matplotlib.pyplot as pltimport seaborn as snsimport warnings%matplotlib inline%config InlineBackend.figure_format = &#x27;retina&#x27;mpl.rc(&#x27;font&#x27;, family=&#x27;NanumGothic&#x27;) # 폰트 설정mpl.rc(&#x27;axes&#x27;, unicode_minus=False) # 유니코드에서 음수 부호 설정# 차트 스타일 설정sns.set(font=&quot;NanumGothic&quot;, rc=&#123;&quot;axes.unicode_minus&quot;:False&#125;, style=&#x27;darkgrid&#x27;)plt.rc(&quot;figure&quot;, figsize=(10,8))warnings.filterwarnings(&quot;ignore&quot;)from sklearn.datasets import load_irisiris = load_iris()feature_names = [&#x27;sepal_length&#x27;,&#x27;sepal_width&#x27;,&#x27;petal_length&#x27;,&#x27;petal_width&#x27;]iris_df = pd.DataFrame(iris.data, columns = feature_names)iris_df[&quot;target&quot;] = iris.targetiris_df.head() 1234567891011from sklearn.mixture import GaussianMixture# GMM: n_components = 모델의 총 수gmm = GaussianMixture(n_components=3, random_state=0)gmm.fit(iris.data)gmm_cluster_labels = gmm.predict(iris.data)# target, gmm_cluster 비교iris_df[&quot;gmm_cluster&quot;] = gmm_cluster_labelsiris_df.groupby([&quot;target&quot;,&quot;gmm_cluster&quot;]).size() 1234567결과target gmm_cluster0 0 501 1 45 2 52 2 50dtype: int64 GMM can be performed using GaussianMixture() in sklearn.mixture() n_components is the total number of models, and like n_clusters in K-Means, determines the number of clusters. Here, when the target of gmm_cluster was 1, only five were mapped differently, and all the rest were well mapped. Korean reference1 Korean reference2","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}],"tags":[{"name":"Clustering","slug":"Clustering","permalink":"https://jmj3047.github.io/tags/Clustering/"},{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/tags/Python/"},{"name":"Gaussian Mixture Model","slug":"Gaussian-Mixture-Model","permalink":"https://jmj3047.github.io/tags/Gaussian-Mixture-Model/"}]},{"title":"K-Means Clustering","slug":"K-Means_Clustering","date":"2022-09-08T15:00:00.000Z","updated":"2023-02-23T01:35:53.766Z","comments":true,"path":"2022/09/09/K-Means_Clustering/","link":"","permalink":"https://jmj3047.github.io/2022/09/09/K-Means_Clustering/","excerpt":"","text":"1. What is K-means Clustering The K-Means clustering algorithm does not automatically identify and group the number of clusters by looking at the data. The number of clusters should be specified and the initial value should be selected accordingly by user. How to determine the number of clusters use the indicator that quantifies how well clustering has been done. After deciding the candidates of cluster numbers and performing clustering about each cluster number, calculate the index At this time, we select the number of clusterings that optimize the index as the optimal number of clusters. The frequently used indicators include Dunn Index, and Silhouette Index. The algorithm assigning the group determines the initial value which is the initial center point of the group. Each center point determines a group, and individual data is assigned to the same group as the center point close to itself. It does not end at once (depending on the form of the data) but repeats the process of updating the center of the group and assigning groups of individual data again. It belongs to the hard clustering algorithm, which is a clustering that unconditionally assigns a group of data points to a point close to the center. 2. Pros &amp; Cons Advantages The process is intuitive, easy to understand, and the algorithm is simple so that it is easy to implement. It does not require complex calculations, so it can be applied to large-scale data. convergence is guaranteed. Disadvantages The result can be very different depending on the initial value. Can be affected by an outlier: because the distance is based on Euclidean Distance, it can be affected by an outlier in the process of updating the center value. Cannot reflect intra-group dispersion structure: Since the distance is based on Euclidean Distance, it cannot properly reflect the intra-group dispersion structure. As the dimensionality increases, the distance between individual data becomes closer, and the effect of clustering may not be effective. This has the meaning of clustering only when the distance between clusters is kept as far apart as possible, but as the dimensionality increases, the distance between individual data becomes closer, so this effect may not be observed. The number of clusters is not set automatically, but must be set in advance. However, the optimal number of clusters can be determined using indicators such as Dunn Index and Silhouette Index. Because of Euclidean Distance, it cannot be used if there is a categorical variable. In this case, it is possible to use the extended K-Modes Clustering algorithm. 3. Using python: scikit-learn Put 3 in n_clusters and init_center in init, and create a KMeans instance. If you put data(X) into the fit function, clustering is performed. The final label can be obtained through the labels_ field. 1234567import warningswarnings.filterwarnings(&#x27;ignore&#x27;)from sklearn.cluster import KMeans kmeans = KMeans(n_clusters=3, init=init_center)kmeans.fit(X)labels = kmeans.labels_ Visualization 1234567891011121314fig = plt.figure(figsize=(7,7))fig.set_facecolor(&#x27;white&#x27;)for i, label in enumerate(labels): if label == 0: color = &#x27;blue&#x27; elif label ==1: color = &#x27;red&#x27; else: color = &#x27;green&#x27; plt.scatter(X[i,0],X[i,1], color=color) plt.xlabel(&#x27;x1&#x27;)plt.ylabel(&#x27;x2&#x27;)plt.show() Korean reference1 Korean reference2","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}],"tags":[{"name":"Clustering","slug":"Clustering","permalink":"https://jmj3047.github.io/tags/Clustering/"},{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/tags/Python/"},{"name":"K-Means Clustering","slug":"K-Means-Clustering","permalink":"https://jmj3047.github.io/tags/K-Means-Clustering/"}]},{"title":"Clustering","slug":"Clustering","date":"2022-09-07T15:00:00.000Z","updated":"2023-02-23T01:35:34.556Z","comments":true,"path":"2022/09/08/Clustering/","link":"","permalink":"https://jmj3047.github.io/2022/09/08/Clustering/","excerpt":"","text":"Clustering is an example of unsupervised learning. Without any label, those with close distances in the data are classified into clusters. It is different from classification, which is supervised learning. In other words, it identifies patterns and groups hidden in the data and binds them together. Even if there’s label in data, there is a possibility that some data with same label can be grouped into different clusters. There are K-Means Clustering, Mean Shift, Gaussian Mixture Model, DBSCAN, Agglomerative Clustering in clustering algorithms and they will be covered in the next post. Korean reference","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"Clustering","slug":"Clustering","permalink":"https://jmj3047.github.io/tags/Clustering/"},{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"}]},{"title":"Data Sampling","slug":"Data_Sampling","date":"2022-09-06T15:00:00.000Z","updated":"2023-02-23T01:35:23.520Z","comments":true,"path":"2022/09/07/Data_Sampling/","link":"","permalink":"https://jmj3047.github.io/2022/09/07/Data_Sampling/","excerpt":"","text":"1. Reason why you need The more input data you have on machine learning, the slower the processing. Therefore, in order to speed up the processing speed of machine learning, acceleration of learning speed of data would be helpful, which can be done with optimization of machine learning with representative data. Then let’s see how we can reduce the data so that we can only use the data we need. 2. What is data sampling The process of organizing the data and making it the best input data. For example, it can speed up the processing of machine learning by using sales of ‘month’ rather than using sales of ‘day’ units to analyze last year’s profits of a pizza house. This is the work of making optiml data. There are probabilistic sampling, nonprobability sampling in data sampling method. The probabilistic sampling method is a sampling method based on statistics, and the nonprobability sampling is a sampling method in which the subjectivity of a person is involved. Depending on each sampling method, you should select and use the sampling method that matches the data and the situation because there are advantages and disadvantages. 3. Probabilistic sampling Probabilistic sampling is a random sampling method that can be divided into simple random sampling, two-step sampling, stratified sampling, cluster&#x2F;collective sampling, and system sampling. Simple Random Sampling: A method of randomly extracting samples from the entire data. Two-Step Sampling: A sampling method that separates the entire n data into m subpopulations. Selects m subpopulations and provides simple random sampling of N data from m subpopulations. It is an accurate sampling method rather than simple random sampling. Stratified Sampling: By separating the population from several layers, it is a method of randomly extracting data from each layer by n. For example, it is a method of dividing the layers of Korean cities and province and extracting n data from each layer. cluster&#x2F;collective sampling: If the population is composed of multiple clusters, it is a method of selecting one or several clusters and using the entire data of the selected clusters. For example, it is a way to use all of the data by setting the Korea as it’s city and province. system sampling: It is a method of extracting data one by one at a regular interval by numbering all data from 1 to n. This method is mainly used to sample representative values of the time series data 4. Nonprobability sampling This is a method of subjectively extracting the probability of being selected as a sample in advance. The advantage and disadvantage of this sampling is that the subjective intention of the sampling person is involved. The implicit population extracted by nonprobability sampling is a good sampling if it matches the ideal population, the most suitable population for the subject. Nonprobability sampling methods include convenience sampling, purpose sampling, and quota sampling methods. Convenience Sampling: A method of sampling by selecting a point or location where data is good for collecting. The sample surveyed by this sampling method has a disadvantage that it is less representative than the population. It’s not possible to go through the statistical inference process. Statistical inference is that we generalize the sample analysis results to speculation about populations. Purpose Sampling: This is how you select the object you think is the most suitable object for your purpose; you will sample the data that is subjectively appropriate for your purpose.The downside is that it has also low representative for the population. Quota Sampling: Divide the population into segments, assigning each segment a quartar that represents the number of samples. Within segments, the characteristics related to the topic must be similar, and the populations must be distributed differently between segments. This is methodologically similar to layer-by-layer sampling. But the difference is that the sample is not selected by probability but by subjective judgment. 5. Conclusion Comparing probabilistic sampling with nonprobability sampling, probabilistic sampling may look good when judged by just words. However, stochastic sampling is advantageous for data that can be analyzed based on statistics, and non-probability sampling is advantageous for data such as language and music. It is better to choose and use probabilistic and nonprobable sampling as appropriate. Korean Reference","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Probabilistic sampling","slug":"Probabilistic-sampling","permalink":"https://jmj3047.github.io/tags/Probabilistic-sampling/"},{"name":"Nonprobability sampling","slug":"Nonprobability-sampling","permalink":"https://jmj3047.github.io/tags/Nonprobability-sampling/"}]},{"title":"Growth Hacking, AARRR, Funnel, Retention","slug":"Growth_Hacking","date":"2022-08-11T15:00:00.000Z","updated":"2023-02-23T01:35:04.494Z","comments":true,"path":"2022/08/12/Growth_Hacking/","link":"","permalink":"https://jmj3047.github.io/2022/08/12/Growth_Hacking/","excerpt":"","text":"1. Growth Hacking 그로스해킹(Growth Hacking)은 성장(Growth)을 위한 모든 수단(Hacking)이란 뜻으로 공격 대상의 미세한 빈틈을 찾아 해킹을 하듯이 성장을 위해 고객과 유통과정 등의 공략지점을 찾아내고 이를 적극적으로 공략하는 마케팅 방법론 브랜드, 기업, 제품 매출 증가 등을 위한 가설을 수립하고 이를 빠르게 MVP 모델로 출시하여 시장의 평가를 받아 본 후, 소비자와 시장의 반응에 따라 제품 또는 서비스가 시장에서 원하는 (고객들이 원하는) 완벽한 상품으로 도달할 때까지 쉬지 않고 개선해 나가는 방식 성장(Growth)을 위한 모든 수단(Hacking)을 통해 효율성을 극대화하고 제품과 서비스를 빠르게 성장시킨다는 장점이 있습니다. 여기서 수단이란, 데이터 기반의 분석과 시장의 피드백을 받아 제품과 서비스를 개선하고 확장해 나간다는 의미로, 자본과 리소스가 한정적인 스타트 기업에게서 효과적인 마케팅 성과를 얻을 수 있음 구매자의 행동 패턴을 분석하고 이를 바탕으로 사용자의 경험을 최적화하는 방법. 그로스해킹은 생산부터 관리에 이르기까지 소비자의 Wants를 충족시키는 상품을 만드는 것 2. AARRR Analysis Acquisition: 어떻게 처음 우리 서비스를 접하게 되는가(사용자 유치) Activation: 사용자가 처음 서비스를 접할 때에 긍정적인 경험을 제공하고 있는가(사용자 활성화) Retention: 이후의 서비스를 다시 사용하는 정도는 얼마나 되는가(사용자 유지) Revenue: 최종 목적(거시전환)으로 연결되고 있는가(매출) Referral: 사용자가 자발적으로 확산이나 공유를 일으키고 있는가(추천) 3. Funnel Analysis 퍼널 분석은 웹 사이트에서 특정 결과에 도달하는데 필요한 단계와, 각 단계를 통과하는 사용자 수를 파악하기 위한 방법 퍼널 분석을 통해 기업은 방문자가 사이트에 가입하는지, 구매자로 변환이 되는지 등의 정보를 특정한 퍼널에 매핑하게 됨. 이때 사용자의 흐름을 시각화하는 모형의 모습이 부엌이나 차고에서 흔하게 쓰이는 깔때기와 유사한 모습을 띠고 있어 ‘퍼널 분석’이라는 이름이 붙여짐 4. Retention Analysis 리텐션은 앱 서비스 성장에 있어서 매우 중요한 지표 리텐션이란 한번 획득한 유저들이 서비스를 이탈하지 않고 계속 서비스를 이용하는 것을 의미 리텐션이 높은 서비스는 리텐션이 낮은 서비스보다 획득비용에 투자한 비용을 빠르게 회수할 수 있으며 이렇게 회수한 비용으로 다시금 빠르게 획득에 투자할 수 있도록 하여 성장을 촉진합니다. 리텐션이 낮다는 것은 획득 이후 다시 돌아오지 않고 이탈하는 유저가 많은 것인데요, 리텐션이 낮은 서비스는 성장이 더딜 뿐만 아니라 한번 획득했다가 이탈한 유저는 다시 획득하기에도 더 많은 획득 비용과 시간이 소요되므로 악순환을 만들어냅니다. 따라서 리텐션은 사업 성장에 있어서 반드시 지켜보아야 할 지표 중 하나입니다. 잔존율(D+1지표) Reference Retention Funnel How to measure Retention","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"}]},{"title":"Deep Embedding Learning for Text-Dependent Speaker Verification","slug":"Deep_Embedding_Learning_for_Text-Dependent_Speaker_Verification","date":"2022-07-04T15:00:00.000Z","updated":"2022-10-15T08:03:47.207Z","comments":true,"path":"2022/07/05/Deep_Embedding_Learning_for_Text-Dependent_Speaker_Verification/","link":"","permalink":"https://jmj3047.github.io/2022/07/05/Deep_Embedding_Learning_for_Text-Dependent_Speaker_Verification/","excerpt":"","text":"Journal&#x2F;Conference: InterspeechYear(published year): 2020Author: Peng Zhang, Peng Hu, Xueliang ZhangSubject: Speaker Verification Deep Embedding Learning for Text-Dependent Speaker Verification Summary 이 논문은 화자 검증 작업을 위한 효과적인 딥 임베딩 학습 아키텍처를 제시한다. 널리 사용되는 잔류 신경망(ResNet) 및 시간 지연 신경망(TDNN) 기반 아키텍처와 비교하여, 두 가지 주요 개선이 제안된다. 우리는 화자의 단기 컨텍스트 정보를 인코딩하기 위해 조밀하게 연결된 컨볼루션 네트워크(DenseNet)를 사용한다. 양방향 주의 풀링 전략이 제안된다. 장기적인 시간적 맥락을 모델링하고 화자 정체성을 반영하는 중요한 프레임을 집계한다. 결과는 제안된 알고리듬이 과제 1과 과제 3의 평가 세트에서 각각 8.06%, 19.70% minDCF 및 9.26%, 16.16% EERs 상대적 감소로 FFSVC2020의 공식 기준선을 능가한다는 것을 보여준다. Introduction딥 러닝 기반 방법은 화자 간 차별에 필수적인 정보를 포함하는 딥 스피커 임베딩 또는 짧게 임베딩과 같은 발화 수준 표현을 얻기 위해 지배적이었다. 본 논문에서는 화자 검증을 위한 효과적인 딥 임베딩 학습 아키텍처를 제안한다. 최근 조밀하게 연결된 컨볼루션 네트워크의 성공에 자극받아 (DenseNet) 이미지 분류 [15], 음악 소스 분리 [16], 화자 분리 [17] 및 화자인식 [18]에서, 우리는 DenseNet을 프레임 레벨 피처 추출기로 채택하여 후속 레이어의 입력의 변화와 훈련 효율성을 증가시킨다. 개념적으로 각 밀도 블록은 작은 CNN 시스템 역할을 한다. 차이점은 레이어의 출력이 채널 차원의 출력 연결에 의해 구현되는 피드 포워드 방식으로 조밀하게 연결된다는 것이다. 또한, 우리는 발화 수준 기능의 표현력을 향상시키기 위해 시간 컨텍스트를 추가로 모델링하기 위한 양방향 주의 풀링 계층을 설계한다. Model ArchitectureDenseNet(프레임 레벨 기능 추출기): 각각 5개의 CNN 레이어가 있는 4개의 밀도 블록(DenseBlock)으로 구성. 프레임 레벨 피처 추출 후, 양방향 주의 풀링 레이어는 프레임 레벨 피처를 고정 차원 벡터로 변환하는 데 사용되며, 이어서 완전히 연결된 두 개의 숨겨진 레이어가 발화 레벨 피처를 형성한다. 출력은 소프트맥스 분류기 계층이며, 각 노드는 화자ID에 대응합니다. Frame level의 특징 추출을 하고, Input과 다음 레이어의 변화에 최적화 시키고, 훈련 효율을 높이기위해서 사용함.개념적으로 각각의 dense block들 이 작은 CNN의 역할을 수행하고 있는 것. 각각의 레이어들이 바로 전에 있는 모든 레이어들과 feature map(CNN에서의 합성곱과 같은 원리)을 통해서 연결이 되어있는 방식.이러한 연결 패턴은 훈련 중 레이어들 사이에서 더 나은 기울기 flow와 각 레이어들에 대한 접근을 모든 feature에게 전달함으로써 일시적인 문맥 정보를 capture 가능. 프레임 레벨 단계의 경우 각 DenseBlock은 5개의 컨볼루션 레이어(Conv2D), 지수 선형 단위(ELU) 및 인스턴스 정규화(IN)로 구성된다. 각 밀도 블록 뒤의 텐서 모양은 featureMaps × timeSteps × 주파수 채널 형식이다. 각 Conv2D 및 Conv2D+IN+ELU는 kernelSize 형식으로 지정됩니다. 시간 × kernelSizeFreq, (보행)시간, 보폭Freq), (패딩시간, 패딩Freq), 맵을 특징으로 한다. 각 밀도 블록(g)은 5개의 Conv2D+증가율이 g인 IN+ELU 블록을 포함합니다. 발화 수준 단계의 경우 숫자는 우리의 구현에서 출력 기능 맵 또는 임베딩 차원의 채널을 나타낸다. DenseBlock[15]에서 처음 제시된 DenseBlock 아키텍처의 주요 아이디어는 CNN의 네트워크 구축 블록에서 각 계층에서 모든 후속 계층에 직접 연결을 도입하는 것이다. 시간 빈도가 긴 컨텍스트 정보를 효율적으로 캡처하도록 설계되었다. 각 계층은 피쳐 맵 연결을 통해 모든 다음 계층에 직접 연결됩니다. 이러한 연결 패턴은 훈련 중에 계층 간에 더 나은 그레이디언트 흐름을 생성하고 각 계층이 이전 계층의 모든 특징 표현에 액세스할 수 있도록 하여 시간적 컨텍스트 정보를 캡처하도록 설계되었다. Bidirectional Attentive Pooling 양방향 주의 풀링(bidirectional attentive pooling, BAP)의 도식입니다. hi는 BAP 입력의 i번째 벡터를 나타내며 wf와 wb는 각각 BGRU의 전방과 후방 숨겨진 상태를 나타낸다. →U 및 ←U는 BGRU 레이어의 양방향 출력입니다. bidirectional gated recurrent unit (BGRU) layer + attentive pooling : utterance level feature 일반적인 average pooling을 사용하지 않은 이유: 평균화 대신 주의 메커니즘[10, 12]은 숨겨진 표현을 적극적으로 선택하고 화자 차별 정보를 강조하기 위한 더 나은 대안이다. 보다 차별적인 고정 차원 발화 수준 표현을 얻고 장기 시퀀스 정보를 캡처하기 위해 [19]는 CNN-BLSTM 모델과 주의 깊은 풀링 레이어를 함께 결합하는 주의 기반 CNN-BLSTM 프레임워크를 제안하였다. BLSTM을 주의 깊은 풀링 계층에 직접 연결하는 [19]와는 달리 주의 깊은 풀링을 사용하여 양방향 반복 신경망에 의해 출력되는 양방향 시간 정보를 캡처한 다음, 양방향 발화 수준 기능을 연결한다. 제안된 풀링 방법인 양방향 주의 풀링(BAP)은 다음과 같이 표현될 수 있다. BAP 계층은 양방향 순차 모델링과 주의 메커니즘을 모두 활용하여 장기적인 시간적 컨텍스트 정보를 캡처한다. Result Dataset: FFSVC2020The first 30 utterances are of fixed content: ‘ni hao mi ya’ in Mandarin Chinese for TD-SV tasks. The remaining utterances are text-independent.In total, training data sets have nearly 1,139,671 utterances and the total duration approximately 950 hours with 374 speakers Conclusion 텍스트 의존적 스피커 검증을 위한 딥 임베딩 학습 아키텍처를 제안 아키텍처는 프레임 수준에서 스피커 ID를 캡처하기 위한 DenseBlock의 스택과 발화 수준에서 스피커 임베딩을 형성하기 위한 양방향 주의 풀링 구조로 구성됨 컨볼루션 레이어의 출력을 조밀하게 연결함으로써, 시간 주파수 컨텍스트 정보의 다양한 측면을 가진 보다 의미 있는 프레임 레벨 표현이 생성됨 양방향 주의 풀링 계층은 BGRU 계층과 주의 풀링의 조합으로 양방향에서 시간 컨텍스트 정보를 추가로 캡처 FFSVC2020에서 점수 제출의 경우, 우리가 제안한 방법은 평가 세트의 과제 1과 과제 3에 대한 최소 DCF와 EER에서 각각 0.52와 4.72%, 0.14%를 달성. 또한 이 결과는 x-벡터 및 ResNet 기준선 시스템에 비해 상당한 성능 향상을 보여줌 Pdf: Deep Embedding Learning for Text-Dependent Speaker Verification","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speaker Verification","slug":"Paper/Speaker-Verification","permalink":"https://jmj3047.github.io/categories/Paper/Speaker-Verification/"}],"tags":[{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"},{"name":"TD-SV","slug":"TD-SV","permalink":"https://jmj3047.github.io/tags/TD-SV/"}]},{"title":"Attention is all you need","slug":"Attention_is_all_you_need","date":"2022-05-09T15:00:00.000Z","updated":"2022-10-15T08:03:40.527Z","comments":true,"path":"2022/05/10/Attention_is_all_you_need/","link":"","permalink":"https://jmj3047.github.io/2022/05/10/Attention_is_all_you_need/","excerpt":"","text":"Journal&#x2F;Conference: NIPSYear(published year): 2017Author: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia PolosukhinSubject: NLP Attention is all you need Summary Attention 만으로 시퀀셜 데이터를 분석하여 병렬화와 연산 속도 향상을 가능하게 한 새로운 모델 제시 Seq2Seq 과 Attention 을 결합한 모델(Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. CoRR, abs&#x2F;1409.0473, 2014.)에서 한층 더 발전한 모델입니다. Recurrent model(재귀 구조)없이 Self-attention 만으로 구성한 첫번째 모델입니다. 재귀 구조 제거로 모델을 병렬화(Parallelization)하여 자연 언어 처리 학습&#x2F;추론 시간을 획기적으로 단축시켰습니다. Introduction기존의 자연언어 처리 모델은 RNN, LSTM, GLU 모델로 대표되는 재귀 모델(Recurrent Model)을 encoder-decoder 구조로 결합하는 seq2seq 과 같은 모델을 주로 사용하였습니다. 이러한 재귀 모델은 순차 처리로 인해서 병렬화가 어렵다는 약점이 있고, 메모리 크기가 제한되어 긴 문장을 처리하기도 어렵다는 단점이 있습니다. 이를 보완하기 위한 어텐션(attention) 매커니즘이 제안되었습니다. 어텐션은 재귀 과정에서 입력에서 출력까지의 거리가 길어지는 문제를 해결할 수 있어 입출력의 전역 의존성을 높여주었지만, 재귀 모델과 결합해서만 사용되어 왔습니다. 이에 해당 논문에서는, 어텐션만으로 모델을 구성하여 쉽게 병렬화 할 수 있고 자연언어처리 과제의 성능을 높인 Transformer 모델을 제시합니다. 이는 기존의 재귀 모델과 다르게 8대의 P100 GPU로 12시간 정도만 학습했음에도 당시 기준 SOTA를 달성하였습니다. WMT 2014 English-to-German Translation task -&gt; 28.4 BLEU WMT 2014 English-to-French Translation task -&gt; 41.0 BLEU Model ArchitectureTransformer 모델은 seq2seq으로 대표되는 인코더-디코더 구조를 self-attention 으로 쌓은 뒤, fully connected layer 로 출력을 생성합니다. Encoder and Decoder Stacks 인코더(Encoder) $N$(&#x3D;6)개의 동일한 레이어로 구성 각 레이어는 2개의 하위 레이어로 구성 Multi-head self-attention position-wise fully connected feed-forward 하위 레이어를 거칠 때마다 Residual connection(Resnet) 과 layer normalization 을 실행 각 레이어 출력의 크기는 $d_{model}$(&#x3D;512)로 고정 디코더(Decoder) 인코더와 같이 $N$(&#x3D;6)개의 동일한 레이어로 구성 인코더와 동일한 2개의 하위 레이어에 한가지를 더 추가하여 3개의 하위 레이어로 구성 Multi-head self-attention position-wise fully connected feed-forward 인코더의 출력으로 실행하는 multi-head attention 순차적으로 결과를 만들어 낼 수 있도록 Self-attention 레이어에 Masking 을추가 : $i$ 번째 출력을 만들 때, $i$번째보다 앞선 출력($i-1, i-2,\\dots$) 만을 참고하도록 함 Attentionattention은 query와 key-value pair들을 output에 맵핑해주는 함수입니다. 출력은 values들의 weighted sum으로, value에 할당된 weight는 query와 대응되는 key의 compatibility function으로 계산합니다. Scaled Dot-Product Attention 여기서 사용하는 attention은 Scaled Dot-Product Attention(SDPA)라 부르는데, input은 dimension이 $d_k$인 query와 key, dimension이 $d_v$인 value들로 이루어집니다. 모든 query와 모든 key들에 대해 dot product로 계산되는데 각각의 결과에 dk로 나누어진다. 다음 value의 가중치를 얻기 위해 softmax 함수를 적용합니다. attention 함수는 additive attention과 dot-product attention이 사용됩니다. additive attention은 single hidden layer와 함께 feed-forward network에 compatibility function을 계산하는데 사용됩니다. dot-product attention이 좀 더 빠르고 실제로 space-efficient합니다. 왜냐하면 optimized matrix multiplication code를 사용해서 구현되었기 때문입니다. $d_k$가 작은 경우 additive attention이 dot product attention보다 성능이 좋습니다. 그러나 $d_k$가 큰 값인 경우 softmax 함수에서 기울기 변화가 거의 없는 영역으로 이동하기 때문에 dot product를 사용하면서 $d_k$으로 나누어 scaling을 적용했습니다. Multi-Head Attention $d_{model}$ dimension의 query, key, value 들로 하나의 attention을 수행하는 대신, query, key, value들에 각각 학습된 linear projection을 $h$번 수행하는 것이 더 좋습니다. 즉, Q,K,V에 각각 다른 weight를 곱해주는 것입니다. 이때, projection이라 하는 이유는 각각의 값들이 parameter matrix와 곱해졌을 때, $d_k,d_v,d_{model}$차원으로 project되기 때문입니다. query, key, value들을 병렬적으로 attention function을 거쳐 dimension이 $d_v$인 output 값으로 나오게 됩니다. 이후 여러개의 head를 concatenate하고 다시 $W^O$와 projection하여 dimension이 $d_{model}$인 output 값으로 나오게 됩니다. Position-wise Feed-Forward Networks인코더와 디코더는 fully connected feed-forward network를 가지고 있습니다. 또한, 두 번의 linear transformations과 activation function ReLU로 구성되어집니다. 각각의 position마다 같은 $W,b$ 를 사용하지만 layer가 달라지면 다른 parameter를 사용합니다. Positional Encoding모델이 recurrence와 convolution을 사용하지 않기 때문에 문장안에 상대적인 혹은 절대적인 위치의 token들에 대한 정보를 주입해야만 했습니다. 이후 positional encoding이라는 것을 encoder와 decoder stack 밑 input embedding에 더해줬습니다. positional encoding은 $d_{model}$인 dimension을 가지고 있기 때문에 둘을 더할 수 있습니다. $pos$는 position, $i$는 dimension $pos$는 sequence에서 단어의 위치이고 해당 단어는 $i$에 0부터 $2d_{model}$까지 대입해 dimension이 $d_{model}$인 positional encoding vector를 얻을 수 있습니다. Why Self-Attention Self-Attention을 사용하는 첫 번째 이유는 layer마다 total computational complexity가 작기 때문입니다. 두 번째 이유는 computation의 양이 parallelized하기때문에 sequential operation의 minimum으로 측정되기 때문입니다. 세 번째 이유로는 네트워크에서의 long-range dependencies사이의 path length때문입니다. long-range dependencies를 학습하는 것은 많은 문장 번역 분야에서의 key challenge가 됩니다. input sequence와 output sequence의 길이가 길어지면 두 position간의 거리가 멀어져 long-range dependencies를 학습하는데 어려워집니다. 테이블을 보면 Recurrent layer의 경우 Sequential operation에서 $O(n)$이 필요하지만 Self-Attention의 경우 상수시간에 실행될 수 있습니다. 또한 Self-Attention은 interpretable(설명가능한) model인 것이 이점입니다. Traning OptimizerAdam optimizer 에 파라미터로 $\\beta_1&#x3D;0.9, \\beta_2&#x3D;0.98, \\epsilon&#x3D;10^{-9}$ 를 사용했습니다. 학습동안 아래의 공식을 통해 learning rate를 변화시켰습니다. 이는 warmup_step에 따라 linear하게 증가시키고 step number에 따라 square root한 값을 통해 점진적으로 줄여갔습니다. 그리고 warmup_step &#x3D; 4000을 사용했습니다. Residual Dropout각 sub-layer에서 input을 더하는 것과 normalization을 하기전에 output에 dropout을 설정했습니다. 또한 encoder와 decoder stacks에 embedding의 합계와 positional encoding에도 dropout을 설정했습니다. dropout rate $P_{drop}&#x3D;0.1$ 을 사용했습니다. Label Smoothing학습하는 동안 label smoothing value $\\epsilon_{ls}&#x3D;0.1$을 적용했습니다. Result Machine Translation 영어→독일어 번역에서는 기존 모델들보다 높은 점수가 나왔고 영어→프랑스어 번역에서는 single 모델보다 좋고 ensemble 모델들과 비슷한 성능을 내주는 것을 볼 수 있습니다. 여기서 중요한 점은 Training Cost인데 기존 모델들보다 훨씬 적은 Cost가 들어가는 것을 볼 수 있습니다. Model Variations (A)를 보면 single-head attention은 head&#x3D;16일때보다 0.9 BLEU 낮고 head&#x3D;32로 늘렸을 때도 head&#x3D;16일때보다 BLEU가 낮습니다. (B)를 보면 dk를 낮추는 것이 model quality를 낮추게 합니다. (C), (D)를 보면 더 큰 모델일수록 좋고, dropout이 overfitting을 피하는데 도움이 되는 것을 볼 수 있습니다. (E)를 보면 sinusoidal position대신 learned positional embeddings를 넣었을 때의 결과가 base model과 동일한 결과인 것을 볼 수 있습니다. Conclusion 재귀 구조 없이 Multi-headed Self-attention 으로 인코더-디코더를 대체한 Transformer 모델을 제시하였습니다. 재귀구조가 없으므로 recurrent 또는 convolutional 레이어 기반 모델보다 빠르게 학습을 할 수 있습니다. 해당 모델은 WMT 2014 영어→독어, 영어→불어 번역 분야에서 기존 모든 앙상블 모델들을 능가하는 SOTA를 달성했습니다. Link: Attention is all you need","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"NLP","slug":"Paper/NLP","permalink":"https://jmj3047.github.io/categories/Paper/NLP/"}],"tags":[{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"},{"name":"Transformer","slug":"Transformer","permalink":"https://jmj3047.github.io/tags/Transformer/"},{"name":"Attention","slug":"Attention","permalink":"https://jmj3047.github.io/tags/Attention/"}]},{"title":"Light Gradient Boosting Machine","slug":"LGBM","date":"2022-05-09T15:00:00.000Z","updated":"2023-02-23T01:34:43.368Z","comments":true,"path":"2022/05/10/LGBM/","link":"","permalink":"https://jmj3047.github.io/2022/05/10/LGBM/","excerpt":"","text":"1. DefinitionEnsemble→ 여러 예측기를 수집해서 단일 예측기 보다 더 좋은 예측기를 만드는 것. 일반적으로 앙상블 기법을 사용하면 , 예측기 하나로 훈련하였을때 보다 , 편향은 비슷하지만 분산이 줄어든다고 알려져 있다. 배깅(bagging) 원데이터 집합으로부터 크기가 같은 표본을 여러 번 단순임의 복원추출하여 각 표본(붓스트랩 표본) 에 대해 분류기를 생성한 후 그 결과를 앙상블하는 방법 반복추출 방법을 사용하기 때문에 같은 데이터가 한 표본에 여러 번 추출될 수도 있고, 어떤 데이터는 추출되지 않을수도 있다. 부스팅(boosting) 배깅의 과정과 유사하나 붓스트랩 표본을 구성하는 sampling 과정에서 각 자료에 동일한 확율을 부여하는 것이 아니라, 분류가 잘못된 데이터에- 더 큰 가중을 주어 표본을 추출한다. Bagging 과 Boosting 의 차이 Bagging 은 독립된 예측기를 통해 더 나은 예측기를 얻는다 Boosting 은 앞의 예측기를 보완해 나가면서 더 나은 예측기를 얻는다 랜덤포레스트(random forest) 배깅에 랜덤 과정을 추가한 방법이다. 각 노드마다 모든 예측변수 안에서 최적의 분할을 선택하는 방법 대신 예측변수들을 임의로 추출하고, 추출된 변수 내에서 최적의 분할을 만들어 나가는 방법을 사용한다. 부트스트랩(Bootstrap) 부트스트랩은 평가를 반복한다는 측면에서 교차검증과 유사하나, 훈련용 자료를 반복 재선정한다는 점에서 차이가 있다. 즉 부트스트랩은 관측치를 한번 이상 훈련용 자료로 사용하는 복원추출법에 기반한다. 부트스트랩은 전체 데이터의 양이 크지않은 경우의 모형평가에 가장 적합하다. 2. GBM Gradient Boosting Machine(GBM)은 Ensemble Learning의 일환 Gradient Boosting&#x3D;Gradient Descent+Boosting Gradient Descent 첫번째 데이터에서 잘 못 맞춘 데이터들에 가중치를 주어, 두번째 모델 에서는 더 많은양을 만들어 준다. 또, 두번째 모델에서 잘못 매칭한 데이터들 에게 가중치를 주어서, 세번째 모델을 만들어주는 그러한 방식으로 모델을 반복한다. Fit an addictive model(ensemble) in a foward stage-wise manner bagging 처럼 한번에 딱 학습을 시키는게 아니고 위에 언급한 것 처럼 하나씩 하나씩 더해가면서 모델을 학습 시켜나가는 모델 Adaptive boosting 모델을 거듭할수록, weak leaner가 만들어 지면서 이전에 가졌던 오류에 대해 해결할 수 있는 능력을 만듦 이러한 weak learner 를 만들기 위해 , 앞서 보였던 모델의 shortcoming 을 더 많이 샘플링 하라는 뜻 Gradient Boosting 그라디언트 부스팅은, Regression,Classification,Ranking 을 다 할 수 있다. 이 셋의 다르기는 loss function 에서 차이가 날 뿐, concept 은 동일 💡 Regression 으로 설명하기가 가장 직관적이기 때문에, Regression 모델로써 concept 을 설명하겠다. 얘는 adaptive boosting 과는 달리, 샘플링을 따로 시행하지는 않는다. 잔차를 목푯값 (y) 으로 놓고 계속해서 반복하면서 잔차에 대한 식을 만들어 낸다. 잔차를 목표값으로 잡아두면, 앞선 모델이 맞추지 못한 만큼만 맞추려고 노력을 하기때문에, 앞선 모델의 결과물과 뒷 모델의 결과물을 더하면 정답이 나온다. 경사도를 통해서 Weak learner 를 boosting 시킴 손실함수의 gradient(경사도) 가 0 에 가까울때 까지 미분을 해준다. Gradient 가 0이 아니라면 weight 를 gradinet 의 반대방향으로 움직이되 얼마만큼 움직이냐에 따라서 달라지니 조금씩 움직인다. 처음, decision tree 로 split point 를 잡아, regression 해준 부분의 잔차를 보면 높은것을 알 수 있다. GBM에서는 이 잔차를 기준으로 또 Split point 를 잡아주면서(이러한 과정에서 손실함수가 들어가며 손실함수의 gradient 를 줄여나가는 과정에서 gradient descent 개념이 들어가는 것) 점점 잔차를 줄여 나가는 것을 볼 수 있다. 💡 처음에는 회귀식이 안좋게 나오는데 iteration 이 반복 될수록 회귀식이 좋아지는것으로 볼 수 있다. Overfitting problem in GBM GBM 의 가장큰 문제점은 오차를 기반으로 모델을 형성 하기때문에(애초에 모델 자체가 반복을 거듭 할수록, 전 모델의 오차를 줄여나가는 모델이기 때문에 오버피팅 문제는 필연적) 우리가 어찌 할수 없는 오차까지도 모델에 학습시키어서 오버피팅 문제를 불러 일으킨다 과적합 해결법 Subsampling→각각의 모델을 만들때 샘플링을 랜덤으로 80% 만해서 모델을 만들어준다 Shrinkage- original 알고리즘들은 전에 만들어진 모델들과 뒤로 갈수록 만들어지는 모델들에, 영향력이 동등했는데 , shrinkage 를 쓰면 , 뒤에 만들어지는 모델들에 대해서 , 가중치를 적게 두어 만들어준다. Early Stopping- validation error 가 증가 할 것 같으면 미리 중지를 시키는것 Information Gain:Split point를 통해서 얼마나 혼잡도,불순도가 낮아지는가. Information Gain 을 통해 그 변수의 영향도를 체크 할 수 있다. 3. LightGBMGOSS 모든 피쳐들을 검사하면 시간이 많이 걸리기 때문에 이를 막기위해서, Gradient-based One-side Sampling (GOSS) 를 사용→ Large gradient 는 keep 하고 small gradient 는 드랍 하는 방식으로, 1000개 데이터를 모두 탐색하는 것이 아니라 gradient 가 큰 것 위주로 탐색하는 방식 → 탐색횟수를 줄이는 것 EFB (Exclusive Feature Bundleing) 모든피쳐를 탐색할 필요를 없애는 것 Bundle 을 찾는 방법 Graph coloring problem 으로 해결가능 각각의 노드는 피쳐이고, edge 는 피쳐들간의 conflict → conflict 가 많은 애들은 중복이 많이 들어가서 bundling 이 되면 안됨 conflict 가 없는 애들 끼리는 bundling 을 해도 됨 Greed bundling 계산법 edge 의 강도: 두 변수의 conflict 강도 edge: 동시에 0이아닌 객체의 수. Degree 시작점을 degree의 내림차순으로 정리 해준 다음, degree가 높은것 부터 시작한다. cutoff 는 hyperparameter 인데, cut-off가 0.2라는 말은, N&#x3D;10 이기 때문에 2회 이상 Nonzero value 가 겹치게 되면, bundleing이 안되는 것. cut-off 기준에 맞지 않기 때문에 x5는 고립이 된다. feature merge 를 쉽게 해주기 위해 feature의 위치를 살짝 조정 하여준다. feature 를 merge 하는방법. Add. offset add offset→bundling 을 하기위한 대상이 되는 변수에다가 기준이 되는 변수가 가질 수 있는 최대 값을 더해준다. conflict 가 일어난 부분은 그대로 기준 변수가 가지는 값을 더해준다. Reference: 고려대학교 산업경영공학부 DSBA 연구실","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}],"tags":[{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"}]},{"title":"Making Chatbot with doc2vec tutorial(1)","slug":"Making_Chatbot_with_doc2vec_tutorial(1)","date":"2022-05-05T15:00:00.000Z","updated":"2022-10-15T08:07:56.742Z","comments":true,"path":"2022/05/06/Making_Chatbot_with_doc2vec_tutorial(1)/","link":"","permalink":"https://jmj3047.github.io/2022/05/06/Making_Chatbot_with_doc2vec_tutorial(1)/","excerpt":"","text":"모델 만들기데이터 만들기doc2vec을 이용해서 FAQ데이터들의 질문들을 벡터화하는 모델을 만들어 본다. word2vec이 단어를 벡터화 하는 것이라면 doc2vec은 단어가 아니라 문서를 기준으로 (여기서는 문장)벡터를 만드는 라이브러리이다. doc2vec을 사용하면 서로 다른 문서들이 같은 차원의 벡터값을 갖게 된다. 각 문서라 갖는 벡터값을 비교해 같으면 같을 수록 유사한 문서라는 것을 알 수 있다. 따라서 doc2vec을 이용해 FAQ의 질문들을 벡터화 한다면 어떤 질문이 들어왔을 때 동일 모델로 질문을 벡터화 한다음, 저장돼 있는 질문들의 벡터와 비교해서 가장 유사한 질문을 찾을 수 있다. 가장 유사한 질문을 찾은 다음 그 질문의 답을 출력하면 FAQ챗봇을 만들 수 있다. **GPU사용 필수..! 1234567891011import osimport warningsfrom gensim.models import doc2vecfrom gensim.models.doc2vec import TaggedDocumentfaqs = [[&quot;1&quot;, &quot;당해년도 납입액은 수정 가능 한가요?&quot;, &quot;네, 당해년도 납입액은 12464 화면 등록전까지 수정 가능합니다.&quot;], [&quot;2&quot;, &quot;대리인통보 대상계좌 기준은 어떻게 되나요?&quot;, &quot;모계좌 기준 가장 최근에 개설된 계좌의 관리점에서 조회 됩니다. 의원폐쇄된 자계좌는 조회대상 계좌에서 제외됩니다. 계좌주 계좌가 사절원 계좌가 아닌 경우만 조회됩니다&quot;], [&quot;3&quot;, &quot;등록가능 단말기수는 어떻게 되나요?&quot;, &quot;5대까지 등록 가능입니다.&quot;], [&quot;4&quot;, &quot;모바일계좌개설 가능한 시간은 어떻게 되나요?&quot;, &quot;08:00 ~ 20:00(영업일만 가능&quot;], [&quot;5&quot;, &quot;미국인일때 미국납세자등록번호 작성 방법은 어떻게 되나요?&quot;, &quot;계좌주가 미국인일 때 계좌주의 미국납세자등록번호(사회보장번호(Social Security Number), 고용주식별번호(Employer Identification Number), 개인납세자번호(Individual Taxpayer Identification Number))를 기재합니다..&quot;]] 위와 같이 5개의 FAQ데이터를 임의로 만들었다. 이제 여기에 5개의 질문을 벡터화 할건데 사실 벡터화 할 때 데이터는 많을 수록 좋다. 적으면 서로 분간이 잘 안됨. 형태소 분석doc2vec으로 문장을 벡터화하기 전에 약간의 전처리 과정이 필요하다. 각 문장을 tokenize해야 한다. 토큰화 하는 과정이 영어랑 한국어랑 조금 다른데 한국어의 경우 형태소 분석(pos tagging)을 통해 형태소 단위로 나눈뒤 토큰으로 사용할 형태소를 결정하고 나눈다. 즉 각 문장을 형태소 단위의 배열로 만든다. 한국어 형태소 분석기는 konlpy를 사용한다. 123456789101112#형태소 분석import jpypefrom konlpy.tag import Kkmakkma = Kkma()def tokenize_kkma(doc): jpype.attachThreadToJVM() #자바를 사용하기 위한 소스 코드 token_doc = [&#x27;/&#x27;.join(word) for word in kkma.pos(doc) ] #형태소 분석한 단어와 형태소 명을 &#x27;단어/형태소&#x27;형태로 출력하기 위한 코드 return token_doctokenize_kkma(faqs[0][1]) Kkma를 import 하고 jpype도 import 한다. jpype는 파이썬에서 자바를 사용할 수 있게 해주는 패키지인데 기본적으로 kkma가 자바 베이스라서 꼭 필요하다. Kkma()로 형태소 분석기를 불러온다음 kkma.pos(doc)로 형태소 분석을 한다. 123456789101112출력 결과:[&#x27;당해/NNG&#x27;, &#x27;년도/NNM&#x27;, &#x27;납입/NNG&#x27;, &#x27;액/XSN&#x27;, &#x27;은/JX&#x27;, &#x27;수정/NNG&#x27;, &#x27;가능/NNG&#x27;, &#x27;한/MDN&#x27;, &#x27;가요/NNG&#x27;, &#x27;?/SF&#x27;] 형태소 분석을 하면 문장이 단어&#x2F;형태소 형태의 배열로 출력된다. 1번 문장은 총 10개의 형태소로 나뉘었다. 형태소 분석기종류에 따라 결과가 조금씩 다를수 있다. Doc2Vec 모델 만들기Doc2Vec을 이용해 모델을 만들기 위해서는 토큰화 된 리스트와 태그 값이 필요하다. 여기서 태그는 문장 번호. [문장의 번호, 문장을 토큰화한 배열] 이렇게 두 개의 값을 가진 리스트를 사용해 doc2vec 모델을 만들 수 있다. 실제로 모델을 만드는데 사용하는 건 토큰 값이지만 비슷한 문장이 무엇인지 찾기 위한 인덱스로 태그 값을 사용하게 된다. 1234567# 리스트에서 각 문장부분 토큰화token_faqs = [(tokenize_kkma(row[1]), row[0]) for row in faqs]# Doc2Vec에서 사용하는 태그문서형으로 변경tagged_faqs = [TaggedDocument(d, [c]) for d, c in token_faqs]tagged_faqs 12345[TaggedDocument(words=[&#x27;당해/NNG&#x27;, &#x27;년도/NNM&#x27;, &#x27;납입/NNG&#x27;, &#x27;액/XSN&#x27;, &#x27;은/JX&#x27;, &#x27;수정/NNG&#x27;, &#x27;가능/NNG&#x27;, &#x27;한/MDN&#x27;, &#x27;가요/NNG&#x27;, &#x27;?/SF&#x27;], tags=[&#x27;1&#x27;]), TaggedDocument(words=[&#x27;대리인/NNG&#x27;, &#x27;통보/NNG&#x27;, &#x27;대상/NNG&#x27;, &#x27;계좌/NNG&#x27;, &#x27;기준/NNG&#x27;, &#x27;은/JX&#x27;, &#x27;어떻/VA&#x27;, &#x27;게/ECD&#x27;, &#x27;되/VV&#x27;, &#x27;나요/EFQ&#x27;, &#x27;?/SF&#x27;], tags=[&#x27;2&#x27;]), TaggedDocument(words=[&#x27;등록/NNG&#x27;, &#x27;가능/NNG&#x27;, &#x27;단말/NNG&#x27;, &#x27;기수/NNG&#x27;, &#x27;는/JX&#x27;, &#x27;어떻/VA&#x27;, &#x27;게/ECD&#x27;, &#x27;되/VV&#x27;, &#x27;나요/EFQ&#x27;, &#x27;?/SF&#x27;], tags=[&#x27;3&#x27;]), TaggedDocument(words=[&#x27;모바일/NNG&#x27;, &#x27;계좌/NNG&#x27;, &#x27;개설/NNG&#x27;, &#x27;가능/NNG&#x27;, &#x27;하/XSV&#x27;, &#x27;ㄴ/ETD&#x27;, &#x27;시간/NNG&#x27;, &#x27;은/JX&#x27;, &#x27;어떻/VA&#x27;, &#x27;게/ECD&#x27;, &#x27;되/VV&#x27;, &#x27;나요/EFQ&#x27;, &#x27;?/SF&#x27;], tags=[&#x27;4&#x27;]), TaggedDocument(words=[&#x27;미국인/NNG&#x27;, &#x27;일/NNG&#x27;, &#x27;때/NNG&#x27;, &#x27;미국/NNP&#x27;, &#x27;납세자/NNG&#x27;, &#x27;등록/NNG&#x27;, &#x27;번호/NNG&#x27;, &#x27;작성/NNG&#x27;, &#x27;방법/NNG&#x27;, &#x27;은/JX&#x27;, &#x27;어떻/VA&#x27;, &#x27;게/ECD&#x27;, &#x27;되/VV&#x27;, &#x27;나요/EFQ&#x27;, &#x27;?/SF&#x27;], tags=[&#x27;5&#x27;])] TaggedDocument function을 사용하면 doc2vec에서 사용할 수 있는 태그 된 문서 형식으로 변경한다. 출력해 보면 words배열과 tags값을 갖는 Dic형태의 자료형이 되었음을 확인할 수 있다. 1234567891011121314151617181920212223# make model import multiprocessing cores = multiprocessing.cpu_count() d2v_faqs = doc2vec.Doc2Vec(vector_size=50, alpha=0.025, min_alpha=0.025, hs=1, negative=0, dm=0, dbow_words = 1, min_count = 1, workers = cores, seed=0) d2v_faqs.build_vocab(tagged_faqs) # train document vectors for epoch in range(10): d2v_faqs.train(tagged_faqs, total_examples = d2v_faqs.corpus_count, epochs = d2v_faqs.epochs ) d2v_faqs.alpha -= 0.0025 # decrease the learning rate d2v_faqs.min_alpha = d2v_faqs.alpha # fix the learning rate, no decay 모델을 만들고 학습 시킨다. doc2vec모델을 만들 때 파라미터는 여러가지가 들어가는데 여기서는 vector_size와 min_count정도를 수정했다. vector_size는 만들어지는 벡터 차원의 크기이고, min_count는 최소 몇 번 이상 나온 단어에 대해 학습할지 정하는 파라미터이다. 여기서는 일단 사이즈 50에 최소 횟수는 1회로 정했다. epoch는 10번으로 해서 train했다. 유사 문장 찾기이제 이 모델로 어떤 문장이 들어왔을 때 1~5번 중에 무엇과 비슷한지 알아보자. 먼저 어떤 문장이 들어오면 그 문장을 벡터화 하고 그 벡터가 어떤 문장과 비슷한지 태그 값을 찾아본다. 12predict_vector = d2v_faqs.infer_vector([&quot;당해년도 납입액은 수정 가능 한가요?&quot;])d2v_faqs.docvecs.most_similar([predict_vector], topn=2) 1[(&#x27;2&#x27;, 0.21605531871318817), (&#x27;3&#x27;, 0.10707802325487137)] 제대로 됐는지 확인하기 위해 1번 문장을 그대로 넣었지만 답은 2, 3번이 나왔다. 학습이 제대로 되지 않아서 이런 결과가 나왔다. 테스트할 문장을 벡터화 할 때도 형태소 분석을 해줘야 한다. 왜냐하면 모델을 학습 할 때 문장들을 형태소로 분석해서 넣어줬기 때문이다. 123test_string = &quot;대리인통보 대상계좌 기준은 어떻게 되나요?&quot;tokened_test_string = tokenize_kkma(test_string)tokened_test_string 1234567891011[&#x27;대리인/NNG&#x27;, &#x27;통보/NNG&#x27;, &#x27;대상/NNG&#x27;, &#x27;계좌/NNG&#x27;, &#x27;기준/NNG&#x27;, &#x27;은/JX&#x27;, &#x27;어떻/VA&#x27;, &#x27;게/ECD&#x27;, &#x27;되/VV&#x27;, &#x27;나요/EFQ&#x27;, &#x27;?/SF&#x27;] 12test_vector = d2v_faqs.infer_vector(tokened_test_string)d2v_faqs.docvecs.most_similar([test_vector], topn=2) 1[(&#x27;1&#x27;, 0.1448383331298828), (&#x27;3&#x27;, 0.0218462273478508)] 2번 문장으로 했을 때 결과입니다. doc2vec이라는 모델은 문서단위로 벡터화 하는 것이기 때문에 문서가 많아야 한다. 여기서는 문장이 많아야 한다. 문장이 많으면 많을 수록 문장 간의 거리를 계산해서 더 잘 구분해준다. 간단하게 생각하면 문장이 적으면 적중률이 높을 것 같지만 사실은 그 반대이다. 데이터가 많을수록 그 데이터간의 차이를 구분할 수 있기 때문에 더 잘 예측하게 된다. Local path :C:\\Users\\jmj30\\Dropbox\\카메라 업로드\\Documentation\\2022\\2022 상반기\\휴먼교육센터\\mj_chatbot_prac\\faq_chatbot Reference: https://cholol.tistory.com/466","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Python","slug":"Python/Python","permalink":"https://jmj3047.github.io/categories/Python/Python/"}],"tags":[{"name":"Doc2vec","slug":"Doc2vec","permalink":"https://jmj3047.github.io/tags/Doc2vec/"},{"name":"Chatbot","slug":"Chatbot","permalink":"https://jmj3047.github.io/tags/Chatbot/"},{"name":"NLP","slug":"NLP","permalink":"https://jmj3047.github.io/tags/NLP/"}]},{"title":"Making English Chatbot with doc2vec(3)","slug":"Making_English_Chatbot_with_doc2vec(3)","date":"2022-05-05T15:00:00.000Z","updated":"2022-10-15T08:08:04.571Z","comments":true,"path":"2022/05/06/Making_English_Chatbot_with_doc2vec(3)/","link":"","permalink":"https://jmj3047.github.io/2022/05/06/Making_English_Chatbot_with_doc2vec(3)/","excerpt":"","text":"많은 데이터로 실험해보기데이터 살펴보기더 많은 학습 데이터로 모델을 학습한다. 데이터 원본 링크: https://www.kaggle.com/jiriroz/qa-jokes총 3만 8천개의 문장 데이터 불러오기, 전처리12345678import osimport warningsfrom gensim.models import doc2vecfrom gensim.models.doc2vec import TaggedDocumentimport pandas as pdfaqs = pd.read_csv(os.path.join(&#x27;data&#x27;,&#x27;jokes.csv&#x27;))faqs 한국어와 다르게 영어는 띄어쓰기로 단어가 잘 구분되기 때문에 형태소 분석은 생략한다. 형태소 분석을 하지 않아도 띄어쓰기로 split하면 단어 단위로 잘 짤리기 때문이다. 하지만 영어 단어를 원형으로 만들어 주는 lemmatization이나 the나 a같은 관사를 제거하는 stopword 제거는 해준다. 12345678910from nltk.tokenize import word_tokenizefrom nltk.stem import WordNetLemmatizerfrom nltk.corpus import stopwordsimport nltknltk.download(&#x27;punkt&#x27;)# 토큰화tokened_questions = [word_tokenize(question.lower()) for question in faqs[&#x27;Question&#x27;]]tokened_questions 123456789101112131415[[&#x27;did&#x27;, &#x27;you&#x27;, &#x27;hear&#x27;, &#x27;about&#x27;, &#x27;the&#x27;, &#x27;native&#x27;, &#x27;american&#x27;, &#x27;man&#x27;, &#x27;that&#x27;, &#x27;drank&#x27;, &#x27;200&#x27;, &#x27;cups&#x27;, &#x27;of&#x27;, &#x27;tea&#x27;, &#x27;?&#x27;], 대문자를 모두 소문자로 바꿔주고 토큰화를 한 다음 띄어쓰기로 출력해주었다. 12345678910lemmatizer = WordNetLemmatizer()nltk.download(&#x27;wordnet&#x27;)# lemmatizationlemmed_questions = [[lemmatizer.lemmatize(word) for word in doc] for doc in tokened_questions]lemmed_questionsnltk.download(&#x27;stopwords&#x27;)# stopword 제거 불용어 제거하기stop_words = stopwords.words(&#x27;english&#x27;)questions = [[w for w in doc if not w in stop_words] for doc in lemmed_questions]questions 123456789101112131415**[[&#x27;hear&#x27;, &#x27;native&#x27;, &#x27;american&#x27;, &#x27;man&#x27;, &#x27;drank&#x27;, &#x27;200&#x27;, &#x27;cup&#x27;, &#x27;tea&#x27;, &#x27;?&#x27;], [&quot;&#x27;s&quot;, &#x27;best&#x27;, &#x27;anti&#x27;, &#x27;diarrheal&#x27;, &#x27;prescription&#x27;, &#x27;?&#x27;], [&#x27;call&#x27;, &#x27;person&#x27;, &#x27;outside&#x27;, &#x27;door&#x27;, &#x27;ha&#x27;, &#x27;arm&#x27;, &#x27;leg&#x27;, &#x27;?&#x27;], [&#x27;star&#x27;, &#x27;trek&#x27;, &#x27;character&#x27;, &#x27;member&#x27;, &#x27;magic&#x27;, &#x27;circle&#x27;, &#x27;?&#x27;], [&quot;&#x27;s&quot;, &#x27;difference&#x27;, &#x27;bullet&#x27;, &#x27;human&#x27;, &#x27;?&#x27;], [&#x27;wa&#x27;, &#x27;ethiopian&#x27;, &#x27;baby&#x27;, &#x27;cry&#x27;, &#x27;?&#x27;], [&quot;&#x27;s&quot;, &#x27;difference&#x27;, &#x27;corn&#x27;, &#x27;husker&#x27;, &#x27;epilepsy&#x27;, &#x27;hooker&#x27;, &#x27;dysentery&#x27;, &#x27;?&#x27;], [&#x27;2016&#x27;, &quot;&#x27;s&quot;, &#x27;biggest&#x27;, &#x27;sellout&#x27;, &#x27;?&#x27;],** lemmatization하고 불용어를 제거하고 난 다음의 결과물. 이제 모든 전처리가 끝났으니 TaggedDocument로 변형시키고 나서 doc2vec에 넣어준다. 12345678# 리스트에서 각 문장부분 토큰화index_questions = []for i in range(len(faqs)): index_questions.append([questions[i], i ])# Doc2Vec에서 사용하는 태그문서형으로 변경tagged_questions = [TaggedDocument(d, [int(c)]) for d, c in index_questions] doc2vec 모델화doc2vec을 훈련하기 전에 모델에 변형을 주었다. for문도 빼고 파라미터도 변경해 주었다. 123456789101112131415161718192021222324252627# make modelimport multiprocessingcores = multiprocessing.cpu_count()d2v_faqs = doc2vec.Doc2Vec(vector_size=200, # alpha=0.025, # min_alpha=0.025, hs=1, negative=0, dm=0, dbow_words = 1, min_count = 5, workers = cores, seed=0, epochs=20)d2v_faqs.build_vocab(tagged_questions)d2v_faqs.train(tagged_questions, total_examples = d2v_faqs.corpus_count, epochs = d2v_faqs.epochs ) # # train document vectors # for epoch in range(50): # d2v_faqs.train(tagged_faqs, # total_examples = d2v_faqs.corpus_count, # epochs = d2v_faqs.epochs # ) # d2v_faqs.alpha -= 0.0025 # decrease the learning rate # d2v_faqs.min_alpha = d2v_faqs.alpha # fix the learning rate, no decay 1234567# 테스트하는 문장도 같은 전처리를 해준다.test_string = &quot;What&#x27;s the best anti diarrheal prescription?&quot;tokened_test_string = word_tokenize(test_string)lemmed_test_string = [lemmatizer.lemmatize(word) for word in tokened_test_string]test_string = [w for w in lemmed_test_string if not w in stop_words]test_string 12345678910111213141516# 성능 측정raten = 5found = 0for i in range(len(faqs)): tstr = faqs[&#x27;Question&#x27;][i] tokened_test_string = word_tokenize(tstr) lemmed_test_string = [lemmatizer.lemmatize(word) for word in tokened_test_string] ttok = [w for w in lemmed_test_string if not w in stop_words] tvec = d2v_faqs.infer_vector(ttok) re = d2v_faqs.docvecs.most_similar([tvec], topn = raten) for j in range(raten): if i == re[j][0]: found = found + 1 breakprint(&quot;정확도 = &#123;&#125; % (&#123;&#125;/&#123;&#125; ) &quot;.format(found/len(faqs),found, len(faqs))) 1정확도 = 0.8626303274190598 % (33012/38269 ) Local path :C:\\Users\\jmj30\\Dropbox\\카메라 업로드\\Documentation\\2022\\2022 상반기\\휴먼교육센터\\mj_chatbot_prac\\faq_chatbot Reference: https://cholol.tistory.com/466","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Python","slug":"Python/Python","permalink":"https://jmj3047.github.io/categories/Python/Python/"}],"tags":[{"name":"Doc2vec","slug":"Doc2vec","permalink":"https://jmj3047.github.io/tags/Doc2vec/"},{"name":"Chatbot","slug":"Chatbot","permalink":"https://jmj3047.github.io/tags/Chatbot/"},{"name":"NLP","slug":"NLP","permalink":"https://jmj3047.github.io/tags/NLP/"}]},{"title":"Making English Chatbot with Django(4)","slug":"Making_English_Chatbot_with_Django(4)","date":"2022-05-05T15:00:00.000Z","updated":"2022-05-06T08:28:34.000Z","comments":true,"path":"2022/05/06/Making_English_Chatbot_with_Django(4)/","link":"","permalink":"https://jmj3047.github.io/2022/05/06/Making_English_Chatbot_with_Django(4)/","excerpt":"","text":"실제 서비스 구현해보기**code: https://github.com/jmj3047/faq_chatbot_example.git vs code로 django 설정하기: https://integer-ji.tistory.com/81 채팅창 만들기 html&#x2F;css를 사용해 간단한 채팅화면을 만들었다. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465&lt;!-- //templates/addresses/chat_test.html --&gt;&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;/static/jquery-3.2.1.min.js&quot;&gt;&lt;/script&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt; &lt;link rel=&quot;preconnect&quot; href=&quot;https://fonts.googleapis.com&quot;&gt; &lt;link rel=&quot;preconnect&quot; href=&quot;https://fonts.gstatic.com&quot; crossorigin&gt; &lt;link href=&quot;https://fonts.googleapis.com/css2?family=Inconsolata:wght@400;600&amp;display=swap&quot; rel=&quot;stylesheet&quot;&gt;&lt;/head&gt;&lt;style&gt;* &#123;font-family: &#x27;Inconsolata&#x27;, monospace;&#125;.chat_wrap &#123;display:none;width: 350px;height: 500px;position: fixed;bottom: 30px;right: 95px;background: #a9bdce;&#125;.chat_content &#123;font-size:16pt; position:relative; height: 600px;width: 500px;overflow-y:scroll;padding:10px 15px;background: cornflowerblue&#125;.chat_input &#123;border:solid 0.5px lightgray; padding:2px 5px;&#125;.chat_header &#123;padding: 10px 15px; width: 500px; border-bottom: 1px solid #95a6b4;&#125;.chat_header .close_btn &#123;border: none;background: lightgray;float: right;&#125;.send_btn &#123;border: none; background: #ffeb33;height: 100%; color: #0a0a0a;&#125;.msg_box:after &#123;content: &#x27;&#x27;;display: block;clear:both;&#125;.msg_box &gt; span &#123;padding: 3px 5px;word-break: break-all;display: block;max-width: 300px;margin-bottom: 10px;border-radius: 4px&#125;.msg_box.send &gt; span &#123;background:#ffeb33;float: right;&#125;.msg_box.receive &gt; span &#123;background:#fff;float: left;&#125;&lt;/style&gt;&lt;body&gt;&lt;div class=&quot;chat_header&quot;&gt; &lt;span style=&quot;font-size:20pt;&quot;&gt;EDITH&lt;/span&gt; &lt;button type=&quot;button&quot; id=&quot;close_chat_btn&quot; class=&quot;close_btn&quot;&gt;X&lt;/button&gt;&lt;/div&gt;&lt;div id=&quot;divbox&quot; class=&quot;chat_content&quot;&gt;&lt;/div&gt;&lt;form id=&quot;form&quot; style=&quot;display: inline&quot;&gt; &lt;input type=&quot;text&quot; placeholder=&quot;write message..&quot; name=&quot;input1&quot; class=&quot;chat_input&quot; id=&quot;input1&quot; size=&quot;74&quot; style=&quot;margin:-3px; display: inline; width: 468px; height: 32px; font-size: 16pt;&quot; /&gt; &lt;input type=&quot;button&quot; value=&quot;SEND&quot; id=&quot;btn_submit&quot; class=&quot;send_btn&quot; style=&quot;margin:-5px; display: inline; width: 53px; height: 38px; font-size: 14pt;&quot; /&gt;&lt;/form&gt;&lt;script&gt; $(&#x27;#btn_submit&#x27;).click(function () &#123; send(); &#125;); $(&#x27;#form&#x27;).on(&#x27;submit&#x27;, function(e)&#123; e.preventDefault(); send(); &#125;); $(&#x27;#close_chat_btn&#x27;).on(&#x27;click&#x27;, function()&#123; $(&#x27;#chat_wrap&#x27;).hide().empty(); &#125;); function send()&#123; $(&#x27;#divbox&#x27;).append(&#x27;&lt;div class=&quot;msg_box send&quot;&gt;&lt;span&gt;&#x27;+$(&#x27;#input1&#x27;).val()+&#x27;&lt;span&gt;&lt;/div&gt;&#x27;); $(&quot;#divbox&quot;).scrollTop($(&quot;#divbox&quot;)[0].scrollHeight); console.log(&quot;serial&quot;+$(&#x27;form&#x27;).serialize()) $.ajax(&#123; url: &#x27;http://127.0.0.1:8000/chat_service/&#x27;, //챗봇 api url type: &#x27;post&#x27;, dataType: &#x27;json&#x27;, data: $(&#x27;form&#x27;).serialize(), success: function(data) &#123; &lt;!--$(&#x27;#reponse&#x27;).html(data.reponse);--&gt; $(&#x27;#divbox&#x27;).append(&#x27;&lt;div class=&quot;msg_box receive&quot;&gt;&lt;span&gt;&#x27;+ data.response +&#x27;&lt;span&gt;&lt;/div&gt;&#x27;); $(&quot;#divbox&quot;).scrollTop($(&quot;#divbox&quot;)[0].scrollHeight); &#125; &#125;); $(&#x27;#input1&#x27;).val(&#x27;&#x27;); &#125;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 간단하게 설명하면 Django로 restfulAPI를 구현하기 위한 소스 위에 챗봇을 붙이기 위한 화면과 모델이 들어가 있는 버전이다. 위에 소스는 화면 역할을 하는 chat_test.html 파일이다. jquery 라이브러리를 사용했기 때문에 jquery를 import 해야 한다. jquery file이 static 폴더에 있어야 한다. jquery는 소스 하단부에 있는 script를 위해 필요하다. 채팅에서 전송 버튼을 누르거나 엔터를 누르면 send()라는 함수가 실행되고 이 함수는 ajax로 질문에 대한 답변을 받아오는 API를 호출한다. 여기서는 localhost&#x2F;chat_service를 호출한다. 채팅을 위한 API화면이 만들어졌으면 이제 질문을 받아 답변을 생성하는 API를 만든다. 아직 FAQ데이터를 학습한 모델은 넣지 않았으니 인풋이 들어오면 더미데이터(dummy)를 리턴하는 API를 만든다. 이런 API 동작들은 view.py에서 구현할 수 있다. 1234567891011#faq_chatbot_example/addresses/views.py@csrf_exemptdef chat_service(request): if request.method == &#x27;POST&#x27;: input1 = request.POST[&#x27;input1&#x27;] output = dict() output[&#x27;response&#x27;] = &quot;이건 응답&quot; return HttpResponse(json.dumps(output), status=200) else: return render(request, &#x27;addresses/chat_test.html&#x27;) Django 프로젝트 안에 addresses 앱에 있는 views.py를 보면 chat_service 함수를 만들었다. POST형식으로 콜이 오면 response에 아웃풋 메세지를 담아서 json형태로 리턴한다. views.py에 함수를 만들고 url로 연결하기 위해서 urls.py에 chat_service를 입력한다. 123456789101112###django 3.8.3 버전 맞춰줘야 함#/faq_chatbot_example/restfulapiserver/urls.py# from django.conf.urls import url, includefrom addresses import viewsfrom django.urls import path, re_path, includefrom django.contrib import adminurlpatterns = [ ... path(&#x27;chat_service/&#x27;, views.chat_service), ...] urls.py에서 ~&#x2F;chat_service를 views.chat_service에 연결시킨다. 이제 ~&#x2F;chat_service로 콜하면 views.chat_service가 실행된다. 아까 위에서 만든 채팅페이지에 전송버튼을 누르면 ajax를 이용해 chat_service를 호출했다. 정상적으로 되는지 테스트 해본다. FAQ 모델 넣기addresses 앱 안에 새로운 py 모델을 만들어서 넣기 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374#/faq_chatbot_example/addresses/faq_chatbot.pyfrom gensim.models import doc2vec, Doc2Vecfrom gensim.models.doc2vec import TaggedDocumentimport pandas as pdfrom nltk.tokenize import word_tokenizefrom nltk.stem import WordNetLemmatizerfrom nltk.corpus import stopwordsimport nltk# 파일로부터 모델을 읽는다. 없으면 생성한다.try: d2v_faqs = Doc2Vec.load(&#x27;d2v_faqs_size200_min5_epoch20_jokes.model&#x27;) lemmatizer = WordNetLemmatizer() stop_words = stopwords.words(&#x27;english&#x27;) faqs = pd.read_csv(&#x27;jokes.csv&#x27;)except: faqs = pd.read_csv(&#x27;jokes.csv&#x27;) nltk.download(&#x27;punkt&#x27;) # 토근화 tokened_questions = [word_tokenize(question.lower()) for question in faqs[&#x27;Question&#x27;]] lemmatizer = WordNetLemmatizer() nltk.download(&#x27;wordnet&#x27;) # lemmatization lemmed_questions = [[lemmatizer.lemmatize(word) for word in doc] for doc in tokened_questions] nltk.download(&#x27;stopwords&#x27;) # stopword 제거 불용어 제거하기 stop_words = stopwords.words(&#x27;english&#x27;) questions = [[w for w in doc if not w in stop_words] for doc in lemmed_questions] # 리스트에서 각 문장부분 토큰화 index_questions = [] for i in range(len(faqs)): index_questions.append([questions[i], i ]) # Doc2Vec에서 사용하는 태그문서형으로 변경 tagged_questions = [TaggedDocument(d, [int(c)]) for d, c in index_questions] # make model import multiprocessing cores = multiprocessing.cpu_count() d2v_faqs = doc2vec.Doc2Vec( vector_size=200, hs=1, negative=0, dm=0, dbow_words=1, min_count=5, workers=cores, seed=0, epochs=20 ) d2v_faqs.build_vocab(tagged_questions) d2v_faqs.train(tagged_questions, total_examples=d2v_faqs.corpus_count, epochs=d2v_faqs.epochs) d2v_faqs.save(&#x27;d2v_faqs_size200_min5_epoch20_jokes.model&#x27;)# FAQ 답변def faq_answer(input): # 테스트하는 문장도 같은 전처리를 해준다. tokened_test_string = word_tokenize(input) lemmed_test_string = [lemmatizer.lemmatize(word) for word in tokened_test_string] test_string = [w for w in lemmed_test_string if not w in stop_words] topn = 5 test_vector = d2v_faqs.infer_vector(test_string) result = d2v_faqs.docvecs.most_similar([test_vector], topn=topn) print(result) for i in range(topn): print(&quot;&#123;&#125;위. &#123;&#125;, &#123;&#125; &#123;&#125; &#123;&#125;&quot;.format(i + 1, result[i][1], result[i][0], faqs[&#x27;Question&#x27;][result[i][0]], faqs[&#x27;Answer&#x27;][result[i][0]])) return faqs[&#x27;Answer&#x27;][result[0][0]]faq_answer(&quot;What do you call a person who is outside a door and has no arms nor legs?&quot;) 위 소스에서 상단에 있는 모델을 만드는 코드는 API 서버를 실행하는 시점에서 호출된다. 무조건 호출하는 건 아니고 views.py에서 import를 써 넣으면 최초 1번은 실행되게 된다. 채팅 웹페이지로부터 faq_chatbot.py에 있는 faq_answer를 호출하는 것 까지 flow를 그려보면 chat_test.html→view.py(chat_service)→faq_chatbot.py(faq_answer) 순서이다. 따라서 views.py에서 faq_answer함수를 호출하기 위해 import를 하게 되는데 django는 최초 실행시 views.py를 한번 읽기 때문에 faq_chatbot.py에 적어놓은 소스가 한번 실행되게 된다. 매번서버를 실행할 때마다 모델을 새로 만들게 되면 서버 기동 속도가 느려지고 비효율적이기 때문에 모델을 만들고 나서 패일로 저장하고, 만들어진 파일이 없다면 모델을 생성하도록 try&#x2F;except를 사용했다. 추가적으로 프로젝트상 소스가 실행되기 때문에 파일경로는 root이다. jokes.csv가 있어야 할 곳과 모델이 생성되는 곳의 경로는 프로젝트의 root폴더이다. 자 이제 질문의 답을 찾아주는 함수가 만들어 졌으니 아까 더미 데이터로 리턴해주던 views.py의 함수를 바꿔보자. 123456789101112#faq_chatbot_example/addresses/views.py@csrf_exemptdef chat_service(request): if request.method == &#x27;POST&#x27;: input1 = request.POST[&#x27;input1&#x27;] response = faq_answer(input1) output = dict() output[&#x27;response&#x27;] = response return HttpResponse(json.dumps(output), status=200) else: return render(request, &#x27;addresses/chat_test1.html&#x27;) 이전에는 response에 무조건 더미 응답을 보냈는데 이제는 faq_answer함수를 사용해 해당 질문에 알맞은 정답을 가져온다. faq_answer함수를 사용하기 위해 제일 상단에 from .faq_chatbot import faq_answer를 선언해야 한다. 실행 결과(html 파일 수정) **code: ‣https://github.com/jmj3047/faq_chatbot_example.git Reference: https://cholol.tistory.com/478","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Django","slug":"Python/Django","permalink":"https://jmj3047.github.io/categories/Python/Django/"}],"tags":[{"name":"Doc2vec","slug":"Doc2vec","permalink":"https://jmj3047.github.io/tags/Doc2vec/"},{"name":"Chatbot","slug":"Chatbot","permalink":"https://jmj3047.github.io/tags/Chatbot/"},{"name":"NLP","slug":"NLP","permalink":"https://jmj3047.github.io/tags/NLP/"}]},{"title":"Making Korean Chatbot with doc2vec(2)","slug":"Making_Korean_Chatbot_with_doc2vec(2)","date":"2022-05-05T15:00:00.000Z","updated":"2022-10-15T08:08:14.439Z","comments":true,"path":"2022/05/06/Making_Korean_Chatbot_with_doc2vec(2)/","link":"","permalink":"https://jmj3047.github.io/2022/05/06/Making_Korean_Chatbot_with_doc2vec(2)/","excerpt":"","text":"모델 다듬기FAQ데이터 늘리기더 많은 학습 데이터로 모델을 학습한다. 데이터 원본 링크: https://www.data.go.kr/dataset/3068685/fileData.do 123456789import osimport warningsfrom gensim.models import doc2vecfrom gensim.models.doc2vec import TaggedDocumentimport pandas as pdfaqs = pd.read_csv(os.path.join(&#x27;data&#x27;,&#x27;kor_elec_faq2.csv&#x27;), encoding=&#x27;CP949&#x27;)faqsfaqs[[&#x27;순번&#x27;, &#x27;제목&#x27;, &#x27;내용&#x27;]] pandas를 사용해 csv파일을 바로 읽어준다. utf-8 인코딩 문제로 에러가 나면 cp949로 넣어준다. 전체 필드에서 필요한 index와 질문(여기서는 제목), 답변(여기서는 내용)만 뽑아낸다. 총 351개의 질문과 답변 데이터가 생겼으니 전 게시물과 동일한 방법으로 모델학습을 시킨다. 전 데이터는 pandas데이터가 아니었기 때문에 수정해 준다. 1234567# 리스트에서 각 문장부분 토큰화token_faqs = []for i in range(len(faqs)): token_faqs.append([tokenize_kkma_noun(faqs[&#x27;제목&#x27;][i]), faqs[&#x27;순번&#x27;][i]])# Doc2Vec에서 사용하는 태그문서형으로 변경tagged_faqs = [TaggedDocument(d, [c]) for d, c in token_faqs] 이렇게 하고 모델을 돌려도 성능이 좋지 않음. doc2vec모델의 경우 최소 만단위의 문장이 있어야 제대로 나온다. 튜닝 시도해보기이전까지 데이터 내에 있는 순번을 인덱스로 사용했는데 정상적으로 작동하지 않아 다시 만들어준다. 123456789# 리스트에서 각 문장부분 토큰화token_faqs = []for i in range(len(faqs)): token_faqs.append([tokenize_kkma_noun(faqs[&#x27;제목&#x27;][i]), i ]) # token_faqs.append([tokenize_kkma_noun(faqs[&#x27;제목&#x27;][i]), faqs[&#x27;순번&#x27;][i]])# Doc2Vec에서 사용하는 태그문서형으로 변경tagged_faqs = [TaggedDocument(d, [int(c)]) for d, c in token_faqs]# tagged_faqs = [TaggedDocument(d, [c]) for d, c in token_faqs] 문서 원본을 수정할 필요는 없고 TaggedDocument만들 때만 잘 넣어준면 된다. 기존에는 faqs[’순번’][i]를 태그 값으로 넣어주었는데 그냥 i를 넣는다. 이렇게 하면 좋은 점이 원본의 index와 태그값이 같아지기 때문에 나중에 원문질문을 복원할 때 faqs[’제목’][tag]로 출력이 가능하다. 그리고 이제 가장 먼저 할 거는 전처리를 조금 수정하는 것이다. 형태소 분석을 할 때 필요 없는 데이터를 제외시키는 방법이다. 보통 문장에서는 명사와 동사가 중요하기 때문에 명사 동사 빼고는 다 날려본다. 1234567891011121314151617#튜닝:형태소 필터링kkma = Kkma()filter_kkma = [&#x27;NNG&#x27;, #보통명사 &#x27;NNP&#x27;, #고유명사 &#x27;OL&#x27; , #외국어 &#x27;VA&#x27;,&#x27;VV&#x27;,&#x27;VXV&#x27; ]def tokenize_kkma(doc): jpype.attachThreadToJVM() token_doc = [&#x27;/&#x27;.join(word) for word in kkma.pos(doc)] return token_docdef tokenize_kkma_noun(doc): jpype.attachThreadToJVM() token_doc = [&#x27;/&#x27;.join(word) for word in kkma.pos(doc) if word[1] in filter_kkma] return token_doc 이런식으로 filter_kkma리스트를 하나 만들어 형태소를 분석했을 때 나오는 형태소가 filter_kkma에 포함되어 있을 경우만 학습 대상에 추가한다. tokenize_kkma를 쓰면 전체 형태소 분석, tokenize_kkmk_noun을 쓰면 동사 명사만 추출한다. 가장 결과가 좋게 나온 조합은 명사만 추출, for 문 50번에 epochs&#x3D;100으로 한 결과값. 12345678910111213141516#튜닝:명사만 추출kkma = Kkma()filter_kkma = [&#x27;NNG&#x27;, #보통명사 &#x27;NNP&#x27;, #고유명사 &#x27;OL&#x27; , #외국어 ]def tokenize_kkma(doc): jpype.attachThreadToJVM() token_doc = [&#x27;/&#x27;.join(word) for word in kkma.pos(doc)] return token_docdef tokenize_kkma_noun(doc): jpype.attachThreadToJVM() token_doc = [&#x27;/&#x27;.join(word) for word in kkma.pos(doc) if word[1] in filter_kkma] return token_doc 123456789101112131415161718192021222324252627# make model import multiprocessing import tensorflow as tf with tf.device(&#x27;/device:GPU:0&#x27;): cores = multiprocessing.cpu_count() d2v_faqs = doc2vec.Doc2Vec(vector_size=20, #100 alpha=0.025, min_alpha=0.025, hs=1, negative=0, dm=0, window=3, dbow_words = 1, min_count = 1, workers = cores, seed=0, epochs=100) d2v_faqs.build_vocab(tagged_faqs) # train document vectors for epoch in range(50): d2v_faqs.train(tagged_faqs, total_examples = d2v_faqs.corpus_count, epochs = d2v_faqs.epochs ) d2v_faqs.alpha -= 0.0025 # decrease the learning rate d2v_faqs.min_alpha = d2v_faqs.alpha # fix the learning rate, no decay 123test_string = &quot;변압기공동이용(모자거래)이란 무엇이며, 요금계산은 어떻게 합니까&quot;tokened_test_string = tokenize_kkma_noun(test_string)tokened_test_string 12345678910111213# 성능 측정# raten = 5 #정확도 = 0.5128205128205128 % (180/351 )raten = 1 #정확도 = 0.24216524216524216 % (85/351 ) found = 0for i in range(len(faqs)): tstr = faqs[&#x27;제목&#x27;][i] ttok = tokenize_kkma_noun(tstr) tvec = d2v_faqs.infer_vector(ttok) re = d2v_faqs.docvecs.most_similar([tvec], topn = raten) for j in range(raten): if i == re[j][0]: found = found + 1print(&quot;정확도 = &#123;&#125; % (&#123;&#125;/&#123;&#125; ) &quot;.format(found/len(faqs),found, len(faqs)) 모델 저장, 불러오기1234567891011121314151617# 모델 저장d2v_faqs.save(os.path.join(&#x27;data&#x27;,&#x27;/d2v_faqs_size100_min1_batch50_epoch100_nounonly_dm0.model&#x27;))# 모델 loadd2v_faqs_1 = doc2vec.Doc2Vec.load(os.path.join(&#x27;data&#x27;,&#x27;/d2v_faqs_size100_min1_batch50_epoch100_nounonly_dm0.model&#x27;))#testtest_string = &quot;건물을 새로 지을 때 임시전력은 어떻게 신청하나요&quot;tokened_test_string = tokenize_kkma_noun(test_string)tokened_test_stringtopn = 5# 모델 추측test_vector1 = d2v_faqs_1.infer_vector(tokened_test_string)result1 = d2v_faqs_1.docvecs.most_similar([test_vector1], topn=topn)for i in range(topn): print(&quot;모델 1 &#123;&#125;위. &#123;&#125;, &#123;&#125; &#123;&#125;&quot;.format(i+1, result1[i][1], result1[i][0],faqs[&#x27;제목&#x27;][result1[i][0]] )) Local path :C:\\Users\\jmj30\\Dropbox\\카메라 업로드\\Documentation\\2022\\2022 상반기\\휴먼교육센터\\mj_chatbot_prac\\faq_chatbot Reference: https://cholol.tistory.com/466","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Python","slug":"Python/Python","permalink":"https://jmj3047.github.io/categories/Python/Python/"}],"tags":[{"name":"Doc2vec","slug":"Doc2vec","permalink":"https://jmj3047.github.io/tags/Doc2vec/"},{"name":"Chatbot","slug":"Chatbot","permalink":"https://jmj3047.github.io/tags/Chatbot/"},{"name":"NLP","slug":"NLP","permalink":"https://jmj3047.github.io/tags/NLP/"}]},{"title":"Setting Git & Virtualenv","slug":"Setting_Git_&_Virtualenv","date":"2022-05-05T15:00:00.000Z","updated":"2023-02-28T07:50:54.784Z","comments":true,"path":"2022/05/06/Setting_Git_&_Virtualenv/","link":"","permalink":"https://jmj3047.github.io/2022/05/06/Setting_Git_&_Virtualenv/","excerpt":"","text":"Put Local folder into git repo Make folder ‘example’ and git repo ‘example 123456789101112131415161718192021222324#in local cmd example foldergit init #add remote repogit remote add origin &#x27;repo https&#x27;#bring files in repo to localgit pull origin master #bring local files to git repogit add .git commit -m &#x27;updated&#x27;git push orgin master #check remotegit remote -v#check current statusgit status#error: failed to push some refs to &#x27;https://github.com/jmj3047/.git&#x27;#force to push git push -f origin master Setting virtual env in window&#x2F;linux1234567#****use virtual env no matter what****&gt;python -m venv env_name&gt;source env_name/Scripts/activate #window&gt;source env_name/bin/activate #linux#put all the version of modules in requirements.txt&gt;pip install -r requirements.txt","categories":[{"name":"Setting","slug":"Setting","permalink":"https://jmj3047.github.io/categories/Setting/"}],"tags":[{"name":"Git","slug":"Git","permalink":"https://jmj3047.github.io/tags/Git/"},{"name":"Virtualenv","slug":"Virtualenv","permalink":"https://jmj3047.github.io/tags/Virtualenv/"}]},{"title":"Support Vector Machine","slug":"SVM","date":"2022-05-05T15:00:00.000Z","updated":"2023-02-23T01:34:32.094Z","comments":true,"path":"2022/05/06/SVM/","link":"","permalink":"https://jmj3047.github.io/2022/05/06/SVM/","excerpt":"","text":"1. 분류에 대한 수적 표현 학습 데이터 X(독립변수),Y(종속변수)가 있을 때 (i&#x3D;1,2,3,4,5 ….데이터의 갯수) Y⇒{-1,1} (두 개의 클래스를 의미) ⇒ 경우에 따라서, 클래스를 1과 -1 로 나눔 Y(정답) * F(x)(예측한 정답) &gt;0 라는 것은 제대로 분류된 형태 ( 같은 부호끼리 곱하면 양수인 경우니까) 2. 선형 분할(Linear Classifier) f(x)&#x3D; W transpose X + b (선형조합, 각각의 항들이 더하기로 이루어진 조합.) 선형분할은 직선으로 나누는 것 (2차원이건 3차원이건 그 이상이건 상관 없음) b(bias) Y 절편을 의미 W는 직선의 기울기 3. 초평면 분할 더 나은(최적) 분류를 위한 초평면(Hyperplane)→선 보다 더 큰 차원 좋은 판별선에 대한 기준 최적화: 좋은 것 을 극대화 시키고 나쁜 것 을 극소화 시키는 것 분류에서의 최적화: 잘 안나뉘는것 , 잘 나뉘는 것 나중에 Testing data 를 돌렸을때, 가장 좋게 나뉜 것은 반절로 나뉜 직선이다. Test data 가 어떻게 들어올지 모르는 것 이기때문에 , 과적합 되어 있는 것보다 확실히 절반으로 나누는것이 좋다. 최적의 분할 초평면 찾기 Margin: c는 선형분할의 각 클래스별 거리 각 클래스별 거리를 합친 것 Margin&#x3D;2c를 최대화 하는, w T x +b&#x3D;0 의 직선을 찾아야 하는것 이다. Marign 을 최대화 시키는 초평면이 최적 “Learning Theory” 에 따르면, Marigin을 최대화 시키는 초평면이 일반화 오류가 가장 낮게 나타남(Test data 에서도 좋은 점수가 나온다) Margin:초평면과 가장 근접한 각 클래스 관측치와의 거리의 합. Margin 수식 유도 일반적인 방법 점과 선 사이의 거리 거리 d 가 2개이니까 2&#x2F;||W|| Margin 최대화 (최적화) ||w|| 가 분모에 있기 때문에 결국 ||w|| 를 최소화 해주는것 이 2&#x2F;||w|| 를 최대화 해주는거랑 같다고 할 수 있다 우리는 결국 w 값을 최소화 시켜주는것이 목적이기 때문에 제곱을 취해주든 상수를 곱해주는 상관이 없다 Lagrange Multiplier(수학적 기법) ⇒ 제약조건을 최적화 조건에 녹여버리는 기법. 💡 라그랑쥬를 다 풀고 나면 판별식이 나온다. Xi tranpose X ( 학습데이터와 분류할 데이터의 내적) 4. SVM(Support Vector Machine) 판별식에 서포트벡터만 사용하기 때문에 아웃라이어에 대한 영향을 안 받음(KKT 조건으로 걸러냄) KNN 또한 이웃을 확인하는 개수인 K의 한계가 있어서 어느 elbow point 를 지나치면 정확도가 떨어진다. → 비슷한 원리 ⇒ svm 또한 분류를 유효하게 하기위해서 support verctor 만 이용해준다. 선형으로 완벽히 나눠지지 않는 데이터라면 테스트 데이터에게는 위의 모델 보다 아래 모델이 더 좋을 것 으로 보인다. 하지만 SVM 의 제약조건에는 트레인데이터가 완벽하게 나누어져야 한다는 제약 조건이 걸려있다. 어떻게 하면 좋을까? Slack Variable for “Soft Margin” Soft Margin SVM Non-linear SVM Reference: 한국공학대학교 경영학과 강지훈 교수님 강의","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}],"tags":[{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"}]},{"title":"What is Transformer","slug":"What_is_Transformer","date":"2022-05-05T15:00:00.000Z","updated":"2022-10-15T08:08:32.440Z","comments":true,"path":"2022/05/06/What_is_Transformer/","link":"","permalink":"https://jmj3047.github.io/2022/05/06/What_is_Transformer/","excerpt":"","text":"Transformer란?트랜스포머(Transformer)는 구글에서 발표한 논문 “Attention is all you need”에 나오는 모델이다. 아래 글은 이 논문 abstract의 일부분이다. The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU … 여기서도 알 수 있듯이, 트랜스포머는 어텐션(Attention) mechanism을 기반으로 여러개의 인코더와 디코더를 연결한 구조를 갖고 있다. 또한 CNN, RNN, LSTM 등의 구조를 사용하지 않았기 때문에 학습 시간이 훨씬 감소된 성능을 내었다고 한다. 그렇다면 그 구조가 무엇인지 더 알아보도록 하자. (1) 트랜스포머의 입력먼저 트랜스포머의 입력부터 알아보자, 단어 벡터 데이터가 트랜스포머의 입력으로 들어가지게 되는데 이 때 각 단어의 위치 정보를 알려주어야 한다. 왜냐하면 트랜스포머에 단어가 입력될 때 순차적으로 받아지지 않기 때문이다. 따라서 순서 정보를 더해주어야 하기 때문에 위치 정보를 각 단어 벡터마다 더해주어야 하는데, 이 과정을 포지셔널 인코딩(positional encoding)이라고 한다. 포지셔널 인코딩 값을 더해주기 위해서는 사인함수와 코사인함수를 사용한 아래 두 함수를 사용한다. 위 식에서 pos는 입력된 데이터의 임베딩 벡터(몇번째 단어인지)를, i는 임베딩 벡터내의 차원의 인덱스(0~512)를 뜻한다. 임베딩 벡터내의 차원이란 트랜스포머 모델의 인코더와 디코더에서 정해진 입력과 출력의 크기를 말한다. 논문상에서 이 차원을 512로 설정했으면 이 차원은 인코더의 값을 디코더로 보낼때 값을 유지하도록 한다. 다시 돌아와서, 위 함수에서 차원이 2i(짝수)인지 2i+1(홀수)인지에 따라서 사용하는 함수가 다르다. 짝수 차원의 경우 사인함수, 홀수차원의 경우 코사인 함수를 사용하게된다. (2)인코더(Encoder)의 구조 위 이미지는 트랜스포머 논문에 함께 실려 있는 이미지로, 트랜스포머의 구조를 나타낸다.여기서 왼쪽 부분이 트랜스포머의 인코더 부분인데, 인코더의 구조는 어떻게 이루어졌을까? 먼저 이미지 왼쪽 아래를 보자. Input data가 들어가게 되면 Input Embedding을 거치게 되는데, 여기서는 문자열인 단어 데이터를 벡터형태로 변환해 준다(단어 길이 X 벡터차원의 행렬). 그리고 나서 위에서 설명한 포지셔널 인코딩을 수행해주게 된다. 그러고 나서 박스로 표현된 인코더에 들어가게 된다. 인코더 안에서는 크게 Multi-Head Attention과 Feed Forward과정이 수행되는데, Multi-Head Attention은 셀프 어텐션이 병렬적으로 사용된 것을 말하며, Feed Forward란 피드포워드 신경망 구조를 의미한다. 한편, 위에서 잠깐 언급했지만 트랜스포머에서는 여러 개의 인코더와 디코더를 쌓은 구조를 갖고 있다. 즉, 인코더가 1개가 아니라는 뜻인데, 논문에서는 6개의 인코더 층을 사용했다고 하니, 6개라고 설정하도록 하겠다. 아무튼, 인코더 과정을 총 6번 반복한다고 생각하면 된다. *셀프 어텐션이란?셀프 어텐션이란 자기 자신에게 어텐션 함수를 수행하는 것을 말하는데, 그렇다면 어텐션이란 무엇일까? 어텐션에서도 다양한 종류가 있는데 간단히 말하자면, 쿼리(Query)가 주어졌을 때, 이 쿼리와 여러개의 키(Key)와의 유사도를 각각 구하고, 구한 유사도를 가중치로 설정하여 각각의 값(value)을 구한 뒤, 이 값(유사도가 반영된 값)들을 모두 가중합하여 반환하는 함수를 말한다. 예를 들어, 한 텍스트 문장이 쿼리로 입력될 때, 각 단어 벡터들과의 유사도를 계산해 이 유사도를 가중합하여 반환된 값이 그 문장의 어텐션 값이 된다. 그렇다면 셀프 어텐션 값을 구하기 위해서 입력된 문장의 단어 벡터(쿼리)에 대해 쿼리(query),키(key), 값(value) 벡터가 정의되어야 할 것이다. 그 과정은 아래 이미지를 통해 쉽게 이해할 수 있다. ‘student’라는 단어 벡터가 입력되었을 때, 각각 쿼리, 키 값의 가중치 행렬을 곱해주어 쿼리, 키, 값 벡터를 얻어낸다. 이렇게 쿼리 벡터, 키 벡터, 값 벡터를 얻어냈다면 쿼리 벡터는 모든 키 벡터에 대해 어텐션 스코어(attention score)를 구하게 되고, 이를 이용하여 모든 값 벡터를 가중합 하여 어텐션 값을 구하게 된다. 한편, 이러한 연산은 각 단어마다가 아닌 문장 전체에 대해서 행렬 연산으로도 일괄적으로 연산이 가능한데, 위와 같이 문장에 대한 쿼리 벡터, 키 벡터의 연산을 통해 값 벡터 행렬을 구할 수 있게 된다. 마지막으로 쿼리벡터와 키 벡터가 연산되어 나온 행렬에 전체적으로 특정 값(key벡터 차원의 제곱근 값)을 나누어 준 뒤, 소프트맥스 함수를 적용해주고, 가중치가 계산된 값 벡터를 곱하게 되면 최종적으로 각 단어의 어텐션 값을 가지는 어텐션 값 행렬이 도출된다. 즉, 요약하자면 어텐션 함수는 쿼리(Query)가 주어졌을 때, 이 쿼리와 여러개의 키(key)와의 유사도를 각각 구하고, 구한 유사도를 가중치로 설정하여 각각의 값(value)을 구한 뒤, 유사도가 반영된 값들을 모두 가중합 하여 반환하는 함수를 말한다. *멀티 헤드 어텐션(Multi-Head Attention)이란?앞에서 트랜스포머의 인코더에서는 어텐션이 병렬적으로 수행되는 멀티 헤드 어텐션이 수행된다고 했다. 논문에서는 512차원의 벡터를 8로 나누어 54차원의 Query, Key, Value 벡터로 바꾸어서 어텐션 함수를 병렬적으로 수행한 것인데, 그렇다면 왜 이렇게 수행한 것일까? 즉, 차원을 나누어서 어텐션 함수를 수행한 뒤, 가중치 행렬을 곱해주고 이를 다시 합치게 되는건데, 논문에 따르면 single attention function을 하는 것보다 병렬적으로 수행하는 것이 모델이 학습하는 데에는 더 효과적이었으며, 모델이 다른 영역(과거시점과 미래시점)에 있는 정보들을 참조할 수 있다고한다. 따라서 출력된 값들은 인코더의 입력 값의 차원과 동일하게 유지된다. *피드 포워드 신경망이란?인코더 안에서 Multi-Head Attention이 수행되고 나면 Feed Forward가 수행된다고 했었는데, Feed Forward는 무엇일까? Feed Forward는 일종의 신경망으로 Feed Forward Neural Network를 줄여서 FFNN이라고 한다. FFNN의 종류도 여러가지가 있는데, 트랜스 포머의 인코더 층에는 포지션 와이즈(Position-wise) FFNN을 사용한다. 포지션 와이즈 FFNN은 Fully-connected FFNN과 같은 기능을 하는데, 아래와 같은 연산을 수행한다. 위 식에서 x의 값은 Multi-Head Attention에서 출력된 행렬 값이다. 반면, 가중치를 의미하는 W1, W2, b1, b2는 가중치 값으로 인코더 마다 다른 값을 가지지만 하나의 인코더 층 안에서는 문장과 단어들마다 동일하게 사용된다고 한다. 이렇게 피드 포워드 신경망까지 거치게 되면 한 인코더의 출력값이 도출 되고, 이 값은 다시 두번째 인코더 입력으로 들어가게 되며 이 과정이 반복된다. *Add &amp; Norm한편 인코더의 구조를 보여준 이밎를 다시 보고 오면 2개의 서브층인 Multi-Head Attention과 Feed Forward가 각각 끝나고 나면 Add &amp; Norm 이라는 단계가 수행된다. 이것은 또 무엇일까? 논문의 일부분을 읽어보면 Add &amp; Norm이란 바로 두개의 서브층을 residual connection 해주고 layer normalization을 해주는 것을 의미한다. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. Residual connection과 layer normalization에 대해 짧게 요약하자면, residual connection은 서브층의 입력과 출력을 더 하는 것이다. 이러한 알고리즘은 RNN, VGG 구조에서도 볼 수 있고, 이러한 연산이 가능한 것은 입력 데이터와 출력 데이터가 동일한 차원을 갖고 있기 때문이라고 한다. 반면, layer normalization은 정규화를 하는 과정으로 출력된 값들에 대해서 평균과 분산을 구해서 정규화를 하는 것을 말한다. 앞에서 입력데이터인 512차원의 벡터를 8로 나누어 어텐션 함수를 병렬적으로 수행하였다고 했는데, 그렇게 출력된 8개의 값들로 layer normalization을 하는 것이다. (3)디코더(Decoder)의 구조 다시 트랜스포머의 구조를 살펴보자. 지금까지 왼쪽에 있는 인코더에 대해 살펴 보았고, 이제 오른쪽에 있는 디코더에 대해 살펴보도록 하겠다. 디코더는 인코더에서 넘겨받은 값에 대해 Multi-Head Attention과 Feed Forward를 수행하기 전 output data에 대해 임베딩과 포지셔널 인코딩을 한 값을 입력 받는다. 그리고 인코더와는 다르게 Masked Multi-Head Attention이라는 것을 해주게 된다. *Masked Multi-Head Attention이란?Masked Multi-Head Attention은 말그대로 Multi-Head Attention에서 Mask기능이 들어간 것이다. 앞에서 Multi-Head Attention은 셀프어텐션을 병렬적으로 수행한 것을 의미했었다. 따라서 다른 영역에 있는, 즉 미래의 시점에 있는 단어의 정보도 알 수 있게 된다고 했었다. 이러한 이유로 트랜스포머의 디코더에는 현재시점보다 미래에 있는 단어를 참고해 예측하지 못하고 이전시점들에 있는 단어들만 참고할 수 있도록 마스킹해줘야 한다. 아마 미래에 있는 단어를 참고해 예측하도록 한다면 학습하는데 도움이 되지 않는가 보다. 답지보고 베끼는 느낌이랄까? 아무튼 마스킹을 하기 위해 lood-ahead mask라는 것을 해주는데, Multi-Head Attention을 통해 나온 행렬 값에 대해 마스킹을 하고자 하는 값에는 1, 마스킹을 하지 않는 값에는 0을 리턴하도록 한다. 그리고 나서 Add &amp; Norm 과정을 수행해준뒤 도출된 결과를 다음 결과로 보내준다. *디코더의 Multi-Head Attention과 Feed Forward디코더에서 Masked Multi-Head Attention이 수행되고 나면 그 다음부터는 인코더와 마찬가지로 Multi-Head Attention과 Feed Forward가 수행된다. 근데 이때 Multi-Head Attention에 입력으로 들어가는 값들을 잘 살펴 봐야 한다. 인코더에서 출력된 값과 디코더 첫번째 서브층에서 출력된 값이 인풋으로 들어가기 때문이다. 두번째 서브층인 Multi-Head Attention에서는 마찬가지로 셀프어텐션 함수를 수행하기 위해 Query, Key, Value 벡터가 입력되어야 한다. 이때 Query는 디코더 첫번째 서브층에서 출력된 값이 해당되고, Key 벡터와 Value 벡터는 마지막 인코더에서 출력된 값으로 입력된다. 그리고 똑같이 Multi-Head Attention을 수행해주게 된다. 이렇게 6개의 디코더마다 Multi-Head Attention의 Query 벡터는 디코더 첫번째 서브층의 output, Key벡터와 Value벡터는 인코더의 output이 입력으로 들어가게 된다. Reference attention 논문: https://arxiv.org/abs/1409.0473 https://wikidocs.net/31379 https:&#x2F;&#x2F;velog.io&#x2F;@seolini43&#x2F;파이썬Transformer로-오피스-챗봇-만들기-이론편","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"NLP","slug":"Paper/NLP","permalink":"https://jmj3047.github.io/categories/Paper/NLP/"}],"tags":[{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"},{"name":"Transformer","slug":"Transformer","permalink":"https://jmj3047.github.io/tags/Transformer/"},{"name":"Attention","slug":"Attention","permalink":"https://jmj3047.github.io/tags/Attention/"}]},{"title":"HTML with Python_CGI","slug":"HTML_with_Python_CGI","date":"2022-05-03T15:00:00.000Z","updated":"2022-05-04T08:38:19.000Z","comments":true,"path":"2022/05/04/HTML_with_Python_CGI/","link":"","permalink":"https://jmj3047.github.io/2022/05/04/HTML_with_Python_CGI/","excerpt":"","text":"** code: https://github.com/jmj3047/mj_chatbot_prac&#x2F;blob&#x2F;c5bec233b833b24345deeffe7391621415dc1dcb&#x2F;cgi_webpython.py Making Website with CGI Constructing Web Server: Download and Install Apche Official: Installing Apache in window(https://httpd.apache.org/docs/2.4/platform/windows.html) Beginner version: Install Bitnami Wamp Stack push 1 or 2 click this button below it takes some time to install it Constructing Web Server: Bitnami Wamp Stack Start Bitnami Wamp Stack Click Go to Application: If you can see the site like the picture below, success! Start or Stop the server: Click Manage Server If program below is shut down, go to the folder where ‘bitnami wamp stack’ installed and click ‘manager-windows.exe’ Using Python in Web(HTML): Setting Apache Install python and apache find folder where apach installed(‘D:\\wamp\\apache2\\conf’) &gt; ‘conf’ folder &gt; httpd.conf open httpd.conf file and search: 1LoadModule cgid_module modules/mod_cgid.so and if ‘#’ exists in front of code, delete it Find tag in httpd.conf and add some lines 1234567891011121314151617181920212223242526272829303132333435&lt;Directory &quot;/Applications/mampstack-8.0.5-0/apache2/htdocs&quot;&gt; # # Possible values for the Options directive are &quot;None&quot;, &quot;All&quot;, # or any combination of: # Indexes Includes FollowSymLinks SymLinksifOwnerMatch ExecCGI MultiViews # # Note that &quot;MultiViews&quot; must be named *explicitly* --- &quot;Options All&quot; # doesn&#x27;t give it to you. # # The Options directive is both complicated and important. Please see # http://httpd.apache.org/docs/2.4/mod/core.html#options # for more information. # Options Indexes FollowSymLinks # # AllowOverride controls what directives may be placed in .htaccess files. # It can be &quot;All&quot;, &quot;None&quot;, or any combination of the keywords: # AllowOverride FileInfo AuthConfig Limit # AllowOverride None # # Controls who can get stuff from this server. # Require all granted ********** add this code ********** &lt;Files *.py&gt; Options ExecCGI AddHandler cgi-script .py &lt;/Files&gt; *********************************** &lt;/Directory&gt; htdocs 디렉토리 내 확장자가 py인 모든 파일은 CGI기능을 활성시키고 CGI로 실행하라는 의미 Restart Apache Web Server in manager-osx Python file setting index.py 가 있는 htdocs 디렉토리에서 index.py 실행 후 아래같이 입력(다른 파이썬 파일을 만들어도 상관 없음) 123#!/usr/local/bin/python3 &gt;&gt;&gt; python.exe 경로 환경변수에 저장해줬다면 !Python만 해도 됨 print(&quot;Content-Type: text/html&quot;) print() index.html 의 코드 넣기( 다른 html 파일 이어도 됨): index.py가 실행되었을 때 index.html 의 코드가 출력되게 해주는 코드 12345678910111213141516171819202122#!/usr/local/bin/python3print(&quot;Content-Type: text/html&quot;)print()print(&#x27;&#x27;&#x27;&lt;!doctype html&gt; # ---&gt; 줄바꿈을 위해 docsting (&#x27;&#x27;&#x27; &#x27;&#x27;&#x27;) 사용&lt;html&gt;&lt;head&gt; &lt;title&gt;WEB1 - Welcome&lt;/title&gt; &lt;meta charset=&quot;utf-8&quot;&gt;&lt;/head&gt;&lt;body&gt; &lt;h1&gt;&lt;a href=&quot;index.html&quot;&gt;WEB&lt;/a&gt;&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;a href=&quot;qs-1.html&quot;&gt;HTML&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;qs-2.html&quot;&gt;CSS&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;qs-3.html&quot;&gt;JavaScript&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h2&gt;WEB&lt;/h2&gt; &lt;p&gt;The World Wide Web (abbreviated WWW or the Web) is an information space where documents and other web resources are identified by Uniform Resource Locators (URLs), interlinked by hypertext links, and can be accessed via the Internet.[1] English scientist Tim Berners-Lee invented the World Wide Web in 1989. He wrote the first web browser computer program in 1990 while employed at CERN in Switzerland.[2][3] The Web browser was released outside of CERN in 1991, first to other research institutions starting in January 1991 and to the general public on the Internet in August 1991. &lt;/p&gt;&lt;/body&gt;&lt;/html&gt;&#x27;&#x27;&#x27;) 웹 브라우저 주소창에 localhost:8080&#x2F;index.py 입력하고 접속 index.html 파일의 내용이 잘 출력된다면 구현 성공 Internal Server Error 가 확인된다면 에디터에서 apache2&#x2F;logs 디렉토리 내 error_log 파일에 있는 에러 코드 확인 및 구글링 EXAMPLE123456789101112131415161718192021222324252627282930#!C:\\Python310\\python.exe ---&gt;파이썬 경로# 한글이 꺠지지 않으려면 꼭 넣어야 함# -*- coding:utf-8 -*-import sysimport codecssys.stdout =codecs.getwriter(&quot;utf-8&quot;)(sys.stdout.detach())import cgi# cgitb는 CGI 프로그래밍시 디버깅을 위한 모듈로 cgitb.enable()할 경우 런타임 에러를 웹브라우저로 전송함# cgitb.enable() 하지 않은 상태로 실행 중 오류가 발생한 경우 웹서버는 클라이언트에게 HTTP응답 코드 500을 전송함import cgitbcgitb.enable()# HTTP 규격에서 헤더 전송 이후에는 반드시 줄 바꿈을 하게되어 있음으로 마지막에 \\r\\n을 전송# 마지막에 \\r\\n을 전송하지 않으면 브라우저 측에서 오류가 발생print(&quot;Content-type: text/html;charset=utf-8\\r\\n&quot;)print(&quot;&quot;&quot; &lt;!doctype html&gt; &lt;html&gt; &lt;head&gt; &lt;meta charset=&#x27;utf-8&#x27;&gt; &lt;h1&gt;안녕?&lt;/h1&gt; &lt;h2&gt;Thank you so much&lt;/h2&gt; &lt;h3&gt;This page is made by Python&lt;/h3&gt; &quot;&quot;&quot;)a = 3+4+5b = a/3print(&#x27;b는 :&#x27;, b)print(&quot;&lt;/head&gt;&quot;)print(&quot;&lt;/html&gt;&quot;) Result Reference https://daekiry.tistory.com/4?category=928946 https://daekiry.tistory.com/5?category=928946 https://daekiry.tistory.com/6 https:&#x2F;&#x2F;velog.io&#x2F;@ssoulll&#x2F;python-웹-페이지를-CGI로-구현","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"HTML","slug":"Python/HTML","permalink":"https://jmj3047.github.io/categories/Python/HTML/"}],"tags":[{"name":"Web Server","slug":"Web-Server","permalink":"https://jmj3047.github.io/tags/Web-Server/"},{"name":"CGI","slug":"CGI","permalink":"https://jmj3047.github.io/tags/CGI/"}]},{"title":"HTML with Python_Flask & Brython","slug":"HTML_with_Python_Flask&Brython","date":"2022-05-03T15:00:00.000Z","updated":"2022-05-04T08:42:26.000Z","comments":true,"path":"2022/05/04/HTML_with_Python_Flask&Brython/","link":"","permalink":"https://jmj3047.github.io/2022/05/04/HTML_with_Python_Flask&Brython/","excerpt":"","text":"Flask 파이썬 기반 마이크로 웹 개발 프레임워크 웹 개발의 핵심기능만 간경하게 유지 필요한 기능은 다른 라이브러리나 프레임워크로 손쉽게 확장 신속하게 최소한의 노력으로 웹 애플리케이션 개발 가능 Installation start virtualenv pip install flask Error → note: could not find a version that satisfies the requirement flask → 네트워크 문제로 외부 라이브러리 저장소에 접근하지 못할 경우 나오는 문제, → 직접 https://github.com/mitsuhiko/flask 위치로 가서 소스 받아 설치 해야 함. Strat Flask**code: https://github.com/jmj3047/mj_chatbot_prac&#x2F;blob&#x2F;c5bec233b833b24345deeffe7391621415dc1dcb&#x2F;flask_prac&#x2F;0.flask_hello.py 12345678910from flask import Flaskapp = Flask(__name__)@app.route(&#x27;/&#x27;)def hello_world(): return &#x27;Hello World!&#x27; if __name__ == &#x27;__main__&#x27;: app.debug =True app.run() 소스를 실행하고, terminal 에서 ‘python flask_test.py’ 입력 후 http://127.0.0.1:5000 으로 접근 Process for starting Flask Application 특정 URL 호출(request) : http://127.0.0.1:5000/ 또는 http://localhost:5000 특정 URL 매핑 검색 : @app.route(‘&#x2F;‘) 특정 URL에 매칭된 함수(def 함수) 실행 : def hello_world() 비즈니스 로직 실행 : result 결과 응답으로 전송(response): return result HTML 로 화면에 출력 쿠키(Cookie), 세션(Session), 로깅(logging) 등 제공 Routing**code: https://github.com/jmj3047/mj_chatbot_prac&#x2F;blob&#x2F;c5bec233b833b24345deeffe7391621415dc1dcb&#x2F;flask_prac&#x2F;1.flask_login.py URL을 통해 처리할 핸들러를 찾는 것 플라스크는 복잡한 URI를 함수로 연결하는 방법을 제공 URI 를 연결하는 route() 데코레이터 함수 제공 &#x2F; 접속 시 root_world() 가 호출 됨 &#x2F;hello 접속 시 hello_world() 가 호출 됨 12345678910111213141516from flask import Flask, redirect, url_forapp = Flask(__name__)@app.route(&#x27;/&#x27;) #127.0.0.1:5000에 가면 함수 실행def root_world(): result = &#x27;root world&#x27; return result@app.route(&#x27;/hello&#x27;) #127.0.0.1:5000/hello 를 가면 실행def hello_world(): result = &#x27;hello world&#x27; return resultif __name__ == &#x27;__main__&#x27;: app.debug =True app.run() app.debug는 개발의 편의를 위해 존재 True값을 경우 코드를 변경하면 자동으로 서버가 재 실행 됨 또한, 웹상에서 파이썬 코드를 수행할 수 있게 되므로, 운영환경에서 사용을 유의해야 함. 현재 접근은 개발 소스가 존재하는 로컬에서만 접근 가능 외부에서도 접근을 가능하게 하려면 app.run(host&#x3D;’0.0.0.0’)로 서버 실행 부를 변경해야 함 1234567891011121314151617181920212223242526272829303132from flask import Flask, redirect, url_forapp = Flask(__name__)@app.route(&#x27;/users/&lt;user_id&gt;&#x27;) #동적 변수를 사용하여 URI 접속# &lt;동적변수&gt;를 뷰함수의 인자로 사용# &lt;동적 변수&gt; 다음에 /를 넣으면 안됨def user_id(userid): result = &#x27;user_id = &#x27; + userid return result@app.route(&#x27;/admin&#x27;)def hello_admin(): return &#x27;Hello Admin&#x27;@app.route(&#x27;/guest/&lt;guest&gt;&#x27;)def hello_guest(guest): return &#x27;Hello %s as Guest&#x27; % guest@app.route(&#x27;/user/&lt;name&gt;&#x27;)def hello_user(name): if name == &#x27;admin&#x27;: return redirect(url_for(&#x27;hello_admin&#x27;)) else: return redirect(url_for(&#x27;hello_guest&#x27;, guest=name))# url_for(): 함수를 호출하는 URI를 반환# redirect(): 다른 route 경로 이동(다른 페이지 이동)if __name__ == &#x27;__main__&#x27;: app.debug = True app.run() Flask GET 방식으로 값 전송 및 처리**code: https://github.com/jmj3047/mj_chatbot_prac&#x2F;blob&#x2F;c5bec233b833b24345deeffe7391621415dc1dcb&#x2F;flask_prac&#x2F;2.flask_app.py https://github.com/jmj3047/mj_chatbot_prac&#x2F;blob&#x2F;c5bec233b833b24345deeffe7391621415dc1dcb&#x2F;flask_prac&#x2F;templates&#x2F;login&#x2F;login_form_get.html mkdir templates 폴더 생성 login_form_get.html 파일 작성 get 방식 지정 : method&#x3D;”get” 1234567891011121314151617181920from flask import Flask, request, session, render_templateapp = Flask(__name__)@app.route(&#x27;/login_form_get&#x27;) def login_form_get(): return render_template(&#x27;login/login_form_get.html&#x27;)@app.route(&#x27;/login_get_proc&#x27;, methods=[&#x27;GET&#x27;]) def login_get_proc(): user_id = request.args.get(&#x27;user_id&#x27;) user_pwd = request.args.get(&#x27;user_pwd&#x27;) if len(user_id) == 0 or len(user_pwd) == 0: return &#x27;no &#123;&#125; or &#123;&#125;&#x27;.format(user_id, user_pwd) return &#x27;welcome &#123;&#125;&#x27;.format(user_id)if __name__ == &#x27;__main__&#x27;: app.debug = True app.run() 12345678910111213141516&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;meta charset = &quot;UTF-8&quot;&gt; &lt;title&gt;login_form_get.html&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt; &lt;form action=&quot;/login_get_proc&quot; method=&quot;get&quot;&gt; ID: &lt;input type=&quot;text&quot;, name=&quot;user_id&quot;&gt;&lt;br&gt; PW: &lt;input type=&quot;password&quot;, name=&quot;user_pwd&quot;&gt;&lt;br&gt; &lt;input type=&quot;submit&quot;, value=&quot;Click&quot;&gt; &lt;/form&gt; &lt;/h1&gt; &lt;/body&gt;&lt;/html&gt; Brython**code:https://github.com/jmj3047/mj_chatbot_prac&#x2F;blob&#x2F;c5bec233b833b24345deeffe7391621415dc1dcb&#x2F;flask_prac&#x2F;templates&#x2F;brython_test.html python을 HTML 코드에 삽입해서 사용 12345678910111213141516171819202122&lt;html&gt; &lt;head&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;/path/to/brython.js&quot;&gt;&lt;/script&gt; &lt;/head&gt; &lt;body onload=&quot;brython()&quot;&gt; &lt;script type=&quot;text/python&quot;&gt; from browser import document, alert def echo(event): alert(document[&quot;zone&quot;].value) document[&quot;mybutton&quot;].bind(&quot;click&quot;, echo) &lt;/script&gt; &lt;input id=&quot;zone&quot;&gt;&lt;button id=&quot;mybutton&quot;&gt;click !&lt;/button&gt; &lt;/body&gt;&lt;/html&gt; Reference: https://essim92.tistory.com/8 https://code-examples.net/ko/q/dc0356 https://github.com/brython-dev/brython #Test Brython online(DEMO)","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"HTML","slug":"Python/HTML","permalink":"https://jmj3047.github.io/categories/Python/HTML/"}],"tags":[{"name":"Web Server","slug":"Web-Server","permalink":"https://jmj3047.github.io/tags/Web-Server/"},{"name":"Flask","slug":"Flask","permalink":"https://jmj3047.github.io/tags/Flask/"},{"name":"Brython","slug":"Brython","permalink":"https://jmj3047.github.io/tags/Brython/"}]},{"title":"Making Office Chatbot with Transformer","slug":"Making_Office_Chatbot_with_Transformer","date":"2022-05-03T15:00:00.000Z","updated":"2022-10-15T08:08:18.515Z","comments":true,"path":"2022/05/04/Making_Office_Chatbot_with_Transformer/","link":"","permalink":"https://jmj3047.github.io/2022/05/04/Making_Office_Chatbot_with_Transformer/","excerpt":"","text":"일상 대화와 오피스 대화 데이터를 Transformer 모델로 학습시켜, 질문에 대한 적절한 답변을 하는 챗봇 Data: 한국어대화데이터셋(오피스데이터) 사용 (AIHUB에 ‘개방데이터-인식기술 언어지능-한국어대화데이터셋’에서 로그인 후 다운로드) GPU 사용 그 외 환경 123batch size = 64buffer size = 20000epochs = 50 1. Environments1!pip install tensorflow_datasets 12345678import pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport reimport urllib.requestimport timeimport tensorflow_datasets as tfdsimport tensorflow as tf 12import osos.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;3&quot; 1234567with tf.device(&#x27;/device:GPU:3&#x27;): # 텐서 생성 a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]) b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]) c = tf.matmul(a, b) print(c) tf.Tensor( [[22. 28.] [49. 64.]], shape=(2, 2), dtype=float32) 2. 데이터 전처리12data = pd.read_csv(&#x27;./ChatbotData.csv&#x27;)data = data[0:5290] 1data[:10] 123456789f = open(r&#x27;./conversation_office.txt&#x27;,&quot;r&quot;)lines = f.readlines()Q = []A = []for i in range(len(lines)) : if i%2 == 0 : Q.append(lines[i][2:-1]) A.append(lines[i+1][2:-1]) 123456import pandas as pddf = pd.DataFrame()df[&#x27;Q&#x27;] = Qdf[&#x27;A&#x27;] = Adf[&#x27;label&#x27;] = 1 1df[:10] 1234#두 데이터를 concat() 함수를 이용하여 합쳐 하나의 데이터프레임으로 나타내주도록 함train_data = pd.concat([data, df],ignore_index=True)train_data = train_data.sample(frac=1).reset_index(drop=True) #데이터를 랜덤으로 섞어주는 코드 3. 단어 집합 생성123456789101112131415# 문장 그대로 학습 모델에 넣으면 모델이 인식을 할 수 없기 때문에 단어 집합을 만들어 줘야 함.# 정수 인코딩과 패딩을 해주는 작업을 해주어야 함#특수기호 띄어쓰기questions = []for sentence in train_data[&#x27;Q&#x27;]: sentence = re.sub(r&quot;([?.!,])&quot;, r&quot; \\1 &quot;, sentence) sentence = sentence.strip() questions.append(sentence) answers = []for sentence in train_data[&#x27;A&#x27;]: sentence = re.sub(r&quot;([?.!,])&quot;, r&quot; \\1 &quot;, sentence) sentence = sentence.strip() answers.append(sentence) 123456789# 서브워드텍스트인코더를 사용하여 질문, 답변 데이터로부터 단어 집합(Vocabulary) 생성tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus( questions + answers, target_vocab_size=2**13) # 시작 토큰과 종료 토큰에 대한 정수 부여START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]# 시작 토큰과 종료 토큰을 고려하여 단어 집합의 크기를 + 2VOCAB_SIZE = tokenizer.vocab_size + 2 123456789101112131415161718192021222324252627#정수 인코딩과 패딩# 서브워드텍스트인코더 토크나이저의 .encode()를 사용하여 텍스트 시퀀스를 정수 시퀀스로 변환.print(&#x27;임의의 질문 샘플을 정수 인코딩 : &#123;&#125;&#x27;.format(tokenizer.encode(questions[20])))#출력 : 임의의 질문 샘플을 정수 인코딩 : [8656, 331]# 최대 길이를 40으로 정의MAX_LENGTH = 40# 토큰화 / 정수 인코딩 / 시작 토큰과 종료 토큰 추가 / 패딩def tokenize_and_filter(inputs, outputs): tokenized_inputs, tokenized_outputs = [], [] for (sentence1, sentence2) in zip(inputs, outputs): # encode(토큰화 + 정수 인코딩), 시작 토큰과 종료 토큰 추가 sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN tokenized_inputs.append(sentence1) tokenized_outputs.append(sentence2) # 패딩 tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences( tokenized_inputs, maxlen=MAX_LENGTH, padding=&#x27;post&#x27;) tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences( tokenized_outputs, maxlen=MAX_LENGTH, padding=&#x27;post&#x27;) return tokenized_inputs, tokenized_outputs 임의의 질문 샘플을 정수 인코딩 : [2704, 1081, 13, 542] 1questions, answers = tokenize_and_filter(questions, answers) 1234#sample# 정수 인코딩과 패딩이 된 결과가 출력print(questions[0])print(answers[0]) [10023 31 121 4282 10024 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [10023 3607 213 13 21 1 10024 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] 12from tensorflow.python.client import device_libdevice_lib.list_local_devices() 1234567891011121314151617181920212223#인코더와 디코더의 입력 데이터가 되도록 배치 크기로 데이터를 묶어줌 with tf.device(&#x27;/device:GPU:3&#x27;): BATCH_SIZE = 64 BUFFER_SIZE = 20000 dataset = tf.data.Dataset.from_tensor_slices(( &#123; &#x27;inputs&#x27;: questions, &#x27;dec_inputs&#x27;: answers[:, :-1] # 디코더의 입력 / 마지막 패딩 토큰 제거 &#125;, &#123; &#x27;outputs&#x27;: answers[:, 1:] # 맨 처음 토큰이 제거 = 시작 토큰 제거 &#125;, )) dataset = dataset.cache() dataset = dataset.shuffle(BUFFER_SIZE) dataset = dataset.batch(BATCH_SIZE) dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE) 4. 트랜스포머 모델 만들기123456789101112131415161718192021222324252627282930313233343536373839def transformer(vocab_size, num_layers, dff, d_model, num_heads, dropout, name=&quot;transformer&quot;): # 인코더의 입력 inputs = tf.keras.Input(shape=(None,), name=&quot;inputs&quot;) # 디코더의 입력 dec_inputs = tf.keras.Input(shape=(None,), name=&quot;dec_inputs&quot;) # 인코더의 패딩 마스크 enc_padding_mask = tf.keras.layers.Lambda( create_padding_mask, output_shape=(1, 1, None), name=&#x27;enc_padding_mask&#x27;)(inputs) # 디코더의 룩어헤드 마스크(첫번째 서브층) look_ahead_mask = tf.keras.layers.Lambda( create_look_ahead_mask, output_shape=(1, None, None), name=&#x27;look_ahead_mask&#x27;)(dec_inputs) # 디코더의 패딩 마스크(두번째 서브층) dec_padding_mask = tf.keras.layers.Lambda( create_padding_mask, output_shape=(1, 1, None), name=&#x27;dec_padding_mask&#x27;)(inputs) # 인코더의 출력은 enc_outputs. 디코더로 전달된다. enc_outputs = encoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff, d_model=d_model, num_heads=num_heads, dropout=dropout, )(inputs=[inputs, enc_padding_mask]) # 인코더의 입력은 입력 문장과 패딩 마스크 # 디코더의 출력은 dec_outputs. 출력층으로 전달된다. dec_outputs = decoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff, d_model=d_model, num_heads=num_heads, dropout=dropout, )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask]) # 다음 단어 예측을 위한 출력층 outputs = tf.keras.layers.Dense(units=vocab_size, name=&quot;outputs&quot;)(dec_outputs) return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name) 1234567891011121314151617181920212223242526272829303132class PositionalEncoding(tf.keras.layers.Layer): def __init__(self, position, d_model): super(PositionalEncoding, self).__init__() self.pos_encoding = self.positional_encoding(position, d_model) def get_angles(self, position, i, d_model): angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32)) return position * angles def positional_encoding(self, position, d_model): angle_rads = self.get_angles( position=tf.range(position, dtype=tf.float32)[:, tf.newaxis], i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :], d_model=d_model) # 배열의 짝수 인덱스(2i)에는 사인 함수 적용 sines = tf.math.sin(angle_rads[:, 0::2]) # 배열의 홀수 인덱스(2i+1)에는 코사인 함수 적용 cosines = tf.math.cos(angle_rads[:, 1::2]) angle_rads = np.zeros(angle_rads.shape) angle_rads[:, 0::2] = sines angle_rads[:, 1::2] = cosines pos_encoding = tf.constant(angle_rads) pos_encoding = pos_encoding[tf.newaxis, ...] print(pos_encoding.shape) return tf.cast(pos_encoding, tf.float32) def call(self, inputs): return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :] 1234567891011121314151617181920212223242526272829303132333435def create_padding_mask(x): mask = tf.cast(tf.math.equal(x, 0), tf.float32) # (batch_size, 1, 1, key의 문장 길이) return mask[:, tf.newaxis, tf.newaxis, :]# 디코더의 첫번째 서브층(sublayer)에서 미래 토큰을 Mask하는 함수def create_look_ahead_mask(x): seq_len = tf.shape(x)[1] look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0) padding_mask = create_padding_mask(x) # 패딩 마스크도 포함 return tf.maximum(look_ahead_mask, padding_mask)#encoderdef encoder(vocab_size, num_layers, dff, d_model, num_heads, dropout, name=&quot;encoder&quot;): inputs = tf.keras.Input(shape=(None,), name=&quot;inputs&quot;) # 인코더는 패딩 마스크 사용 padding_mask = tf.keras.Input(shape=(1, 1, None), name=&quot;padding_mask&quot;) # 포지셔널 인코딩 + 드롭아웃 embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs) embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32)) embeddings = PositionalEncoding(vocab_size, d_model)(embeddings) outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings) # 인코더를 num_layers개 쌓기 for i in range(num_layers): outputs = encoder_layer(dff=dff, d_model=d_model, num_heads=num_heads, dropout=dropout, name=&quot;encoder_layer_&#123;&#125;&quot;.format(i), )([outputs, padding_mask]) return tf.keras.Model( inputs=[inputs, padding_mask], outputs=outputs, name=name) 1234567891011121314151617181920212223242526272829def encoder_layer(dff, d_model, num_heads, dropout, name=&quot;encoder_layer&quot;): inputs = tf.keras.Input(shape=(None, d_model), name=&quot;inputs&quot;) # 인코더는 패딩 마스크 사용 padding_mask = tf.keras.Input(shape=(1, 1, None), name=&quot;padding_mask&quot;) # 멀티-헤드 어텐션 (첫번째 서브층 / 셀프 어텐션) attention = MultiHeadAttention( d_model, num_heads, name=&quot;attention&quot;)(&#123; &#x27;query&#x27;: inputs, &#x27;key&#x27;: inputs, &#x27;value&#x27;: inputs, # Q = K = V &#x27;mask&#x27;: padding_mask # 패딩 마스크 사용 &#125;) # 드롭아웃 + 잔차 연결과 층 정규화 attention = tf.keras.layers.Dropout(rate=dropout)(attention) attention = tf.keras.layers.LayerNormalization( epsilon=1e-6)(inputs + attention) # 포지션 와이즈 피드 포워드 신경망 (두번째 서브층) outputs = tf.keras.layers.Dense(units=dff, activation=&#x27;relu&#x27;)(attention) outputs = tf.keras.layers.Dense(units=d_model)(outputs) # 드롭아웃 + 잔차 연결과 층 정규화 outputs = tf.keras.layers.Dropout(rate=dropout)(outputs) outputs = tf.keras.layers.LayerNormalization( epsilon=1e-6)(attention + outputs) return tf.keras.Model( inputs=[inputs, padding_mask], outputs=outputs, name=name) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960class MultiHeadAttention(tf.keras.layers.Layer): def __init__(self, d_model, num_heads, name=&quot;multi_head_attention&quot;): super(MultiHeadAttention, self).__init__(name=name) self.num_heads = num_heads self.d_model = d_model assert d_model % self.num_heads == 0 # d_model을 num_heads로 나눈 값. # 논문 기준 : 64 self.depth = d_model // self.num_heads # WQ, WK, WV에 해당하는 밀집층 정의 self.query_dense = tf.keras.layers.Dense(units=d_model) self.key_dense = tf.keras.layers.Dense(units=d_model) self.value_dense = tf.keras.layers.Dense(units=d_model) # WO에 해당하는 밀집층 정의 self.dense = tf.keras.layers.Dense(units=d_model) # num_heads 개수만큼 q, k, v를 split하는 함수 def split_heads(self, inputs, batch_size): inputs = tf.reshape( inputs, shape=(batch_size, -1, self.num_heads, self.depth)) return tf.transpose(inputs, perm=[0, 2, 1, 3]) def call(self, inputs): query, key, value, mask = inputs[&#x27;query&#x27;], inputs[&#x27;key&#x27;], inputs[ &#x27;value&#x27;], inputs[&#x27;mask&#x27;] batch_size = tf.shape(query)[0] query = self.query_dense(query) key = self.key_dense(key) value = self.value_dense(value) # 2. 헤드 나누기 # q : (batch_size, num_heads, query의 문장 길이, d_model/num_heads) # k : (batch_size, num_heads, key의 문장 길이, d_model/num_heads) # v : (batch_size, num_heads, value의 문장 길이, d_model/num_heads) query = self.split_heads(query, batch_size) key = self.split_heads(key, batch_size) value = self.split_heads(value, batch_size) # 3. 스케일드 닷 프로덕트 어텐션. 앞서 구현한 함수 사용. # (batch_size, num_heads, query의 문장 길이, d_model/num_heads) scaled_attention, _ = scaled_dot_product_attention(query, key, value, mask) # (batch_size, query의 문장 길이, num_heads, d_model/num_heads) scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3]) # 4. 헤드 연결(concatenate)하기 # (batch_size, query의 문장 길이, d_model) concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model)) # 5. WO에 해당하는 밀집층 지나기 # (batch_size, query의 문장 길이, d_model) outputs = self.dense(concat_attention) return outputs 123456789101112131415161718192021222324252627def scaled_dot_product_attention(query, key, value, mask): # query 크기 : (batch_size, num_heads, query의 문장 길이, d_model/num_heads) # key 크기 : (batch_size, num_heads, key의 문장 길이, d_model/num_heads) # value 크기 : (batch_size, num_heads, value의 문장 길이, d_model/num_heads) # padding_mask : (batch_size, 1, 1, key의 문장 길이) # Q와 K의 곱. 어텐션 스코어 행렬. matmul_qk = tf.matmul(query, key, transpose_b=True) # 스케일링 # dk의 루트값으로 나눠준다. depth = tf.cast(tf.shape(key)[-1], tf.float32) logits = matmul_qk / tf.math.sqrt(depth) # 마스킹. 어텐션 스코어 행렬의 마스킹 할 위치에 매우 작은 음수값을 넣는다. # 매우 작은 값이므로 소프트맥스 함수를 지나면 행렬의 해당 위치의 값은 0이 된다. if mask is not None: logits += (mask * -1e9) # 소프트맥스 함수는 마지막 차원인 key의 문장 길이 방향으로 수행된다. # attention weight : (batch_size, num_heads, query의 문장 길이, key의 문장 길이) attention_weights = tf.nn.softmax(logits, axis=-1) # output : (batch_size, num_heads, query의 문장 길이, d_model/num_heads) output = tf.matmul(attention_weights, value) return output, attention_weights 123456789101112131415161718192021222324252627def decoder(vocab_size, num_layers, dff, d_model, num_heads, dropout, name=&#x27;decoder&#x27;): inputs = tf.keras.Input(shape=(None,), name=&#x27;inputs&#x27;) enc_outputs = tf.keras.Input(shape=(None, d_model), name=&#x27;encoder_outputs&#x27;) # 디코더는 룩어헤드 마스크(첫번째 서브층)와 패딩 마스크(두번째 서브층) 둘 다 사용. look_ahead_mask = tf.keras.Input( shape=(1, None, None), name=&#x27;look_ahead_mask&#x27;) padding_mask = tf.keras.Input(shape=(1, 1, None), name=&#x27;padding_mask&#x27;) # 포지셔널 인코딩 + 드롭아웃 embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs) embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32)) embeddings = PositionalEncoding(vocab_size, d_model)(embeddings) outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings) # 디코더를 num_layers개 쌓기 for i in range(num_layers): outputs = decoder_layer(dff=dff, d_model=d_model, num_heads=num_heads, dropout=dropout, name=&#x27;decoder_layer_&#123;&#125;&#x27;.format(i), )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask]) return tf.keras.Model( inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask], outputs=outputs, name=name) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def decoder_layer(dff, d_model, num_heads, dropout, name=&quot;decoder_layer&quot;): inputs = tf.keras.Input(shape=(None, d_model), name=&quot;inputs&quot;) enc_outputs = tf.keras.Input(shape=(None, d_model), name=&quot;encoder_outputs&quot;) # 룩어헤드 마스크(첫번째 서브층) look_ahead_mask = tf.keras.Input( shape=(1, None, None), name=&quot;look_ahead_mask&quot;) # 패딩 마스크(두번째 서브층) padding_mask = tf.keras.Input(shape=(1, 1, None), name=&#x27;padding_mask&#x27;) # 멀티-헤드 어텐션 (첫번째 서브층 / 마스크드 셀프 어텐션) attention1 = MultiHeadAttention( d_model, num_heads, name=&quot;attention_1&quot;)(inputs=&#123; &#x27;query&#x27;: inputs, &#x27;key&#x27;: inputs, &#x27;value&#x27;: inputs, # Q = K = V &#x27;mask&#x27;: look_ahead_mask # 룩어헤드 마스크 &#125;) # 잔차 연결과 층 정규화 attention1 = tf.keras.layers.LayerNormalization( epsilon=1e-6)(attention1 + inputs) # 멀티-헤드 어텐션 (두번째 서브층 / 디코더-인코더 어텐션) attention2 = MultiHeadAttention( d_model, num_heads, name=&quot;attention_2&quot;)(inputs=&#123; &#x27;query&#x27;: attention1, &#x27;key&#x27;: enc_outputs, &#x27;value&#x27;: enc_outputs, # Q != K = V &#x27;mask&#x27;: padding_mask # 패딩 마스크 &#125;) # 드롭아웃 + 잔차 연결과 층 정규화 attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2) attention2 = tf.keras.layers.LayerNormalization( epsilon=1e-6)(attention2 + attention1) # 포지션 와이즈 피드 포워드 신경망 (세번째 서브층) outputs = tf.keras.layers.Dense(units=dff, activation=&#x27;relu&#x27;)(attention2) outputs = tf.keras.layers.Dense(units=d_model)(outputs) # 드롭아웃 + 잔차 연결과 층 정규화 outputs = tf.keras.layers.Dropout(rate=dropout)(outputs) outputs = tf.keras.layers.LayerNormalization( epsilon=1e-6)(outputs + attention2) return tf.keras.Model( inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask], outputs=outputs, name=name) 1234567891011121314151617tf.keras.backend.clear_session()# Hyper-parametersD_MODEL = 256NUM_LAYERS = 2NUM_HEADS = 8DFF = 512DROPOUT = 0.1model = transformer( vocab_size=VOCAB_SIZE, num_layers=NUM_LAYERS, dff=DFF, d_model=D_MODEL, num_heads=NUM_HEADS, dropout=DROPOUT) (1, 10025, 256) (1, 10025, 256) 123456789101112131415161718192021222324class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule): def __init__(self, d_model, warmup_steps=4000): super(CustomSchedule, self).__init__() self.d_model = d_model self.d_model = tf.cast(self.d_model, tf.float32) self.warmup_steps = warmup_steps def __call__(self, step): arg1 = tf.math.rsqrt(step) arg2 = step * (self.warmup_steps**-1.5) return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)def loss_function(y_true, y_pred): y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1)) loss = tf.keras.losses.SparseCategoricalCrossentropy( from_logits=True, reduction=&#x27;none&#x27;)(y_true, y_pred) mask = tf.cast(tf.not_equal(y_true, 0), tf.float32) loss = tf.multiply(loss, mask) return tf.reduce_mean(loss) 1234567891011learning_rate = CustomSchedule(D_MODEL)optimizer = tf.keras.optimizers.Adam( learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)def accuracy(y_true, y_pred): # 레이블의 크기는 (batch_size, MAX_LENGTH - 1) y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1)) return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy]) 123## 모델 학습EPOCHS = 50model.fit(dataset, epochs=EPOCHS) Epoch 1/50 104/104 [==============================] - 12s 54ms/step - loss: 1.2164 - accuracy: 0.0149 Epoch 2/50 104/104 [==============================] - 6s 54ms/step - loss: 1.0725 - accuracy: 0.0285 Epoch 3/50 104/104 [==============================] - 6s 54ms/step - loss: 0.9082 - accuracy: 0.0472 Epoch 4/50 104/104 [==============================] - 6s 54ms/step - loss: 0.7714 - accuracy: 0.0482 ... 104/104 [==============================] - 6s 54ms/step - loss: 0.0160 - accuracy: 0.1321 Epoch 46/50 104/104 [==============================] - 6s 56ms/step - loss: 0.0158 - accuracy: 0.1320 Epoch 47/50 104/104 [==============================] - 6s 55ms/step - loss: 0.0152 - accuracy: 0.1320 Epoch 48/50 104/104 [==============================] - 6s 54ms/step - loss: 0.0151 - accuracy: 0.1322 Epoch 49/50 104/104 [==============================] - 6s 55ms/step - loss: 0.0149 - accuracy: 0.1321 Epoch 50/50 104/104 [==============================] - 6s 53ms/step - loss: 0.0148 - accuracy: 0.1321 &lt;keras.callbacks.History at 0x7f794c0f0880&gt; 5. 챗봇 평가하기 학습시킨 챗봇에 새로운 문장을 넣어서 평가 12345678910111213141516171819202122232425262728293031323334# 새로운 문장도 인코더 입력 형식으로 변형하는 코드def preprocess_sentence(sentence): # 단어와 구두점 사이에 공백 추가. # ex) 12시 땡! -&gt; 12시 땡 ! sentence = re.sub(r&quot;([?.!,])&quot;, r&quot; \\1 &quot;, sentence) sentence = sentence.strip() return sentencedef evaluate(sentence): sentence = preprocess_sentence(sentence) sentence = tf.expand_dims( START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0) output = tf.expand_dims(START_TOKEN, 0) # 디코더의 예측 시작 for i in range(MAX_LENGTH): predictions = model(inputs=[sentence, output], training=False) # 현재(마지막) 시점의 예측 단어를 받아온다. predictions = predictions[:, -1:, :] predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32) # 만약 마지막 시점의 예측 단어가 종료 토큰이라면 예측을 중단 if tf.equal(predicted_id, END_TOKEN[0]): break # 마지막 시점의 예측 단어를 출력에 연결한다. # 이는 for문을 통해서 디코더의 입력으로 사용될 예정이다. output = tf.concat([output, predicted_id], axis=-1) return tf.squeeze(output, axis=0) 123456789101112def predict(sentence): prediction = evaluate(sentence) predicted_sentence = tokenizer.decode( [i for i in prediction if i &lt; tokenizer.vocab_size]) print(&#x27;Master: &#123;&#125;&#x27;.format(sentence)) # print(&#x27;Output: &#123;&#125;&#x27;.format(predicted_sentence)) print(&#x27;Chatbot: &#123;&#125;&#x27;.format(predicted_sentence)) return predicted_sentence 12#predict() 함수에 문장을 입력하면 해당 문장에 대한 결과가 출력됨output = predict(&quot;굿모닝&quot;) Input: 굿모닝 Output: 좋은 아침이에요 . 1output = predict(&quot;오늘 날씨&quot;) Input: 오늘 날씨 Output: 충분히 아름다워요 . 1output = predict(&quot;오늘 날씨 어때?&quot;) Input: 오늘 날씨 어때? Output: 오전엔 화창하지만 오후에는 비가 올 것입니다 . 1output = predict(&quot;집중력&quot;) Input: 집중력 Output: 병원 가보세요 . 1output = predict(&quot;퇴근&quot;) Input: 퇴근 Output: 인생은 채워나가는거죠 . 1output = predict(&quot;야근 싫어&quot;) Input: 야근 싫어 Output: 얼른 집에 가서 쉬시길 바랄게요 . 123output = str(input(&quot;오피스 챗봇입니다. 무엇을 도와드릴까요?:&quot;))output = predict(output) Master: 일하기 싫어 Chatbot: 저도요 ! ! Reference https:&#x2F;&#x2F;velog.io&#x2F;@seolini43&#x2F;파이썬Transformer로-오피스-챗봇-만들기-코드 **code: https://github.com/jmj3047/mj_chatbot_prac&#x2F;blob&#x2F;c5bec233b833b24345deeffe7391621415dc1dcb&#x2F;chatbot_backend.py","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Python","slug":"Python/Python","permalink":"https://jmj3047.github.io/categories/Python/Python/"}],"tags":[{"name":"Transformer","slug":"Transformer","permalink":"https://jmj3047.github.io/tags/Transformer/"},{"name":"Chatbot","slug":"Chatbot","permalink":"https://jmj3047.github.io/tags/Chatbot/"},{"name":"NLP","slug":"NLP","permalink":"https://jmj3047.github.io/tags/NLP/"}]},{"title":"MongoDB Update Operator","slug":"MongoDB_update","date":"2022-04-27T15:00:00.000Z","updated":"2023-02-28T07:40:20.366Z","comments":true,"path":"2022/04/28/MongoDB_update/","link":"","permalink":"https://jmj3047.github.io/2022/04/28/MongoDB_update/","excerpt":"","text":"$set: 필드값을 설정하고 필드가 존재하지 않으면 새 필드가 생성됨. 스키마를 갱신하거나 사용자 정의 키를 추가 할때 편리함. $unset: 키와 값을 모두 제거함 1234567891011121314&gt; db.users.insertOne(&#123;&quot;name&quot;:&quot;joe&quot;&#125;)&gt; db.users.updateOne(&#123;&quot;_id&quot; : ObjectId(&quot;6269e32b7b15b7097fbad433&quot;)&#125;,&#123;&quot;$set&quot;:&#123;&quot;favorite book&quot; : &quot;War and Peace&quot;&#125;&#125;)&#123; &quot;acknowledged&quot; : true, &quot;matchedCount&quot; : 1, &quot;modifiedCount&quot; : 1 &#125;&gt; db.users.findOne()&#123; &quot;_id&quot; : ObjectId(&quot;6269e32b7b15b7097fbad433&quot;), &quot;name&quot; : &quot;joe&quot;, &quot;favorite book&quot; : &quot;War and Peace&quot;&#125;&gt; db.users.updateOne(&#123;&quot;name&quot; : &quot;joe&quot;&#125;,&#123;&quot;$unset&quot;:&#123;&quot;favorite book&quot;:1&#125;&#125;)&#123; &quot;acknowledged&quot; : true, &quot;matchedCount&quot; : 1, &quot;modifiedCount&quot; : 1 &#125;&gt; db.users.find()&#123; &quot;_id&quot; : ObjectId(&quot;6269e32b7b15b7097fbad433&quot;), &quot;name&quot; : &quot;joe&quot; &#125; $inc: $set과 비슷하지만, 숫자를 증감하기 위해 사용. int, long, double, decimal 타입 값에만 사용 가능 12345678910111213141516171819202122&gt;db.games.insertOne(&#123;&quot;game&quot; :&quot;pinball&quot;,&quot;user&quot; :&quot;joe&quot;&#125;)&#123; &quot;acknowledged&quot; : true, &quot;insertedId&quot; : ObjectId(&quot;6269e4be7b15b7097fbad436&quot;)&#125;&gt; db.games.find()&#123; &quot;_id&quot; : ObjectId(&quot;6269e4be7b15b7097fbad436&quot;), &quot;game&quot; : &quot;pinball&quot;, &quot;user&quot; : &quot;joe&quot; &#125;&gt; db.games.updateOne(&#123;&quot;game&quot; :&quot;pinball&quot;,&quot;user&quot; :&quot;joe&quot;&#125;,&#123;&quot;$inc&quot;:&#123;&quot;score&quot;:50&#125;&#125;)&#123; &quot;acknowledged&quot; : true, &quot;matchedCount&quot; : 1, &quot;modifiedCount&quot; : 1 &#125;&gt; db.games.find()&#123; &quot;_id&quot; : ObjectId(&quot;6269e4be7b15b7097fbad436&quot;), &quot;game&quot; : &quot;pinball&quot;, &quot;user&quot; : &quot;joe&quot;, &quot;score&quot; : 50 &#125;&gt; db.games.updateOne(&#123;&quot;game&quot; :&quot;pinball&quot;,&quot;user&quot; :&quot;joe&quot;&#125;,&#123;&quot;$inc&quot;:&#123;&quot;score&quot;:10000&#125;&#125;)&#123; &quot;acknowledged&quot; : true, &quot;matchedCount&quot; : 1, &quot;modifiedCount&quot; : 1 &#125;&gt; db.games.find()&#123; &quot;_id&quot; : ObjectId(&quot;6269e4be7b15b7097fbad436&quot;), &quot;game&quot; : &quot;pinball&quot;, &quot;user&quot; : &quot;joe&quot;, &quot;score&quot; : 10050 &#125;&gt; $push: 배열이 이미 존재하지만 배열 끝에 요소를 추가하고, 존재하지 않으면 새로운 배열을 생성함. $each: $push에 $each제한자를 사용하면 작업 한 번으로 값을 여러개 추가할 수 있음. 12345678910111213141516171819202122&gt; db.blog.posts.findOne() &#123; &quot;_id&quot; : ObjectId(&quot;4b2d75476cc613d5ee930164&quot;), &quot;title&quot; : &quot;A blog post&quot;, &quot;content&quot; : &quot;...&quot;&#125; &gt; db.blog.posts.updateOne(&#123;&quot;title&quot; : &quot;A blog post&quot;&#125;, &#123;&quot;$push&quot; : &#123;&quot;comments&quot; : &#123;&quot;name&quot; : &quot;joe&quot;, &quot;email&quot; : &quot;joe@example.com&quot;, &quot;content&quot; : &quot;nice post.&quot;&#125;&#125;&#125;) &#123; &quot;acknowledged&quot; : true, &quot;matchedCount&quot; : 1, &quot;modifiedCount&quot; : 1 &#125; &gt; db.blog.posts.findOne() &#123; &quot;_id&quot; : ObjectId(&quot;4b2d75476cc613d5ee930164&quot;), &quot;title&quot; : &quot;A blog post&quot;, &quot;content&quot; : &quot;...&quot;, &quot;comments&quot; : [ &#123;&quot;name&quot; : &quot;joe&quot;, &quot;email&quot; : &quot;joe@example.com&quot;, &quot;content&quot; : &quot;nice post.&quot; &#125; ] &#125; $ne: 배열이 존재하지 않을 때 해당 값을 추가하면서 배열을 집합처럼 처리할 때 사용. $addToSet: 다른주소를 추가할 때 중복을 피할 수 있음 고유한 값을 여러개 추가하려면 $addToSet&#x2F;$each조합을 활용해야 함. $ne&#x2F;$push조합으로는 할 수 없음. 123456789101112&gt; db.users.updateOne(&#123;&quot;_id&quot; : ObjectId(&quot;4b2d75476cc613d5ee930164&quot;)&#125;, &#123;&quot;$addToSet&quot; : &#123;&quot;emails&quot; : &#123;&quot;$each&quot; : [&quot;joe@php.net&quot;, &quot;joe@example.com&quot;, &quot;joe@python.org&quot;]&#125;&#125;&#125;) &#123; &quot;acknowledged&quot; : true, &quot;matchedCount&quot; : 1, &quot;modifiedCount&quot; : 1 &#125; &gt; db.users.findOne(&#123;&quot;_id&quot; : ObjectId(&quot;4b2d75476cc613d5ee930164&quot;)&#125;) &#123; &quot;_id&quot; : ObjectId(&quot;4b2d75476cc613d5ee930164&quot;), &quot;username&quot; : &quot;joe&quot;, &quot;emails&quot; : [ &quot;joe@example.com&quot;, &quot;joe@gmail.com&quot;, &quot;joe@yahoo.com&quot;, &quot;joe@hotmail.com&quot; &quot;joe@php.net&quot; &quot;joe@python.org&quot; ] &#125; Reference 몽고DB 완벽 가이드: 실전 예제로 배우는 NoSQL 데이터베이스 기초부터 활용까지","categories":[{"name":"Data Base","slug":"Data-Base","permalink":"https://jmj3047.github.io/categories/Data-Base/"},{"name":"MongoDB","slug":"Data-Base/MongoDB","permalink":"https://jmj3047.github.io/categories/Data-Base/MongoDB/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://jmj3047.github.io/tags/MongoDB/"}]},{"title":"MongoDB CRUD","slug":"MongoDB_CRUD","date":"2022-04-24T15:00:00.000Z","updated":"2023-02-28T07:39:43.709Z","comments":true,"path":"2022/04/25/MongoDB_CRUD/","link":"","permalink":"https://jmj3047.github.io/2022/04/25/MongoDB_CRUD/","excerpt":"","text":"Initial setting 12345678&gt;dbtest&gt;use videoswitched to db video # if video doesnt exist, created&gt;dbvideo&gt;db.movies # created movies collectionvideo.movies Create: insertOne 함수 1234567891011121314151617181920&gt;movie = &#123;&quot;title&quot; : &quot;Star Wars: Episode IV - A New Hope&quot;, &quot;director&quot; : &quot;George Lucas&quot;, &quot;year&quot; : 1977&#125;&#123; &quot;title&quot; : &quot;Star Wars: Episode IV - A New Hope&quot;, &quot;director&quot; : &quot;George Lucas&quot;, &quot;year&quot; : 1977&#125;&gt;db.movies.insertOne(movie) #영화가 데이터 베이스에 저장됨&#123; &quot;acknowledged&quot; : true, &quot;insertedId&quot; : ObjectId(&quot;6266057e4619361339f0c881&quot;)&#125;#Find 함수로 호출&gt;db.movies.find().pretty()&#123; &quot;_id&quot; : ObjectId(&quot;6266057e4619361339f0c881&quot;), &quot;title&quot; : &quot;Star Wars: Episode IV - A New Hope&quot;, &quot;director&quot; : &quot;George Lucas&quot;, &quot;year&quot; : 1977&#125; Read: find, findOne 함수 1234567&gt; db.movies.findOne(movie) &#123; &quot;_id&quot; : ObjectId(&quot;6266057e4619361339f0c881&quot;), &quot;title&quot; : &quot;Star Wars: Episode IV - A New Hope&quot;, &quot;director&quot; : &quot;George Lucas&quot;, &quot;year&quot; : 1977&#125; Update : updateOne 함수 1234567891011&gt; db.movies.updateOne(&#123;title : &quot;Star Wars: Episode IV - A New Hope&quot;&#125;, &#123;$set : &#123;reviews: []&#125;&#125;) &#123; &quot;acknowledged&quot; : true, &quot;matchedCount&quot; : 1, &quot;modifiedCount&quot; : 1 &#125;&gt; db.movies.find().pretty()&#123; &quot;_id&quot; : ObjectId(&quot;6266057e4619361339f0c881&quot;), &quot;title&quot; : &quot;Star Wars: Episode IV - A New Hope&quot;, &quot;director&quot; : &quot;George Lucas&quot;, &quot;year&quot; : 1977, &quot;reviews&quot; : [ ]&#125; Delete: deleteOne, deleteMany 함수 12&gt;db.movies.deleteOne(&#123;title : &quot;Star Wars: Episode IV - A New Hope&quot;&#125;)&#123; &quot;acknowledged&quot; : true, &quot;deletedCount&quot; : 1 &#125; Reference 몽고DB 완벽 가이드: 실전 예제로 배우는 NoSQL 데이터베이스 기초부터 활용까지","categories":[{"name":"Data Base","slug":"Data-Base","permalink":"https://jmj3047.github.io/categories/Data-Base/"},{"name":"MongoDB","slug":"Data-Base/MongoDB","permalink":"https://jmj3047.github.io/categories/Data-Base/MongoDB/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://jmj3047.github.io/tags/MongoDB/"}]},{"title":"MongoDB Install & Basic Command","slug":"MongoDB_Install","date":"2022-04-24T15:00:00.000Z","updated":"2023-02-28T07:40:15.695Z","comments":true,"path":"2022/04/25/MongoDB_Install/","link":"","permalink":"https://jmj3047.github.io/2022/04/25/MongoDB_Install/","excerpt":"","text":"Link: www.mongodb.com/try/download/enterprise Download proper version of Mongodb Install이 완료된 후에는 MongoDB 환경변수 설정을 위해 시스템 환경 변수 편집을 진행하여 줍니다. 환경변수 편집을 위해 환경변수 &gt;시스템 변수 Path 설정을 선택하여 줍니다. 설치된 MongoDB의 bin폴더 경로를 입력하여 줍니다.(C:\\Program Files\\MongoDB\\Server\\5.0\\bin) 저장 후 cmd창에서 mongdb –version을 통해 정상 설치를 확인하여 줍니다. cmd 창에 mongodb 실행 1&gt;mongo 명령어 두줄로 잘 실행되는지 간단히 확인 12&gt; db.world.insert(&#123; &quot;speech&quot; : &quot;Hello World!&quot; &#125;);&gt; cur = db.world.find();x=cur.next();print(x[&quot;speech&quot;]); Basic Command사용 가능한 모든 데이터베이스 표시 : 1show dbs; 액세스 할 특정 데이터베이스를 선택 (Ex: mydb . 이미 존재하지 않으면 mydb 가 생성됩니다 : 1use mydb; 데이터베이스에 모든 콜렉션을 표시. 먼저 콜렉션을 선택하십시오 (위 참조). 1show collections; 데이터베이스와 함께 사용할 수있는 모든 기능 표시 : 1db.mydb.help(); 현재 선택한 데이터베이스를 확인 12&gt; dbmydb db.dropDatabase() 명령은 기존 데이터베이스를 삭제하는 데 사용됩니다. 1db.dropDatabase() Reference https://khj93.tistory.com/entry/MongoDB-Window에-MongoDB-설치하기 https://learntutorials.net/ko/mongodb/topic/691/mongodb-시작하기","categories":[{"name":"Data Base","slug":"Data-Base","permalink":"https://jmj3047.github.io/categories/Data-Base/"},{"name":"MongoDB","slug":"Data-Base/MongoDB","permalink":"https://jmj3047.github.io/categories/Data-Base/MongoDB/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://jmj3047.github.io/tags/MongoDB/"}]},{"title":"BeautifulSoup Quick Start","slug":"BeautifulSoup_QuickStart","date":"2022-04-21T15:00:00.000Z","updated":"2022-04-22T02:53:21.000Z","comments":true,"path":"2022/04/22/BeautifulSoup_QuickStart/","link":"","permalink":"https://jmj3047.github.io/2022/04/22/BeautifulSoup_QuickStart/","excerpt":"","text":"Index_prac.html123456789101112131415161718192021222324252627&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;title&gt; The Dormouse&#x27;s story &lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class=&quot;title&quot;&gt;&lt;b&gt;The Dormouse&#x27;s story&lt;/b&gt;&lt;/p&gt; &lt;p class=&quot;story&quot;&gt; Once upon a time there were three little sisters; and their names were &lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;, &lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and &lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class=&quot;story&quot;&gt;To Be Continued...&lt;/p&gt; &lt;/body&gt;&lt;/html&gt; Quick Start.py123456789101112131415161718192021222324252627282930313233343536373839404142434445from bs4 import BeautifulSoupsoup = BeautifulSoup(open(&quot;index_prac.html&quot;), &#x27;html.parser&#x27;)# print allprint(soup.prettify())# navigate that data structureprint(soup.title)# &lt;title&gt;The Dormouse&#x27;s story&lt;/title&gt;print(soup.title.name)# u&#x27;title&#x27;print(soup.title.string)# u&#x27;The Dormouse&#x27;s story&#x27;print(soup.title.parent.name)# u&#x27;head&#x27;print(soup.p)# &lt;p class=&quot;title&quot;&gt;&lt;b&gt;The Dormouse&#x27;s story&lt;/b&gt;&lt;/p&gt;print(soup.p[&#x27;class&#x27;])# u&#x27;title&#x27;print(soup.a)# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;print(soup.find_all(&#x27;a&#x27;))# [&lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;,# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;,# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;]print(soup.find(id=&quot;link3&quot;))# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;## extracting all the URLsfor link in soup.find_all(&#x27;a&#x27;): print(link.get(&#x27;href&#x27;))# http://example.com/elsie# http://example.com/lacie# http://example.com/tillie## extracting all the text from a pageprint(soup.get_text()) Reference: https://www.crummy.com/software/BeautifulSoup/bs4/doc/#quick-start","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"HTML","slug":"Python/HTML","permalink":"https://jmj3047.github.io/categories/Python/HTML/"}],"tags":[{"name":"HTML","slug":"HTML","permalink":"https://jmj3047.github.io/tags/HTML/"},{"name":"Web Crawling","slug":"Web-Crawling","permalink":"https://jmj3047.github.io/tags/Web-Crawling/"},{"name":"BeautifulSoup","slug":"BeautifulSoup","permalink":"https://jmj3047.github.io/tags/BeautifulSoup/"}]},{"title":"Improved Training of Wasserstein GANs","slug":"WGAN","date":"2022-04-21T15:00:00.000Z","updated":"2022-10-15T08:05:10.371Z","comments":true,"path":"2022/04/22/WGAN/","link":"","permalink":"https://jmj3047.github.io/2022/04/22/WGAN/","excerpt":"","text":"Journal&#x2F;Conference: NIPSYear(published year): 2017Author: Ishaan Gulrajani, Faruk Ahmed, Martín Arjovsky, Vincent Dumoulin, Aaron C. CourvilleSubject: DCGAN, Generative Model Improved Training of Wasserstein GANs Summary 기존의 Wasserstein-GAN 모델의 weight clipping 을 대체할 수 있는 gradient penalty 방법을 제시 hyperparameter tuning 없이도 안정적인 학습이 가능해졌음을 제시 IntroductionGAN 모델을 안정적으로 학습하기 위한 많은 방법들이 존재해왔습니다. 특히, 가치함수가 수렴하는 성질을 분석하여 Discriminator(이후 Critic)가 1-Lipschitz function 공간에 있도록 하는 Wasserstein GAN(WGAN) 이 제시된 바 있습니다. 논문은 WGAN 의 단점을 개선한 WGAN-GP 모델을 제시합니다. Toy datasets에 대해 critic의 weight clipping이 undesired behavior를 유발할 수 있음을 증명 “Gradient penalty”(WGAN-GP) 기법으로 제안 다양한 GAN 구조에대해 안정적인 학습을 수행할 수 있고, 고품질 이미지 생성을 수행하며, 개별 샘플링이 필필요하지 않는 문자수준 언어 모델을 제시 BackgroundGenerative adversarial networks일반적인 GAN 구조에 대해 다시 한번 개념을 되짚습니다. Wasserstein GANsWGAN 은 GAN 의 목적함수인 JSD 가 parameter 에 연속적이지 않에 학습에 문제가 발생함을 지적하였습니다. 이에, Earth-Mover distance 로 모든 구간에서 연속적이고 대부분의 구간에서 미분 가능하게 하여 문제를 해결하였습니다. 이외에도 WGAN 의 특징에 대해 서술하며, 가장 중요한 특징으로 Lipschitz 조건을 만족하기 위해 시행하는 weight clipping 을 언급합니다. Properties of the optimal WGAN critic최적의 WGAN critic 을 가정했을 때, ****weight clipping이 WGAN critic에서 문제를 발생시킴을 언급하고 증명한 결과를 제시합니다. Difficulties with weight constraintsWGAN의 weight clipping이 최적화에 문제를 발생시킬 수 있고, 최적화가 잘 되더라도 critic이 pathological value surface 을 가질 수 있음을 증명하였던 내용을 확인하기 위한 실험을 진행합니다. 논문은 기존 WGAN 이 제시하였던 hard clipping 방식 이외에도, L2 norm clipping&#x2F;weight normalization&#x2F;L1 and L2 weight decay 등의 weight constraint 를 가정하였을 때 모두 비슷한 문제가 발생하였음을 언급합니다. Capacity underuse &amp; Exploding and vanishing gradientsk-Lipshitz 조건을 달성하기 위해 weight clipping 을 수행하였을 때, critic은 더욱 단순한 형태의 함수를 취하게 됩니다. 논문은 이를 증명하기 위해 Generator 의 분포를 toy distribution + unit-variance 가우시안-노이즈에 고정한뒤, weight clipping 과 함께 WGAN critic 을 학습한 결과를 제시합니다. 왼쪽 그림에서 Weight clipping 수행한 경우의 value surface 모양이 단순해졌음을 확인할 수 있습니다. 또한, 우측 그림과 같이, Gradient penalty 를 수행한 경우에 gradient vanishing 이나 exploding 이 발생하지 않았음을 제시합니다. Gradient penaltyWeight Clipping 을 사용하지 않고 Lipschitz constraint 를 유지할 수 있는 방법을 설명합니다. 입력에 대한 Critic 출력 gradient 의 크기를 직접 제약합니다. 이 때, tractability issue 를 피하기 위해 무작위로 추출한 샘플 $\\hat{x}$ 의 gradient norm 을 사용해 soft 한 제약을 줍니다. 이렇게 새롭게 정의되는 목적함수는 아래와 같습니다. Sampling distribution논문은 데이터 분포와 generator 분포에서 샘플링한 점의 쌍을 이은 뒤, 점을 잇는 선분을 따라 $\\hat{x}$ 를 샘플링하였고, 실험적으로 좋은 성능을 얻었음을 언급합니다. Penalty coefficientgradient penalty 를 가하는 정도를 경정하는 계수로, 논문에서는 모두 $\\lambda&#x3D;10$ 을 사용했음을 언급합니다. No critic batch normalization기존 GAN 모델은 batch normalization 을 모든 곳에서 사용했지만, 이는 discriminator의 단일 입력을 단일 출력으로 매핑하는 문제에서, 입력의 전체 배치로부터 출력의 배치로 매핑하는 문제로 변화시킵니다. 이 때문에 gradient penalty 를 수행하면 batch normalization 이 유효하지 않은 결과가 발생한다고 합니다. 따라서 논문은 critic 에 batch normalization 을 제거하였고 그럼에도 적절한 성능을 보였음을 언급합니다. Two-sided penaltygradient penalty 는 norm이 1 아래에 머무르지 않고(one-sided penalty), 1로 향하기(two-sided penalty)는 것을 촉진한다는 점을 제시합니다. ExperimentsTraining random architectures within a set 일반적인 DCGAN 구조에서 위의 표의 설정을 랜덤하게 설정하여 모델을 구성합니다. 이렇게 무작위로 200개의 모델을 구성한뒤, 32x32 ImageNet 에 대해 WGAN-GP, standard GAN을 합니다. 구성한 모델의 inception_score 가 min_score 보다 큰 경우 성공으로 분류합니다. WGAN-GP 는 많은 구조를 학습하는데 성공했다는 결과를 제시합니다. Training varied architectures on LSUN bedrooms아래와 같이 6개의 모델을 기본 모델로 사용합니다. 여기에 DCGAN, LSGAN, WGAN, WGAN-GP 를 각각 적용하였을 때의 성능을 비교합니다. 단, WGAN-GP는 discriminator 에서 Batch normalization 을 사용할 수 없기에 layer normalization 을 사용합니다. WGAN-GP 를 제외한 모든 모델에서 불안정하거나 mode collapse 에 빠진 모습을 보입니다. Improved performance over weight clippingWGAN-GP 가 weight clipping 에 비해 더 빠른 학습 속도와 샘플 효율을 보인다는 점을 증명하기 위한 실험 결과를 제시합니다. 이를 위해, WGAN 과 WGAN-GP 모델을 CIFAR-10 으로 학습하여 Inception Score 를 계산합니다. 왼쪽은 iteration에 따른 Inception Score이며, 오른쪽은 시간에 따른 Inception Score입니다. WGAN-GP는 weight clipping 보다 항상 더 좋은 성능을 보입니다. 이는 같은 optimizer 를 사용했을 때도 마찬가지이며, 비록 DCGAN 보다는 느리지만 수렴에 있어서 안정적인 점수를 보일 수 있음을 제시합니다. Sample quality on CIFAR-10 and LSUN bedrooms CIFAR-10으로 학습한 모델의 Inception score 를 계산하여 다양한 구조의 GAN을 비교한 표를 제시합니다. WGAN-GP 는 Supervised 의 경우 SGAN 을 제외했을 때 가장 좋은 성능을 보입니다. 또한, WGAN-GP 로 deep ResNet 모델을 사용하여 128X128 LSUN 침대 이미지를 생성하여 위와같은 결과를 제시합니다. Modeling discrete data with a continuous generator Generator 는 연속적인 분포의 함수를 가정합니다. 따라서언어 모델은 비연속적인 분포를 모델링 해야하므로 GAN 으로 학습하기에 부적절할 수 있습니다. 위는 Google Billion Word 데이터셋을 사용해 문자 수준 언어 모델을 WGAN-GP 로 학습한 결과입니다. 모델이 빈번하게 철자를 틀리지만, 언어의 통계에 대해서는 어느정도 학습을 수행하였음을 볼 수 있습니다. Meaningful loss curves and detecting overfitting기존의 weight clipping 은 loss 가 sample quality 와 연관되어 최소값으로 수렴할 수 있다는 점입니다. WGAN-GP 가 해당 특성을 유지하는지 확인하기 위한 테스크를 진행한 결과를 제시합니다. (a)에서 LSUN 침대 데이터셋을 학습하고 critic 의 negative loss 를 그렸을 때, Gnerator 가 학습됨에 따라 값이 줄어드는 것을 확인할 수 있습니다. 이 경우, WGAN-GP가 critic에서의 과적합을 완화했다고 볼 수 있습니다. 또한, MNIST 무작위 숫자 1000개로 학습한 결과는, 적은 데이터셋을 사용한 만큼 과적합이 발생하기 쉽습니다. 때문에, critic이 generator보다 더 빨리 과적합되어 training loss를 점차 증가시키고 validation loss를 감소시켰음을 확인할 수 있습니다. ConclusionWGAN에 Gradient penalty를 적용하여 기존의 weight clipping 을 적용함으로 인해 발생하는 문제를 해결할 수 있음을 제시하였습니다. Summarize GAN의 가장 큰 문제는 학습환경이 매우 불안정하다는 것이다. 생성자와 구분자 둘 중에 하나가 실력이 월등이 좋아진다면 밸런스가 붕괴되고 모델이 정확히 학습되지 않고 학습이 완료된 후에도 mode dropping 이 생기는데 이는 구분자가 그 역할을 충분히 하지 못해 모델이 최적점까지 학습이 안 된 것이다. 따라서 이 문제를 해결하기 위해 본 논문에서는 WGAN 방법을 도입했다. 간단히 설명하면 GAN의 discriminator보다 선생님 역할을 잘 할 수 있는 critic을 사용함으로써 gradient를 잘 전달시키고 critic과 generator를 최적점까지 학습할 수 있다는 것이다. 그렇다면 이를 적용하면 학습시킬 때 생성자와 구분자의 밸런스가 잘 맞는지 주의깊게 보지 않아도 되고 학습한 이후에 발생하는 mode droppin이 해결 가능하다. 식을 해석해보면 생성자가 Lipschitz 함수 조건을 만족하는가 하지않는가에 대한 기준이 하나 더 생기는것 이다. Link: Improved Training of Wasserstein GANs","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Generative Model","slug":"Paper/Generative-Model","permalink":"https://jmj3047.github.io/categories/Paper/Generative-Model/"}],"tags":[{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"},{"name":"Image Classification","slug":"Image-Classification","permalink":"https://jmj3047.github.io/tags/Image-Classification/"},{"name":"WGAN-GP","slug":"WGAN-GP","permalink":"https://jmj3047.github.io/tags/WGAN-GP/"},{"name":"WGAN","slug":"WGAN","permalink":"https://jmj3047.github.io/tags/WGAN/"}]},{"title":"Basic Web Crawling","slug":"Web_Crawling_Basic","date":"2022-04-21T15:00:00.000Z","updated":"2022-04-22T07:30:27.000Z","comments":true,"path":"2022/04/22/Web_Crawling_Basic/","link":"","permalink":"https://jmj3047.github.io/2022/04/22/Web_Crawling_Basic/","excerpt":"","text":"Crawling Tools -Beautifulsoup: 파이썬에서 가장 일반적인 수집도구(CSS 통해서 수집) -Scrapy (CSS, XPATH 형태로 수집) -Selenium (CSS, XPATH 통해서 데이터 수집 + Java Script) →자바 필요 + 몇개 설치 도구 필요 웹사이트 만드는 3대 조건 +1 :HTML, CSS, JavaScript, Ajax(비동기처리) 웹사이트 구동방식 :GET &#x2F; POST Create virtual env(git bash)123pip install virtualenvpython -m virtualenv venvsource venv/Scripts/activate Installing Library123pip install beautifulsoup4pip install numpy pandas matplotlib seabornpip install requests Reference: https://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-beautiful-soup","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"HTML","slug":"Python/HTML","permalink":"https://jmj3047.github.io/categories/Python/HTML/"}],"tags":[{"name":"HTML","slug":"HTML","permalink":"https://jmj3047.github.io/tags/HTML/"},{"name":"Web Crawling","slug":"Web-Crawling","permalink":"https://jmj3047.github.io/tags/Web-Crawling/"},{"name":"BeautifulSoup","slug":"BeautifulSoup","permalink":"https://jmj3047.github.io/tags/BeautifulSoup/"}]},{"title":"Web Crawling Practice","slug":"Web_Crawling_Headline","date":"2022-04-21T15:00:00.000Z","updated":"2022-04-22T08:17:11.000Z","comments":true,"path":"2022/04/22/Web_Crawling_Headline/","link":"","permalink":"https://jmj3047.github.io/2022/04/22/Web_Crawling_Headline/","excerpt":"","text":"1. Crawling Headline news from Naver12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import requestsfrom bs4 import BeautifulSoupimport pandas as pddef crawling(soup): div = soup.find(&quot;div&quot;, class_=&#x27;list_issue&#x27;) # print(type(div)) print(div.find_all(&#x27;a&#x27;)) #list형태 result = [] urls = [] for a in div.find_all(&quot;a&quot;): # print(a.get_text()) urls.append(a[&#x27;href&#x27;]) result.append(a.get_text()) # print(result) #save as csv file df = pd.DataFrame(&#123;&#x27;news_title&#x27;: result, &quot;url&quot;: urls&#125;) print(df) df.to_csv(&quot;newscrawling.csv&quot;) print(&quot;Crawling Success!&quot;) def main(): CUSTOM_HEADER = &#123; &#x27;referer&#x27; : &#x27;https://www.naver.com/&#x27;, &#x27;user-agent&#x27; : &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36&#x27; &#125; url = &#x27;https://www.naver.com/&#x27; req = requests.get(url = url, headers=CUSTOM_HEADER) print(req.status_code) #200: 정상적으로 사이트 돌아가고 있음 #404: 주소 오류 #503: 서버 내려진 상태 #print(req.text)이거를 beautifulsoup객체로 바꿔줌 soup = BeautifulSoup(req.text, &#x27;html.parser&#x27;) print(type(soup)) # print(soup.find(&quot;strong&quot;, class_=&#x27;new&#x27;)) # print(soup.find_all(&quot;span&quot;, class_=&#x27;blind&#x27;)) #print(soup.find(&quot;a&quot;, class_=&#x27;link_newsstand&#x27;)) crawling(soup)if __name__ == &quot;__main__&quot;: main() 2. Crawling Product List from ACBF1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import requestsfrom bs4 import BeautifulSoupimport pandas as pddef crawling(soup): div = soup.find_all(&quot;div&quot;, class_=&#x27;name&#x27;) print(type(div)) #&lt;class &#x27;bs4.element.ResultSet&#x27;&gt; # print(div) product_name = [] # urls =[] for a in div: # print(a.get_text()) # urls.append(a.get(&#x27;href&#x27;)) product_name.append(a.get_text()) print(product_name) # df = pd.DataFrame(&#123;&#x27;news_title&#x27;: product_name&#125;) # print(df) # df.to_csv(&quot;suit_product.csv&quot;) print(&quot;Crawling Success!&quot;) def main(): CUSTOM_HEADER = &#123; &#x27;referer&#x27; : &#x27;https://anyonecanbeafuse.com/category/%EC%88%98%ED%8A%B8/89/&#x27;, &#x27;user-agent&#x27; : &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36&#x27; &#125; url = &#x27;https://anyonecanbeafuse.com/category/%EC%88%98%ED%8A%B8/89/&#x27; req = requests.get(url = url, headers=CUSTOM_HEADER) print(req.status_code) #200: 정상적으로 사이트 돌아가고 있음 #404: 주소 오류 #503: 서버 내려진 상태 #print(req.text)이거를 beautifulsoup객체로 바꿔줌 soup = BeautifulSoup(req.text, &#x27;html.parser&#x27;) #print(type(soup)) # print(soup.find_all(&quot;div&quot;, class_=&#x27;name&#x27;)) crawling(soup)if __name__ == &quot;__main__&quot;: main() 3. Crawling Music Title from Chart123456789101112131415161718192021222324252627282930313233343536373839404142import requestsfrom bs4 import BeautifulSoupdef crawling(soup): tbody_df = soup.find(&quot;tbody&quot;) # print(tbody_df) result = [] for a in tbody_df.find_all(&#x27;p&#x27;, class_=&#x27;title&#x27;): # print(a.get_text()) # print(type(a.get_text())) result.append(a.get_text().strip(&quot;\\n&quot;)) print(result) print(&quot;Crawling Success!&quot;) def main(): CUSTOM_HEADER = &#123; &#x27;referer&#x27; : &#x27;https://music.bugs.co.kr/chart&#x27;, #필수 아님 &#x27;user-agent&#x27; : &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36&#x27; &#125; url = &#x27;https://music.bugs.co.kr/chart&#x27; req = requests.get(url = url, headers=CUSTOM_HEADER) print(req.status_code) #200: 정상적으로 사이트 돌아가고 있음 #404: 주소 오류 #503: 서버 내려진 상태 #print(req.text)이거를 beautifulsoup객체로 바꿔줌 soup = BeautifulSoup(req.text, &#x27;html.parser&#x27;) print(type(soup)) #&lt;class &#x27;bs4.BeautifulSoup&#x27;&gt; # print(soup.find_all(&quot;p&quot;, class_=&#x27;title&#x27;)) # print(soup.find_all(&quot;span&quot;, class_=&#x27;blind&#x27;)) #print(soup.find(&quot;a&quot;, class_=&#x27;link_newsstand&#x27;)) crawling(soup)if __name__ == &quot;__main__&quot;: main()","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"HTML","slug":"Python/HTML","permalink":"https://jmj3047.github.io/categories/Python/HTML/"}],"tags":[{"name":"HTML","slug":"HTML","permalink":"https://jmj3047.github.io/tags/HTML/"},{"name":"Web Crawling","slug":"Web-Crawling","permalink":"https://jmj3047.github.io/tags/Web-Crawling/"},{"name":"BeautifulSoup","slug":"BeautifulSoup","permalink":"https://jmj3047.github.io/tags/BeautifulSoup/"}]},{"title":"Unsupervised representation learning with deep convolutional generative adversarial networks","slug":"DCGAN","date":"2022-04-20T15:00:00.000Z","updated":"2022-10-15T08:03:41.524Z","comments":true,"path":"2022/04/21/DCGAN/","link":"","permalink":"https://jmj3047.github.io/2022/04/21/DCGAN/","excerpt":"","text":"Journal&#x2F;Conference: ICLRYear(published year): 2016Author: Alec Radford, Luke Metz, Soumith ChintalaSubject: DCGAN, Generative Model Unsupervised representation learning with deep convolutional generative adversarial networks Summary CNN 과 GAN framework 를 결합한 DCGAN 모델을 제시 Introduction논문 당시 GAN 은 불안정한 학습과 Generator 의 오작동으로 인해 제한적으로만 쓰였습니다. 이에 논문은 이미지 생성 모델을 만들기 위한 CNN 기반 GAN framework인 DCGAN(Deep Convolutional GANs) 을 제시합니다. 모델 구조에 제약을 가하여 대부분의 상황에서 안정적으로 학습할 수 있게 함 Discriminator 로 image classification 을 수행하였을 때 기타 비지도 학습 모델과 비슷한 성능을 보임 특정 필터가 특정 object를 그려낸다는 것을 시각화하여 제시함 Generator 에 입력하는 noise 를 제어하여 생성되는 샘플의 다양한 속성이 변화하는 것을 탐구함 Approach and Model Architecture논문 이전에도 GAN에 CNN을 써서 이미지 품질을 높이려는 시도가 있었으나 좋은 성과를 거두지 못하였다고 설명합니다. 이후, 다양한 데이터셋에서 안정적이고 높은 해상도의 이미지를 생성하기 위한 DCGAN 모델 설계 가이드라인을 제시합니다. Details of Adversarial Training3가지 데이터셋을 사용합니다. Large-scale Scene Understanding(LSUN) Imagenet-1k Faces 그외에 학습 디테일을 아래와 같이 제시합니다. pre-processing 제외 batch size 128 가중치는 N(0, 0.02) 로 초기화 Leaky ReLU의 기울기는 0.2로 설정함 AdamOptimizer, $\\beta_1 &#x3D;0.0002, \\beta_2&#x3D;0.9$ Generator 구조의 모식도는 위와 같습니다. LSUN 데이터셋으로 1 epoch 를 학습시킨 후 침실을 생성했을 때의 결과입니다. 이론적으로 모델이 훈련 예시를 기억했을 수도 있으나, 작은 학습률과 미니배치를 사용했음을 감안할 때 가능성이 낮다고 설명합니다. LSUN 데이터셋으로 5 epoch 학습 후 침실을 생성한 결과입니다. 침대 등의 근처에서 오히려 underfitting 이 발생했음을 확인할 수 있습니다. Empirical Validation of DCGANs CapabilitiesUnsupervised representation learning 알고리즘을 평가하는 일반적인 방법은 supervised 데이터셋으로 특징을 추출한 뒤 performance를 측정하는 것입니다. CIFAR-10 데이터셋에 대해 검증한 결과, 다른 방법들(K-means, Exemplar CNN 등)과 비교하여 결과에 큰 차이가 존재하지 않습니다. StreetView House Numbers dataset(SVHN) 데이터셋에서는 state-of-the-art 결과를 얻었음을 제시합니다. Investigating and Visualizing the Internals of the Networks가장 가까운 학습 데이터 이미지를 찾거나, 최근접 픽셀&#x2F;특징을 확인하거나 log-likelihood metric 으로 평가를 하는 방법은 모두 성능이 떨어지는 metric 이기에 사용하지 않았음을 언급합니다. 논문은 대신, 2개의 이미지를 생성할 때 사용한 noise 2개를 interpolation 하고, interpolated z 로 이미지를 생성한 결과를 제시합니다. 한 이미지에서 다른 이미지로 점진적으로 변해가는 모습을 관측할 수 있습니다. 또한 노이즈 벡터 z 의 산술 연산을 통해, vec(웃는 여자) −− vec(무표정 여자) ++ vec(무표정 남자) &#x3D;&#x3D; vec(웃는 남자) 같은 결과를 얻을 수 있었음을 제시합니다. 또한, 랜덤하게 생성한 필터와 학습된 필터의 activation 을 아래와 같이 시각화 하였습니다. 이해할 수 없는 feature 가 아닌 특정 object나 특징을 추출하고 있음을 확인할 수 있습니다. Conclusions and future work논문은 CNN 기반의 안정적인 이미지 생성모델인 DCGAN을 제안하였으며, image representation에 적합한 성능을 보임을 제시합니다. 그러나 여전히, 학습이 길어지는 경우 필터 일부가 요동치는 등의 현상을 관측하기도 하였음을 언급합니다. Link: Unsupervised representation learning with deep convolutional generative adversarial networks","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Generative Model","slug":"Paper/Generative-Model","permalink":"https://jmj3047.github.io/categories/Paper/Generative-Model/"}],"tags":[{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"},{"name":"DCGAN","slug":"DCGAN","permalink":"https://jmj3047.github.io/tags/DCGAN/"},{"name":"Convolutional Neural Networks","slug":"Convolutional-Neural-Networks","permalink":"https://jmj3047.github.io/tags/Convolutional-Neural-Networks/"},{"name":"Image Classification","slug":"Image-Classification","permalink":"https://jmj3047.github.io/tags/Image-Classification/"},{"name":"GAN","slug":"GAN","permalink":"https://jmj3047.github.io/tags/GAN/"}]},{"title":"Generative Adversarial Nets","slug":"Generative_Adversarial_Nets","date":"2022-04-20T15:00:00.000Z","updated":"2022-10-15T08:03:52.639Z","comments":true,"path":"2022/04/21/Generative_Adversarial_Nets/","link":"","permalink":"https://jmj3047.github.io/2022/04/21/Generative_Adversarial_Nets/","excerpt":"","text":"Journal&#x2F;Conference: NIPSYear(published year): 2014Author: I. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, Yoshua BengioSubject: GAN, Generative Model Generative Adversarial Nets Summary 적대적으로 동작하는 두개의 네트워크를 사용해 새로운 데이터를 생성할 수 있는 GAN(Generative Adversarial Nets) 구조를 제안 생성자(Generator) 와 감별자(Discriminator) 모두 마르코프 체인 등의 구조없이 back-propagation 으로 학습이 가능한 인공신경망 구조를 사용 이후 등장하는 수많은 GAN 기반 모델의 기원이 되는 논문 Introduction &amp; Related Works분류 문제에 제한되어 사용되던 딥러닝 모델의 용도를 새로운 데이터를 생성하는 문제에도 적용할 수 있는 적대적 생성 신경망(Generative Adversarial Nets)을 최초로 제시한 논문입니다. GAN은 아래와 같은 목표를 가진, 적대적인 두 모델을 학습합니다. 감별자(Discriminator) 모델 데이터가 원본 데이터셋에서 온것인지, 생성자가 만든 것인지를 판별 예시) 경찰이 지폐가 위조되었는지를 판별 생성자(Generator) 모델 감별자가 구분할 수 없는 가짜 데이터를 생성 예시) 위폐범이 경찰이 구분할 수 없는 위조 지폐를 제작함 논문은 해당 방법이 특별한 모델이나 학습 방법을 필요로 하지 않는 방법이라고 하며, MLP(multi-layer perception) 구조를 사용해 학습한 결과를 소개합니다. Adversarial nets적대적 신경망의 가장 직관적인 예시로 MLP 모델을 사용한 경우를 가정하여 설명합니다. 이 때 사용하는 표기법은 다음과 같습니다. $x\\sim p_{data}$ : 실제 데이터로부터 뽑은 샘플 $p_g$ : 생성자가 생성하는 데이터의 분포 $p_z(z)$ : 데이터를 생성하기 위해 사용하는 입력 노이즈 분포 $G(z;\\theta_g)$ : 생성자 모델 $\\theta_g$ : 생성자 모델 파라미터 $D(x;\\theta_d)$ : 감별자 모델 $\\theta_D$ : 감별자 모델 파라미터 D 는 실제 데이터와 생성된 데이터에 정확히 구분할 수 있는 확률을 최대화 하려고 합니다. G 는 D가 실제 데이터로 착각할 만한 데이터를 생성하는 것을 목적으로 $\\log(1-D(G(z))$ 를 최소화 하려고 합니다. 따라서 아래와 같이 가치함수 $V(G,D)$ 가 주어졌을 때, G 는 최소화, D는 최대화를 목적으로 경쟁합니다. 실제 계산에서 V 를 최대로 하는 D 를 구할 때 많은 계산이 필요하고, 데이터셋이 제한된 상황에서 과적합이 발생할 수도 있습니다. 따라서 실제 훈련에서는 D 를 k 번만 학습하고 G 를 학습합니다. 또한, 학습 초기에는 G가 생성하는 데이터의 품질이 낮으므로 D가 판별을 하기 쉬워, $\\log(1−D(G(z)))$ 항이 소실될수 있습니다. 따라서, $\\log D(G(z))$ 를 최대화 하는 문제로 변환하여 초기에 학습이 잘 이뤄질 수 있도록 합니다. 학습 과정의 모식도입니다. 파란 점선은 감별자 D의 분포, 검은 점은 원본 데이터 분포, 초록 실선은 생성자 G의 분포를 나타냅니다. 파란색점선: discriminator 검정색점선: real data에서나온sample 초록색실선: generator Z옆의검정색실선: domain from which z is sampled 화살표: 생성자가 noise를 real data와 얼마나 비슷하게 만들어주는지에 대한 지표 (a) 와 같이 학습이 완료되기 전의 상태에서 시작합니다.(model training 초기상태) (b) 와 같이 D 를 업데이트 할 때, 최적의 D( $D^{*}_G(x)$ ) 는 $\\frac{p_{data}(x)}{p_{data}(x) + p_g(x)}$ 로 수렴합니다.(내부의 알고리즘에 의해서 구분자가 train됨) (c) G를 업데이트하면, D 를 교란할 수 있도록 G 가 생성하는 분포가 실제 데이터 분포에 가까워집니다.(구분자가 학습한 걸 생성자에게 업데이트) (d) 학습 과정을 반복하면 생성자는 데이터 분포와 일치하는 데이터를 생성($p_g &#x3D; p_{data}$) 하며, 감별자는 어떠한 샘플도 구분할 수 없게 됩니다. ($D(x)&#x3D;\\frac{1}{2}$) (real data와f ake data가 같은 모습이 된 단계. 구분자는 fake와 real data를 구분할 수 없게 됨.) Fake data가왜noise인지? 명확한 이유는 명시되어 있지 않음. 대략적인 이유를 추론해보자면 생성자에 편향되지 않은 데이터가 들어가야 실험의 결과가 더 clear하기 때문. 생성자에 넣어서 만들어진 데이터가 m개라면, 그 데이터 m개가 만들어지려면 같은 숫자의 real data가 있어야 함. 따라서 총데이터는 2m개 구분자의 결과값은 fake data일때 0, real data 일 때 1인 하나의 스칼라 값 따라서 가장 이상적인 구분자가 될 때의 값은 0.5 GAN 모델은 markov 모델이 해야하는 훈련 과정과 overfitting에 문제점을 보완하기 위해 이를 한번에 하기 위해 만들어진 네트워크 Theoretical Results적대적 신경망 문제에서 생성자가 원본 데이터와 유사한 분포의 데이터를 생성할 수 있다는 증명을 제시합니다. 또한, 실제 적대적 신경망을 학습하기 위해 설계한 아래 알고리즘 또한 같은 결과에 수렴한다는 증명을 제시합니다. Global Optimality of $p_g &#x3D; p_{data}$먼저 임의의 G 가 주어졌을 때 최적의 D 를 계산하는 과정을 보입니다. 최적의 D 를 이용하여 Equation 1 을 G에 관한 수식으로 표현할 수 있습니다. 이 때 새롭게 정리한 가치함수가, G가 생성하는 데이터의 분포가 실제 분포를 따르는 경우에만 최소화된다는 것을 다음과 같이 증명합니다. Convergence of Algorithm 1G 와 D 가 $p_g$ 충분한 표현력을 갖고 있을 때, 제시한 알고리즘이 $p_g&#x3D;p_{data}$ 로 수렴함을 아래와 같이 증명합니다. 실제로 MLP 를 사용한 G 로는 모든 형태의 $p_g$ 를 표현할 수 없으므로 이론적인 최고 성능을 보장하기 어렵습니다. 논문은 그럼에도 불구하고 GAN이 실제 훈련결과에서 좋은 성능을 보임을 제시합니다. Experiments실험에 사용한 조건은 다음과 같습니다. Dataset : MNIST, Toronto Face Database(TFD), CIFAR-10 사용 Generator : ReLU&#x2F;sigmoid 활성함수를 혼합하여 사용 Discriminator : maxout 활성함수를 사용 D를 학습시킬 때만 Dropout을 사용 G에서 데이터를 생성하는 경우에만 noise를 input 으로 사용 GAN 은 데이터 분포 자체를 구하기 위한 tractable likelihood 를 가정하지 않습니다. 이러한 모델을 평가하기 위해 기존에 제안된 방법은 다음과 같습니다. Generator 에서 생성한 데이터를 Gaussian Parzen window 에 fitting fitting 한 분포가 주어졌을 때 log-likelihood 를 계산 Validation set 으로 교차 검증을 수행해 표준 편차를 계산 논문은 해당 방법의 분산이 크고 높은 차원의 데이터에서 잘 작동하지 않지만, GAN 이 기존 모델에 비해 상대적으로 좋은 결과를 보이고 있음을 제시합니다. 다음으로 GAN 모델로 생성한 데이터를 제시합니다. 가장 우측에는 원본 데이터셋 중 생성된 데이터에 가장 가까운 데이터를 배치하였습니다. 논문은 해당 모델이 기존의 생성 모델보다 낫다고 주장하기는 어렵지만, 비슷한 성과와 응용 가능성을 보여줄 수 있다는 의견을 제시합니다. 또한 위 그림과 같이, Generator 의 Input noise 를 점진적으로 변형시킬 때, 점점 interploation 되어가는 생성 데이터를 확인할 수 있습니다. Advantages and disadvantagesGAN 의 단점을 아래와 같이 정리합니다. Generator 가 생성하는 데이터의 분포가 명시적으로 존재하지 않습니다. Generator 와 Discriminator 의 균형이 깨지는 경우 학습이 원활이 이루어지지 않습니다. 또한, GAN 의 장점을 아래와 같이 정리합니다. 마르코프 체인 같은 구조 없이 역전파 만으로도 학습이 가능합니다. Generator 의 분포로 특별한 모델을 가정하지 않습니다. 더욱 복잡한 데이터 분포를 모사할 수 있어 선명한 데이터를 생성할 수 있습니다. Conclusions and future workGAN 프레임워크를 확장하고 개선할 수 있는 다양한 방법을 제시합니다. 주어진 조건에 따라 데이터를 생성하는 모델로 발전 가능 x가 주어졌을 때 z를 예측하는 보조 네트워크를 학습한다면 생성자의 데이터 분포를 예측할 수 있음 parameters를 공유하는 conditionals model를 학습함으로써 다른 conditionals models을 근사적으로 모델링할 수 있음 Semi-supervised learning에도 활용 가능 : 데이터가 제한된 경우 Discriminator 를 활용하여 classifier의 성능을 향상시킬 수 있음 효율성 개선: G와 D를 균형있게 학습할 수 있는 방법이나 새로운 z 분포를 제시하여 학습 속도 개선 가능 Summarize GAN 모델은 생성자(Generator)와 구분자(Discriminator) 둘의 적대적인 경쟁을 통해서 학습하는 딥러닝 네트워크 실제 우리가 학습시키려는 데이터와 생성자가 만든 Fake 데이터를 구분자에 모두 학습시켜서 구분을 더 잘 짓게 하는 방식으로이루어진네트워크이며, 생성자는랜덤노이즈를학습데이터와유사한패턴으로만들어주는네트워크구조를가진다. 이를 테스트하기 위해서 확인할 지표는 바이너리크로스엔트로피와 손실함수의 값이 구분자가 출력한 확률값이 정답에 가까우면 낮아지기 때문에 이것이 모델 학습의 목표가 된다. 구분자의 손실함수는 그래서 두가지 합인데 하나는 가짜이미지를 입력했을 때의 출력값과 1의차이, 그리고 가짜 이미지를 입력했을 때의 출력값과 0의 차이. 이 둘의 합이 구분자의 손실함수이며 이를 최소화하는 방향으로 구분자의 파라미터가 업데이트 된다. 이 업데이트는 최적화 함수를 통해 이루어진다. 데이터가 어떤 유형인지에 따라서 fake data를 어떤 것을 사용할지도 달라지는데 이 논문에서는 fake data를 데이터 분포를 통해서 샘플을 사용하며 이는 대체적으로 차원이 낮은 랜덤노이즈이다. 최악의 경우(max)를 가정했을 때 손실을 최소화(min)하는 것을 minimax게임이라고 하며 이것이 GAN 기저에 깔려있는 이론이라고 할 수 있다. GAN의 가장 큰문제는 학습환경이 매우 불안정하다는 것이다. 생성자와 구분자 둘 중에 하나가 실력이 월등이 좋아진다면 밸런스가 붕괴되고 모델이 정확히 학습되지 않고 학습이 완료된 후에도 mode dropping 이 생기는데 이는 구분자가 그 역할을 충분히 하지 못해 모델이 최적점까지 학습이 안 된 것이다. 이런 문제를 해결하기 위해 이후 논문에서 다양한 해결 방법이 제시된다. Link: Generative Adversarial Nets","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Generative Model","slug":"Paper/Generative-Model","permalink":"https://jmj3047.github.io/categories/Paper/Generative-Model/"}],"tags":[{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"},{"name":"GAN","slug":"GAN","permalink":"https://jmj3047.github.io/tags/GAN/"},{"name":"Generative Model","slug":"Generative-Model","permalink":"https://jmj3047.github.io/tags/Generative-Model/"}]},{"title":"Hexo Hueman Tutorial","slug":"Hexo_Hueman_tutorial","date":"2022-04-20T15:00:00.000Z","updated":"2023-02-28T07:50:13.607Z","comments":true,"path":"2022/04/21/Hexo_Hueman_tutorial/","link":"","permalink":"https://jmj3047.github.io/2022/04/21/Hexo_Hueman_tutorial/","excerpt":"","text":"1.Starting Hexo Blog1234567891011121314151617181920212223242526username@LAPTOP-D1EUIRLS MINGW64 ~/Desktop$ hexo init your_blog_folderusername@LAPTOP-D1EUIRLS MINGW64 ~/Desktop$ cd your_blog_folder/username@LAPTOP-D1EUIRLS MINGW64 ~/Desktop/your_blog_folder$ echo &quot;# your_blog_folder&quot; &gt;&gt; README.mdgit initgit add README.mdgit commit -m &quot;first commit&quot;git branch -M mastergit remote add origin https://github.com/your_id/your_blog_folder.gitgit push -u origin masterusername@LAPTOP-D1EUIRLS MINGW64 ~/Desktop/your_blog_folder (master)$ git add .username@LAPTOP-D1EUIRLS MINGW64 ~/Desktop/your_blog_folder (master)$ git commit -m &quot;updated&quot;username@LAPTOP-D1EUIRLS MINGW64 ~/Desktop/your_blog_folder (master)$ git pushusername@LAPTOP-D1EUIRLS MINGW64 ~/Desktop/your_blog_folder (master)$ code . 2.Applying Hueman Theme3.Basic Hexo Tutorial4.Hexo Tag Plugins5.Add Math Formula(without changing from Notion) Creat File name mathjax.ejs on themes/hueman/layout folder 123456789101112MathJax.Hub.Config(&#123; jax: [&quot;input/TeX&quot;, &quot;output/HTML-CSS&quot;], # mathjax tex2jax: &#123; inlineMath: [ [&#x27;$&#x27;, &#x27;$&#x27;] ], displayMath: [ [&#x27;$$&#x27;, &#x27;$$&#x27;]], processEscapes: true, skipTags: [&#x27;script&#x27;, &#x27;noscript&#x27;, &#x27;style&#x27;, &#x27;textarea&#x27;, &#x27;pre&#x27;, &#x27;code&#x27;] &#125;, messageStyle: &quot;none&quot;, &quot;HTML-CSS&quot;: &#123; preferredFont: &quot;TeX&quot;, availableFonts: [&quot;STIX&quot;,&quot;TeX&quot;] &#125;&#125;); Check #Plugins in themes/hueman/_config.yml file and change mathjax: false to true Add mathjax:true at the header when you post Reference: Math Formula 6.Font Change7.Deleting Posts8.Error in Hueman ThemeTo Be Continued..","categories":[{"name":"Setting","slug":"Setting","permalink":"https://jmj3047.github.io/categories/Setting/"}],"tags":[{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Hexo","slug":"Hexo","permalink":"https://jmj3047.github.io/tags/Hexo/"},{"name":"Hueman","slug":"Hueman","permalink":"https://jmj3047.github.io/tags/Hueman/"}]},{"title":"ImageNet Classification with Deep Convolutional Neural Networks","slug":"ImageNet_Classification","date":"2022-04-20T15:00:00.000Z","updated":"2022-10-15T08:04:25.032Z","comments":true,"path":"2022/04/21/ImageNet_Classification/","link":"","permalink":"https://jmj3047.github.io/2022/04/21/ImageNet_Classification/","excerpt":"","text":"Journal&#x2F;Conference: NIPSYear(published year): 2012Author: Alex Krizhevsky, Ilya Sutskever, Geoffrey E. HintonSubject: AlexNet, Computer Vision ImageNet Classification with Deep Convolutional Neural Networks Summary 기존 머신러닝 모델을 제치고 딥러닝 모델이 더 우수한 성능을 보일 수 있음을 증명한 최초의 모델 ReLU 활성화 함수와 Dropout 의 유용함, Data Augmentation 기법을 제시 2012년 ImageNet 대회 ILSVRC 에서 우승을 차지한 모델 IntroductionAlexNet 이전의 객체 인식 모델은 대부분 고전적인 ML 모델로, 수만개 정도의 작은 데이터셋(NORB, Caltech-101&#x2F;256, CIFAR-10&#x2F;100)을 사용합니다. 그러나 이후, 수십만 개의 완전 분할 된 이미지로 구성된 LabelMe 와 1500 만 개 이상의 고해상도 이미지로 구성된 ImageNet 이 등장합니다. 이런 데이터셋을 처리하기 위해서는 높은 학습 역량을 가진 모델이 필요합니다. 또한, 학습과정에 사용되지 않은 수많은 데이터에 대해서도 추론을 할 수 있는 방대한 사전 지식을 담아내야합니다. 이에 논문은 컨볼루션 신경망(CNN) 모델을 기반으로 하는 AlexNet 을 제시합니다. CNN 은 FFNN(feed-forward NN)에 비해 더 적은 매개 변수를 가지므로 훈련이 용이합니다. 이를 통해 ILSVRC-2010, ILSVRC-2012 대회에 사용된 ImageNet subset에서 최고의 성능을 달성했습니다. 또한, 네트워크 성능 향상과 훈련시간 감소를 위한 여러가지 방법을 제시합니다. ALexNet 은 2개의 GTX 580 3GB GPU에서 5-6 일동안 훈련을 수행하였습니다. The Dataset지금은 대부분의 딥러닝 모델에서 기본적으로 사용하는 ImageNet 에 대한 소개입니다. 22,000 개 범주로 구분되는 1,500 만개 고해상도 이미지 웹에서 수집한 이미지를 Amazon 의 Mechanical Turk 크라우드 소싱 도구로 라벨링 2010 년부터 Pascal Visual Object Challenge의 일환으로 ImageNet 대규모 시각 인식 도전 (ILSVRC)이라는 연례 대회가 열렸습니다. ILSVRC는 1000 개의 카테고리 각각에 약 1000 개의 이미지가있는 ImageNet의 하위 집합을 사용합니다. 이는 약 120 만 개의 훈련 이미지, 50,000 개의 검증 이미지, 150,000 개의 테스트 이미지로 구성됩니다. 대부분의 실험 결과는 테스트 이미지가 공개된 ILSVRC-2010 를 사용합니다. 별도로, AlexNet 이 참가했던 ILSVRC-2012 실험 결과 또한 제시합니다. ImageNet 데이터셋 성능 지표로는 Top-1&#x2F;Top-5 Accuracy 를 사용합니다. 가변 해상도로 구성된 ImageNet 을 처리하기 위해 256 × 256의 고정 해상도로 다운 샘플링을 수행합니다. 직사각형 이미지는 scaling 후 중앙 256x256 패치를 잘라냅니다. 이외의 전처리는 수행하지 않습니다. The ArchitectureReLU Nonlinearity논문 발표 당시 일반적으로 사용된 perceptron 의 activation 함수는 tanh 혹은 sigmoid 입니다. 이들은 출력값이 무한대로 발산하지 않고 특정한 영역으로 제한되는 saturating 함수입니다. 반면 ReLU(Recitified Linear Unit) activation 은 출력값이 0 에서 무한대까지 발산할 수 있는 non-saturating 함수입니다. 논문은 4 layer CNN 으로 CIFAR-10 데이터셋을 사용하여 학습하였을 때, ReLU 가 6배 빠른 학습 속도를 보여주었음을 제시합니다. 이를 통해, non-saturating 한 함수가 gradient 를 더 빠르게 update 할 수 있음을 제시합니다. Training on Multiple GPUsGPU 메모리 제한과 느린 학습 속도를 개선할 수 있는 병렬학습 방법을 제시합니다. 기본 골자는 네트워크를 분할(커널, 뉴런 등)하여 서로 다른 GPU 에서 병렬적으로 연산을 수행하는 것입니다. 이 때, 메모리의 한계 및 병목 현상을 고려하여, 특정한 레이어에서만 연산 결과를 교환합니다. 논문은 이를 통해 half-size kernel 을 사용한 단일 GPU 모델보다 Top-1&#x2F;Top-5 accuracy 를 1.7% &#x2F; 1.2% 감소시켰음을 제시합니다. Local Response NormalizationReLU 활성 함수는 입력을 normalization 하지 않아도 saturation 이 발생하지 않습니다. 그러나 positive value 를 그대로 출력하는 ReLU 함수의 특성으로 인해 CNN 의 일부 구역에서 강한 신호가 생성될 수 있습니다. 이에 논문은 아래와 같은 local response normalization 방법을 제시합니다. 요약하면, CNN 에서 인접한 필터를 사용하여 normalization 을 진행한 것으로, 논문에서는 Top-1&#x2F;Top-5 Accuracy 를 1.4%, 1.2% 개선할 수 있었음을 제시합니다. 또한, CIFAR-10 으로 학습을 수행하였을 때도 2% 의 오차율 감소를 보였음을 제시합니다.(논문 당시에는 Batch Normalization 이 소개되지 않았습니다.) Overlapping PoolingCNN의 풀링 레이어는 같은 채널에 존재하는 인접한 뉴런의 출력을 요약해줍니다. 논문 이전에는 pooling 을 수행하는 영역이 겹치지 않도록 구성하여 사용하는 것이 일반적이었습니다. 논문은 풀링을 수행하는 영역이 이동하는 거리를 조절하여 풀링 영역이 겹치도록 한 결과, Top-1&#x2F;Top-5 Accuracy 를 0.4 %&#x2F;0.3 % 감소했다고 합니다. 또한 이를 통해 모델의 과적합 가능성을 줄일 수 있었다고 합니다. Overall Architecture AlexNet 의 전체 구조도 입니다. 2GPU 로 병렬학습을 수행하기 위해 두 갈래로 나뉘어 표현되어 있습니다. 총 5개의 convolution layer 와 3개의 max pooling layer, 3개의 dense layer 로 구성되어 있으며, 필요한 경우에만 GPU 연산 결과를 공유합니다. 또한 convolution&#x2F;dense layer 의 활성함수는 ReLU 를 사용합니다. Input : 224 x 224 x 3 &#x3D; 150,528 Convolution 1 : 11x11 kernel, 4 stride : 54x54x96 Max pooling 1 : 3x3 kernel, 2 stride : 26x26x96 Convolution 2 : 5x5 kernel, 2 padding : 26x26x256 Max pooling 2 : 3x3 kernel, 2 stride : 12x12x256 Convolution 3 : 3x3 kernel, 1 padding : 12x12x384 Convolution 4 : 3x3 kernel, 1 padding : 12x12x384 Convolution 5 : 3x3 kernel, 1 padding : 12x12x384 Max pooling 3 : 3x3 kernel, 2 stride : 5x5x256 Dense 1 : 4096 Dense 2 : 4096 Dense 3 : 1000 Reducing Overfitting6천만개의 파라미터로 구성된 모델의 과적합을 막기 위해 사용한 방법을 소개합니다. Data Augmentation학습 데이터를 인위적으로 변환하여 훈련 데이터를 증가시키는 방법입니다. 변환된 이미지를 저장하지 않고 GPU 학습시에 CPU에서 계산하도록 하여, 추가적인 계산 비용을 소모하지 않았다고 합니다. 주요 방법은 두가지로 요약됩니다. 256 × 256 이미지에서 224 × 224 패치를 추출하고, 수평 방향으로 뒤짚기 기존 데이터 셋의 2048 배 확장 가능 실제 : 5 개의 224 × 224 패치 (4 개의 코너 패치 및 중앙 패치)와 수평 반사를 수행한 10개의 패치 사용 RGB 채널 강도 조정 학습 데이터셋의 픽셀값으로 PCA 를 수행 PCA eigenvector 에 N(0,0.1) 인 정규분포에 추출한 랜덤값을 곱해 색상을 조정 Top-1 오차율을 1% 감소할 수 있었음 DropoutDense Layer 의 Output 에 Dropout rate 0.5 를 사용한 Dropout layer 를 추가합니다. 학습에 필요한 Epoch 를 2배 정도 늘렸으나, 과적합을 성공적으로 방지했음을 제시합니다. Details of learning모델 학습의 세부내용입니다. Batch size : 128 SGD (momentum 0.9, weight decay 0.0005) weight decay 가 모델을 정규화 할 뿐만 아니라 직접적으로 모델의 학습 오차를 줄였음을 제시합니다. 가중치 업데이트 과정은 아래와 같습니다. 가중치는 평균이 0, 표준 편차가 0.01인 정규 분포를 따르도록 초기화합니다. 2&#x2F;4&#x2F;5 번째 convolution 과 dense layer의 bias 는 1로 초기화하여, 학습을 가속할 수 있었음을 제시합니다. 학습률은 모든 layer 에 대해서 동일하되, 훈련을 수행하면서 메뉴얼하게 조정합니다. 학습률 0.01 에서 시작하여, 학습이 개선되지 않을 때 학습률을 10으로 나누는 방식으로 수행합니다. RESULT ILSVRC-2010 데이터에 대해서 기존 모델의 결과를 압도적으로 상회하는 결과를 제시하였습니다. AlexNet 이 직접 참가했던 ILSVRC-2012 에서도 다른 최고 성능의 모델에 비해 압도적인 결과를 보였음을 확인할 수 있습니다. 또한, CNN Layer 갯수를 추가할 때마다 성능이 상승함을 제시합니다. Qualitative Evaluations CNN kernel 을 시각화한 그림을 제시하면서, 각 커널이 이미지의 다양한 Feature 를 효과적으로 추출해냈음을 제시합니다. AlexNet 은 중앙을 벗어나는 데이터도 효과적으로 분류해냈습니다. 또한, Top-5 예측이 대부분 유사한 범주인 것으로 보아 합리적인 예측을 수행하고 있음을 제시합니다. 또한, 자세가 서로 다른 코끼리의 사례와 같이, Pixel 차원에서 완전히 다른 데이터임에도 유사한 범주로 분류할 수 있는 결과를 보여줍니다. Discussion“깊은” CNN 이 효과적으로 작동하였음을 제시합니다. Convolution layer 를 제거할 때마다 Top-1 Accuracy 가 2%씩 감소하는 점에 미루어, “깊이”의 중요성을 다시 한번 제시합니다. Link: ImageNet Classification with Deep Convolutional Neural Networks","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Computer Vision","slug":"Paper/Computer-Vision","permalink":"https://jmj3047.github.io/categories/Paper/Computer-Vision/"}],"tags":[{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"},{"name":"Convolutional Neural Networks","slug":"Convolutional-Neural-Networks","permalink":"https://jmj3047.github.io/tags/Convolutional-Neural-Networks/"},{"name":"Image Classification","slug":"Image-Classification","permalink":"https://jmj3047.github.io/tags/Image-Classification/"}]},{"title":"K-Nearest Neighbor","slug":"KNN","date":"2022-04-20T15:00:00.000Z","updated":"2023-02-23T01:34:13.754Z","comments":true,"path":"2022/04/21/KNN/","link":"","permalink":"https://jmj3047.github.io/2022/04/21/KNN/","excerpt":"","text":"1. Classification 분류나 예측을 진행할때 나랑 가장 가까운 이웃 k개를 고려하겠다. 나랑 가까운 이웃 한명이 검정색이면 검정색으로 판단 파란색의 가장 가까운 이웃을 확인해본 결과 검정색 이므로 파란색도 검정색으로 분류되었다 K&#x3D;3 일 경우 형광색 친구를 분류한다고 하였을때 이웃중 파란색이 2개 검정색이 한개이기 때문에 파란색으로 분류된다. 분류를 원하는 관측치의 주변 N개의 데이터(근접 이웃)을 골라서, 주변대세를 확인 (다수결의 원칙으로) 2. Prediction 인접 K개의 데이터의 수치를 확인해줘서 그 데이터의 평균을 검은점의 예측치로 설정해준다. 3. How to find optimal k?k의 결정 k가 너무 큰 경우, KNN모델이 지나치게 일반화됨 K가 너무 작은 경우,KNN 모델의 예측 결과의 분산이 큼 주로 이것저것 해보고 error이 가장 작은 k를 설정하여준다. 거리 척도의 결정 상황에 맞는 거리척도를 사용하여야 한다. 거리척도의 종류:Minkowski distance , Euclidean distance, Citi block distance, Mahalanobis distance, Correlation distance 등 Reference: 한국공학대학교 강지훈교수님 강의","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"}],"tags":[{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"}]},{"title":"Pyspark Tutorial(1)","slug":"Pyspark_Tutorial_1","date":"2022-04-20T15:00:00.000Z","updated":"2022-04-22T07:33:10.000Z","comments":true,"path":"2022/04/21/Pyspark_Tutorial_1/","link":"","permalink":"https://jmj3047.github.io/2022/04/21/Pyspark_Tutorial_1/","excerpt":"","text":"Reference: https://spark.apache.org/docs/latest/quick-start.html Get Started01.basic.py1234567891011121314# -*- coding: utf-8 -*-import pysparkprint(pyspark.__version__)from pyspark.sql import SparkSession#스파크 세션 초기화 :spark session이 하나 만들어진것spark = SparkSession.builder.master(&#x27;local[1]&#x27;).appName(&#x27;SampleTutorial&#x27;).getOrCreate()rdd = spark.sparkContext.parallelize([1,2,3,4,5])print(&quot;rdd Count&quot;, rdd.count())spark.stop() 02.rating.py1234567891011121314151617181920212223242526272829303132333435#SparkContext#RDDfrom pyspark import SparkConf, SparkContextimport collectionsprint(&quot;Hello&quot;)def main(): # MasterNode = local # MapReduce conf = SparkConf().setMaster(&quot;local&quot;).setAppName(&#x27;RatingHistogram&#x27;) sc = SparkContext(conf = conf) lines = sc.textFile(&quot;ml-100k/u.logs&quot;) #print(lines) ratings = lines.map(lambda x: x.split()[2]) #print(&quot;ratings:&quot;,ratings) #rdd라는 객체가 만들어진것 result = ratings.countByValue() #print(&quot;result:&quot;,result) #정렬하기 sortedResults = collections.OrderedDict(sorted(result.items())) for key, value in sortedResults.items(): print(&quot;%s %i&quot; % (key, value)) if __name__ == &quot;__main__&quot;: main() ##spark를 쓰는 이유:로그 데이터를 가져와서 규칙을 대입해서 정렬한다음에 정형데이터로 치환하기위해#실제로 의미있는 로그라면 분석도 의미가 있다#분석과 로그데이터를 처리할 수 있는 환경을 지원해줌#과거에는 두개가 따로 있었음 03.dataloading.py1234567891011121314151617181920212223242526272829303132333435363738394041424344#Spark SQL 적용#Spark Sessionfrom pyspark.sql import SparkSession# #스파크 세션 생성# my_spark = SparkSession.builder.getOrCreate()# print(my_spark)# #테이블을 확인하는 코드# print(my_spark.catalog.listDatabases())# #show database# my_spark.sql(&quot;show databases&quot;).show()# #check current DB# my_spark.catalog.currentDatabase()# my_spark.stop()#loading csv filespark = SparkSession.builder.master(&#x27;local[1]&#x27;).appName(&quot;DBTutorial&quot;).getOrCreate()flights = spark.read.option(&#x27;header&#x27;,&#x27;true&#x27;).csv(&#x27;data/flight_small.csv&#x27;)#flights.show(4)#spark.catalog.currentDatabase()#flights 테이블을 default DB에 추가함flights.createOrReplaceTempView(&#x27;flights&#x27;)#print(spark.catalog.listTables(&#x27;default&#x27;))#spark.sql(&#x27;show tables from default&#x27;).show()#쿼리 통해서 데이터 저장query = &quot;FROM flights SELECT * LIMIT 10&quot;query2 = &quot;SELECT * FROM flights LIMIT 10&quot;# 스파크 세션 할당flights10 = spark.sql(query2)#flights10.show()#spark 데이터 프레임을 pandas data frame으로 변환import pandas as pdpd_flights10 = flights10.toPandas()print(pd_flights10.head()) 04.struct_type.py Reference: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.types.StructType.html 1234567891011121314151617181920212223242526272829303132from pyspark.sql import SparkSessionfrom pyspark.sql import functions as func #aliasfrom pyspark.sql.types import StructType, StructField, IntegerType, LongTypeprint(&quot;Hello&quot;)#세션 할당spark = SparkSession.builder.appName(&quot;PopularMovies&quot;).getOrCreate()#스키마 작성(u.logs 데이터)schema = StructType( [ StructField(&quot;userID&quot;,IntegerType(), True) , StructField(&quot;movieID&quot;,IntegerType(), True) , StructField(&quot;rating&quot;, IntegerType(), True) , StructField(&quot;timestamp&quot;, LongType(), True) ])print(&quot;Schema is done&quot;)#데이터 불러오기movies_df = spark.read.option(&quot;sep&quot;,&quot;\\t&quot;).schema(schema).csv(&quot;ml-100k/u.logs&quot;)#내림차순으로 인기 있는 영화 정렬#movieID로 그룹바이, count() 진행, orderbytopMovieIds = movies_df.groupby(&quot;movieID&quot;).count().orderBy(func.desc(&#x27;count&#x27;))print(topMovieIds.show(10))#세션 종료spark.stop() 05.advance_structtype.py1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162from pyspark.sql import SparkSessionfrom pyspark.sql import functions as func #aliasfrom pyspark.sql.types import StructType, StructField, IntegerType, LongTypeimport codecsprint(&quot;Starting Session&quot;)def loadMovieNames(): #u.item에서 영화 이름 가져옴 movieNames = &#123;&#125; with codecs.open(&quot;ml-100k/u.item&quot;,&quot;r&quot;, encoding=&quot;ISO-8859-1&quot;, errors =&quot;ignore&quot;) as f: for line in f: fields = line.split(&quot;|&quot;) movieNames[int(fields[0])] = fields[1] #데이터 추가하는 딕셔너리 문법 return movieNames #세션 할당spark = SparkSession.builder.appName(&quot;PopularMovies&quot;).getOrCreate()#파이썬 딕셔너리 객체를 spark 객체로 변환nameDict = spark.sparkContext.broadcast(loadMovieNames())#스키마 작성(u.logs 데이터)schema = StructType( [ StructField(&quot;userID&quot;,IntegerType(), True) , StructField(&quot;movieID&quot;,IntegerType(), True) , StructField(&quot;rating&quot;, IntegerType(), True) , StructField(&quot;timestamp&quot;, LongType(), True) ])print(&quot;Schema is done&quot;)#데이터 불러오기movies_df = spark.read.option(&quot;sep&quot;,&quot;\\t&quot;).schema(schema).csv(&quot;ml-100k/u.logs&quot;)#내림차순으로 인기 있는 영화 정렬할 필요 없음#movieID로 그룹바이, count() 진행, orderby#topMovieIds = movies_df.groupby(&quot;movieID&quot;).count().orderBy(func.desc(&#x27;count&#x27;))topMovieIds = movies_df.groupby(&quot;movieID&quot;).count()# 딕셔너리 # key-value# 키값을 알면 value자동으로 가져옴(movietitle)def lookupName(movieID): return nameDict.value[movieID]# 사용자 정의 함수 사용할 때 쓰는 spark 문법lookupNameUDF = func.udf(lookupName)# MovieTitle을 기존 topMovieIds 데이터에 추가#컬럼 추가moviesWithNames = topMovieIds.withColumn(&quot;movietitle&quot;,lookupNameUDF(func.col(&quot;movieID&quot;)))#정렬final_df = moviesWithNames.orderBy(func.desc(&quot;count&quot;))print(final_df.show(10))#세션 종료spark.stop()","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Pyspark","slug":"Python/Pyspark","permalink":"https://jmj3047.github.io/categories/Python/Pyspark/"}],"tags":[{"name":"pyspark","slug":"pyspark","permalink":"https://jmj3047.github.io/tags/pyspark/"}]},{"title":"Pyspark Tutorial(2)","slug":"Pyspark_Tutorial_2","date":"2022-04-20T15:00:00.000Z","updated":"2022-04-22T07:33:14.000Z","comments":true,"path":"2022/04/21/Pyspark_Tutorial_2/","link":"","permalink":"https://jmj3047.github.io/2022/04/21/Pyspark_Tutorial_2/","excerpt":"","text":"Data cleansing01.pipeline.py123456789101112131415161718192021222324from pyspark.sql import SparkSessionfrom pyspark.sql import *from pyspark.sql import functions as F#Create Spark Sessionspark = SparkSession.builder.master(&quot;local[1]&quot;).appName(&quot;MLSampleTutorial&quot;).getOrCreate()#Load Datadf = spark.read.csv(&quot;data/AA_DFW_2015_Departures_Short.csv.gz&quot;, header = True)print(&quot;file loaded&quot;)print(df.show())#remove duration = 0df = df.filter(df[3] &gt; 0) #Actual elapsed time (Minutes) 여기 컬럼 값이 0보다 작은건 보여주지 않음# df.show()#ADD ID columndf = df.withColumn(&#x27;id&#x27;,F.monotonically_increasing_id()) #id값을 자동으로 넣어줌df.write.csv(&quot;data/output.csv&quot;, mode = &#x27;overwrite&#x27;)spark.stop() 02.total_spent.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# #라이브러리 불러오기# from pyspark import SparkConf, SparkContext# #사용자 정의 함수# #main함수# def main():# conf = SparkConf.setMaster(&quot;local&quot;).setAppName(&#x27;SpentbyCustomer&#x27;)# sc = SparkContext(conf= conf)# # 파이썬 코드# # 실행코드 작성# if __name__ == &quot;__main__&quot;:# main()########## 이게 spark 기본 세팅 ##########라이브러리 불러오기from pyspark import SparkConf, SparkContext#사용자 정의 함수def extractCusPrice(line): fields = line.split(&quot;,&quot;) return(int(fields[0]), float(fields[2]))#main함수def main(): #스파크 설정 conf = SparkConf().setMaster(&quot;local&quot;).setAppName(&#x27;SpentbyCustomer&#x27;) sc = SparkContext(conf= conf) #데이터 불러오기 input = sc.textFile(&#x27;data/customer-orders.csv&#x27;) #print(&#x27;is data?&#x27;) mappedInput = input.map(extractCusPrice) #튜플 형태로 나옴 totalByCustomer = mappedInput.reduceByKey(lambda x, y : x + y) #정렬 flipped = totalByCustomer.map(lambda x: (x[1], x[0])) totalByCustomerSorted = flipped.sortByKey() results = totalByCustomerSorted.collect() for result in results: print(result) #파이썬 코드 # 실행코드 작성if __name__ == &quot;__main__&quot;: main() 03.friends_by_age.py123456789101112131415161718from pyspark import SparkConf, SparkContextconf = SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;FriendsByAge&quot;)sc = SparkContext(conf = conf)def parseLine(line): fields = line.split(&#x27;,&#x27;) age = int(fields[2]) numFriends = int(fields[3]) return (age, numFriends)lines = sc.textFile(&quot;data/fakefriends.csv&quot;)rdd = lines.map(parseLine)totalsByAge = rdd.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))averagesByAge = totalsByAge.mapValues(lambda x: x[0] / x[1])results = averagesByAge.collect()for result in results: print(result) 04.min_temp.py123456789101112131415161718192021222324252627282930313233#온도를 측정하는 프로그램 만들기from dataclasses import fieldfrom pyspark import SparkConf, SparkContextconf = SparkConf().setMaster(&#x27;local&#x27;).setAppName(&#x27;MinTemperatures&#x27;) #마스터 노드에다가 올린다sc = SparkContext(conf = conf)print(&quot;Start&quot;)def parseLine(line): fields = line.split(&quot;,&quot;) #쉼표로 다 끊어줌 -&gt; 리스트로 반환됨 stationID = fields[0] entryType = fields[2] temperature = float(fields[3]) * 0.1 * (9.0/5.0) + 32.0 return (stationID, entryType, temperature)lines = sc.textFile(&#x27;data/1800.csv&#x27;)#print(lines)parseLines = lines.map(parseLine)#print(parseLine)minTemps = parseLines.filter(lambda x: &quot;TMIN&quot; in x[1])stationTemps = minTemps.map(lambda x: (x[0],x[2]))minTemps = stationTemps.map(lambda x, y: min(x, y))results = minTemps.collect()#print(results)for result in results: print(result[0]+ &quot;\\t&#123;:.2f&#125;F&quot;.format(result[1]))","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Pyspark","slug":"Python/Pyspark","permalink":"https://jmj3047.github.io/categories/Python/Pyspark/"}],"tags":[{"name":"pyspark","slug":"pyspark","permalink":"https://jmj3047.github.io/tags/pyspark/"}]},{"title":"Pyspark Tutorial(3)","slug":"Pyspark_Tutorial_3","date":"2022-04-20T15:00:00.000Z","updated":"2022-04-22T07:33:20.000Z","comments":true,"path":"2022/04/21/Pyspark_Tutorial_3/","link":"","permalink":"https://jmj3047.github.io/2022/04/21/Pyspark_Tutorial_3/","excerpt":"","text":"Machine Learning01.regression.py1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950from pyspark.ml.regression import DecisionTreeRegressorfrom pyspark.sql import SparkSessionfrom pyspark.ml.feature import VectorAssemblerprint(&quot;Starting Session&quot;)#세션 할당spark = SparkSession.builder.appName(&quot;DecisionTree&quot;).getOrCreate()#데이터 불러오기#StructType 과정 생략 가능data = spark.read.option(&quot;header&quot;, &quot;true&quot;).option(&quot;inferSchema&quot;, &quot;true&quot;).csv(&quot;data/realestate.csv&quot;)#print(data.show())#데이터 프레임을 행렬로 변환assembler = VectorAssembler().setInputCols([&quot;HouseAge&quot;, &quot;DistanceToMRT&quot;,&quot;NumberConvenienceStores&quot;]).setOutputCol(&quot;features&quot;) #데이터 컬럼 값 아무거나 넣어도 됨#타겟 데이터 설정df = assembler.transform(data).select(&quot;PriceofUnitArea&quot;,&quot;features&quot;)#데이터 분리trainTest = df.randomSplit([0.5,0.5])trainingDF = trainTest[0]testDF = trainTest[1]#Decision Tree 클래스 정의dtr = DecisionTreeRegressor().setFeaturesCol(&quot;features&quot;).setLabelCol(&quot;PriceofUnitArea&quot;)#모델 학습model = dtr.fit(trainingDF)#print(model)#모델 예측fullPredictions = model.transform(testDF).cache()#예측값과 label확인predictions = fullPredictions.select(&quot;prediction&quot;).rdd.map(lambda x: x[0])#실제데이터labels = fullPredictions.select(&quot;PriceofUnitArea&quot;).rdd.map(lambda x: x[0])#예측값과 label을 zip으로 묶어줌preds_label = predictions.zip(labels).collect()for prediction in preds_label: print(prediction)#세션 종료spark.stop() 02.logistic_regression.py1234567891011121314151617181920212223from pyspark.sql import SparkSessionfrom pyspark.ml.classification import LogisticRegression #Important# 세션 할당spark = SparkSession.builder.appName(&quot;AppName&quot;).getOrCreate()# load Datatraining = spark.read.format(&quot;libsvm&quot;).load(&quot;data/mllib/sample_libsvm_data.txt&quot;)print(&quot;Data loaded&quot;)# model# Scikit-Learn 문법과 비슷mlr = LogisticRegression() # Importantmlr_model = mlr.fit(training) # Important# 로지스텍 회귀, 선형 모델 .. 계수와 상수를 뽑아낼 수 있음print(&quot;Coefficients :&quot; + str(mlr_model.coefficients))print(&quot;Intercept :&quot; + str(mlr_model.intercept))spark.stop() 03.pipeline.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657from tokenize import Tokenfrom pyspark.ml import Pipelinefrom pyspark.ml.classification import LogisticRegressionfrom pyspark.ml.feature import HashingTF, Tokenizerfrom pyspark.sql import SparkSession# 세션 할당 spark = SparkSession.builder.appName(&quot;MLPipeline&quot;).getOrCreate()# 가상의 데이터 만들기training = spark.createDataFrame([ (0, &quot;a b c d e spark&quot;, 1.0), (1, &quot;b d&quot;, 0.0), (2, &quot;spark f g h&quot;, 1.0), (3, &quot;hadoop mapreduce&quot;, 0.0)], [&quot;id&quot;, &quot;text&quot;, &quot;label&quot;])# Feature Engineering# 1. Prparation# step01. 텍스트를 단어로 분리tokenizer = Tokenizer(inputCol=&#x27;text&#x27;, outputCol=&#x27;words&#x27;)# step02. 변환된 텍스트를 숫자로 변환hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=&quot;features&quot;)# step03. 모델을 가져옴lr = LogisticRegression(maxIter=5, regParam=0.01)# 2. Starting pipeplinepipeline = Pipeline(stages=[tokenizer, hashingTF, lr])# 3. Model Trainingmodel = pipeline.fit(training)# 4. Prepare test documents, which are unlabeled (id, text) tuples.test = spark.createDataFrame([ (4, &quot;spark i j k&quot;), (5, &quot;l m n&quot;), (6, &quot;spark hadoop spark&quot;), (7, &quot;apache hadoop&quot;)], [&quot;id&quot;, &quot;text&quot;])# 5. Predictionprediction = model.transform(test)selected = prediction.select(&quot;id&quot;, &quot;text&quot;, &quot;probability&quot;, &quot;prediction&quot;)for row in selected.collect(): row_id, text, prob, prediction = row #튜플 형태로 반환 print( # 문자열 포맷팅 &quot;(%d, %s) -------&gt; probability=%s, prediction=%f&quot; % (row_id, text, str(prob), prediction) )# training.show()# 세션 종료spark.stop() 04.randomforest.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960from cProfile import labelfrom pyspark.sql import SparkSession# 머신러닝 라이브러리from pyspark.ml import Pipelinefrom pyspark.ml.classification import RandomForestClassifierfrom pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexerfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator# 데이터 불러오기 spark = SparkSession.builder.appName(&quot;RandomForest&quot;).getOrCreate()data = spark.read.format(&quot;libsvm&quot;).load(&quot;data/mllib/sample_libsvm_data.txt&quot;)print(type(data))# Feature Engineering# label column labelIndexer = StringIndexer(inputCol=&#x27;label&#x27;, outputCol=&#x27;indexedLabel&#x27;).fit(data)# 범주형 데이터 체크, 인덱스화featureIndexer = VectorIndexer(inputCol=&#x27;features&#x27;, outputCol=&#x27;IndexedFeatures&#x27;, maxCategories=4).fit(data)# 데이터 분리(trainingData, testData) = data.randomSplit([0.7, 0.3])# 모델 rf = RandomForestClassifier(labelCol=&#x27;indexedLabel&#x27;, # 종속변수 featuresCol=&#x27;IndexedFeatures&#x27;, # 독립변수 numTrees=10)# outputCol=&#x27;indexedLabel&#x27; --&gt; original label로 변환labelConvereter = IndexToString(inputCol=&#x27;prediction&#x27;, outputCol=&#x27;predictedLabel&#x27;, labels=labelIndexer.labels)# 파이프라인 구축pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConvereter])# 모델 학습model = pipeline.fit(trainingData)# 모델 예측predictions = model.transform(testData)# 행에 표시할 것 추출 predictions.select(&quot;predictedLabel&quot;, &#x27;label&#x27;, &#x27;features&#x27;).show(5)# 모형 평가evaluator = MulticlassClassificationEvaluator( labelCol=&quot;indexedLabel&quot;, predictionCol=&quot;prediction&quot;, metricName=&quot;accuracy&quot;)accuracy = evaluator.evaluate(predictions)print(&quot;Test Error = %f &quot; % (1.0 - accuracy))spark.stop()","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Pyspark","slug":"Python/Pyspark","permalink":"https://jmj3047.github.io/categories/Python/Pyspark/"}],"tags":[{"name":"pyspark","slug":"pyspark","permalink":"https://jmj3047.github.io/tags/pyspark/"}]},{"title":"How to install PySpark","slug":"install_PySpark","date":"2022-04-18T15:00:00.000Z","updated":"2022-04-22T02:53:55.000Z","comments":true,"path":"2022/04/19/install_PySpark/","link":"","permalink":"https://jmj3047.github.io/2022/04/19/install_PySpark/","excerpt":"","text":"Preparation installing spark need python3 if you are first using python, install anaconda Installing JAVA Installing file: Java SE 8 Archive Downloads (JDK 8u211 and later) Need to login Oracle Run the download file as admin → Click Next button → Changing the path on file (Space between words like Program Files can be problem during installation) Changing Path Same changes to folders in the JAVA runtime environment folder (Click ‘Change’ and modify) Create and save jre folder in the path right after the C dirve Installing Spark Installing site: https://spark.apache.org/downloads.html Download installation file After clicking Download Spark: [spark-3.2.0-bin-hadoop3.2.tgz](https://www.apache.org/dyn/closer.lua/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz), you can download it by clicking the HTTP 하단 page like picture below Installation URL: https://www.apache.org/dyn/closer.lua/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz (2022.01) Download WinRAR Program You need to install WinRAR, to unzip .tgz file. Installation file: https://www.rarlab.com/download.htm Install what fits your computer Create Spark folder and move files Moving files Copy all the file in spark-3.2.0-bin-hadoop3.2 folder After that, create spark folder below C drive and move all of them to it. Modify log4j.properties file • Open the fileconf - [log4j.properties](http://log4j.properties) Open the log file as notebook and change INFO → ERROR just like example below. During the process, all the output values can be removed. 1234567# Set everything to be logged to the console# log4j.rootCategory=INFO, consolelog4j.rootCategory=ERROR, consolelog4j.appender.console=org.apache.log4j.ConsoleAppenderlog4j.appender.console.target=System.errlog4j.appender.console.layout=org.apache.log4j.PatternLayoutlog4j.appender.console.layout.ConversionPattern=%d&#123;yy/MM/dd HH:mm:ss&#125; %p %c&#123;1&#125;: %m%n Installing winutils This time, we need program that makes local computer mistakes Sparks for Hadoop. Installing file: https://github.com/cdarlint/winutils Download winutils programs that fit installation version. I downloaded version 3.2.0 Create winutils&#x2F;bin folder on C drive and save the downloaded file. Ensure this file is authorized to be used so that it can be executed without errors whne running Spark This time, open CMD as admin and run the file If ChangeFileModeByMask error (3) occurs, create tmp\\hive folder below C drive. 12C:\\Windows\\system32&gt;cd c:\\winutils\\binc:\\winutils\\bin&gt; winutils.exe chmod 777 \\tmp\\hive Setting environment variables Set the system environment variable Click the 사용자 변수 - 새로 만들기 button on each user account Set SPARK_HOME variable Set JAVA_HOME variable Set HADOOP_HOME variable Edit PATH variable. Add the code below. Add code below %SPARK_HOME%\\bin %JAVA_HOME%\\bin Testing Spark Open CMD file, set the path as c:\\spark folder if the logo appears when input ‘spark’, success Check whether the code below works 1234&gt;&gt;&gt; rd = sc.textFile(&quot;README.md&quot;)&gt;&gt;&gt; rd.count()109&gt;&gt;&gt;","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Pyspark","slug":"Python/Pyspark","permalink":"https://jmj3047.github.io/categories/Python/Pyspark/"}],"tags":[{"name":"pyspark","slug":"pyspark","permalink":"https://jmj3047.github.io/tags/pyspark/"}]},{"title":"Adversarial Speaker Verification","slug":"Adversaria 2d6a8","date":"2022-04-16T15:00:00.000Z","updated":"2022-10-15T08:03:42.984Z","comments":true,"path":"2022/04/17/Adversaria 2d6a8/","link":"","permalink":"https://jmj3047.github.io/2022/04/17/Adversaria%202d6a8/","excerpt":"","text":"Journal&#x2F;Conference: ICASSP IEEEYear(published year): 2019Author: Zhong Meng, Yong Zhao, Jinyu Li, Yifan GongSubject: Speaker Verification Adversarial Speaker Verification GoalWith ASV, our goal is to learn a condition-invariant and speaker-discriminative deep hidden feature in the background DNN through adversarial multi-task learning such that a noise-robust deep embedding can be obtained from these deep features for an enrolled speaker or a test utterance. Data“Hey Cortana” from the Windows 10 desktop Cortana service logs.CHiME-3: buses (BUS), in cafes (CAF), in pedestrian areas (PED), at street junctions (STR))From the clean Cortana data, we select 6 utterances from each of the 3k speakers as the enrollment data (called “Enroll A”). We select 60k utterances from 3k target speakers and 3k impostors in Cortana dataset and mix them with CHiME-3 real noise to generate the noisy evaluation set. Result Why? In ASV, a speaker classification network and a condition identification network are jointly trained to minimize the speaker classification loss and to mini-maximize the condition loss through adversarial multitask learning.The target labels of the condition network can be categorical (environment types) and continuous (SNR values). With ASV, speaker-discriminative and condition-invariant deep embeddings can be extracted for both enrollment and test speech. 적대적 학습은 [22] 논문에서 먼저 적용되었는데 이 논문과의 차이점은, 두가지 소음 컨디션을 서로 다른 방법으로 막은 것(22 논문에서는 환경 개선 보다는 unlabeled 타겟 도메인 데이터를 훈련하여 적응 시키는 걸 목표로 함), 그리고 본 논문은 네트워크에 직접적으로 음성 피처를 인풋으로 넣어 훈련하는 반면, 22 논문은 i-벡터를 인풋으로 넣었고 이는 computational한 시간과 자원이 더 들어감. ** Train ASV model to adapt to noise ** Experiment Embeddings : 인공신경망에서 원래 차원보다 저차원의 벡터로 만드는 것을 의미원래 차원은 매우 많은 범주형 변수들로 구성되어 있고 이것들이 학습방식을 통해 저차원으로 대응됨. 수천 수만개의 고차원 변수들을 몇백개의 저차원 변수로 만들어 주고, 또한 변형된 저차원 공간에서도 충분히 카테고리형 의미를 내재함.출처: 인공신경망(딥러닝)의 Embedding 이란 무엇일까? - 임베딩의 의미(1&#x2F;3) 훈련단계에서 background DNN을 화자들을 구별하기 위해 훈련 시킴. F &#x3D; {f1 ,…, fT }, ft ∈ Rrf : deep hidden featuresX &#x3D; {x1 ,…, xT}, xt ∈ Rrx , t &#x3D; {1 ,…, T} : input speech frames from training set to intermediate deep hidden featuresΘf: parameters maps input speech framesMf: the hidden layers of the background DNN as a feature extractor network with parameters Θf P(a|ft;Θy), a ∈ A : Speaker posteriors, where A is the set of all speakers in the training setΘy: maps the deep features F to the speaker posteriors.My: the upper layers of the background DNN as a speaker classifier network with parameters Θy Θf and Θy are optimized by Minimizing cross entropy loss of speaker classification. Y &#x3D; {y1 ,…, yT }, yt ∈A : sequence of speaker labels aligned with X1[.]: indicator function equals to 1 if the condition in the bracket is satisfied and 0 other wise. Categorical Condition Classification Loss: to address the conditions that are characterized as a categorical variable additional condition classification network Mc: which predicts the condition posteriors p(b| ft;Θf ); b ∈ B given the deep features F from the training setB : the set of all conditions in the training set With a sequence of condition labels C &#x3D; {c1 ,…, cT} that is aligned with X, compute the condition classification loss through cross-entropy Continuous Condition Regression Loss: an additional condition regression network Mc to predict the frame-level condition value (SNR value) compute the condition regression loss through mean-square error Deep feature F 를 condition invariant 하게 만들려면, 소음들 각각의 환경에서 나오는 피처들의 차이가 최대한 적어야 함.따라서 Mf 와 Mc 는 같이 적대적으로 train 하게 되고, Θf 가 frame-level condition loss, Lcondition 을 최대화 시키고 Θc가 Lcondition을 최소화 시키는 방향으로 감.이 둘의 경쟁은 처음에 Mc에 대한 차별성을 높여주고, speaker invariance 의 deep feature 가 Mf에 의해 만들어짐.결국 Mf가 극단적으로 Mc가 구별하지 못하는 피처를 만드는 지점에 수렴.그와 동시에 논문에서는 화자 차별적인 deep feature 들을 Lspeaker(Eq3)의 speaker classification 손실함수를 최소화 하면서 만듦. 최적의 파라미터를 찾는 식: 여기서 λ가 speaker classification 손실함수와 condition 함수 사이의 균형을 통제.GRL은 forward propagation 에서 identity transform 역할을 하며 back propagation 에서 경사도를 – λ로 곱함. Link: ADVERSARIAL SPEAKER VERIFICATION","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speaker Verification","slug":"Paper/Speaker-Verification","permalink":"https://jmj3047.github.io/categories/Paper/Speaker-Verification/"}],"tags":[{"name":"Adversarial Speaker Verification","slug":"Adversarial-Speaker-Verification","permalink":"https://jmj3047.github.io/tags/Adversarial-Speaker-Verification/"},{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"}]},{"title":"Definition of Distance","slug":"Definition_of_Distance","date":"2022-04-16T15:00:00.000Z","updated":"2023-02-23T01:33:50.993Z","comments":true,"path":"2022/04/17/Definition_of_Distance/","link":"","permalink":"https://jmj3047.github.io/2022/04/17/Definition_of_Distance/","excerpt":"","text":"1. Euclidean distance 가장 흔히 사용하는 거리측도 대응되는 x,y값 간 차이 제곱합의 제곱근으로써, 두 관측치 사이의 직선 거리를 의미함. 다차원 데이터에서도 마찬가지 이다. 2. Manhattan Distance 맨하탄은 블럭이 나누어져 있어 직선으로 갈 수가 없다. 직선거리가 아닌 격자거리. 격자:바둑판처럼 가로세로를 일정한 간격으로 직각이 되게 짠 구조나 물건. 각 좌표의 차이의 절댓값의 합 3. Mahalanobis Distance 변수 내 분산,변수 간 공분산을 모두 반영하여 x,y,간 거리를 계산하는 방식⇒변수간 상관관계를 고려한 거리지표이다. 데이터의 공분산 행렬이 단위행렬인 경우는 유클리디안 거리와 동일함 공분산 행렬의 역행렬을 취했다는 것 → 분산이 분모에 들어간다는 뜻 → 분산이 커지면 거리가 작아지고 , 분산이 작아지면 거리가 길어짐 마할라노비스거리가 제곱근이 취해져 있기 때문에 제곱근을 없앴다. 2차원 행렬로 비유를 했을시 , 쭈욱 대입하면 아래의 식으로 나타난다 y값에 0,0 을주고 대입하면 타원의 방정식이 나온다. 유클리디안 관점에서는 중앙점과 비교했을때, A가 더 멀다. 상관관계를 고려한 마할라노비스 거리로 보면 B가 더 멀다 Reference: 한국공학대학교 강지훈교수님 강의","categories":[{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"}],"tags":[{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"}]}],"categories":[{"name":"Data Base","slug":"Data-Base","permalink":"https://jmj3047.github.io/categories/Data-Base/"},{"name":"GCP","slug":"Data-Base/GCP","permalink":"https://jmj3047.github.io/categories/Data-Base/GCP/"},{"name":"Setting","slug":"Setting","permalink":"https://jmj3047.github.io/categories/Setting/"},{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://jmj3047.github.io/categories/Data-Analysis/"},{"name":"Basic","slug":"Data-Analysis/Basic","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Basic/"},{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Generative Model","slug":"Paper/Generative-Model","permalink":"https://jmj3047.github.io/categories/Paper/Generative-Model/"},{"name":"Multi-Task Learning","slug":"Paper/Multi-Task-Learning","permalink":"https://jmj3047.github.io/categories/Paper/Multi-Task-Learning/"},{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Python","slug":"Python/Python","permalink":"https://jmj3047.github.io/categories/Python/Python/"},{"name":"Model","slug":"Data-Analysis/Model","permalink":"https://jmj3047.github.io/categories/Data-Analysis/Model/"},{"name":"Speaker Verification","slug":"Paper/Speaker-Verification","permalink":"https://jmj3047.github.io/categories/Paper/Speaker-Verification/"},{"name":"NLP","slug":"Paper/NLP","permalink":"https://jmj3047.github.io/categories/Paper/NLP/"},{"name":"Django","slug":"Python/Django","permalink":"https://jmj3047.github.io/categories/Python/Django/"},{"name":"HTML","slug":"Python/HTML","permalink":"https://jmj3047.github.io/categories/Python/HTML/"},{"name":"MongoDB","slug":"Data-Base/MongoDB","permalink":"https://jmj3047.github.io/categories/Data-Base/MongoDB/"},{"name":"Computer Vision","slug":"Paper/Computer-Vision","permalink":"https://jmj3047.github.io/categories/Paper/Computer-Vision/"},{"name":"Pyspark","slug":"Python/Pyspark","permalink":"https://jmj3047.github.io/categories/Python/Pyspark/"}],"tags":[{"name":"Big Query","slug":"Big-Query","permalink":"https://jmj3047.github.io/tags/Big-Query/"},{"name":"BigQueryML","slug":"BigQueryML","permalink":"https://jmj3047.github.io/tags/BigQueryML/"},{"name":"Looker Studio","slug":"Looker-Studio","permalink":"https://jmj3047.github.io/tags/Looker-Studio/"},{"name":"DataFlow","slug":"DataFlow","permalink":"https://jmj3047.github.io/tags/DataFlow/"},{"name":"GCP","slug":"GCP","permalink":"https://jmj3047.github.io/tags/GCP/"},{"name":"Auth","slug":"Auth","permalink":"https://jmj3047.github.io/tags/Auth/"},{"name":"Error","slug":"Error","permalink":"https://jmj3047.github.io/tags/Error/"},{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"ML Process","slug":"ML-Process","permalink":"https://jmj3047.github.io/tags/ML-Process/"},{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/tags/Python/"},{"name":"kaggle","slug":"kaggle","permalink":"https://jmj3047.github.io/tags/kaggle/"},{"name":"Hexo","slug":"Hexo","permalink":"https://jmj3047.github.io/tags/Hexo/"},{"name":"Git","slug":"Git","permalink":"https://jmj3047.github.io/tags/Git/"},{"name":"Automation","slug":"Automation","permalink":"https://jmj3047.github.io/tags/Automation/"},{"name":"Bash","slug":"Bash","permalink":"https://jmj3047.github.io/tags/Bash/"},{"name":"ACF","slug":"ACF","permalink":"https://jmj3047.github.io/tags/ACF/"},{"name":"PACF","slug":"PACF","permalink":"https://jmj3047.github.io/tags/PACF/"},{"name":"Time Series","slug":"Time-Series","permalink":"https://jmj3047.github.io/tags/Time-Series/"},{"name":"Probability Distribution Function","slug":"Probability-Distribution-Function","permalink":"https://jmj3047.github.io/tags/Probability-Distribution-Function/"},{"name":"Probability Density Function","slug":"Probability-Density-Function","permalink":"https://jmj3047.github.io/tags/Probability-Density-Function/"},{"name":"Normal Distribution","slug":"Normal-Distribution","permalink":"https://jmj3047.github.io/tags/Normal-Distribution/"},{"name":"Standard Normal Distribution","slug":"Standard-Normal-Distribution","permalink":"https://jmj3047.github.io/tags/Standard-Normal-Distribution/"},{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Speaker GAN","slug":"Speaker-GAN","permalink":"https://jmj3047.github.io/tags/Speaker-GAN/"},{"name":"Speaker Identification","slug":"Speaker-Identification","permalink":"https://jmj3047.github.io/tags/Speaker-Identification/"},{"name":"Generative Adversarial Network","slug":"Generative-Adversarial-Network","permalink":"https://jmj3047.github.io/tags/Generative-Adversarial-Network/"},{"name":"Multi-Task Learning","slug":"Multi-Task-Learning","permalink":"https://jmj3047.github.io/tags/Multi-Task-Learning/"},{"name":"Voice Trigger Detection","slug":"Voice-Trigger-Detection","permalink":"https://jmj3047.github.io/tags/Voice-Trigger-Detection/"},{"name":"Speaker Verification","slug":"Speaker-Verification","permalink":"https://jmj3047.github.io/tags/Speaker-Verification/"},{"name":"Pandas Dataframe","slug":"Pandas-Dataframe","permalink":"https://jmj3047.github.io/tags/Pandas-Dataframe/"},{"name":"Threshold Adjustment","slug":"Threshold-Adjustment","permalink":"https://jmj3047.github.io/tags/Threshold-Adjustment/"},{"name":"Confusion Matrix","slug":"Confusion-Matrix","permalink":"https://jmj3047.github.io/tags/Confusion-Matrix/"},{"name":"Precision-Recall","slug":"Precision-Recall","permalink":"https://jmj3047.github.io/tags/Precision-Recall/"},{"name":"F1 measure","slug":"F1-measure","permalink":"https://jmj3047.github.io/tags/F1-measure/"},{"name":"ROC curve","slug":"ROC-curve","permalink":"https://jmj3047.github.io/tags/ROC-curve/"},{"name":"Grid Search CV","slug":"Grid-Search-CV","permalink":"https://jmj3047.github.io/tags/Grid-Search-CV/"},{"name":"Ensemble Model","slug":"Ensemble-Model","permalink":"https://jmj3047.github.io/tags/Ensemble-Model/"},{"name":"Decision Tree Classifier","slug":"Decision-Tree-Classifier","permalink":"https://jmj3047.github.io/tags/Decision-Tree-Classifier/"},{"name":"Clustering","slug":"Clustering","permalink":"https://jmj3047.github.io/tags/Clustering/"},{"name":"DBSCAN","slug":"DBSCAN","permalink":"https://jmj3047.github.io/tags/DBSCAN/"},{"name":"Gaussian Mixture Model","slug":"Gaussian-Mixture-Model","permalink":"https://jmj3047.github.io/tags/Gaussian-Mixture-Model/"},{"name":"K-Means Clustering","slug":"K-Means-Clustering","permalink":"https://jmj3047.github.io/tags/K-Means-Clustering/"},{"name":"Probabilistic sampling","slug":"Probabilistic-sampling","permalink":"https://jmj3047.github.io/tags/Probabilistic-sampling/"},{"name":"Nonprobability sampling","slug":"Nonprobability-sampling","permalink":"https://jmj3047.github.io/tags/Nonprobability-sampling/"},{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"},{"name":"TD-SV","slug":"TD-SV","permalink":"https://jmj3047.github.io/tags/TD-SV/"},{"name":"Transformer","slug":"Transformer","permalink":"https://jmj3047.github.io/tags/Transformer/"},{"name":"Attention","slug":"Attention","permalink":"https://jmj3047.github.io/tags/Attention/"},{"name":"Doc2vec","slug":"Doc2vec","permalink":"https://jmj3047.github.io/tags/Doc2vec/"},{"name":"Chatbot","slug":"Chatbot","permalink":"https://jmj3047.github.io/tags/Chatbot/"},{"name":"NLP","slug":"NLP","permalink":"https://jmj3047.github.io/tags/NLP/"},{"name":"Virtualenv","slug":"Virtualenv","permalink":"https://jmj3047.github.io/tags/Virtualenv/"},{"name":"Web Server","slug":"Web-Server","permalink":"https://jmj3047.github.io/tags/Web-Server/"},{"name":"CGI","slug":"CGI","permalink":"https://jmj3047.github.io/tags/CGI/"},{"name":"Flask","slug":"Flask","permalink":"https://jmj3047.github.io/tags/Flask/"},{"name":"Brython","slug":"Brython","permalink":"https://jmj3047.github.io/tags/Brython/"},{"name":"MongoDB","slug":"MongoDB","permalink":"https://jmj3047.github.io/tags/MongoDB/"},{"name":"HTML","slug":"HTML","permalink":"https://jmj3047.github.io/tags/HTML/"},{"name":"Web Crawling","slug":"Web-Crawling","permalink":"https://jmj3047.github.io/tags/Web-Crawling/"},{"name":"BeautifulSoup","slug":"BeautifulSoup","permalink":"https://jmj3047.github.io/tags/BeautifulSoup/"},{"name":"Image Classification","slug":"Image-Classification","permalink":"https://jmj3047.github.io/tags/Image-Classification/"},{"name":"WGAN-GP","slug":"WGAN-GP","permalink":"https://jmj3047.github.io/tags/WGAN-GP/"},{"name":"WGAN","slug":"WGAN","permalink":"https://jmj3047.github.io/tags/WGAN/"},{"name":"DCGAN","slug":"DCGAN","permalink":"https://jmj3047.github.io/tags/DCGAN/"},{"name":"Convolutional Neural Networks","slug":"Convolutional-Neural-Networks","permalink":"https://jmj3047.github.io/tags/Convolutional-Neural-Networks/"},{"name":"GAN","slug":"GAN","permalink":"https://jmj3047.github.io/tags/GAN/"},{"name":"Generative Model","slug":"Generative-Model","permalink":"https://jmj3047.github.io/tags/Generative-Model/"},{"name":"Hueman","slug":"Hueman","permalink":"https://jmj3047.github.io/tags/Hueman/"},{"name":"pyspark","slug":"pyspark","permalink":"https://jmj3047.github.io/tags/pyspark/"},{"name":"Adversarial Speaker Verification","slug":"Adversarial-Speaker-Verification","permalink":"https://jmj3047.github.io/tags/Adversarial-Speaker-Verification/"}]}