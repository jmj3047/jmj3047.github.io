{"meta":{"title":"Jang Minjee","subtitle":"","description":"","author":"Jang Minjee","url":"https://jmj3047.github.io","root":"/"},"pages":[],"posts":[{"title":"Hexo Blog 생성 및 재연결","slug":"Hexo_Create","date":"2022-10-05T15:00:00.000Z","updated":"2022-10-15T08:05:32.462Z","comments":true,"path":"2022/10/06/Hexo_Create/","link":"","permalink":"https://jmj3047.github.io/2022/10/06/Hexo_Create/","excerpt":"","text":"Hexo Blog 생성 간단하게 Hexo 블로그를 만들어 본다. I. 필수 파일 설치 1단계: nodejs.org 다운로드 설치가 완료 되었다면 간단하게 확인해본다. 1$ node -v 2단계: git-scm.com 다운로드 설치가 완료 되었다면 간단하게 확인해본다. 1$ git --version 3단계: hexo 설치 hexo는 npm을 통해서 설치가 가능하다. 1$ npm install -g hexo-cli II. 깃허브 설정 두개의 깃허브 Repo를 생성한다. 포스트 버전관리 (name: myblog) 포스트 배포용 관리 (name: rain0430.github.io) rain0430 대신에 각자의 username을 입력하면 된다. 이 때, myblog repo를 git clone을 통해 적당한 경로로 내려 받는다. $ git clone your_git_repo_address.git III. 블로그 만들기 (옵션) 적당한 곳에 경로를 지정한 다음 다음과 같이 폴더를 만든다. 12$ mkdir makeBlog # 만약 Powershell 이라면 mkdir 대신에 md를 쓴다. $ cd makeBlog 임의의 블로그 파일명을 만든다. 12345$ hexo init myblog$ cd myblog$ npm install$ npm install hexo-server --save$ npm install hexo-deployer-git --save ERROR Deployer not found: git hexo-deployer-git을 설치 하지 않으면 deploy시 위와 같은 ERROR가 발생합니다. _config.yml 파일 설정 싸이트 정보 수정 1234title: 제목을 지어주세요subtitle: 부제목을 지어주세요description: description을 지어주세요author: YourName 블로그 URL 정보 설정 1234url: https://rain0430.github.ioroot: /permalink: :year/:month/:day/:title/permalink_defaults: 깃허브 연동 12345# Deploymentdeploy: type: git repo: https://github.com/rain0430/rain0430.github.io.git branch: main IV. 깃허브에 배포하기 배포 전, 터미널에서 localhost:4000 접속을 통해 화면이 뜨는지 확인해본다. 1234$ hexo generate$ hexo serverINFO Start processingINFO Hexo is running at http://localhost:4000 . Press Ctrl+C to stop. 화면 확인이 된 이후에는 깃허브에 배포한다. 사전에, gitignore 파일에서 아래와 같이 설정을 진행한다. 1234567.DS_StoreThumbs.dbdb.json*.lognode_modules/public/.deploy*/ 최종적으로 배포를 진행한다. 1$ hexo deploy 배포가 완료가 되면 브라우저에서 USERNAME.github.io로 접속해 정상적으로 배포가 되었는지 확인한다. Hexo Blog 재연결 기존 블로그 폴더 파일 압축해서 백업한 후 진행해야 한다. → theme 같은 경우 받아오는거부터 다시 해야 하기 때문 재연결보다는 재생성이라고 말하는게 더 적합하다. 다른 로컬에서 블로그를 재연결해서 사용할 경우 아래와 같이 순차적으로 진행하면 된다. 1234$ hexo init your_blog_repo # 여기는 각자 소스 레포 확인$ cd myblog$ git init $ git remote add origin https://github.com/your_name/your_blog_repo.git # 각자 소스 레포 주소 아래 명령어에서 에러가 발생이 있다. 1$ git pull --set-upstream origin main # 에러 발생 그런 경우, 아래 명령어를 추가한다. 기존의 디렉토리와 파일을 모두 삭제한다는 뜻이다. 1$ git clean -d -f 그리고 에러가 발생했던 명령어를 다시 실행한다. 이 때에는 이제 정상적으로 실행되는 것을 확인할 수 있다. 1$ git pull --set-upstream origin main # 에러 발생 안함 / 소스 확인 이제 정상적으로 환경 세팅은 된 것이다. 순차적으로 아래와 같이 진행하도록 한다. 이 때, theme 폴더에 본인의 테마 소스코드가 잘 있는지 확인을 하도록 한다. 1234$ npm install $ hexo clean$ hexo generate$ hexo server reference 생성 재연결","categories":[{"name":"Setting","slug":"Setting","permalink":"https://jmj3047.github.io/categories/Setting/"},{"name":"Hexo","slug":"Setting/Hexo","permalink":"https://jmj3047.github.io/categories/Setting/Hexo/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://jmj3047.github.io/tags/Hexo/"}]},{"title":"Comparison K means & GMM","slug":"Kmeans_VS_GMM","date":"2022-09-10T15:00:00.000Z","updated":"2022-10-15T08:03:02.547Z","comments":true,"path":"2022/09/11/Kmeans_VS_GMM/","link":"","permalink":"https://jmj3047.github.io/2022/09/11/Kmeans_VS_GMM/","excerpt":"","text":"1. K-Means It can be used for easy, concise, and large data. If the number of features becomes too large with distance-based algorithms, the performance of clustering is degraded. Therefore, in some cases, it is applied with reducing dimensions using PCA In addition, since it is an iterative algorithm, it is a model that is quite sensitive to outliers and learning execution time could slow down when the number of iterations increases rapidly. The K-means API in Scikit-learn provides the following key parameters. n_clusters : As the number of clusters defined in advance, it defines how many clusters K-means will cluster into, that is, the “K” value. init : It is a method of initially setting the coordinates of the cluster center point and is usually set in the ‘k-means++’ method. It is used because setting it in a random way can lead to out-of-the-way results. max_iter : It is to set the number of iterations. If clustering is completed before the set number of times is reached, it ends in the middle even if the number of iterations is not filled. The following are attribute values returned by the K-means API provided by Skikit-learn. In other words, these are values returned by K-means after performing all clustering. labels_ : Returns the cluster label to which each individual data point belongs. (However, keep in mind that this label value is not mapped to the same value as the label value of the actual original data!) cluster_centers_ : After clustering into K clusters, the coordinates of the center points of each K cluster are returned. Using this, the center point coordinates can be visualized. 1234567891011121314151617181920212223242526from sklearn.datasets import make_blobs# X에는 데이터, y에는 클러스터링 된 label값 반환X, y = make_blobs(n_samples=200, n_features=2, centers=3, cluster_std=0.8, random_state=1)print(X.shape, y.shape)# y Target값 분포 확인# return_counts=True 추가하면 array요소마다 value_counts()해줌unique, counts = np.unique(y, return_counts=True)# 클러스터링용으로 생성한 데이터 데이터프레임으로 만들기cluster_df = pd.DataFrame(data=X, columns=[&#x27;ftr1&#x27;,&#x27;ftr2&#x27;])cluster_df[&#x27;target&#x27;] = ycluster_df.head()# 생성 데이터포인트들 시각화해보기target_lst = np.unique(y)markers = [&#x27;o&#x27;,&#x27;s&#x27;,&#x27;^&#x27;,&#x27;P&#x27;,&#x27;D&#x27;]for target in target_lst: target_cluster = cluster_df[cluster_df[&#x27;target&#x27;]==target] plt.scatter(x=target_cluster[&#x27;ftr1&#x27;], y=target_cluster[&#x27;ftr2&#x27;], edgecolor=&#x27;k&#x27;, marker=markers[target])plt.show() Let’s look at the distribution of individual data made with the make_blobs function that creates a clustering dataset. Now, let’s apply K-means to the above data to visualize the coordinates of the center points of each cluster. 123456789101112131415161718192021222324252627282930313233# K-means 클러스터링 수행하고 개별 클러스터 중심을 시각화# 1.K-means 할당kmeans = KMeans(n_clusters=3, init=&#x27;k-means++&#x27;, max_iter=200, random_state=12) # X는 cluster_df의 feature array임.cluster_labels = kmeans.fit_predict(X)cluster_df[&#x27;kmeans_label&#x27;] = cluster_labels# 2.K-means속성의 cluster_centers_는 개별 클러스터의 중심 위치 좌표를 반환centers = kmeans.cluster_centers_unique_labels = np.unique(cluster_labels)markers = [&#x27;o&#x27;,&#x27;s&#x27;,&#x27;^&#x27;,&#x27;P&#x27;]# 3. label별로 루프돌아서 개별 클러스터링의 중심 시각화for label in unique_labels: label_cluster = cluster_df[cluster_df[&#x27;kmeans_label&#x27;] == label] # 각 클러스터의 중심 좌표 할당 center_x_y = centers[label] # 각 클러스터 데이터들 시각화 plt.scatter(x=label_cluster[&#x27;ftr1&#x27;], y=label_cluster[&#x27;ftr2&#x27;], marker=markers[label]) # 각 클러스터의 중심 시각화 # 중심 표현하는 모형 설정 plt.scatter(x=center_x_y[0], y=center_x_y[1], s=200, color=&#x27;white&#x27;, alpha=0.9, edgecolor=&#x27;k&#x27;, marker=markers[label]) # 중심 표현하는 글자 설정 plt.scatter(x=center_x_y[0], y=center_x_y[1], s=70, color=&#x27;k&#x27;, edgecolor=&#x27;k&#x27;, marker=&#x27;$%d$&#x27; % label)# label값에 따라 숫자로 표현한다는 의미plt.show() 2. GMM It is a parametric model and is a representative clustering model using EM algorithms. It is to estimate the probability that individual data belongs to a specific normal distribution under the assumption that they belong to a Gaussian distribution. n_components is main parameter of the API provided by Scikit-learn which refers to the number of clustering predefined GMM has a particularly well-applied data distribution, which is mainly easy to apply to elliptical elongated data distributions 1234567891011121314151617181920212223242526from sklearn.datasets import load_irisfrom sklearn.cluster import KMeansimport matplotlib.pyplot as pltimport numpy as npimport pandas as pd%matplotlib inlineiris = load_iris()feature_names = [&#x27;sepal_length&#x27;,&#x27;sepal_width&#x27;,&#x27;petal_length&#x27;,&#x27;petal_width&#x27;]# 보다 편리한 데이타 Handling을 위해 DataFrame으로 변환irisDF = pd.DataFrame(data=iris.data, columns=feature_names)irisDF[&#x27;target&#x27;] = iris.target# GMM 적용from sklearn.mixture import GaussianMixture# n_components로 미리 군집 개수 설정gmm = GaussianMixture(n_components=3, random_state=42)gmm_labels = gmm.fit_predict(iris.data)# GMM 후 클러스터링 레이블을 따로 설정irisDF[&#x27;gmm_cluster&#x27;] = gmm_labels# 실제 레이블과 GMM 클러스터링 후 레이블과 비교해보기(두 레이블 수치가 동일해야 똑같은 레이블 의미 아님!)print(irisDF.groupby(&#x27;target&#x27;)[&#x27;gmm_cluster&#x27;].value_counts()) The result values are as follows, where target is the label of the actual original data, and gmm_cluster is the label of clustering after clustering. To interpret the above results, all of the labels with a target of 0 were clustered into a clustering label of 2. It is 100% well clustered. On the other hand, among the labels with a target of 1, there are 45 clusters with 0 and 5 clusters with 1, so there are 5 incorrectly clustered data. Korean reference","categories":[{"name":"Basic ML","slug":"Basic-ML","permalink":"https://jmj3047.github.io/categories/Basic-ML/"}],"tags":[{"name":"Clustering","slug":"Clustering","permalink":"https://jmj3047.github.io/tags/Clustering/"},{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Gaussian Mixture Model","slug":"Gaussian-Mixture-Model","permalink":"https://jmj3047.github.io/tags/Gaussian-Mixture-Model/"},{"name":"K-Means Clustering","slug":"K-Means-Clustering","permalink":"https://jmj3047.github.io/tags/K-Means-Clustering/"},{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/tags/Python/"}]},{"title":"Gaussian Mixture Model","slug":"GMM","date":"2022-09-09T15:00:00.000Z","updated":"2022-10-15T07:49:03.734Z","comments":true,"path":"2022/09/10/GMM/","link":"","permalink":"https://jmj3047.github.io/2022/09/10/GMM/","excerpt":"","text":"It is one of several models applying the Expectation Maximum (EM) algorithm. What is EM algorithm? EM algorithm is basically an algorithm mainly used for Unsupervised learning. It is also used in clustering. The EM algorithm can be largely divided into two stages: E-step and M-step. In conclusion, it is a flow that finds the optimal parameter value by repeating E-step and M-step. E-step: Calculate the likelihood value that is as close as possible to the likelihood of the initial value of a given arbitrary parameter. M-step: Obtained a new parameter value that maximizes the likelihood calculated in the E-step The above two steps are continuously repeated until the parameter value does not change significantly. The purpose of the EM algorithm is to find parameters that maximize the likelihood. Maximum Likelihood Estimation (MLE) is a Convex or Convex function whose objective function can be differentiated (which can obtain a global optimum) because the optimal parameter must be obtained directly by partial differentiation of the parameter variable. However, EM is a process of finding a value close to the optimal parameter while repeating the E-M step. Similar to the ANN (artificial neural network) model, it does not guarantee to find the Global Optimum, and even if it does, it does not recognize that the point is the real Global Optimum. Therefore, it is very likely that the local minimum is found and the objective function is not composed of Convex or Concave functions. The EM algorithm defines the function of the parameters to be obtained (ex. probability distribution) as a probability variable and optimizes it. It is also used in clustering models and is mainly used in speech recognition modeling. This is a post that Kakao, which develops voice recognition technology using GMM algorithms. Clustering is performed on the assumption that data are generated by datasets with multiple normal distributions. Several normal distribution curves are extracted and individual data are determined to which normal distribution belongs. This process is called parameter estimation in GMM, and two typical estimates are made and the EM method is applied for this parameter estimation. Means and variance of individual normal distributions Probability of which normal distribution each data corresponds Code 123456789101112131415161718192021222324252627282930import numpy as npimport pandas as pdimport matplotlib as mplimport matplotlib.pyplot as pltimport seaborn as snsimport warnings%matplotlib inline%config InlineBackend.figure_format = &#x27;retina&#x27;mpl.rc(&#x27;font&#x27;, family=&#x27;NanumGothic&#x27;) # 폰트 설정mpl.rc(&#x27;axes&#x27;, unicode_minus=False) # 유니코드에서 음수 부호 설정# 차트 스타일 설정sns.set(font=&quot;NanumGothic&quot;, rc=&#123;&quot;axes.unicode_minus&quot;:False&#125;, style=&#x27;darkgrid&#x27;)plt.rc(&quot;figure&quot;, figsize=(10,8))warnings.filterwarnings(&quot;ignore&quot;)from sklearn.datasets import load_irisiris = load_iris()feature_names = [&#x27;sepal_length&#x27;,&#x27;sepal_width&#x27;,&#x27;petal_length&#x27;,&#x27;petal_width&#x27;]iris_df = pd.DataFrame(iris.data, columns = feature_names)iris_df[&quot;target&quot;] = iris.targetiris_df.head() 1234567891011from sklearn.mixture import GaussianMixture# GMM: n_components = 모델의 총 수gmm = GaussianMixture(n_components=3, random_state=0)gmm.fit(iris.data)gmm_cluster_labels = gmm.predict(iris.data)# target, gmm_cluster 비교iris_df[&quot;gmm_cluster&quot;] = gmm_cluster_labelsiris_df.groupby([&quot;target&quot;,&quot;gmm_cluster&quot;]).size() 1234567결과target gmm_cluster0 0 501 1 45 2 52 2 50dtype: int64 GMM can be performed using GaussianMixture() in sklearn.mixture() n_components is the total number of models, and like n_clusters in K-Means, determines the number of clusters. Here, when the target of gmm_cluster was 1, only five were mapped differently, and all the rest were well mapped. Korean reference1 Korean reference2","categories":[{"name":"Basic ML","slug":"Basic-ML","permalink":"https://jmj3047.github.io/categories/Basic-ML/"}],"tags":[{"name":"Clustering","slug":"Clustering","permalink":"https://jmj3047.github.io/tags/Clustering/"},{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Gaussian Mixture Model","slug":"Gaussian-Mixture-Model","permalink":"https://jmj3047.github.io/tags/Gaussian-Mixture-Model/"},{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/tags/Python/"}]},{"title":"K-Means Clustering","slug":"K-Means_Clustering","date":"2022-09-08T15:00:00.000Z","updated":"2022-10-15T07:49:06.457Z","comments":true,"path":"2022/09/09/K-Means_Clustering/","link":"","permalink":"https://jmj3047.github.io/2022/09/09/K-Means_Clustering/","excerpt":"","text":"The K-Means clustering algorithm does not automatically identify and group the number of clusters by looking at the data. The number of clusters should be specified and the initial value should be selected accordingly by user. How to determine the number of clusters use the indicator that quantifies how well clustering has been done. After deciding the candidates of cluster numbers and performing clustering about each cluster number, calculate the index At this time, we select the number of clusterings that optimize the index as the optimal number of clusters. The frequently used indicators include Dunn Index, and Silhouette Index. The algorithm assigning the group determines the initial value which is the initial center point of the group. Each center point determines a group, and individual data is assigned to the same group as the center point close to itself. It does not end at once (depending on the form of the data) but repeats the process of updating the center of the group and assigning groups of individual data again. It belongs to the hard clustering algorithm, which is a clustering that unconditionally assigns a group of data points to a point close to the center. Advantages The process is intuitive, easy to understand, and the algorithm is simple so that it is easy to implement. It does not require complex calculations, so it can be applied to large-scale data. convergence is guaranteed. Disadvantages The result can be very different depending on the initial value. Can be affected by an outlier: because the distance is based on Euclidean Distance, it can be affected by an outlier in the process of updating the center value. Cannot reflect intra-group dispersion structure: Since the distance is based on Euclidean Distance, it cannot properly reflect the intra-group dispersion structure. As the dimensionality increases, the distance between individual data becomes closer, and the effect of clustering may not be effective. This has the meaning of clustering only when the distance between clusters is kept as far apart as possible, but as the dimensionality increases, the distance between individual data becomes closer, so this effect may not be observed. The number of clusters is not set automatically, but must be set in advance. However, the optimal number of clusters can be determined using indicators such as Dunn Index and Silhouette Index. Because of Euclidean Distance, it cannot be used if there is a categorical variable. In this case, it is possible to use the extended K-Modes Clustering algorithm. Using python: scikit-learn Put 3 in n_clusters and init_center in init, and create a KMeans instance. If you put data(X) into the fit function, clustering is performed. The final label can be obtained through the labels_ field. 1234567import warningswarnings.filterwarnings(&#x27;ignore&#x27;)from sklearn.cluster import KMeans kmeans = KMeans(n_clusters=3, init=init_center)kmeans.fit(X)labels = kmeans.labels_ Visualization 1234567891011121314fig = plt.figure(figsize=(7,7))fig.set_facecolor(&#x27;white&#x27;)for i, label in enumerate(labels): if label == 0: color = &#x27;blue&#x27; elif label ==1: color = &#x27;red&#x27; else: color = &#x27;green&#x27; plt.scatter(X[i,0],X[i,1], color=color) plt.xlabel(&#x27;x1&#x27;)plt.ylabel(&#x27;x2&#x27;)plt.show() Korean reference1 Korean reference2","categories":[{"name":"Basic ML","slug":"Basic-ML","permalink":"https://jmj3047.github.io/categories/Basic-ML/"}],"tags":[{"name":"Clustering","slug":"Clustering","permalink":"https://jmj3047.github.io/tags/Clustering/"},{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"K-Means Clustering","slug":"K-Means-Clustering","permalink":"https://jmj3047.github.io/tags/K-Means-Clustering/"},{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/tags/Python/"}]},{"title":"Clustering","slug":"Clustering","date":"2022-09-07T15:00:00.000Z","updated":"2022-10-15T08:07:34.400Z","comments":true,"path":"2022/09/08/Clustering/","link":"","permalink":"https://jmj3047.github.io/2022/09/08/Clustering/","excerpt":"","text":"Clustering is an example of unsupervised learning. Without any label, those with close distances in the data are classified into clusters. It is different from classification, which is supervised learning. In other words, it identifies patterns and groups hidden in the data and binds them together. Even if there’s label in data, there is a possibility that some data with same label can be grouped into different clusters. There are K-Means Clustering, Mean Shift, Gaussian Mixture Model, DBSCAN, Agglomerative Clustering in clustering algorithms and they will be covered in the next post. Korean reference","categories":[{"name":"Basic ML","slug":"Basic-ML","permalink":"https://jmj3047.github.io/categories/Basic-ML/"}],"tags":[{"name":"Clustering","slug":"Clustering","permalink":"https://jmj3047.github.io/tags/Clustering/"},{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"}]},{"title":"Data Sampling","slug":"Data_Sampling","date":"2022-09-06T15:00:00.000Z","updated":"2022-10-15T07:49:25.840Z","comments":true,"path":"2022/09/07/Data_Sampling/","link":"","permalink":"https://jmj3047.github.io/2022/09/07/Data_Sampling/","excerpt":"","text":"1. Reason why you need The more input data you have on machine learning, the slower the processing. Therefore, in order to speed up the processing speed of machine learning, acceleration of learning speed of data would be helpful, which can be done with optimization of machine learning with representative data. Then let’s see how we can reduce the data so that we can only use the data we need. 2. What is data sampling The process of organizing the data and making it the best input data. For example, it can speed up the processing of machine learning by using sales of ‘month’ rather than using sales of ‘day’ units to analyze last year’s profits of a pizza house. This is the work of making optiml data. There are probabilistic sampling, nonprobability sampling in data sampling method. The probabilistic sampling method is a sampling method based on statistics, and the nonprobability sampling is a sampling method in which the subjectivity of a person is involved. Depending on each sampling method, you should select and use the sampling method that matches the data and the situation because there are advantages and disadvantages. 3. Probabilistic sampling Probabilistic sampling is a random sampling method that can be divided into simple random sampling, two-step sampling, stratified sampling, cluster&#x2F;collective sampling, and system sampling. Simple Random Sampling: A method of randomly extracting samples from the entire data. Two-Step Sampling: A sampling method that separates the entire n data into m subpopulations. Selects m subpopulations and provides simple random sampling of N data from m subpopulations. It is an accurate sampling method rather than simple random sampling. Stratified Sampling: By separating the population from several layers, it is a method of randomly extracting data from each layer by n. For example, it is a method of dividing the layers of Korean cities and province and extracting n data from each layer. cluster&#x2F;collective sampling: If the population is composed of multiple clusters, it is a method of selecting one or several clusters and using the entire data of the selected clusters. For example, it is a way to use all of the data by setting the Korea as it’s city and province. system sampling: It is a method of extracting data one by one at a regular interval by numbering all data from 1 to n. This method is mainly used to sample representative values of the time series data 4. Nonprobability sampling This is a method of subjectively extracting the probability of being selected as a sample in advance. The advantage and disadvantage of this sampling is that the subjective intention of the sampling person is involved. The implicit population extracted by nonprobability sampling is a good sampling if it matches the ideal population, the most suitable population for the subject. Nonprobability sampling methods include convenience sampling, purpose sampling, and quota sampling methods. Convenience Sampling: A method of sampling by selecting a point or location where data is good for collecting. The sample surveyed by this sampling method has a disadvantage that it is less representative than the population. It’s not possible to go through the statistical inference process. Statistical inference is that we generalize the sample analysis results to speculation about populations. Purpose Sampling: This is how you select the object you think is the most suitable object for your purpose; you will sample the data that is subjectively appropriate for your purpose.The downside is that it has also low representative for the population. Quota Sampling: Divide the population into segments, assigning each segment a quartar that represents the number of samples. Within segments, the characteristics related to the topic must be similar, and the populations must be distributed differently between segments. This is methodologically similar to layer-by-layer sampling. But the difference is that the sample is not selected by probability but by subjective judgment. 5. Conclusion Comparing probabilistic sampling with nonprobability sampling, probabilistic sampling may look good when judged by just words. However, stochastic sampling is advantageous for data that can be analyzed based on statistics, and non-probability sampling is advantageous for data such as language and music. It is better to choose and use probabilistic and nonprobable sampling as appropriate. Korean Reference","categories":[{"name":"Basic ML","slug":"Basic-ML","permalink":"https://jmj3047.github.io/categories/Basic-ML/"}],"tags":[{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Probabilistic sampling","slug":"Probabilistic-sampling","permalink":"https://jmj3047.github.io/tags/Probabilistic-sampling/"},{"name":"Nonprobability sampling","slug":"Nonprobability-sampling","permalink":"https://jmj3047.github.io/tags/Nonprobability-sampling/"}]},{"title":"Growth Hacking, AARRR, Funnel, Retention","slug":"Growth_Hacking","date":"2022-08-11T15:00:00.000Z","updated":"2022-10-15T07:49:35.914Z","comments":true,"path":"2022/08/12/Growth_Hacking/","link":"","permalink":"https://jmj3047.github.io/2022/08/12/Growth_Hacking/","excerpt":"","text":"1. Growth Hacking 그로스해킹(Growth Hacking)은 성장(Growth)을 위한 모든 수단(Hacking)이란 뜻으로 공격 대상의 미세한 빈틈을 찾아 해킹을 하듯이 성장을 위해 고객과 유통과정 등의 공략지점을 찾아내고 이를 적극적으로 공략하는 마케팅 방법론 브랜드, 기업, 제품 매출 증가 등을 위한 가설을 수립하고 이를 빠르게 MVP 모델로 출시하여 시장의 평가를 받아 본 후, 소비자와 시장의 반응에 따라 제품 또는 서비스가 시장에서 원하는 (고객들이 원하는) 완벽한 상품으로 도달할 때까지 쉬지 않고 개선해 나가는 방식 성장(Growth)을 위한 모든 수단(Hacking)을 통해 효율성을 극대화하고 제품과 서비스를 빠르게 성장시킨다는 장점이 있습니다. 여기서 수단이란, 데이터 기반의 분석과 시장의 피드백을 받아 제품과 서비스를 개선하고 확장해 나간다는 의미로, 자본과 리소스가 한정적인 스타트 기업에게서 효과적인 마케팅 성과를 얻을 수 있음 구매자의 행동 패턴을 분석하고 이를 바탕으로 사용자의 경험을 최적화하는 방법. 그로스해킹은 생산부터 관리에 이르기까지 소비자의 Wants를 충족시키는 상품을 만드는 것 2. AARRR Analysis Acquisition: 어떻게 처음 우리 서비스를 접하게 되는가(사용자 유치) Activation: 사용자가 처음 서비스를 접할 때에 긍정적인 경험을 제공하고 있는가(사용자 활성화) Retention: 이후의 서비스를 다시 사용하는 정도는 얼마나 되는가(사용자 유지) Revenue: 최종 목적(거시전환)으로 연결되고 있는가(매출) Referral: 사용자가 자발적으로 확산이나 공유를 일으키고 있는가(추천) 3. Funnel Analysis 퍼널 분석은 웹 사이트에서 특정 결과에 도달하는데 필요한 단계와, 각 단계를 통과하는 사용자 수를 파악하기 위한 방법 퍼널 분석을 통해 기업은 방문자가 사이트에 가입하는지, 구매자로 변환이 되는지 등의 정보를 특정한 퍼널에 매핑하게 됨. 이때 사용자의 흐름을 시각화하는 모형의 모습이 부엌이나 차고에서 흔하게 쓰이는 깔때기와 유사한 모습을 띠고 있어 ‘퍼널 분석’이라는 이름이 붙여짐 4. Retention Analysis 리텐션은 앱 서비스 성장에 있어서 매우 중요한 지표 리텐션이란 한번 획득한 유저들이 서비스를 이탈하지 않고 계속 서비스를 이용하는 것을 의미 리텐션이 높은 서비스는 리텐션이 낮은 서비스보다 획득비용에 투자한 비용을 빠르게 회수할 수 있으며 이렇게 회수한 비용으로 다시금 빠르게 획득에 투자할 수 있도록 하여 성장을 촉진합니다. 리텐션이 낮다는 것은 획득 이후 다시 돌아오지 않고 이탈하는 유저가 많은 것인데요, 리텐션이 낮은 서비스는 성장이 더딜 뿐만 아니라 한번 획득했다가 이탈한 유저는 다시 획득하기에도 더 많은 획득 비용과 시간이 소요되므로 악순환을 만들어냅니다. 따라서 리텐션은 사업 성장에 있어서 반드시 지켜보아야 할 지표 중 하나입니다. 잔존율(D+1지표) Reference Retention Funnel How to measure Retention","categories":[{"name":"Basic ML","slug":"Basic-ML","permalink":"https://jmj3047.github.io/categories/Basic-ML/"}],"tags":[{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"}]},{"title":"Deep Embedding Learning for Text-Dependent Speaker Verification","slug":"Deep_Embedding_Learning_for_Text-Dependent_Speaker_Verification","date":"2022-07-04T15:00:00.000Z","updated":"2022-10-15T08:03:47.207Z","comments":true,"path":"2022/07/05/Deep_Embedding_Learning_for_Text-Dependent_Speaker_Verification/","link":"","permalink":"https://jmj3047.github.io/2022/07/05/Deep_Embedding_Learning_for_Text-Dependent_Speaker_Verification/","excerpt":"","text":"Journal&#x2F;Conference: InterspeechYear(published year): 2020Author: Peng Zhang, Peng Hu, Xueliang ZhangSubject: Speaker Verification Deep Embedding Learning for Text-Dependent Speaker Verification Summary 이 논문은 화자 검증 작업을 위한 효과적인 딥 임베딩 학습 아키텍처를 제시한다. 널리 사용되는 잔류 신경망(ResNet) 및 시간 지연 신경망(TDNN) 기반 아키텍처와 비교하여, 두 가지 주요 개선이 제안된다. 우리는 화자의 단기 컨텍스트 정보를 인코딩하기 위해 조밀하게 연결된 컨볼루션 네트워크(DenseNet)를 사용한다. 양방향 주의 풀링 전략이 제안된다. 장기적인 시간적 맥락을 모델링하고 화자 정체성을 반영하는 중요한 프레임을 집계한다. 결과는 제안된 알고리듬이 과제 1과 과제 3의 평가 세트에서 각각 8.06%, 19.70% minDCF 및 9.26%, 16.16% EERs 상대적 감소로 FFSVC2020의 공식 기준선을 능가한다는 것을 보여준다. Introduction딥 러닝 기반 방법은 화자 간 차별에 필수적인 정보를 포함하는 딥 스피커 임베딩 또는 짧게 임베딩과 같은 발화 수준 표현을 얻기 위해 지배적이었다. 본 논문에서는 화자 검증을 위한 효과적인 딥 임베딩 학습 아키텍처를 제안한다. 최근 조밀하게 연결된 컨볼루션 네트워크의 성공에 자극받아 (DenseNet) 이미지 분류 [15], 음악 소스 분리 [16], 화자 분리 [17] 및 화자인식 [18]에서, 우리는 DenseNet을 프레임 레벨 피처 추출기로 채택하여 후속 레이어의 입력의 변화와 훈련 효율성을 증가시킨다. 개념적으로 각 밀도 블록은 작은 CNN 시스템 역할을 한다. 차이점은 레이어의 출력이 채널 차원의 출력 연결에 의해 구현되는 피드 포워드 방식으로 조밀하게 연결된다는 것이다. 또한, 우리는 발화 수준 기능의 표현력을 향상시키기 위해 시간 컨텍스트를 추가로 모델링하기 위한 양방향 주의 풀링 계층을 설계한다. Model ArchitectureDenseNet(프레임 레벨 기능 추출기): 각각 5개의 CNN 레이어가 있는 4개의 밀도 블록(DenseBlock)으로 구성. 프레임 레벨 피처 추출 후, 양방향 주의 풀링 레이어는 프레임 레벨 피처를 고정 차원 벡터로 변환하는 데 사용되며, 이어서 완전히 연결된 두 개의 숨겨진 레이어가 발화 레벨 피처를 형성한다. 출력은 소프트맥스 분류기 계층이며, 각 노드는 화자ID에 대응합니다. Frame level의 특징 추출을 하고, Input과 다음 레이어의 변화에 최적화 시키고, 훈련 효율을 높이기위해서 사용함.개념적으로 각각의 dense block들 이 작은 CNN의 역할을 수행하고 있는 것. 각각의 레이어들이 바로 전에 있는 모든 레이어들과 feature map(CNN에서의 합성곱과 같은 원리)을 통해서 연결이 되어있는 방식.이러한 연결 패턴은 훈련 중 레이어들 사이에서 더 나은 기울기 flow와 각 레이어들에 대한 접근을 모든 feature에게 전달함으로써 일시적인 문맥 정보를 capture 가능. 프레임 레벨 단계의 경우 각 DenseBlock은 5개의 컨볼루션 레이어(Conv2D), 지수 선형 단위(ELU) 및 인스턴스 정규화(IN)로 구성된다. 각 밀도 블록 뒤의 텐서 모양은 featureMaps × timeSteps × 주파수 채널 형식이다. 각 Conv2D 및 Conv2D+IN+ELU는 kernelSize 형식으로 지정됩니다. 시간 × kernelSizeFreq, (보행)시간, 보폭Freq), (패딩시간, 패딩Freq), 맵을 특징으로 한다. 각 밀도 블록(g)은 5개의 Conv2D+증가율이 g인 IN+ELU 블록을 포함합니다. 발화 수준 단계의 경우 숫자는 우리의 구현에서 출력 기능 맵 또는 임베딩 차원의 채널을 나타낸다. DenseBlock[15]에서 처음 제시된 DenseBlock 아키텍처의 주요 아이디어는 CNN의 네트워크 구축 블록에서 각 계층에서 모든 후속 계층에 직접 연결을 도입하는 것이다. 시간 빈도가 긴 컨텍스트 정보를 효율적으로 캡처하도록 설계되었다. 각 계층은 피쳐 맵 연결을 통해 모든 다음 계층에 직접 연결됩니다. 이러한 연결 패턴은 훈련 중에 계층 간에 더 나은 그레이디언트 흐름을 생성하고 각 계층이 이전 계층의 모든 특징 표현에 액세스할 수 있도록 하여 시간적 컨텍스트 정보를 캡처하도록 설계되었다. Bidirectional Attentive Pooling 양방향 주의 풀링(bidirectional attentive pooling, BAP)의 도식입니다. hi는 BAP 입력의 i번째 벡터를 나타내며 wf와 wb는 각각 BGRU의 전방과 후방 숨겨진 상태를 나타낸다. →U 및 ←U는 BGRU 레이어의 양방향 출력입니다. bidirectional gated recurrent unit (BGRU) layer + attentive pooling : utterance level feature 일반적인 average pooling을 사용하지 않은 이유: 평균화 대신 주의 메커니즘[10, 12]은 숨겨진 표현을 적극적으로 선택하고 화자 차별 정보를 강조하기 위한 더 나은 대안이다. 보다 차별적인 고정 차원 발화 수준 표현을 얻고 장기 시퀀스 정보를 캡처하기 위해 [19]는 CNN-BLSTM 모델과 주의 깊은 풀링 레이어를 함께 결합하는 주의 기반 CNN-BLSTM 프레임워크를 제안하였다. BLSTM을 주의 깊은 풀링 계층에 직접 연결하는 [19]와는 달리 주의 깊은 풀링을 사용하여 양방향 반복 신경망에 의해 출력되는 양방향 시간 정보를 캡처한 다음, 양방향 발화 수준 기능을 연결한다. 제안된 풀링 방법인 양방향 주의 풀링(BAP)은 다음과 같이 표현될 수 있다. BAP 계층은 양방향 순차 모델링과 주의 메커니즘을 모두 활용하여 장기적인 시간적 컨텍스트 정보를 캡처한다. Result Dataset: FFSVC2020The first 30 utterances are of fixed content: ‘ni hao mi ya’ in Mandarin Chinese for TD-SV tasks. The remaining utterances are text-independent.In total, training data sets have nearly 1,139,671 utterances and the total duration approximately 950 hours with 374 speakers Conclusion 텍스트 의존적 스피커 검증을 위한 딥 임베딩 학습 아키텍처를 제안 아키텍처는 프레임 수준에서 스피커 ID를 캡처하기 위한 DenseBlock의 스택과 발화 수준에서 스피커 임베딩을 형성하기 위한 양방향 주의 풀링 구조로 구성됨 컨볼루션 레이어의 출력을 조밀하게 연결함으로써, 시간 주파수 컨텍스트 정보의 다양한 측면을 가진 보다 의미 있는 프레임 레벨 표현이 생성됨 양방향 주의 풀링 계층은 BGRU 계층과 주의 풀링의 조합으로 양방향에서 시간 컨텍스트 정보를 추가로 캡처 FFSVC2020에서 점수 제출의 경우, 우리가 제안한 방법은 평가 세트의 과제 1과 과제 3에 대한 최소 DCF와 EER에서 각각 0.52와 4.72%, 0.14%를 달성. 또한 이 결과는 x-벡터 및 ResNet 기준선 시스템에 비해 상당한 성능 향상을 보여줌 Pdf: Deep Embedding Learning for Text-Dependent Speaker Verification","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speaker Verification","slug":"Paper/Speaker-Verification","permalink":"https://jmj3047.github.io/categories/Paper/Speaker-Verification/"}],"tags":[{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"},{"name":"TD-SV","slug":"TD-SV","permalink":"https://jmj3047.github.io/tags/TD-SV/"}]},{"title":"Attention is all you need","slug":"Attention_is_all_you_need","date":"2022-05-09T15:00:00.000Z","updated":"2022-10-15T08:03:40.527Z","comments":true,"path":"2022/05/10/Attention_is_all_you_need/","link":"","permalink":"https://jmj3047.github.io/2022/05/10/Attention_is_all_you_need/","excerpt":"","text":"Journal&#x2F;Conference: NIPSYear(published year): 2017Author: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia PolosukhinSubject: NLP Attention is all you need Summary Attention 만으로 시퀀셜 데이터를 분석하여 병렬화와 연산 속도 향상을 가능하게 한 새로운 모델 제시 Seq2Seq 과 Attention 을 결합한 모델(Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. CoRR, abs&#x2F;1409.0473, 2014.)에서 한층 더 발전한 모델입니다. Recurrent model(재귀 구조)없이 Self-attention 만으로 구성한 첫번째 모델입니다. 재귀 구조 제거로 모델을 병렬화(Parallelization)하여 자연 언어 처리 학습&#x2F;추론 시간을 획기적으로 단축시켰습니다. Introduction기존의 자연언어 처리 모델은 RNN, LSTM, GLU 모델로 대표되는 재귀 모델(Recurrent Model)을 encoder-decoder 구조로 결합하는 seq2seq 과 같은 모델을 주로 사용하였습니다. 이러한 재귀 모델은 순차 처리로 인해서 병렬화가 어렵다는 약점이 있고, 메모리 크기가 제한되어 긴 문장을 처리하기도 어렵다는 단점이 있습니다. 이를 보완하기 위한 어텐션(attention) 매커니즘이 제안되었습니다. 어텐션은 재귀 과정에서 입력에서 출력까지의 거리가 길어지는 문제를 해결할 수 있어 입출력의 전역 의존성을 높여주었지만, 재귀 모델과 결합해서만 사용되어 왔습니다. 이에 해당 논문에서는, 어텐션만으로 모델을 구성하여 쉽게 병렬화 할 수 있고 자연언어처리 과제의 성능을 높인 Transformer 모델을 제시합니다. 이는 기존의 재귀 모델과 다르게 8대의 P100 GPU로 12시간 정도만 학습했음에도 당시 기준 SOTA를 달성하였습니다. WMT 2014 English-to-German Translation task -&gt; 28.4 BLEU WMT 2014 English-to-French Translation task -&gt; 41.0 BLEU Model ArchitectureTransformer 모델은 seq2seq으로 대표되는 인코더-디코더 구조를 self-attention 으로 쌓은 뒤, fully connected layer 로 출력을 생성합니다. Encoder and Decoder Stacks 인코더(Encoder) $N$(&#x3D;6)개의 동일한 레이어로 구성 각 레이어는 2개의 하위 레이어로 구성 Multi-head self-attention position-wise fully connected feed-forward 하위 레이어를 거칠 때마다 Residual connection(Resnet) 과 layer normalization 을 실행 각 레이어 출력의 크기는 $d_{model}$(&#x3D;512)로 고정 디코더(Decoder) 인코더와 같이 $N$(&#x3D;6)개의 동일한 레이어로 구성 인코더와 동일한 2개의 하위 레이어에 한가지를 더 추가하여 3개의 하위 레이어로 구성 Multi-head self-attention position-wise fully connected feed-forward 인코더의 출력으로 실행하는 multi-head attention 순차적으로 결과를 만들어 낼 수 있도록 Self-attention 레이어에 Masking 을추가 : $i$ 번째 출력을 만들 때, $i$번째보다 앞선 출력($i-1, i-2,\\dots$) 만을 참고하도록 함 Attentionattention은 query와 key-value pair들을 output에 맵핑해주는 함수입니다. 출력은 values들의 weighted sum으로, value에 할당된 weight는 query와 대응되는 key의 compatibility function으로 계산합니다. Scaled Dot-Product Attention 여기서 사용하는 attention은 Scaled Dot-Product Attention(SDPA)라 부르는데, input은 dimension이 $d_k$인 query와 key, dimension이 $d_v$인 value들로 이루어집니다. 모든 query와 모든 key들에 대해 dot product로 계산되는데 각각의 결과에 dk로 나누어진다. 다음 value의 가중치를 얻기 위해 softmax 함수를 적용합니다. attention 함수는 additive attention과 dot-product attention이 사용됩니다. additive attention은 single hidden layer와 함께 feed-forward network에 compatibility function을 계산하는데 사용됩니다. dot-product attention이 좀 더 빠르고 실제로 space-efficient합니다. 왜냐하면 optimized matrix multiplication code를 사용해서 구현되었기 때문입니다. $d_k$가 작은 경우 additive attention이 dot product attention보다 성능이 좋습니다. 그러나 $d_k$가 큰 값인 경우 softmax 함수에서 기울기 변화가 거의 없는 영역으로 이동하기 때문에 dot product를 사용하면서 $d_k$으로 나누어 scaling을 적용했습니다. Multi-Head Attention $d_{model}$ dimension의 query, key, value 들로 하나의 attention을 수행하는 대신, query, key, value들에 각각 학습된 linear projection을 $h$번 수행하는 것이 더 좋습니다. 즉, Q,K,V에 각각 다른 weight를 곱해주는 것입니다. 이때, projection이라 하는 이유는 각각의 값들이 parameter matrix와 곱해졌을 때, $d_k,d_v,d_{model}$차원으로 project되기 때문입니다. query, key, value들을 병렬적으로 attention function을 거쳐 dimension이 $d_v$인 output 값으로 나오게 됩니다. 이후 여러개의 head를 concatenate하고 다시 $W^O$와 projection하여 dimension이 $d_{model}$인 output 값으로 나오게 됩니다. Position-wise Feed-Forward Networks인코더와 디코더는 fully connected feed-forward network를 가지고 있습니다. 또한, 두 번의 linear transformations과 activation function ReLU로 구성되어집니다. 각각의 position마다 같은 $W,b$ 를 사용하지만 layer가 달라지면 다른 parameter를 사용합니다. Positional Encoding모델이 recurrence와 convolution을 사용하지 않기 때문에 문장안에 상대적인 혹은 절대적인 위치의 token들에 대한 정보를 주입해야만 했습니다. 이후 positional encoding이라는 것을 encoder와 decoder stack 밑 input embedding에 더해줬습니다. positional encoding은 $d_{model}$인 dimension을 가지고 있기 때문에 둘을 더할 수 있습니다. $pos$는 position, $i$는 dimension $pos$는 sequence에서 단어의 위치이고 해당 단어는 $i$에 0부터 $2d_{model}$까지 대입해 dimension이 $d_{model}$인 positional encoding vector를 얻을 수 있습니다. Why Self-Attention Self-Attention을 사용하는 첫 번째 이유는 layer마다 total computational complexity가 작기 때문입니다. 두 번째 이유는 computation의 양이 parallelized하기때문에 sequential operation의 minimum으로 측정되기 때문입니다. 세 번째 이유로는 네트워크에서의 long-range dependencies사이의 path length때문입니다. long-range dependencies를 학습하는 것은 많은 문장 번역 분야에서의 key challenge가 됩니다. input sequence와 output sequence의 길이가 길어지면 두 position간의 거리가 멀어져 long-range dependencies를 학습하는데 어려워집니다. 테이블을 보면 Recurrent layer의 경우 Sequential operation에서 $O(n)$이 필요하지만 Self-Attention의 경우 상수시간에 실행될 수 있습니다. 또한 Self-Attention은 interpretable(설명가능한) model인 것이 이점입니다. Traning OptimizerAdam optimizer 에 파라미터로 $\\beta_1&#x3D;0.9, \\beta_2&#x3D;0.98, \\epsilon&#x3D;10^{-9}$ 를 사용했습니다. 학습동안 아래의 공식을 통해 learning rate를 변화시켰습니다. 이는 warmup_step에 따라 linear하게 증가시키고 step number에 따라 square root한 값을 통해 점진적으로 줄여갔습니다. 그리고 warmup_step &#x3D; 4000을 사용했습니다. Residual Dropout각 sub-layer에서 input을 더하는 것과 normalization을 하기전에 output에 dropout을 설정했습니다. 또한 encoder와 decoder stacks에 embedding의 합계와 positional encoding에도 dropout을 설정했습니다. dropout rate $P_{drop}&#x3D;0.1$ 을 사용했습니다. Label Smoothing학습하는 동안 label smoothing value $\\epsilon_{ls}&#x3D;0.1$을 적용했습니다. Result Machine Translation 영어→독일어 번역에서는 기존 모델들보다 높은 점수가 나왔고 영어→프랑스어 번역에서는 single 모델보다 좋고 ensemble 모델들과 비슷한 성능을 내주는 것을 볼 수 있습니다. 여기서 중요한 점은 Training Cost인데 기존 모델들보다 훨씬 적은 Cost가 들어가는 것을 볼 수 있습니다. Model Variations (A)를 보면 single-head attention은 head&#x3D;16일때보다 0.9 BLEU 낮고 head&#x3D;32로 늘렸을 때도 head&#x3D;16일때보다 BLEU가 낮습니다. (B)를 보면 dk를 낮추는 것이 model quality를 낮추게 합니다. (C), (D)를 보면 더 큰 모델일수록 좋고, dropout이 overfitting을 피하는데 도움이 되는 것을 볼 수 있습니다. (E)를 보면 sinusoidal position대신 learned positional embeddings를 넣었을 때의 결과가 base model과 동일한 결과인 것을 볼 수 있습니다. Conclusion 재귀 구조 없이 Multi-headed Self-attention 으로 인코더-디코더를 대체한 Transformer 모델을 제시하였습니다. 재귀구조가 없으므로 recurrent 또는 convolutional 레이어 기반 모델보다 빠르게 학습을 할 수 있습니다. 해당 모델은 WMT 2014 영어→독어, 영어→불어 번역 분야에서 기존 모든 앙상블 모델들을 능가하는 SOTA를 달성했습니다. Link: Attention is all you need","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"NLP","slug":"Paper/NLP","permalink":"https://jmj3047.github.io/categories/Paper/NLP/"}],"tags":[{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"},{"name":"Transformer","slug":"Transformer","permalink":"https://jmj3047.github.io/tags/Transformer/"},{"name":"Attention","slug":"Attention","permalink":"https://jmj3047.github.io/tags/Attention/"}]},{"title":"Light Gradient Boosting Machine","slug":"LGBM","date":"2022-05-09T15:00:00.000Z","updated":"2022-07-09T05:59:36.000Z","comments":true,"path":"2022/05/10/LGBM/","link":"","permalink":"https://jmj3047.github.io/2022/05/10/LGBM/","excerpt":"","text":"1. DefinitionEnsemble→ 여러 예측기를 수집해서 단일 예측기 보다 더 좋은 예측기를 만드는 것. 일반적으로 앙상블 기법을 사용하면 , 예측기 하나로 훈련하였을때 보다 , 편향은 비슷하지만 분산이 줄어든다고 알려져 있다. 배깅(bagging) 원데이터 집합으로부터 크기가 같은 표본을 여러 번 단순임의 복원추출하여 각 표본(붓스트랩 표본) 에 대해 분류기를 생성한 후 그 결과를 앙상블하는 방법 반복추출 방법을 사용하기 때문에 같은 데이터가 한 표본에 여러 번 추출될 수도 있고, 어떤 데이터는 추출되지 않을수도 있다. 부스팅(boosting) 배깅의 과정과 유사하나 붓스트랩 표본을 구성하는 sampling 과정에서 각 자료에 동일한 확율을 부여하는 것이 아니라, 분류가 잘못된 데이터에- 더 큰 가중을 주어 표본을 추출한다. Bagging 과 Boosting 의 차이 Bagging 은 독립된 예측기를 통해 더 나은 예측기를 얻는다 Boosting 은 앞의 예측기를 보완해 나가면서 더 나은 예측기를 얻는다 랜덤포레스트(random forest) 배깅에 랜덤 과정을 추가한 방법이다. 각 노드마다 모든 예측변수 안에서 최적의 분할을 선택하는 방법 대신 예측변수들을 임의로 추출하고, 추출된 변수 내에서 최적의 분할을 만들어 나가는 방법을 사용한다. 부트스트랩(Bootstrap) 부트스트랩은 평가를 반복한다는 측면에서 교차검증과 유사하나, 훈련용 자료를 반복 재선정한다는 점에서 차이가 있다. 즉 부트스트랩은 관측치를 한번 이상 훈련용 자료로 사용하는 복원추출법에 기반한다. 부트스트랩은 전체 데이터의 양이 크지않은 경우의 모형평가에 가장 적합하다. 2. GBM Gradient Boosting Machine(GBM)은 Ensemble Learning의 일환 Gradient Boosting&#x3D;Gradient Descent+Boosting Gradient Descent 첫번째 데이터에서 잘 못 맞춘 데이터들에 가중치를 주어, 두번째 모델 에서는 더 많은양을 만들어 준다. 또, 두번째 모델에서 잘못 매칭한 데이터들 에게 가중치를 주어서, 세번째 모델을 만들어주는 그러한 방식으로 모델을 반복한다. Fit an addictive model(ensemble) in a foward stage-wise manner bagging 처럼 한번에 딱 학습을 시키는게 아니고 위에 언급한 것 처럼 하나씩 하나씩 더해가면서 모델을 학습 시켜나가는 모델 Adaptive boosting 모델을 거듭할수록, weak leaner가 만들어 지면서 이전에 가졌던 오류에 대해 해결할 수 있는 능력을 만듦 이러한 weak learner 를 만들기 위해 , 앞서 보였던 모델의 shortcoming 을 더 많이 샘플링 하라는 뜻 Gradient Boosting 그라디언트 부스팅은, Regression,Classification,Ranking 을 다 할 수 있다. 이 셋의 다르기는 loss function 에서 차이가 날 뿐, concept 은 동일 💡 Regression 으로 설명하기가 가장 직관적이기 때문에, Regression 모델로써 concept 을 설명하겠다. 얘는 adaptive boosting 과는 달리, 샘플링을 따로 시행하지는 않는다. 잔차를 목푯값 (y) 으로 놓고 계속해서 반복하면서 잔차에 대한 식을 만들어 낸다. 잔차를 목표값으로 잡아두면, 앞선 모델이 맞추지 못한 만큼만 맞추려고 노력을 하기때문에, 앞선 모델의 결과물과 뒷 모델의 결과물을 더하면 정답이 나온다. 경사도를 통해서 Weak learner 를 boosting 시킴 손실함수의 gradient(경사도) 가 0 에 가까울때 까지 미분을 해준다. Gradient 가 0이 아니라면 weight 를 gradinet 의 반대방향으로 움직이되 얼마만큼 움직이냐에 따라서 달라지니 조금씩 움직인다. 처음, decision tree 로 split point 를 잡아, regression 해준 부분의 잔차를 보면 높은것을 알 수 있다. GBM에서는 이 잔차를 기준으로 또 Split point 를 잡아주면서(이러한 과정에서 손실함수가 들어가며 손실함수의 gradient 를 줄여나가는 과정에서 gradient descent 개념이 들어가는 것) 점점 잔차를 줄여 나가는 것을 볼 수 있다. 💡 처음에는 회귀식이 안좋게 나오는데 iteration 이 반복 될수록 회귀식이 좋아지는것으로 볼 수 있다. Overfitting problem in GBM GBM 의 가장큰 문제점은 오차를 기반으로 모델을 형성 하기때문에(애초에 모델 자체가 반복을 거듭 할수록, 전 모델의 오차를 줄여나가는 모델이기 때문에 오버피팅 문제는 필연적) 우리가 어찌 할수 없는 오차까지도 모델에 학습시키어서 오버피팅 문제를 불러 일으킨다 과적합 해결법 Subsampling→각각의 모델을 만들때 샘플링을 랜덤으로 80% 만해서 모델을 만들어준다 Shrinkage- original 알고리즘들은 전에 만들어진 모델들과 뒤로 갈수록 만들어지는 모델들에, 영향력이 동등했는데 , shrinkage 를 쓰면 , 뒤에 만들어지는 모델들에 대해서 , 가중치를 적게 두어 만들어준다. Early Stopping- validation error 가 증가 할 것 같으면 미리 중지를 시키는것 Information Gain:Split point를 통해서 얼마나 혼잡도,불순도가 낮아지는가. Information Gain 을 통해 그 변수의 영향도를 체크 할 수 있다. 3. LightGBMGOSS 모든 피쳐들을 검사하면 시간이 많이 걸리기 때문에 이를 막기위해서, Gradient-based One-side Sampling (GOSS) 를 사용→ Large gradient 는 keep 하고 small gradient 는 드랍 하는 방식으로, 1000개 데이터를 모두 탐색하는 것이 아니라 gradient 가 큰 것 위주로 탐색하는 방식 → 탐색횟수를 줄이는 것 EFB (Exclusive Feature Bundleing) 모든피쳐를 탐색할 필요를 없애는 것 Bundle 을 찾는 방법 Graph coloring problem 으로 해결가능 각각의 노드는 피쳐이고, edge 는 피쳐들간의 conflict → conflict 가 많은 애들은 중복이 많이 들어가서 bundling 이 되면 안됨 conflict 가 없는 애들 끼리는 bundling 을 해도 됨 Greed bundling 계산법 edge 의 강도: 두 변수의 conflict 강도 edge: 동시에 0이아닌 객체의 수. Degree 시작점을 degree의 내림차순으로 정리 해준 다음, degree가 높은것 부터 시작한다. cutoff 는 hyperparameter 인데, cut-off가 0.2라는 말은, N&#x3D;10 이기 때문에 2회 이상 Nonzero value 가 겹치게 되면, bundleing이 안되는 것. cut-off 기준에 맞지 않기 때문에 x5는 고립이 된다. feature merge 를 쉽게 해주기 위해 feature의 위치를 살짝 조정 하여준다. feature 를 merge 하는방법. Add. offset add offset→bundling 을 하기위한 대상이 되는 변수에다가 기준이 되는 변수가 가질 수 있는 최대 값을 더해준다. conflict 가 일어난 부분은 그대로 기준 변수가 가지는 값을 더해준다. Reference: 고려대학교 산업경영공학부 DSBA 연구실","categories":[{"name":"Basic ML","slug":"Basic-ML","permalink":"https://jmj3047.github.io/categories/Basic-ML/"}],"tags":[{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"}]},{"title":"Making Chatbot with doc2vec tutorial(1)","slug":"Making_Chatbot_with_doc2vec_tutorial(1)","date":"2022-05-05T15:00:00.000Z","updated":"2022-10-15T08:07:56.742Z","comments":true,"path":"2022/05/06/Making_Chatbot_with_doc2vec_tutorial(1)/","link":"","permalink":"https://jmj3047.github.io/2022/05/06/Making_Chatbot_with_doc2vec_tutorial(1)/","excerpt":"","text":"모델 만들기데이터 만들기doc2vec을 이용해서 FAQ데이터들의 질문들을 벡터화하는 모델을 만들어 본다. word2vec이 단어를 벡터화 하는 것이라면 doc2vec은 단어가 아니라 문서를 기준으로 (여기서는 문장)벡터를 만드는 라이브러리이다. doc2vec을 사용하면 서로 다른 문서들이 같은 차원의 벡터값을 갖게 된다. 각 문서라 갖는 벡터값을 비교해 같으면 같을 수록 유사한 문서라는 것을 알 수 있다. 따라서 doc2vec을 이용해 FAQ의 질문들을 벡터화 한다면 어떤 질문이 들어왔을 때 동일 모델로 질문을 벡터화 한다음, 저장돼 있는 질문들의 벡터와 비교해서 가장 유사한 질문을 찾을 수 있다. 가장 유사한 질문을 찾은 다음 그 질문의 답을 출력하면 FAQ챗봇을 만들 수 있다. **GPU사용 필수..! 1234567891011import osimport warningsfrom gensim.models import doc2vecfrom gensim.models.doc2vec import TaggedDocumentfaqs = [[&quot;1&quot;, &quot;당해년도 납입액은 수정 가능 한가요?&quot;, &quot;네, 당해년도 납입액은 12464 화면 등록전까지 수정 가능합니다.&quot;], [&quot;2&quot;, &quot;대리인통보 대상계좌 기준은 어떻게 되나요?&quot;, &quot;모계좌 기준 가장 최근에 개설된 계좌의 관리점에서 조회 됩니다. 의원폐쇄된 자계좌는 조회대상 계좌에서 제외됩니다. 계좌주 계좌가 사절원 계좌가 아닌 경우만 조회됩니다&quot;], [&quot;3&quot;, &quot;등록가능 단말기수는 어떻게 되나요?&quot;, &quot;5대까지 등록 가능입니다.&quot;], [&quot;4&quot;, &quot;모바일계좌개설 가능한 시간은 어떻게 되나요?&quot;, &quot;08:00 ~ 20:00(영업일만 가능&quot;], [&quot;5&quot;, &quot;미국인일때 미국납세자등록번호 작성 방법은 어떻게 되나요?&quot;, &quot;계좌주가 미국인일 때 계좌주의 미국납세자등록번호(사회보장번호(Social Security Number), 고용주식별번호(Employer Identification Number), 개인납세자번호(Individual Taxpayer Identification Number))를 기재합니다..&quot;]] 위와 같이 5개의 FAQ데이터를 임의로 만들었다. 이제 여기에 5개의 질문을 벡터화 할건데 사실 벡터화 할 때 데이터는 많을 수록 좋다. 적으면 서로 분간이 잘 안됨. 형태소 분석doc2vec으로 문장을 벡터화하기 전에 약간의 전처리 과정이 필요하다. 각 문장을 tokenize해야 한다. 토큰화 하는 과정이 영어랑 한국어랑 조금 다른데 한국어의 경우 형태소 분석(pos tagging)을 통해 형태소 단위로 나눈뒤 토큰으로 사용할 형태소를 결정하고 나눈다. 즉 각 문장을 형태소 단위의 배열로 만든다. 한국어 형태소 분석기는 konlpy를 사용한다. 123456789101112#형태소 분석import jpypefrom konlpy.tag import Kkmakkma = Kkma()def tokenize_kkma(doc): jpype.attachThreadToJVM() #자바를 사용하기 위한 소스 코드 token_doc = [&#x27;/&#x27;.join(word) for word in kkma.pos(doc) ] #형태소 분석한 단어와 형태소 명을 &#x27;단어/형태소&#x27;형태로 출력하기 위한 코드 return token_doctokenize_kkma(faqs[0][1]) Kkma를 import 하고 jpype도 import 한다. jpype는 파이썬에서 자바를 사용할 수 있게 해주는 패키지인데 기본적으로 kkma가 자바 베이스라서 꼭 필요하다. Kkma()로 형태소 분석기를 불러온다음 kkma.pos(doc)로 형태소 분석을 한다. 123456789101112출력 결과:[&#x27;당해/NNG&#x27;, &#x27;년도/NNM&#x27;, &#x27;납입/NNG&#x27;, &#x27;액/XSN&#x27;, &#x27;은/JX&#x27;, &#x27;수정/NNG&#x27;, &#x27;가능/NNG&#x27;, &#x27;한/MDN&#x27;, &#x27;가요/NNG&#x27;, &#x27;?/SF&#x27;] 형태소 분석을 하면 문장이 단어&#x2F;형태소 형태의 배열로 출력된다. 1번 문장은 총 10개의 형태소로 나뉘었다. 형태소 분석기종류에 따라 결과가 조금씩 다를수 있다. Doc2Vec 모델 만들기Doc2Vec을 이용해 모델을 만들기 위해서는 토큰화 된 리스트와 태그 값이 필요하다. 여기서 태그는 문장 번호. [문장의 번호, 문장을 토큰화한 배열] 이렇게 두 개의 값을 가진 리스트를 사용해 doc2vec 모델을 만들 수 있다. 실제로 모델을 만드는데 사용하는 건 토큰 값이지만 비슷한 문장이 무엇인지 찾기 위한 인덱스로 태그 값을 사용하게 된다. 1234567# 리스트에서 각 문장부분 토큰화token_faqs = [(tokenize_kkma(row[1]), row[0]) for row in faqs]# Doc2Vec에서 사용하는 태그문서형으로 변경tagged_faqs = [TaggedDocument(d, [c]) for d, c in token_faqs]tagged_faqs 12345[TaggedDocument(words=[&#x27;당해/NNG&#x27;, &#x27;년도/NNM&#x27;, &#x27;납입/NNG&#x27;, &#x27;액/XSN&#x27;, &#x27;은/JX&#x27;, &#x27;수정/NNG&#x27;, &#x27;가능/NNG&#x27;, &#x27;한/MDN&#x27;, &#x27;가요/NNG&#x27;, &#x27;?/SF&#x27;], tags=[&#x27;1&#x27;]), TaggedDocument(words=[&#x27;대리인/NNG&#x27;, &#x27;통보/NNG&#x27;, &#x27;대상/NNG&#x27;, &#x27;계좌/NNG&#x27;, &#x27;기준/NNG&#x27;, &#x27;은/JX&#x27;, &#x27;어떻/VA&#x27;, &#x27;게/ECD&#x27;, &#x27;되/VV&#x27;, &#x27;나요/EFQ&#x27;, &#x27;?/SF&#x27;], tags=[&#x27;2&#x27;]), TaggedDocument(words=[&#x27;등록/NNG&#x27;, &#x27;가능/NNG&#x27;, &#x27;단말/NNG&#x27;, &#x27;기수/NNG&#x27;, &#x27;는/JX&#x27;, &#x27;어떻/VA&#x27;, &#x27;게/ECD&#x27;, &#x27;되/VV&#x27;, &#x27;나요/EFQ&#x27;, &#x27;?/SF&#x27;], tags=[&#x27;3&#x27;]), TaggedDocument(words=[&#x27;모바일/NNG&#x27;, &#x27;계좌/NNG&#x27;, &#x27;개설/NNG&#x27;, &#x27;가능/NNG&#x27;, &#x27;하/XSV&#x27;, &#x27;ㄴ/ETD&#x27;, &#x27;시간/NNG&#x27;, &#x27;은/JX&#x27;, &#x27;어떻/VA&#x27;, &#x27;게/ECD&#x27;, &#x27;되/VV&#x27;, &#x27;나요/EFQ&#x27;, &#x27;?/SF&#x27;], tags=[&#x27;4&#x27;]), TaggedDocument(words=[&#x27;미국인/NNG&#x27;, &#x27;일/NNG&#x27;, &#x27;때/NNG&#x27;, &#x27;미국/NNP&#x27;, &#x27;납세자/NNG&#x27;, &#x27;등록/NNG&#x27;, &#x27;번호/NNG&#x27;, &#x27;작성/NNG&#x27;, &#x27;방법/NNG&#x27;, &#x27;은/JX&#x27;, &#x27;어떻/VA&#x27;, &#x27;게/ECD&#x27;, &#x27;되/VV&#x27;, &#x27;나요/EFQ&#x27;, &#x27;?/SF&#x27;], tags=[&#x27;5&#x27;])] TaggedDocument function을 사용하면 doc2vec에서 사용할 수 있는 태그 된 문서 형식으로 변경한다. 출력해 보면 words배열과 tags값을 갖는 Dic형태의 자료형이 되었음을 확인할 수 있다. 1234567891011121314151617181920212223# make model import multiprocessing cores = multiprocessing.cpu_count() d2v_faqs = doc2vec.Doc2Vec(vector_size=50, alpha=0.025, min_alpha=0.025, hs=1, negative=0, dm=0, dbow_words = 1, min_count = 1, workers = cores, seed=0) d2v_faqs.build_vocab(tagged_faqs) # train document vectors for epoch in range(10): d2v_faqs.train(tagged_faqs, total_examples = d2v_faqs.corpus_count, epochs = d2v_faqs.epochs ) d2v_faqs.alpha -= 0.0025 # decrease the learning rate d2v_faqs.min_alpha = d2v_faqs.alpha # fix the learning rate, no decay 모델을 만들고 학습 시킨다. doc2vec모델을 만들 때 파라미터는 여러가지가 들어가는데 여기서는 vector_size와 min_count정도를 수정했다. vector_size는 만들어지는 벡터 차원의 크기이고, min_count는 최소 몇 번 이상 나온 단어에 대해 학습할지 정하는 파라미터이다. 여기서는 일단 사이즈 50에 최소 횟수는 1회로 정했다. epoch는 10번으로 해서 train했다. 유사 문장 찾기이제 이 모델로 어떤 문장이 들어왔을 때 1~5번 중에 무엇과 비슷한지 알아보자. 먼저 어떤 문장이 들어오면 그 문장을 벡터화 하고 그 벡터가 어떤 문장과 비슷한지 태그 값을 찾아본다. 12predict_vector = d2v_faqs.infer_vector([&quot;당해년도 납입액은 수정 가능 한가요?&quot;])d2v_faqs.docvecs.most_similar([predict_vector], topn=2) 1[(&#x27;2&#x27;, 0.21605531871318817), (&#x27;3&#x27;, 0.10707802325487137)] 제대로 됐는지 확인하기 위해 1번 문장을 그대로 넣었지만 답은 2, 3번이 나왔다. 학습이 제대로 되지 않아서 이런 결과가 나왔다. 테스트할 문장을 벡터화 할 때도 형태소 분석을 해줘야 한다. 왜냐하면 모델을 학습 할 때 문장들을 형태소로 분석해서 넣어줬기 때문이다. 123test_string = &quot;대리인통보 대상계좌 기준은 어떻게 되나요?&quot;tokened_test_string = tokenize_kkma(test_string)tokened_test_string 1234567891011[&#x27;대리인/NNG&#x27;, &#x27;통보/NNG&#x27;, &#x27;대상/NNG&#x27;, &#x27;계좌/NNG&#x27;, &#x27;기준/NNG&#x27;, &#x27;은/JX&#x27;, &#x27;어떻/VA&#x27;, &#x27;게/ECD&#x27;, &#x27;되/VV&#x27;, &#x27;나요/EFQ&#x27;, &#x27;?/SF&#x27;] 12test_vector = d2v_faqs.infer_vector(tokened_test_string)d2v_faqs.docvecs.most_similar([test_vector], topn=2) 1[(&#x27;1&#x27;, 0.1448383331298828), (&#x27;3&#x27;, 0.0218462273478508)] 2번 문장으로 했을 때 결과입니다. doc2vec이라는 모델은 문서단위로 벡터화 하는 것이기 때문에 문서가 많아야 한다. 여기서는 문장이 많아야 한다. 문장이 많으면 많을 수록 문장 간의 거리를 계산해서 더 잘 구분해준다. 간단하게 생각하면 문장이 적으면 적중률이 높을 것 같지만 사실은 그 반대이다. 데이터가 많을수록 그 데이터간의 차이를 구분할 수 있기 때문에 더 잘 예측하게 된다. Local path :C:\\Users\\jmj30\\Dropbox\\카메라 업로드\\Documentation\\2022\\2022 상반기\\휴먼교육센터\\mj_chatbot_prac\\faq_chatbot Reference: https://cholol.tistory.com/466","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Python","slug":"Python/Python","permalink":"https://jmj3047.github.io/categories/Python/Python/"}],"tags":[{"name":"Doc2vec","slug":"Doc2vec","permalink":"https://jmj3047.github.io/tags/Doc2vec/"},{"name":"Chatbot","slug":"Chatbot","permalink":"https://jmj3047.github.io/tags/Chatbot/"},{"name":"NLP","slug":"NLP","permalink":"https://jmj3047.github.io/tags/NLP/"}]},{"title":"Making English Chatbot with Django(4)","slug":"Making_English_Chatbot_with_Django(4)","date":"2022-05-05T15:00:00.000Z","updated":"2022-05-06T08:28:34.000Z","comments":true,"path":"2022/05/06/Making_English_Chatbot_with_Django(4)/","link":"","permalink":"https://jmj3047.github.io/2022/05/06/Making_English_Chatbot_with_Django(4)/","excerpt":"","text":"실제 서비스 구현해보기**code: https://github.com/jmj3047/faq_chatbot_example.git vs code로 django 설정하기: https://integer-ji.tistory.com/81 채팅창 만들기 html&#x2F;css를 사용해 간단한 채팅화면을 만들었다. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465&lt;!-- //templates/addresses/chat_test.html --&gt;&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;/static/jquery-3.2.1.min.js&quot;&gt;&lt;/script&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt; &lt;link rel=&quot;preconnect&quot; href=&quot;https://fonts.googleapis.com&quot;&gt; &lt;link rel=&quot;preconnect&quot; href=&quot;https://fonts.gstatic.com&quot; crossorigin&gt; &lt;link href=&quot;https://fonts.googleapis.com/css2?family=Inconsolata:wght@400;600&amp;display=swap&quot; rel=&quot;stylesheet&quot;&gt;&lt;/head&gt;&lt;style&gt;* &#123;font-family: &#x27;Inconsolata&#x27;, monospace;&#125;.chat_wrap &#123;display:none;width: 350px;height: 500px;position: fixed;bottom: 30px;right: 95px;background: #a9bdce;&#125;.chat_content &#123;font-size:16pt; position:relative; height: 600px;width: 500px;overflow-y:scroll;padding:10px 15px;background: cornflowerblue&#125;.chat_input &#123;border:solid 0.5px lightgray; padding:2px 5px;&#125;.chat_header &#123;padding: 10px 15px; width: 500px; border-bottom: 1px solid #95a6b4;&#125;.chat_header .close_btn &#123;border: none;background: lightgray;float: right;&#125;.send_btn &#123;border: none; background: #ffeb33;height: 100%; color: #0a0a0a;&#125;.msg_box:after &#123;content: &#x27;&#x27;;display: block;clear:both;&#125;.msg_box &gt; span &#123;padding: 3px 5px;word-break: break-all;display: block;max-width: 300px;margin-bottom: 10px;border-radius: 4px&#125;.msg_box.send &gt; span &#123;background:#ffeb33;float: right;&#125;.msg_box.receive &gt; span &#123;background:#fff;float: left;&#125;&lt;/style&gt;&lt;body&gt;&lt;div class=&quot;chat_header&quot;&gt; &lt;span style=&quot;font-size:20pt;&quot;&gt;EDITH&lt;/span&gt; &lt;button type=&quot;button&quot; id=&quot;close_chat_btn&quot; class=&quot;close_btn&quot;&gt;X&lt;/button&gt;&lt;/div&gt;&lt;div id=&quot;divbox&quot; class=&quot;chat_content&quot;&gt;&lt;/div&gt;&lt;form id=&quot;form&quot; style=&quot;display: inline&quot;&gt; &lt;input type=&quot;text&quot; placeholder=&quot;write message..&quot; name=&quot;input1&quot; class=&quot;chat_input&quot; id=&quot;input1&quot; size=&quot;74&quot; style=&quot;margin:-3px; display: inline; width: 468px; height: 32px; font-size: 16pt;&quot; /&gt; &lt;input type=&quot;button&quot; value=&quot;SEND&quot; id=&quot;btn_submit&quot; class=&quot;send_btn&quot; style=&quot;margin:-5px; display: inline; width: 53px; height: 38px; font-size: 14pt;&quot; /&gt;&lt;/form&gt;&lt;script&gt; $(&#x27;#btn_submit&#x27;).click(function () &#123; send(); &#125;); $(&#x27;#form&#x27;).on(&#x27;submit&#x27;, function(e)&#123; e.preventDefault(); send(); &#125;); $(&#x27;#close_chat_btn&#x27;).on(&#x27;click&#x27;, function()&#123; $(&#x27;#chat_wrap&#x27;).hide().empty(); &#125;); function send()&#123; $(&#x27;#divbox&#x27;).append(&#x27;&lt;div class=&quot;msg_box send&quot;&gt;&lt;span&gt;&#x27;+$(&#x27;#input1&#x27;).val()+&#x27;&lt;span&gt;&lt;/div&gt;&#x27;); $(&quot;#divbox&quot;).scrollTop($(&quot;#divbox&quot;)[0].scrollHeight); console.log(&quot;serial&quot;+$(&#x27;form&#x27;).serialize()) $.ajax(&#123; url: &#x27;http://127.0.0.1:8000/chat_service/&#x27;, //챗봇 api url type: &#x27;post&#x27;, dataType: &#x27;json&#x27;, data: $(&#x27;form&#x27;).serialize(), success: function(data) &#123; &lt;!--$(&#x27;#reponse&#x27;).html(data.reponse);--&gt; $(&#x27;#divbox&#x27;).append(&#x27;&lt;div class=&quot;msg_box receive&quot;&gt;&lt;span&gt;&#x27;+ data.response +&#x27;&lt;span&gt;&lt;/div&gt;&#x27;); $(&quot;#divbox&quot;).scrollTop($(&quot;#divbox&quot;)[0].scrollHeight); &#125; &#125;); $(&#x27;#input1&#x27;).val(&#x27;&#x27;); &#125;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 간단하게 설명하면 Django로 restfulAPI를 구현하기 위한 소스 위에 챗봇을 붙이기 위한 화면과 모델이 들어가 있는 버전이다. 위에 소스는 화면 역할을 하는 chat_test.html 파일이다. jquery 라이브러리를 사용했기 때문에 jquery를 import 해야 한다. jquery file이 static 폴더에 있어야 한다. jquery는 소스 하단부에 있는 script를 위해 필요하다. 채팅에서 전송 버튼을 누르거나 엔터를 누르면 send()라는 함수가 실행되고 이 함수는 ajax로 질문에 대한 답변을 받아오는 API를 호출한다. 여기서는 localhost&#x2F;chat_service를 호출한다. 채팅을 위한 API화면이 만들어졌으면 이제 질문을 받아 답변을 생성하는 API를 만든다. 아직 FAQ데이터를 학습한 모델은 넣지 않았으니 인풋이 들어오면 더미데이터(dummy)를 리턴하는 API를 만든다. 이런 API 동작들은 view.py에서 구현할 수 있다. 1234567891011#faq_chatbot_example/addresses/views.py@csrf_exemptdef chat_service(request): if request.method == &#x27;POST&#x27;: input1 = request.POST[&#x27;input1&#x27;] output = dict() output[&#x27;response&#x27;] = &quot;이건 응답&quot; return HttpResponse(json.dumps(output), status=200) else: return render(request, &#x27;addresses/chat_test.html&#x27;) Django 프로젝트 안에 addresses 앱에 있는 views.py를 보면 chat_service 함수를 만들었다. POST형식으로 콜이 오면 response에 아웃풋 메세지를 담아서 json형태로 리턴한다. views.py에 함수를 만들고 url로 연결하기 위해서 urls.py에 chat_service를 입력한다. 123456789101112###django 3.8.3 버전 맞춰줘야 함#/faq_chatbot_example/restfulapiserver/urls.py# from django.conf.urls import url, includefrom addresses import viewsfrom django.urls import path, re_path, includefrom django.contrib import adminurlpatterns = [ ... path(&#x27;chat_service/&#x27;, views.chat_service), ...] urls.py에서 ~&#x2F;chat_service를 views.chat_service에 연결시킨다. 이제 ~&#x2F;chat_service로 콜하면 views.chat_service가 실행된다. 아까 위에서 만든 채팅페이지에 전송버튼을 누르면 ajax를 이용해 chat_service를 호출했다. 정상적으로 되는지 테스트 해본다. FAQ 모델 넣기addresses 앱 안에 새로운 py 모델을 만들어서 넣기 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374#/faq_chatbot_example/addresses/faq_chatbot.pyfrom gensim.models import doc2vec, Doc2Vecfrom gensim.models.doc2vec import TaggedDocumentimport pandas as pdfrom nltk.tokenize import word_tokenizefrom nltk.stem import WordNetLemmatizerfrom nltk.corpus import stopwordsimport nltk# 파일로부터 모델을 읽는다. 없으면 생성한다.try: d2v_faqs = Doc2Vec.load(&#x27;d2v_faqs_size200_min5_epoch20_jokes.model&#x27;) lemmatizer = WordNetLemmatizer() stop_words = stopwords.words(&#x27;english&#x27;) faqs = pd.read_csv(&#x27;jokes.csv&#x27;)except: faqs = pd.read_csv(&#x27;jokes.csv&#x27;) nltk.download(&#x27;punkt&#x27;) # 토근화 tokened_questions = [word_tokenize(question.lower()) for question in faqs[&#x27;Question&#x27;]] lemmatizer = WordNetLemmatizer() nltk.download(&#x27;wordnet&#x27;) # lemmatization lemmed_questions = [[lemmatizer.lemmatize(word) for word in doc] for doc in tokened_questions] nltk.download(&#x27;stopwords&#x27;) # stopword 제거 불용어 제거하기 stop_words = stopwords.words(&#x27;english&#x27;) questions = [[w for w in doc if not w in stop_words] for doc in lemmed_questions] # 리스트에서 각 문장부분 토큰화 index_questions = [] for i in range(len(faqs)): index_questions.append([questions[i], i ]) # Doc2Vec에서 사용하는 태그문서형으로 변경 tagged_questions = [TaggedDocument(d, [int(c)]) for d, c in index_questions] # make model import multiprocessing cores = multiprocessing.cpu_count() d2v_faqs = doc2vec.Doc2Vec( vector_size=200, hs=1, negative=0, dm=0, dbow_words=1, min_count=5, workers=cores, seed=0, epochs=20 ) d2v_faqs.build_vocab(tagged_questions) d2v_faqs.train(tagged_questions, total_examples=d2v_faqs.corpus_count, epochs=d2v_faqs.epochs) d2v_faqs.save(&#x27;d2v_faqs_size200_min5_epoch20_jokes.model&#x27;)# FAQ 답변def faq_answer(input): # 테스트하는 문장도 같은 전처리를 해준다. tokened_test_string = word_tokenize(input) lemmed_test_string = [lemmatizer.lemmatize(word) for word in tokened_test_string] test_string = [w for w in lemmed_test_string if not w in stop_words] topn = 5 test_vector = d2v_faqs.infer_vector(test_string) result = d2v_faqs.docvecs.most_similar([test_vector], topn=topn) print(result) for i in range(topn): print(&quot;&#123;&#125;위. &#123;&#125;, &#123;&#125; &#123;&#125; &#123;&#125;&quot;.format(i + 1, result[i][1], result[i][0], faqs[&#x27;Question&#x27;][result[i][0]], faqs[&#x27;Answer&#x27;][result[i][0]])) return faqs[&#x27;Answer&#x27;][result[0][0]]faq_answer(&quot;What do you call a person who is outside a door and has no arms nor legs?&quot;) 위 소스에서 상단에 있는 모델을 만드는 코드는 API 서버를 실행하는 시점에서 호출된다. 무조건 호출하는 건 아니고 views.py에서 import를 써 넣으면 최초 1번은 실행되게 된다. 채팅 웹페이지로부터 faq_chatbot.py에 있는 faq_answer를 호출하는 것 까지 flow를 그려보면 chat_test.html→view.py(chat_service)→faq_chatbot.py(faq_answer) 순서이다. 따라서 views.py에서 faq_answer함수를 호출하기 위해 import를 하게 되는데 django는 최초 실행시 views.py를 한번 읽기 때문에 faq_chatbot.py에 적어놓은 소스가 한번 실행되게 된다. 매번서버를 실행할 때마다 모델을 새로 만들게 되면 서버 기동 속도가 느려지고 비효율적이기 때문에 모델을 만들고 나서 패일로 저장하고, 만들어진 파일이 없다면 모델을 생성하도록 try&#x2F;except를 사용했다. 추가적으로 프로젝트상 소스가 실행되기 때문에 파일경로는 root이다. jokes.csv가 있어야 할 곳과 모델이 생성되는 곳의 경로는 프로젝트의 root폴더이다. 자 이제 질문의 답을 찾아주는 함수가 만들어 졌으니 아까 더미 데이터로 리턴해주던 views.py의 함수를 바꿔보자. 123456789101112#faq_chatbot_example/addresses/views.py@csrf_exemptdef chat_service(request): if request.method == &#x27;POST&#x27;: input1 = request.POST[&#x27;input1&#x27;] response = faq_answer(input1) output = dict() output[&#x27;response&#x27;] = response return HttpResponse(json.dumps(output), status=200) else: return render(request, &#x27;addresses/chat_test1.html&#x27;) 이전에는 response에 무조건 더미 응답을 보냈는데 이제는 faq_answer함수를 사용해 해당 질문에 알맞은 정답을 가져온다. faq_answer함수를 사용하기 위해 제일 상단에 from .faq_chatbot import faq_answer를 선언해야 한다. 실행 결과(html 파일 수정) **code: ‣https://github.com/jmj3047/faq_chatbot_example.git Reference: https://cholol.tistory.com/478","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Django","slug":"Python/Django","permalink":"https://jmj3047.github.io/categories/Python/Django/"}],"tags":[{"name":"Doc2vec","slug":"Doc2vec","permalink":"https://jmj3047.github.io/tags/Doc2vec/"},{"name":"Chatbot","slug":"Chatbot","permalink":"https://jmj3047.github.io/tags/Chatbot/"},{"name":"NLP","slug":"NLP","permalink":"https://jmj3047.github.io/tags/NLP/"}]},{"title":"Making English Chatbot with doc2vec(3)","slug":"Making_English_Chatbot_with_doc2vec(3)","date":"2022-05-05T15:00:00.000Z","updated":"2022-10-15T08:08:04.571Z","comments":true,"path":"2022/05/06/Making_English_Chatbot_with_doc2vec(3)/","link":"","permalink":"https://jmj3047.github.io/2022/05/06/Making_English_Chatbot_with_doc2vec(3)/","excerpt":"","text":"많은 데이터로 실험해보기데이터 살펴보기더 많은 학습 데이터로 모델을 학습한다. 데이터 원본 링크: https://www.kaggle.com/jiriroz/qa-jokes총 3만 8천개의 문장 데이터 불러오기, 전처리12345678import osimport warningsfrom gensim.models import doc2vecfrom gensim.models.doc2vec import TaggedDocumentimport pandas as pdfaqs = pd.read_csv(os.path.join(&#x27;data&#x27;,&#x27;jokes.csv&#x27;))faqs 한국어와 다르게 영어는 띄어쓰기로 단어가 잘 구분되기 때문에 형태소 분석은 생략한다. 형태소 분석을 하지 않아도 띄어쓰기로 split하면 단어 단위로 잘 짤리기 때문이다. 하지만 영어 단어를 원형으로 만들어 주는 lemmatization이나 the나 a같은 관사를 제거하는 stopword 제거는 해준다. 12345678910from nltk.tokenize import word_tokenizefrom nltk.stem import WordNetLemmatizerfrom nltk.corpus import stopwordsimport nltknltk.download(&#x27;punkt&#x27;)# 토큰화tokened_questions = [word_tokenize(question.lower()) for question in faqs[&#x27;Question&#x27;]]tokened_questions 123456789101112131415[[&#x27;did&#x27;, &#x27;you&#x27;, &#x27;hear&#x27;, &#x27;about&#x27;, &#x27;the&#x27;, &#x27;native&#x27;, &#x27;american&#x27;, &#x27;man&#x27;, &#x27;that&#x27;, &#x27;drank&#x27;, &#x27;200&#x27;, &#x27;cups&#x27;, &#x27;of&#x27;, &#x27;tea&#x27;, &#x27;?&#x27;], 대문자를 모두 소문자로 바꿔주고 토큰화를 한 다음 띄어쓰기로 출력해주었다. 12345678910lemmatizer = WordNetLemmatizer()nltk.download(&#x27;wordnet&#x27;)# lemmatizationlemmed_questions = [[lemmatizer.lemmatize(word) for word in doc] for doc in tokened_questions]lemmed_questionsnltk.download(&#x27;stopwords&#x27;)# stopword 제거 불용어 제거하기stop_words = stopwords.words(&#x27;english&#x27;)questions = [[w for w in doc if not w in stop_words] for doc in lemmed_questions]questions 123456789101112131415**[[&#x27;hear&#x27;, &#x27;native&#x27;, &#x27;american&#x27;, &#x27;man&#x27;, &#x27;drank&#x27;, &#x27;200&#x27;, &#x27;cup&#x27;, &#x27;tea&#x27;, &#x27;?&#x27;], [&quot;&#x27;s&quot;, &#x27;best&#x27;, &#x27;anti&#x27;, &#x27;diarrheal&#x27;, &#x27;prescription&#x27;, &#x27;?&#x27;], [&#x27;call&#x27;, &#x27;person&#x27;, &#x27;outside&#x27;, &#x27;door&#x27;, &#x27;ha&#x27;, &#x27;arm&#x27;, &#x27;leg&#x27;, &#x27;?&#x27;], [&#x27;star&#x27;, &#x27;trek&#x27;, &#x27;character&#x27;, &#x27;member&#x27;, &#x27;magic&#x27;, &#x27;circle&#x27;, &#x27;?&#x27;], [&quot;&#x27;s&quot;, &#x27;difference&#x27;, &#x27;bullet&#x27;, &#x27;human&#x27;, &#x27;?&#x27;], [&#x27;wa&#x27;, &#x27;ethiopian&#x27;, &#x27;baby&#x27;, &#x27;cry&#x27;, &#x27;?&#x27;], [&quot;&#x27;s&quot;, &#x27;difference&#x27;, &#x27;corn&#x27;, &#x27;husker&#x27;, &#x27;epilepsy&#x27;, &#x27;hooker&#x27;, &#x27;dysentery&#x27;, &#x27;?&#x27;], [&#x27;2016&#x27;, &quot;&#x27;s&quot;, &#x27;biggest&#x27;, &#x27;sellout&#x27;, &#x27;?&#x27;],** lemmatization하고 불용어를 제거하고 난 다음의 결과물. 이제 모든 전처리가 끝났으니 TaggedDocument로 변형시키고 나서 doc2vec에 넣어준다. 12345678# 리스트에서 각 문장부분 토큰화index_questions = []for i in range(len(faqs)): index_questions.append([questions[i], i ])# Doc2Vec에서 사용하는 태그문서형으로 변경tagged_questions = [TaggedDocument(d, [int(c)]) for d, c in index_questions] doc2vec 모델화doc2vec을 훈련하기 전에 모델에 변형을 주었다. for문도 빼고 파라미터도 변경해 주었다. 123456789101112131415161718192021222324252627# make modelimport multiprocessingcores = multiprocessing.cpu_count()d2v_faqs = doc2vec.Doc2Vec(vector_size=200, # alpha=0.025, # min_alpha=0.025, hs=1, negative=0, dm=0, dbow_words = 1, min_count = 5, workers = cores, seed=0, epochs=20)d2v_faqs.build_vocab(tagged_questions)d2v_faqs.train(tagged_questions, total_examples = d2v_faqs.corpus_count, epochs = d2v_faqs.epochs ) # # train document vectors # for epoch in range(50): # d2v_faqs.train(tagged_faqs, # total_examples = d2v_faqs.corpus_count, # epochs = d2v_faqs.epochs # ) # d2v_faqs.alpha -= 0.0025 # decrease the learning rate # d2v_faqs.min_alpha = d2v_faqs.alpha # fix the learning rate, no decay 1234567# 테스트하는 문장도 같은 전처리를 해준다.test_string = &quot;What&#x27;s the best anti diarrheal prescription?&quot;tokened_test_string = word_tokenize(test_string)lemmed_test_string = [lemmatizer.lemmatize(word) for word in tokened_test_string]test_string = [w for w in lemmed_test_string if not w in stop_words]test_string 12345678910111213141516# 성능 측정raten = 5found = 0for i in range(len(faqs)): tstr = faqs[&#x27;Question&#x27;][i] tokened_test_string = word_tokenize(tstr) lemmed_test_string = [lemmatizer.lemmatize(word) for word in tokened_test_string] ttok = [w for w in lemmed_test_string if not w in stop_words] tvec = d2v_faqs.infer_vector(ttok) re = d2v_faqs.docvecs.most_similar([tvec], topn = raten) for j in range(raten): if i == re[j][0]: found = found + 1 breakprint(&quot;정확도 = &#123;&#125; % (&#123;&#125;/&#123;&#125; ) &quot;.format(found/len(faqs),found, len(faqs))) 1정확도 = 0.8626303274190598 % (33012/38269 ) Local path :C:\\Users\\jmj30\\Dropbox\\카메라 업로드\\Documentation\\2022\\2022 상반기\\휴먼교육센터\\mj_chatbot_prac\\faq_chatbot Reference: https://cholol.tistory.com/466","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Python","slug":"Python/Python","permalink":"https://jmj3047.github.io/categories/Python/Python/"}],"tags":[{"name":"Doc2vec","slug":"Doc2vec","permalink":"https://jmj3047.github.io/tags/Doc2vec/"},{"name":"Chatbot","slug":"Chatbot","permalink":"https://jmj3047.github.io/tags/Chatbot/"},{"name":"NLP","slug":"NLP","permalink":"https://jmj3047.github.io/tags/NLP/"}]},{"title":"Making Korean Chatbot with doc2vec(2)","slug":"Making_Korean_Chatbot_with_doc2vec(2)","date":"2022-05-05T15:00:00.000Z","updated":"2022-10-15T08:08:14.439Z","comments":true,"path":"2022/05/06/Making_Korean_Chatbot_with_doc2vec(2)/","link":"","permalink":"https://jmj3047.github.io/2022/05/06/Making_Korean_Chatbot_with_doc2vec(2)/","excerpt":"","text":"모델 다듬기FAQ데이터 늘리기더 많은 학습 데이터로 모델을 학습한다. 데이터 원본 링크: https://www.data.go.kr/dataset/3068685/fileData.do 123456789import osimport warningsfrom gensim.models import doc2vecfrom gensim.models.doc2vec import TaggedDocumentimport pandas as pdfaqs = pd.read_csv(os.path.join(&#x27;data&#x27;,&#x27;kor_elec_faq2.csv&#x27;), encoding=&#x27;CP949&#x27;)faqsfaqs[[&#x27;순번&#x27;, &#x27;제목&#x27;, &#x27;내용&#x27;]] pandas를 사용해 csv파일을 바로 읽어준다. utf-8 인코딩 문제로 에러가 나면 cp949로 넣어준다. 전체 필드에서 필요한 index와 질문(여기서는 제목), 답변(여기서는 내용)만 뽑아낸다. 총 351개의 질문과 답변 데이터가 생겼으니 전 게시물과 동일한 방법으로 모델학습을 시킨다. 전 데이터는 pandas데이터가 아니었기 때문에 수정해 준다. 1234567# 리스트에서 각 문장부분 토큰화token_faqs = []for i in range(len(faqs)): token_faqs.append([tokenize_kkma_noun(faqs[&#x27;제목&#x27;][i]), faqs[&#x27;순번&#x27;][i]])# Doc2Vec에서 사용하는 태그문서형으로 변경tagged_faqs = [TaggedDocument(d, [c]) for d, c in token_faqs] 이렇게 하고 모델을 돌려도 성능이 좋지 않음. doc2vec모델의 경우 최소 만단위의 문장이 있어야 제대로 나온다. 튜닝 시도해보기이전까지 데이터 내에 있는 순번을 인덱스로 사용했는데 정상적으로 작동하지 않아 다시 만들어준다. 123456789# 리스트에서 각 문장부분 토큰화token_faqs = []for i in range(len(faqs)): token_faqs.append([tokenize_kkma_noun(faqs[&#x27;제목&#x27;][i]), i ]) # token_faqs.append([tokenize_kkma_noun(faqs[&#x27;제목&#x27;][i]), faqs[&#x27;순번&#x27;][i]])# Doc2Vec에서 사용하는 태그문서형으로 변경tagged_faqs = [TaggedDocument(d, [int(c)]) for d, c in token_faqs]# tagged_faqs = [TaggedDocument(d, [c]) for d, c in token_faqs] 문서 원본을 수정할 필요는 없고 TaggedDocument만들 때만 잘 넣어준면 된다. 기존에는 faqs[’순번’][i]를 태그 값으로 넣어주었는데 그냥 i를 넣는다. 이렇게 하면 좋은 점이 원본의 index와 태그값이 같아지기 때문에 나중에 원문질문을 복원할 때 faqs[’제목’][tag]로 출력이 가능하다. 그리고 이제 가장 먼저 할 거는 전처리를 조금 수정하는 것이다. 형태소 분석을 할 때 필요 없는 데이터를 제외시키는 방법이다. 보통 문장에서는 명사와 동사가 중요하기 때문에 명사 동사 빼고는 다 날려본다. 1234567891011121314151617#튜닝:형태소 필터링kkma = Kkma()filter_kkma = [&#x27;NNG&#x27;, #보통명사 &#x27;NNP&#x27;, #고유명사 &#x27;OL&#x27; , #외국어 &#x27;VA&#x27;,&#x27;VV&#x27;,&#x27;VXV&#x27; ]def tokenize_kkma(doc): jpype.attachThreadToJVM() token_doc = [&#x27;/&#x27;.join(word) for word in kkma.pos(doc)] return token_docdef tokenize_kkma_noun(doc): jpype.attachThreadToJVM() token_doc = [&#x27;/&#x27;.join(word) for word in kkma.pos(doc) if word[1] in filter_kkma] return token_doc 이런식으로 filter_kkma리스트를 하나 만들어 형태소를 분석했을 때 나오는 형태소가 filter_kkma에 포함되어 있을 경우만 학습 대상에 추가한다. tokenize_kkma를 쓰면 전체 형태소 분석, tokenize_kkmk_noun을 쓰면 동사 명사만 추출한다. 가장 결과가 좋게 나온 조합은 명사만 추출, for 문 50번에 epochs&#x3D;100으로 한 결과값. 12345678910111213141516#튜닝:명사만 추출kkma = Kkma()filter_kkma = [&#x27;NNG&#x27;, #보통명사 &#x27;NNP&#x27;, #고유명사 &#x27;OL&#x27; , #외국어 ]def tokenize_kkma(doc): jpype.attachThreadToJVM() token_doc = [&#x27;/&#x27;.join(word) for word in kkma.pos(doc)] return token_docdef tokenize_kkma_noun(doc): jpype.attachThreadToJVM() token_doc = [&#x27;/&#x27;.join(word) for word in kkma.pos(doc) if word[1] in filter_kkma] return token_doc 123456789101112131415161718192021222324252627# make model import multiprocessing import tensorflow as tf with tf.device(&#x27;/device:GPU:0&#x27;): cores = multiprocessing.cpu_count() d2v_faqs = doc2vec.Doc2Vec(vector_size=20, #100 alpha=0.025, min_alpha=0.025, hs=1, negative=0, dm=0, window=3, dbow_words = 1, min_count = 1, workers = cores, seed=0, epochs=100) d2v_faqs.build_vocab(tagged_faqs) # train document vectors for epoch in range(50): d2v_faqs.train(tagged_faqs, total_examples = d2v_faqs.corpus_count, epochs = d2v_faqs.epochs ) d2v_faqs.alpha -= 0.0025 # decrease the learning rate d2v_faqs.min_alpha = d2v_faqs.alpha # fix the learning rate, no decay 123test_string = &quot;변압기공동이용(모자거래)이란 무엇이며, 요금계산은 어떻게 합니까&quot;tokened_test_string = tokenize_kkma_noun(test_string)tokened_test_string 12345678910111213# 성능 측정# raten = 5 #정확도 = 0.5128205128205128 % (180/351 )raten = 1 #정확도 = 0.24216524216524216 % (85/351 ) found = 0for i in range(len(faqs)): tstr = faqs[&#x27;제목&#x27;][i] ttok = tokenize_kkma_noun(tstr) tvec = d2v_faqs.infer_vector(ttok) re = d2v_faqs.docvecs.most_similar([tvec], topn = raten) for j in range(raten): if i == re[j][0]: found = found + 1print(&quot;정확도 = &#123;&#125; % (&#123;&#125;/&#123;&#125; ) &quot;.format(found/len(faqs),found, len(faqs)) 모델 저장, 불러오기1234567891011121314151617# 모델 저장d2v_faqs.save(os.path.join(&#x27;data&#x27;,&#x27;/d2v_faqs_size100_min1_batch50_epoch100_nounonly_dm0.model&#x27;))# 모델 loadd2v_faqs_1 = doc2vec.Doc2Vec.load(os.path.join(&#x27;data&#x27;,&#x27;/d2v_faqs_size100_min1_batch50_epoch100_nounonly_dm0.model&#x27;))#testtest_string = &quot;건물을 새로 지을 때 임시전력은 어떻게 신청하나요&quot;tokened_test_string = tokenize_kkma_noun(test_string)tokened_test_stringtopn = 5# 모델 추측test_vector1 = d2v_faqs_1.infer_vector(tokened_test_string)result1 = d2v_faqs_1.docvecs.most_similar([test_vector1], topn=topn)for i in range(topn): print(&quot;모델 1 &#123;&#125;위. &#123;&#125;, &#123;&#125; &#123;&#125;&quot;.format(i+1, result1[i][1], result1[i][0],faqs[&#x27;제목&#x27;][result1[i][0]] )) Local path :C:\\Users\\jmj30\\Dropbox\\카메라 업로드\\Documentation\\2022\\2022 상반기\\휴먼교육센터\\mj_chatbot_prac\\faq_chatbot Reference: https://cholol.tistory.com/466","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Python","slug":"Python/Python","permalink":"https://jmj3047.github.io/categories/Python/Python/"}],"tags":[{"name":"Doc2vec","slug":"Doc2vec","permalink":"https://jmj3047.github.io/tags/Doc2vec/"},{"name":"Chatbot","slug":"Chatbot","permalink":"https://jmj3047.github.io/tags/Chatbot/"},{"name":"NLP","slug":"NLP","permalink":"https://jmj3047.github.io/tags/NLP/"}]},{"title":"Support Vector Machine","slug":"SVM","date":"2022-05-05T15:00:00.000Z","updated":"2022-07-09T05:59:13.000Z","comments":true,"path":"2022/05/06/SVM/","link":"","permalink":"https://jmj3047.github.io/2022/05/06/SVM/","excerpt":"","text":"1. 분류에 대한 수적 표현 학습 데이터 X(독립변수),Y(종속변수)가 있을 때 (i&#x3D;1,2,3,4,5 ….데이터의 갯수) Y⇒{-1,1} (두 개의 클래스를 의미) ⇒ 경우에 따라서, 클래스를 1과 -1 로 나눔 Y(정답) * F(x)(예측한 정답) &gt;0 라는 것은 제대로 분류된 형태 ( 같은 부호끼리 곱하면 양수인 경우니까) 2. 선형 분할(Linear Classifier) f(x)&#x3D; W transpose X + b (선형조합, 각각의 항들이 더하기로 이루어진 조합.) 선형분할은 직선으로 나누는 것 (2차원이건 3차원이건 그 이상이건 상관 없음) b(bias) Y 절편을 의미 W는 직선의 기울기 3. 초평면 분할 더 나은(최적) 분류를 위한 초평면(Hyperplane)→선 보다 더 큰 차원 좋은 판별선에 대한 기준 최적화: 좋은 것 을 극대화 시키고 나쁜 것 을 극소화 시키는 것 분류에서의 최적화: 잘 안나뉘는것 , 잘 나뉘는 것 나중에 Testing data 를 돌렸을때, 가장 좋게 나뉜 것은 반절로 나뉜 직선이다. Test data 가 어떻게 들어올지 모르는 것 이기때문에 , 과적합 되어 있는 것보다 확실히 절반으로 나누는것이 좋다. 최적의 분할 초평면 찾기 Margin: c는 선형분할의 각 클래스별 거리 각 클래스별 거리를 합친 것 Margin&#x3D;2c를 최대화 하는, w T x +b&#x3D;0 의 직선을 찾아야 하는것 이다. Marign 을 최대화 시키는 초평면이 최적 “Learning Theory” 에 따르면, Marigin을 최대화 시키는 초평면이 일반화 오류가 가장 낮게 나타남(Test data 에서도 좋은 점수가 나온다) Margin:초평면과 가장 근접한 각 클래스 관측치와의 거리의 합. Margin 수식 유도 일반적인 방법 점과 선 사이의 거리 거리 d 가 2개이니까 2&#x2F;||W|| Margin 최대화 (최적화) ||w|| 가 분모에 있기 때문에 결국 ||w|| 를 최소화 해주는것 이 2&#x2F;||w|| 를 최대화 해주는거랑 같다고 할 수 있다 우리는 결국 w 값을 최소화 시켜주는것이 목적이기 때문에 제곱을 취해주든 상수를 곱해주는 상관이 없다 Lagrange Multiplier(수학적 기법) ⇒ 제약조건을 최적화 조건에 녹여버리는 기법. 💡 라그랑쥬를 다 풀고 나면 판별식이 나온다. Xi tranpose X ( 학습데이터와 분류할 데이터의 내적) 4. SVM(Support Vector Machine) 판별식에 서포트벡터만 사용하기 때문에 아웃라이어에 대한 영향을 안 받음(KKT 조건으로 걸러냄) KNN 또한 이웃을 확인하는 개수인 K의 한계가 있어서 어느 elbow point 를 지나치면 정확도가 떨어진다. → 비슷한 원리 ⇒ svm 또한 분류를 유효하게 하기위해서 support verctor 만 이용해준다. 선형으로 완벽히 나눠지지 않는 데이터라면 테스트 데이터에게는 위의 모델 보다 아래 모델이 더 좋을 것 으로 보인다. 하지만 SVM 의 제약조건에는 트레인데이터가 완벽하게 나누어져야 한다는 제약 조건이 걸려있다. 어떻게 하면 좋을까? Slack Variable for “Soft Margin” Soft Margin SVM Non-linear SVM Reference: 한국공학대학교 경영학과 강지훈 교수님 강의","categories":[{"name":"Basic ML","slug":"Basic-ML","permalink":"https://jmj3047.github.io/categories/Basic-ML/"}],"tags":[{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"}]},{"title":"Setting Git & Virtualenv","slug":"Setting_Git_&_Virtualenv","date":"2022-05-05T15:00:00.000Z","updated":"2022-10-15T08:05:47.884Z","comments":true,"path":"2022/05/06/Setting_Git_&_Virtualenv/","link":"","permalink":"https://jmj3047.github.io/2022/05/06/Setting_Git_&_Virtualenv/","excerpt":"","text":"Put Local folder into git repo Make folder ‘example’ and git repo ‘example 123456789101112131415161718192021222324#in local cmd example foldergit init #add remote repogit remote add origin &#x27;repo https&#x27;#bring files in repo to localgit pull origin master #bring local files to git repogit add .git commit -m &#x27;updated&#x27;git push orgin master #check remotegit remote -v#check current statusgit status#error: failed to push some refs to &#x27;https://github.com/jmj3047/.git&#x27;#force to push git push -f origin master Setting virtual env in window&#x2F;linux1234567#****use virtual env no matter what****&gt;python -m venv env_name&gt;source env_name/Scripts/activate #window&gt;source env_name/bin/activate #linux#put all the version of modules in requirements.txt&gt;pip install -r requirements.txt","categories":[{"name":"Setting","slug":"Setting","permalink":"https://jmj3047.github.io/categories/Setting/"},{"name":"Git","slug":"Setting/Git","permalink":"https://jmj3047.github.io/categories/Setting/Git/"}],"tags":[{"name":"Git","slug":"Git","permalink":"https://jmj3047.github.io/tags/Git/"},{"name":"Virtualenv","slug":"Virtualenv","permalink":"https://jmj3047.github.io/tags/Virtualenv/"}]},{"title":"What is Transformer","slug":"What_is_Transformer","date":"2022-05-05T15:00:00.000Z","updated":"2022-10-15T08:05:13.624Z","comments":true,"path":"2022/05/06/What_is_Transformer/","link":"","permalink":"https://jmj3047.github.io/2022/05/06/What_is_Transformer/","excerpt":"","text":"Transformer란?트랜스포머(Transformer)는 구글에서 발표한 논문 “Attention is all you need”에 나오는 모델이다. 아래 글은 이 논문 abstract의 일부분이다. The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU … 여기서도 알 수 있듯이, 트랜스포머는 어텐션(Attention) mechanism을 기반으로 여러개의 인코더와 디코더를 연결한 구조를 갖고 있다. 또한 CNN, RNN, LSTM 등의 구조를 사용하지 않았기 때문에 학습 시간이 훨씬 감소된 성능을 내었다고 한다. 그렇다면 그 구조가 무엇인지 더 알아보도록 하자. (1) 트랜스포머의 입력먼저 트랜스포머의 입력부터 알아보자, 단어 벡터 데이터가 트랜스포머의 입력으로 들어가지게 되는데 이 때 각 단어의 위치 정보를 알려주어야 한다. 왜냐하면 트랜스포머에 단어가 입력될 때 순차적으로 받아지지 않기 때문이다. 따라서 순서 정보를 더해주어야 하기 때문에 위치 정보를 각 단어 벡터마다 더해주어야 하는데, 이 과정을 포지셔널 인코딩(positional encoding)이라고 한다. 포지셔널 인코딩 값을 더해주기 위해서는 사인함수와 코사인함수를 사용한 아래 두 함수를 사용한다. 위 식에서 pos는 입력된 데이터의 임베딩 벡터(몇번째 단어인지)를, i는 임베딩 벡터내의 차원의 인덱스(0~512)를 뜻한다. 임베딩 벡터내의 차원이란 트랜스포머 모델의 인코더와 디코더에서 정해진 입력과 출력의 크기를 말한다. 논문상에서 이 차원을 512로 설정했으면 이 차원은 인코더의 값을 디코더로 보낼때 값을 유지하도록 한다. 다시 돌아와서, 위 함수에서 차원이 2i(짝수)인지 2i+1(홀수)인지에 따라서 사용하는 함수가 다르다. 짝수 차원의 경우 사인함수, 홀수차원의 경우 코사인 함수를 사용하게된다. (2)인코더(Encoder)의 구조 위 이미지는 트랜스포머 논문에 함께 실려 있는 이미지로, 트랜스포머의 구조를 나타낸다.여기서 왼쪽 부분이 트랜스포머의 인코더 부분인데, 인코더의 구조는 어떻게 이루어졌을까? 먼저 이미지 왼쪽 아래를 보자. Input data가 들어가게 되면 Input Embedding을 거치게 되는데, 여기서는 문자열인 단어 데이터를 벡터형태로 변환해 준다(단어 길이 X 벡터차원의 행렬). 그리고 나서 위에서 설명한 포지셔널 인코딩을 수행해주게 된다. 그러고 나서 박스로 표현된 인코더에 들어가게 된다. 인코더 안에서는 크게 Multi-Head Attention과 Feed Forward과정이 수행되는데, Multi-Head Attention은 셀프 어텐션이 병렬적으로 사용된 것을 말하며, Feed Forward란 피드포워드 신경망 구조를 의미한다. 한편, 위에서 잠깐 언급했지만 트랜스포머에서는 여러 개의 인코더와 디코더를 쌓은 구조를 갖고 있다. 즉, 인코더가 1개가 아니라는 뜻인데, 논문에서는 6개의 인코더 층을 사용했다고 하니, 6개라고 설정하도록 하겠다. 아무튼, 인코더 과정을 총 6번 반복한다고 생각하면 된다. *셀프 어텐션이란?셀프 어텐션이란 자기 자신에게 어텐션 함수를 수행하는 것을 말하는데, 그렇다면 어텐션이란 무엇일까? 어텐션에서도 다양한 종류가 있는데 간단히 말하자면, 쿼리(Query)가 주어졌을 때, 이 쿼리와 여러개의 키(Key)와의 유사도를 각각 구하고, 구한 유사도를 가중치로 설정하여 각각의 값(value)을 구한 뒤, 이 값(유사도가 반영된 값)들을 모두 가중합하여 반환하는 함수를 말한다. 예를 들어, 한 텍스트 문장이 쿼리로 입력될 때, 각 단어 벡터들과의 유사도를 계산해 이 유사도를 가중합하여 반환된 값이 그 문장의 어텐션 값이 된다. 그렇다면 셀프 어텐션 값을 구하기 위해서 입력된 문장의 단어 벡터(쿼리)에 대해 쿼리(query),키(key), 값(value) 벡터가 정의되어야 할 것이다. 그 과정은 아래 이미지를 통해 쉽게 이해할 수 있다. ‘student’라는 단어 벡터가 입력되었을 때, 각각 쿼리, 키 값의 가중치 행렬을 곱해주어 쿼리, 키, 값 벡터를 얻어낸다. 이렇게 쿼리 벡터, 키 벡터, 값 벡터를 얻어냈다면 쿼리 벡터는 모든 키 벡터에 대해 어텐션 스코어(attention score)를 구하게 되고, 이를 이용하여 모든 값 벡터를 가중합 하여 어텐션 값을 구하게 된다. 한편, 이러한 연산은 각 단어마다가 아닌 문장 전체에 대해서 행렬 연산으로도 일괄적으로 연산이 가능한데, 위와 같이 문장에 대한 쿼리 벡터, 키 벡터의 연산을 통해 값 벡터 행렬을 구할 수 있게 된다. 마지막으로 쿼리벡터와 키 벡터가 연산되어 나온 행렬에 전체적으로 특정 값(key벡터 차원의 제곱근 값)을 나누어 준 뒤, 소프트맥스 함수를 적용해주고, 가중치가 계산된 값 벡터를 곱하게 되면 최종적으로 각 단어의 어텐션 값을 가지는 어텐션 값 행렬이 도출된다. 즉, 요약하자면 어텐션 함수는 쿼리(Query)가 주어졌을 때, 이 쿼리와 여러개의 키(key)와의 유사도를 각각 구하고, 구한 유사도를 가중치로 설정하여 각각의 값(value)을 구한 뒤, 유사도가 반영된 값들을 모두 가중합 하여 반환하는 함수를 말한다. *멀티 헤드 어텐션(Multi-Head Attention)이란?앞에서 트랜스포머의 인코더에서는 어텐션이 병렬적으로 수행되는 멀티 헤드 어텐션이 수행된다고 했다. 논문에서는 512차원의 벡터를 8로 나누어 54차원의 Query, Key, Value 벡터로 바꾸어서 어텐션 함수를 병렬적으로 수행한 것인데, 그렇다면 왜 이렇게 수행한 것일까? 즉, 차원을 나누어서 어텐션 함수를 수행한 뒤, 가중치 행렬을 곱해주고 이를 다시 합치게 되는건데, 논문에 따르면 single attention function을 하는 것보다 병렬적으로 수행하는 것이 모델이 학습하는 데에는 더 효과적이었으며, 모델이 다른 영역(과거시점과 미래시점)에 있는 정보들을 참조할 수 있다고한다. 따라서 출력된 값들은 인코더의 입력 값의 차원과 동일하게 유지된다. *피드 포워드 신경망이란?인코더 안에서 Multi-Head Attention이 수행되고 나면 Feed Forward가 수행된다고 했었는데, Feed Forward는 무엇일까? Feed Forward는 일종의 신경망으로 Feed Forward Neural Network를 줄여서 FFNN이라고 한다. FFNN의 종류도 여러가지가 있는데, 트랜스 포머의 인코더 층에는 포지션 와이즈(Position-wise) FFNN을 사용한다. 포지션 와이즈 FFNN은 Fully-connected FFNN과 같은 기능을 하는데, 아래와 같은 연산을 수행한다. 위 식에서 x의 값은 Multi-Head Attention에서 출력된 행렬 값이다. 반면, 가중치를 의미하는 W1, W2, b1, b2는 가중치 값으로 인코더 마다 다른 값을 가지지만 하나의 인코더 층 안에서는 문장과 단어들마다 동일하게 사용된다고 한다. 이렇게 피드 포워드 신경망까지 거치게 되면 한 인코더의 출력값이 도출 되고, 이 값은 다시 두번째 인코더 입력으로 들어가게 되며 이 과정이 반복된다. *Add &amp; Norm한편 인코더의 구조를 보여준 이밎를 다시 보고 오면 2개의 서브층인 Multi-Head Attention과 Feed Forward가 각각 끝나고 나면 Add &amp; Norm 이라는 단계가 수행된다. 이것은 또 무엇일까? 논문의 일부분을 읽어보면 Add &amp; Norm이란 바로 두개의 서브층을 residual connection 해주고 layer normalization을 해주는 것을 의미한다. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. Residual connection과 layer normalization에 대해 짧게 요약하자면, residual connection은 서브층의 입력과 출력을 더 하는 것이다. 이러한 알고리즘은 RNN, VGG 구조에서도 볼 수 있고, 이러한 연산이 가능한 것은 입력 데이터와 출력 데이터가 동일한 차원을 갖고 있기 때문이라고 한다. 반면, layer normalization은 정규화를 하는 과정으로 출력된 값들에 대해서 평균과 분산을 구해서 정규화를 하는 것을 말한다. 앞에서 입력데이터인 512차원의 벡터를 8로 나누어 어텐션 함수를 병렬적으로 수행하였다고 했는데, 그렇게 출력된 8개의 값들로 layer normalization을 하는 것이다. (3)디코더(Decoder)의 구조 다시 트랜스포머의 구조를 살펴보자. 지금까지 왼쪽에 있는 인코더에 대해 살펴 보았고, 이제 오른쪽에 있는 디코더에 대해 살펴보도록 하겠다. 디코더는 인코더에서 넘겨받은 값에 대해 Multi-Head Attention과 Feed Forward를 수행하기 전 output data에 대해 임베딩과 포지셔널 인코딩을 한 값을 입력 받는다. 그리고 인코더와는 다르게 Masked Multi-Head Attention이라는 것을 해주게 된다. *Masked Multi-Head Attention이란?Masked Multi-Head Attention은 말그대로 Multi-Head Attention에서 Mask기능이 들어간 것이다. 앞에서 Multi-Head Attention은 셀프어텐션을 병렬적으로 수행한 것을 의미했었다. 따라서 다른 영역에 있는, 즉 미래의 시점에 있는 단어의 정보도 알 수 있게 된다고 했었다. 이러한 이유로 트랜스포머의 디코더에는 현재시점보다 미래에 있는 단어를 참고해 예측하지 못하고 이전시점들에 있는 단어들만 참고할 수 있도록 마스킹해줘야 한다. 아마 미래에 있는 단어를 참고해 예측하도록 한다면 학습하는데 도움이 되지 않는가 보다. 답지보고 베끼는 느낌이랄까? 아무튼 마스킹을 하기 위해 lood-ahead mask라는 것을 해주는데, Multi-Head Attention을 통해 나온 행렬 값에 대해 마스킹을 하고자 하는 값에는 1, 마스킹을 하지 않는 값에는 0을 리턴하도록 한다. 그리고 나서 Add &amp; Norm 과정을 수행해준뒤 도출된 결과를 다음 결과로 보내준다. *디코더의 Multi-Head Attention과 Feed Forward디코더에서 Masked Multi-Head Attention이 수행되고 나면 그 다음부터는 인코더와 마찬가지로 Multi-Head Attention과 Feed Forward가 수행된다. 근데 이때 Multi-Head Attention에 입력으로 들어가는 값들을 잘 살펴 봐야 한다. 인코더에서 출력된 값과 디코더 첫번째 서브층에서 출력된 값이 인풋으로 들어가기 때문이다. 두번째 서브층인 Multi-Head Attention에서는 마찬가지로 셀프어텐션 함수를 수행하기 위해 Query, Key, Value 벡터가 입력되어야 한다. 이때 Query는 디코더 첫번째 서브층에서 출력된 값이 해당되고, Key 벡터와 Value 벡터는 마지막 인코더에서 출력된 값으로 입력된다. 그리고 똑같이 Multi-Head Attention을 수행해주게 된다. 이렇게 6개의 디코더마다 Multi-Head Attention의 Query 벡터는 디코더 첫번째 서브층의 output, Key벡터와 Value벡터는 인코더의 output이 입력으로 들어가게 된다. Reference attention 논문: https://arxiv.org/abs/1409.0473 https://wikidocs.net/31379 https:&#x2F;&#x2F;velog.io&#x2F;@seolini43&#x2F;파이썬Transformer로-오피스-챗봇-만들기-이론편","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"NLP","slug":"Paper/NLP","permalink":"https://jmj3047.github.io/categories/Paper/NLP/"}],"tags":[{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"},{"name":"Transformer","slug":"Transformer","permalink":"https://jmj3047.github.io/tags/Transformer/"},{"name":"Attention","slug":"Attention","permalink":"https://jmj3047.github.io/tags/Attention/"}]},{"title":"HTML with Python_CGI","slug":"HTML_with_Python_CGI","date":"2022-05-03T15:00:00.000Z","updated":"2022-05-04T08:38:19.000Z","comments":true,"path":"2022/05/04/HTML_with_Python_CGI/","link":"","permalink":"https://jmj3047.github.io/2022/05/04/HTML_with_Python_CGI/","excerpt":"","text":"** code: https://github.com/jmj3047/mj_chatbot_prac&#x2F;blob&#x2F;c5bec233b833b24345deeffe7391621415dc1dcb&#x2F;cgi_webpython.py Making Website with CGI Constructing Web Server: Download and Install Apche Official: Installing Apache in window(https://httpd.apache.org/docs/2.4/platform/windows.html) Beginner version: Install Bitnami Wamp Stack push 1 or 2 click this button below it takes some time to install it Constructing Web Server: Bitnami Wamp Stack Start Bitnami Wamp Stack Click Go to Application: If you can see the site like the picture below, success! Start or Stop the server: Click Manage Server If program below is shut down, go to the folder where ‘bitnami wamp stack’ installed and click ‘manager-windows.exe’ Using Python in Web(HTML): Setting Apache Install python and apache find folder where apach installed(‘D:\\wamp\\apache2\\conf’) &gt; ‘conf’ folder &gt; httpd.conf open httpd.conf file and search: 1LoadModule cgid_module modules/mod_cgid.so and if ‘#’ exists in front of code, delete it Find tag in httpd.conf and add some lines 1234567891011121314151617181920212223242526272829303132333435&lt;Directory &quot;/Applications/mampstack-8.0.5-0/apache2/htdocs&quot;&gt; # # Possible values for the Options directive are &quot;None&quot;, &quot;All&quot;, # or any combination of: # Indexes Includes FollowSymLinks SymLinksifOwnerMatch ExecCGI MultiViews # # Note that &quot;MultiViews&quot; must be named *explicitly* --- &quot;Options All&quot; # doesn&#x27;t give it to you. # # The Options directive is both complicated and important. Please see # http://httpd.apache.org/docs/2.4/mod/core.html#options # for more information. # Options Indexes FollowSymLinks # # AllowOverride controls what directives may be placed in .htaccess files. # It can be &quot;All&quot;, &quot;None&quot;, or any combination of the keywords: # AllowOverride FileInfo AuthConfig Limit # AllowOverride None # # Controls who can get stuff from this server. # Require all granted ********** add this code ********** &lt;Files *.py&gt; Options ExecCGI AddHandler cgi-script .py &lt;/Files&gt; *********************************** &lt;/Directory&gt; htdocs 디렉토리 내 확장자가 py인 모든 파일은 CGI기능을 활성시키고 CGI로 실행하라는 의미 Restart Apache Web Server in manager-osx Python file setting index.py 가 있는 htdocs 디렉토리에서 index.py 실행 후 아래같이 입력(다른 파이썬 파일을 만들어도 상관 없음) 123#!/usr/local/bin/python3 &gt;&gt;&gt; python.exe 경로 환경변수에 저장해줬다면 !Python만 해도 됨 print(&quot;Content-Type: text/html&quot;) print() index.html 의 코드 넣기( 다른 html 파일 이어도 됨): index.py가 실행되었을 때 index.html 의 코드가 출력되게 해주는 코드 12345678910111213141516171819202122#!/usr/local/bin/python3print(&quot;Content-Type: text/html&quot;)print()print(&#x27;&#x27;&#x27;&lt;!doctype html&gt; # ---&gt; 줄바꿈을 위해 docsting (&#x27;&#x27;&#x27; &#x27;&#x27;&#x27;) 사용&lt;html&gt;&lt;head&gt; &lt;title&gt;WEB1 - Welcome&lt;/title&gt; &lt;meta charset=&quot;utf-8&quot;&gt;&lt;/head&gt;&lt;body&gt; &lt;h1&gt;&lt;a href=&quot;index.html&quot;&gt;WEB&lt;/a&gt;&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;a href=&quot;qs-1.html&quot;&gt;HTML&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;qs-2.html&quot;&gt;CSS&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;qs-3.html&quot;&gt;JavaScript&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h2&gt;WEB&lt;/h2&gt; &lt;p&gt;The World Wide Web (abbreviated WWW or the Web) is an information space where documents and other web resources are identified by Uniform Resource Locators (URLs), interlinked by hypertext links, and can be accessed via the Internet.[1] English scientist Tim Berners-Lee invented the World Wide Web in 1989. He wrote the first web browser computer program in 1990 while employed at CERN in Switzerland.[2][3] The Web browser was released outside of CERN in 1991, first to other research institutions starting in January 1991 and to the general public on the Internet in August 1991. &lt;/p&gt;&lt;/body&gt;&lt;/html&gt;&#x27;&#x27;&#x27;) 웹 브라우저 주소창에 localhost:8080&#x2F;index.py 입력하고 접속 index.html 파일의 내용이 잘 출력된다면 구현 성공 Internal Server Error 가 확인된다면 에디터에서 apache2&#x2F;logs 디렉토리 내 error_log 파일에 있는 에러 코드 확인 및 구글링 EXAMPLE123456789101112131415161718192021222324252627282930#!C:\\Python310\\python.exe ---&gt;파이썬 경로# 한글이 꺠지지 않으려면 꼭 넣어야 함# -*- coding:utf-8 -*-import sysimport codecssys.stdout =codecs.getwriter(&quot;utf-8&quot;)(sys.stdout.detach())import cgi# cgitb는 CGI 프로그래밍시 디버깅을 위한 모듈로 cgitb.enable()할 경우 런타임 에러를 웹브라우저로 전송함# cgitb.enable() 하지 않은 상태로 실행 중 오류가 발생한 경우 웹서버는 클라이언트에게 HTTP응답 코드 500을 전송함import cgitbcgitb.enable()# HTTP 규격에서 헤더 전송 이후에는 반드시 줄 바꿈을 하게되어 있음으로 마지막에 \\r\\n을 전송# 마지막에 \\r\\n을 전송하지 않으면 브라우저 측에서 오류가 발생print(&quot;Content-type: text/html;charset=utf-8\\r\\n&quot;)print(&quot;&quot;&quot; &lt;!doctype html&gt; &lt;html&gt; &lt;head&gt; &lt;meta charset=&#x27;utf-8&#x27;&gt; &lt;h1&gt;안녕?&lt;/h1&gt; &lt;h2&gt;Thank you so much&lt;/h2&gt; &lt;h3&gt;This page is made by Python&lt;/h3&gt; &quot;&quot;&quot;)a = 3+4+5b = a/3print(&#x27;b는 :&#x27;, b)print(&quot;&lt;/head&gt;&quot;)print(&quot;&lt;/html&gt;&quot;) Result Reference https://daekiry.tistory.com/4?category=928946 https://daekiry.tistory.com/5?category=928946 https://daekiry.tistory.com/6 https:&#x2F;&#x2F;velog.io&#x2F;@ssoulll&#x2F;python-웹-페이지를-CGI로-구현","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"HTML","slug":"Python/HTML","permalink":"https://jmj3047.github.io/categories/Python/HTML/"}],"tags":[{"name":"Web Server","slug":"Web-Server","permalink":"https://jmj3047.github.io/tags/Web-Server/"},{"name":"CGI","slug":"CGI","permalink":"https://jmj3047.github.io/tags/CGI/"}]},{"title":"HTML with Python_Flask & Brython","slug":"HTML_with_Python_Flask&Brython","date":"2022-05-03T15:00:00.000Z","updated":"2022-05-04T08:42:26.000Z","comments":true,"path":"2022/05/04/HTML_with_Python_Flask&Brython/","link":"","permalink":"https://jmj3047.github.io/2022/05/04/HTML_with_Python_Flask&Brython/","excerpt":"","text":"Flask 파이썬 기반 마이크로 웹 개발 프레임워크 웹 개발의 핵심기능만 간경하게 유지 필요한 기능은 다른 라이브러리나 프레임워크로 손쉽게 확장 신속하게 최소한의 노력으로 웹 애플리케이션 개발 가능 Installation start virtualenv pip install flask Error → note: could not find a version that satisfies the requirement flask → 네트워크 문제로 외부 라이브러리 저장소에 접근하지 못할 경우 나오는 문제, → 직접 https://github.com/mitsuhiko/flask 위치로 가서 소스 받아 설치 해야 함. Strat Flask**code: https://github.com/jmj3047/mj_chatbot_prac&#x2F;blob&#x2F;c5bec233b833b24345deeffe7391621415dc1dcb&#x2F;flask_prac&#x2F;0.flask_hello.py 12345678910from flask import Flaskapp = Flask(__name__)@app.route(&#x27;/&#x27;)def hello_world(): return &#x27;Hello World!&#x27; if __name__ == &#x27;__main__&#x27;: app.debug =True app.run() 소스를 실행하고, terminal 에서 ‘python flask_test.py’ 입력 후 http://127.0.0.1:5000 으로 접근 Process for starting Flask Application 특정 URL 호출(request) : http://127.0.0.1:5000/ 또는 http://localhost:5000 특정 URL 매핑 검색 : @app.route(‘&#x2F;‘) 특정 URL에 매칭된 함수(def 함수) 실행 : def hello_world() 비즈니스 로직 실행 : result 결과 응답으로 전송(response): return result HTML 로 화면에 출력 쿠키(Cookie), 세션(Session), 로깅(logging) 등 제공 Routing**code: https://github.com/jmj3047/mj_chatbot_prac&#x2F;blob&#x2F;c5bec233b833b24345deeffe7391621415dc1dcb&#x2F;flask_prac&#x2F;1.flask_login.py URL을 통해 처리할 핸들러를 찾는 것 플라스크는 복잡한 URI를 함수로 연결하는 방법을 제공 URI 를 연결하는 route() 데코레이터 함수 제공 &#x2F; 접속 시 root_world() 가 호출 됨 &#x2F;hello 접속 시 hello_world() 가 호출 됨 12345678910111213141516from flask import Flask, redirect, url_forapp = Flask(__name__)@app.route(&#x27;/&#x27;) #127.0.0.1:5000에 가면 함수 실행def root_world(): result = &#x27;root world&#x27; return result@app.route(&#x27;/hello&#x27;) #127.0.0.1:5000/hello 를 가면 실행def hello_world(): result = &#x27;hello world&#x27; return resultif __name__ == &#x27;__main__&#x27;: app.debug =True app.run() app.debug는 개발의 편의를 위해 존재 True값을 경우 코드를 변경하면 자동으로 서버가 재 실행 됨 또한, 웹상에서 파이썬 코드를 수행할 수 있게 되므로, 운영환경에서 사용을 유의해야 함. 현재 접근은 개발 소스가 존재하는 로컬에서만 접근 가능 외부에서도 접근을 가능하게 하려면 app.run(host&#x3D;’0.0.0.0’)로 서버 실행 부를 변경해야 함 1234567891011121314151617181920212223242526272829303132from flask import Flask, redirect, url_forapp = Flask(__name__)@app.route(&#x27;/users/&lt;user_id&gt;&#x27;) #동적 변수를 사용하여 URI 접속# &lt;동적변수&gt;를 뷰함수의 인자로 사용# &lt;동적 변수&gt; 다음에 /를 넣으면 안됨def user_id(userid): result = &#x27;user_id = &#x27; + userid return result@app.route(&#x27;/admin&#x27;)def hello_admin(): return &#x27;Hello Admin&#x27;@app.route(&#x27;/guest/&lt;guest&gt;&#x27;)def hello_guest(guest): return &#x27;Hello %s as Guest&#x27; % guest@app.route(&#x27;/user/&lt;name&gt;&#x27;)def hello_user(name): if name == &#x27;admin&#x27;: return redirect(url_for(&#x27;hello_admin&#x27;)) else: return redirect(url_for(&#x27;hello_guest&#x27;, guest=name))# url_for(): 함수를 호출하는 URI를 반환# redirect(): 다른 route 경로 이동(다른 페이지 이동)if __name__ == &#x27;__main__&#x27;: app.debug = True app.run() Flask GET 방식으로 값 전송 및 처리**code: https://github.com/jmj3047/mj_chatbot_prac&#x2F;blob&#x2F;c5bec233b833b24345deeffe7391621415dc1dcb&#x2F;flask_prac&#x2F;2.flask_app.py https://github.com/jmj3047/mj_chatbot_prac&#x2F;blob&#x2F;c5bec233b833b24345deeffe7391621415dc1dcb&#x2F;flask_prac&#x2F;templates&#x2F;login&#x2F;login_form_get.html mkdir templates 폴더 생성 login_form_get.html 파일 작성 get 방식 지정 : method&#x3D;”get” 1234567891011121314151617181920from flask import Flask, request, session, render_templateapp = Flask(__name__)@app.route(&#x27;/login_form_get&#x27;) def login_form_get(): return render_template(&#x27;login/login_form_get.html&#x27;)@app.route(&#x27;/login_get_proc&#x27;, methods=[&#x27;GET&#x27;]) def login_get_proc(): user_id = request.args.get(&#x27;user_id&#x27;) user_pwd = request.args.get(&#x27;user_pwd&#x27;) if len(user_id) == 0 or len(user_pwd) == 0: return &#x27;no &#123;&#125; or &#123;&#125;&#x27;.format(user_id, user_pwd) return &#x27;welcome &#123;&#125;&#x27;.format(user_id)if __name__ == &#x27;__main__&#x27;: app.debug = True app.run() 12345678910111213141516&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;meta charset = &quot;UTF-8&quot;&gt; &lt;title&gt;login_form_get.html&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt; &lt;form action=&quot;/login_get_proc&quot; method=&quot;get&quot;&gt; ID: &lt;input type=&quot;text&quot;, name=&quot;user_id&quot;&gt;&lt;br&gt; PW: &lt;input type=&quot;password&quot;, name=&quot;user_pwd&quot;&gt;&lt;br&gt; &lt;input type=&quot;submit&quot;, value=&quot;Click&quot;&gt; &lt;/form&gt; &lt;/h1&gt; &lt;/body&gt;&lt;/html&gt; Brython**code:https://github.com/jmj3047/mj_chatbot_prac&#x2F;blob&#x2F;c5bec233b833b24345deeffe7391621415dc1dcb&#x2F;flask_prac&#x2F;templates&#x2F;brython_test.html python을 HTML 코드에 삽입해서 사용 12345678910111213141516171819202122&lt;html&gt; &lt;head&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;/path/to/brython.js&quot;&gt;&lt;/script&gt; &lt;/head&gt; &lt;body onload=&quot;brython()&quot;&gt; &lt;script type=&quot;text/python&quot;&gt; from browser import document, alert def echo(event): alert(document[&quot;zone&quot;].value) document[&quot;mybutton&quot;].bind(&quot;click&quot;, echo) &lt;/script&gt; &lt;input id=&quot;zone&quot;&gt;&lt;button id=&quot;mybutton&quot;&gt;click !&lt;/button&gt; &lt;/body&gt;&lt;/html&gt; Reference: https://essim92.tistory.com/8 https://code-examples.net/ko/q/dc0356 https://github.com/brython-dev/brython #Test Brython online(DEMO)","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"HTML","slug":"Python/HTML","permalink":"https://jmj3047.github.io/categories/Python/HTML/"}],"tags":[{"name":"Web Server","slug":"Web-Server","permalink":"https://jmj3047.github.io/tags/Web-Server/"},{"name":"Flask","slug":"Flask","permalink":"https://jmj3047.github.io/tags/Flask/"},{"name":"Brython","slug":"Brython","permalink":"https://jmj3047.github.io/tags/Brython/"}]},{"title":"Making Office Chatbot with Transformer","slug":"Making_Office_Chatbot_with_Transformer","date":"2022-05-03T15:00:00.000Z","updated":"2022-10-15T08:08:18.515Z","comments":true,"path":"2022/05/04/Making_Office_Chatbot_with_Transformer/","link":"","permalink":"https://jmj3047.github.io/2022/05/04/Making_Office_Chatbot_with_Transformer/","excerpt":"","text":"일상 대화와 오피스 대화 데이터를 Transformer 모델로 학습시켜, 질문에 대한 적절한 답변을 하는 챗봇 Data: 한국어대화데이터셋(오피스데이터) 사용 (AIHUB에 ‘개방데이터-인식기술 언어지능-한국어대화데이터셋’에서 로그인 후 다운로드) GPU 사용 그 외 환경 123batch size = 64buffer size = 20000epochs = 50 1. Environments1!pip install tensorflow_datasets 12345678import pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport reimport urllib.requestimport timeimport tensorflow_datasets as tfdsimport tensorflow as tf 12import osos.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;3&quot; 1234567with tf.device(&#x27;/device:GPU:3&#x27;): # 텐서 생성 a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]) b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]) c = tf.matmul(a, b) print(c) tf.Tensor( [[22. 28.] [49. 64.]], shape=(2, 2), dtype=float32) 2. 데이터 전처리12data = pd.read_csv(&#x27;./ChatbotData.csv&#x27;)data = data[0:5290] 1data[:10] 123456789f = open(r&#x27;./conversation_office.txt&#x27;,&quot;r&quot;)lines = f.readlines()Q = []A = []for i in range(len(lines)) : if i%2 == 0 : Q.append(lines[i][2:-1]) A.append(lines[i+1][2:-1]) 123456import pandas as pddf = pd.DataFrame()df[&#x27;Q&#x27;] = Qdf[&#x27;A&#x27;] = Adf[&#x27;label&#x27;] = 1 1df[:10] 1234#두 데이터를 concat() 함수를 이용하여 합쳐 하나의 데이터프레임으로 나타내주도록 함train_data = pd.concat([data, df],ignore_index=True)train_data = train_data.sample(frac=1).reset_index(drop=True) #데이터를 랜덤으로 섞어주는 코드 3. 단어 집합 생성123456789101112131415# 문장 그대로 학습 모델에 넣으면 모델이 인식을 할 수 없기 때문에 단어 집합을 만들어 줘야 함.# 정수 인코딩과 패딩을 해주는 작업을 해주어야 함#특수기호 띄어쓰기questions = []for sentence in train_data[&#x27;Q&#x27;]: sentence = re.sub(r&quot;([?.!,])&quot;, r&quot; \\1 &quot;, sentence) sentence = sentence.strip() questions.append(sentence) answers = []for sentence in train_data[&#x27;A&#x27;]: sentence = re.sub(r&quot;([?.!,])&quot;, r&quot; \\1 &quot;, sentence) sentence = sentence.strip() answers.append(sentence) 123456789# 서브워드텍스트인코더를 사용하여 질문, 답변 데이터로부터 단어 집합(Vocabulary) 생성tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus( questions + answers, target_vocab_size=2**13) # 시작 토큰과 종료 토큰에 대한 정수 부여START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]# 시작 토큰과 종료 토큰을 고려하여 단어 집합의 크기를 + 2VOCAB_SIZE = tokenizer.vocab_size + 2 123456789101112131415161718192021222324252627#정수 인코딩과 패딩# 서브워드텍스트인코더 토크나이저의 .encode()를 사용하여 텍스트 시퀀스를 정수 시퀀스로 변환.print(&#x27;임의의 질문 샘플을 정수 인코딩 : &#123;&#125;&#x27;.format(tokenizer.encode(questions[20])))#출력 : 임의의 질문 샘플을 정수 인코딩 : [8656, 331]# 최대 길이를 40으로 정의MAX_LENGTH = 40# 토큰화 / 정수 인코딩 / 시작 토큰과 종료 토큰 추가 / 패딩def tokenize_and_filter(inputs, outputs): tokenized_inputs, tokenized_outputs = [], [] for (sentence1, sentence2) in zip(inputs, outputs): # encode(토큰화 + 정수 인코딩), 시작 토큰과 종료 토큰 추가 sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN tokenized_inputs.append(sentence1) tokenized_outputs.append(sentence2) # 패딩 tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences( tokenized_inputs, maxlen=MAX_LENGTH, padding=&#x27;post&#x27;) tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences( tokenized_outputs, maxlen=MAX_LENGTH, padding=&#x27;post&#x27;) return tokenized_inputs, tokenized_outputs 임의의 질문 샘플을 정수 인코딩 : [2704, 1081, 13, 542] 1questions, answers = tokenize_and_filter(questions, answers) 1234#sample# 정수 인코딩과 패딩이 된 결과가 출력print(questions[0])print(answers[0]) [10023 31 121 4282 10024 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [10023 3607 213 13 21 1 10024 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] 12from tensorflow.python.client import device_libdevice_lib.list_local_devices() 1234567891011121314151617181920212223#인코더와 디코더의 입력 데이터가 되도록 배치 크기로 데이터를 묶어줌 with tf.device(&#x27;/device:GPU:3&#x27;): BATCH_SIZE = 64 BUFFER_SIZE = 20000 dataset = tf.data.Dataset.from_tensor_slices(( &#123; &#x27;inputs&#x27;: questions, &#x27;dec_inputs&#x27;: answers[:, :-1] # 디코더의 입력 / 마지막 패딩 토큰 제거 &#125;, &#123; &#x27;outputs&#x27;: answers[:, 1:] # 맨 처음 토큰이 제거 = 시작 토큰 제거 &#125;, )) dataset = dataset.cache() dataset = dataset.shuffle(BUFFER_SIZE) dataset = dataset.batch(BATCH_SIZE) dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE) 4. 트랜스포머 모델 만들기123456789101112131415161718192021222324252627282930313233343536373839def transformer(vocab_size, num_layers, dff, d_model, num_heads, dropout, name=&quot;transformer&quot;): # 인코더의 입력 inputs = tf.keras.Input(shape=(None,), name=&quot;inputs&quot;) # 디코더의 입력 dec_inputs = tf.keras.Input(shape=(None,), name=&quot;dec_inputs&quot;) # 인코더의 패딩 마스크 enc_padding_mask = tf.keras.layers.Lambda( create_padding_mask, output_shape=(1, 1, None), name=&#x27;enc_padding_mask&#x27;)(inputs) # 디코더의 룩어헤드 마스크(첫번째 서브층) look_ahead_mask = tf.keras.layers.Lambda( create_look_ahead_mask, output_shape=(1, None, None), name=&#x27;look_ahead_mask&#x27;)(dec_inputs) # 디코더의 패딩 마스크(두번째 서브층) dec_padding_mask = tf.keras.layers.Lambda( create_padding_mask, output_shape=(1, 1, None), name=&#x27;dec_padding_mask&#x27;)(inputs) # 인코더의 출력은 enc_outputs. 디코더로 전달된다. enc_outputs = encoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff, d_model=d_model, num_heads=num_heads, dropout=dropout, )(inputs=[inputs, enc_padding_mask]) # 인코더의 입력은 입력 문장과 패딩 마스크 # 디코더의 출력은 dec_outputs. 출력층으로 전달된다. dec_outputs = decoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff, d_model=d_model, num_heads=num_heads, dropout=dropout, )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask]) # 다음 단어 예측을 위한 출력층 outputs = tf.keras.layers.Dense(units=vocab_size, name=&quot;outputs&quot;)(dec_outputs) return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name) 1234567891011121314151617181920212223242526272829303132class PositionalEncoding(tf.keras.layers.Layer): def __init__(self, position, d_model): super(PositionalEncoding, self).__init__() self.pos_encoding = self.positional_encoding(position, d_model) def get_angles(self, position, i, d_model): angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32)) return position * angles def positional_encoding(self, position, d_model): angle_rads = self.get_angles( position=tf.range(position, dtype=tf.float32)[:, tf.newaxis], i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :], d_model=d_model) # 배열의 짝수 인덱스(2i)에는 사인 함수 적용 sines = tf.math.sin(angle_rads[:, 0::2]) # 배열의 홀수 인덱스(2i+1)에는 코사인 함수 적용 cosines = tf.math.cos(angle_rads[:, 1::2]) angle_rads = np.zeros(angle_rads.shape) angle_rads[:, 0::2] = sines angle_rads[:, 1::2] = cosines pos_encoding = tf.constant(angle_rads) pos_encoding = pos_encoding[tf.newaxis, ...] print(pos_encoding.shape) return tf.cast(pos_encoding, tf.float32) def call(self, inputs): return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :] 1234567891011121314151617181920212223242526272829303132333435def create_padding_mask(x): mask = tf.cast(tf.math.equal(x, 0), tf.float32) # (batch_size, 1, 1, key의 문장 길이) return mask[:, tf.newaxis, tf.newaxis, :]# 디코더의 첫번째 서브층(sublayer)에서 미래 토큰을 Mask하는 함수def create_look_ahead_mask(x): seq_len = tf.shape(x)[1] look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0) padding_mask = create_padding_mask(x) # 패딩 마스크도 포함 return tf.maximum(look_ahead_mask, padding_mask)#encoderdef encoder(vocab_size, num_layers, dff, d_model, num_heads, dropout, name=&quot;encoder&quot;): inputs = tf.keras.Input(shape=(None,), name=&quot;inputs&quot;) # 인코더는 패딩 마스크 사용 padding_mask = tf.keras.Input(shape=(1, 1, None), name=&quot;padding_mask&quot;) # 포지셔널 인코딩 + 드롭아웃 embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs) embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32)) embeddings = PositionalEncoding(vocab_size, d_model)(embeddings) outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings) # 인코더를 num_layers개 쌓기 for i in range(num_layers): outputs = encoder_layer(dff=dff, d_model=d_model, num_heads=num_heads, dropout=dropout, name=&quot;encoder_layer_&#123;&#125;&quot;.format(i), )([outputs, padding_mask]) return tf.keras.Model( inputs=[inputs, padding_mask], outputs=outputs, name=name) 1234567891011121314151617181920212223242526272829def encoder_layer(dff, d_model, num_heads, dropout, name=&quot;encoder_layer&quot;): inputs = tf.keras.Input(shape=(None, d_model), name=&quot;inputs&quot;) # 인코더는 패딩 마스크 사용 padding_mask = tf.keras.Input(shape=(1, 1, None), name=&quot;padding_mask&quot;) # 멀티-헤드 어텐션 (첫번째 서브층 / 셀프 어텐션) attention = MultiHeadAttention( d_model, num_heads, name=&quot;attention&quot;)(&#123; &#x27;query&#x27;: inputs, &#x27;key&#x27;: inputs, &#x27;value&#x27;: inputs, # Q = K = V &#x27;mask&#x27;: padding_mask # 패딩 마스크 사용 &#125;) # 드롭아웃 + 잔차 연결과 층 정규화 attention = tf.keras.layers.Dropout(rate=dropout)(attention) attention = tf.keras.layers.LayerNormalization( epsilon=1e-6)(inputs + attention) # 포지션 와이즈 피드 포워드 신경망 (두번째 서브층) outputs = tf.keras.layers.Dense(units=dff, activation=&#x27;relu&#x27;)(attention) outputs = tf.keras.layers.Dense(units=d_model)(outputs) # 드롭아웃 + 잔차 연결과 층 정규화 outputs = tf.keras.layers.Dropout(rate=dropout)(outputs) outputs = tf.keras.layers.LayerNormalization( epsilon=1e-6)(attention + outputs) return tf.keras.Model( inputs=[inputs, padding_mask], outputs=outputs, name=name) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960class MultiHeadAttention(tf.keras.layers.Layer): def __init__(self, d_model, num_heads, name=&quot;multi_head_attention&quot;): super(MultiHeadAttention, self).__init__(name=name) self.num_heads = num_heads self.d_model = d_model assert d_model % self.num_heads == 0 # d_model을 num_heads로 나눈 값. # 논문 기준 : 64 self.depth = d_model // self.num_heads # WQ, WK, WV에 해당하는 밀집층 정의 self.query_dense = tf.keras.layers.Dense(units=d_model) self.key_dense = tf.keras.layers.Dense(units=d_model) self.value_dense = tf.keras.layers.Dense(units=d_model) # WO에 해당하는 밀집층 정의 self.dense = tf.keras.layers.Dense(units=d_model) # num_heads 개수만큼 q, k, v를 split하는 함수 def split_heads(self, inputs, batch_size): inputs = tf.reshape( inputs, shape=(batch_size, -1, self.num_heads, self.depth)) return tf.transpose(inputs, perm=[0, 2, 1, 3]) def call(self, inputs): query, key, value, mask = inputs[&#x27;query&#x27;], inputs[&#x27;key&#x27;], inputs[ &#x27;value&#x27;], inputs[&#x27;mask&#x27;] batch_size = tf.shape(query)[0] query = self.query_dense(query) key = self.key_dense(key) value = self.value_dense(value) # 2. 헤드 나누기 # q : (batch_size, num_heads, query의 문장 길이, d_model/num_heads) # k : (batch_size, num_heads, key의 문장 길이, d_model/num_heads) # v : (batch_size, num_heads, value의 문장 길이, d_model/num_heads) query = self.split_heads(query, batch_size) key = self.split_heads(key, batch_size) value = self.split_heads(value, batch_size) # 3. 스케일드 닷 프로덕트 어텐션. 앞서 구현한 함수 사용. # (batch_size, num_heads, query의 문장 길이, d_model/num_heads) scaled_attention, _ = scaled_dot_product_attention(query, key, value, mask) # (batch_size, query의 문장 길이, num_heads, d_model/num_heads) scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3]) # 4. 헤드 연결(concatenate)하기 # (batch_size, query의 문장 길이, d_model) concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model)) # 5. WO에 해당하는 밀집층 지나기 # (batch_size, query의 문장 길이, d_model) outputs = self.dense(concat_attention) return outputs 123456789101112131415161718192021222324252627def scaled_dot_product_attention(query, key, value, mask): # query 크기 : (batch_size, num_heads, query의 문장 길이, d_model/num_heads) # key 크기 : (batch_size, num_heads, key의 문장 길이, d_model/num_heads) # value 크기 : (batch_size, num_heads, value의 문장 길이, d_model/num_heads) # padding_mask : (batch_size, 1, 1, key의 문장 길이) # Q와 K의 곱. 어텐션 스코어 행렬. matmul_qk = tf.matmul(query, key, transpose_b=True) # 스케일링 # dk의 루트값으로 나눠준다. depth = tf.cast(tf.shape(key)[-1], tf.float32) logits = matmul_qk / tf.math.sqrt(depth) # 마스킹. 어텐션 스코어 행렬의 마스킹 할 위치에 매우 작은 음수값을 넣는다. # 매우 작은 값이므로 소프트맥스 함수를 지나면 행렬의 해당 위치의 값은 0이 된다. if mask is not None: logits += (mask * -1e9) # 소프트맥스 함수는 마지막 차원인 key의 문장 길이 방향으로 수행된다. # attention weight : (batch_size, num_heads, query의 문장 길이, key의 문장 길이) attention_weights = tf.nn.softmax(logits, axis=-1) # output : (batch_size, num_heads, query의 문장 길이, d_model/num_heads) output = tf.matmul(attention_weights, value) return output, attention_weights 123456789101112131415161718192021222324252627def decoder(vocab_size, num_layers, dff, d_model, num_heads, dropout, name=&#x27;decoder&#x27;): inputs = tf.keras.Input(shape=(None,), name=&#x27;inputs&#x27;) enc_outputs = tf.keras.Input(shape=(None, d_model), name=&#x27;encoder_outputs&#x27;) # 디코더는 룩어헤드 마스크(첫번째 서브층)와 패딩 마스크(두번째 서브층) 둘 다 사용. look_ahead_mask = tf.keras.Input( shape=(1, None, None), name=&#x27;look_ahead_mask&#x27;) padding_mask = tf.keras.Input(shape=(1, 1, None), name=&#x27;padding_mask&#x27;) # 포지셔널 인코딩 + 드롭아웃 embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs) embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32)) embeddings = PositionalEncoding(vocab_size, d_model)(embeddings) outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings) # 디코더를 num_layers개 쌓기 for i in range(num_layers): outputs = decoder_layer(dff=dff, d_model=d_model, num_heads=num_heads, dropout=dropout, name=&#x27;decoder_layer_&#123;&#125;&#x27;.format(i), )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask]) return tf.keras.Model( inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask], outputs=outputs, name=name) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def decoder_layer(dff, d_model, num_heads, dropout, name=&quot;decoder_layer&quot;): inputs = tf.keras.Input(shape=(None, d_model), name=&quot;inputs&quot;) enc_outputs = tf.keras.Input(shape=(None, d_model), name=&quot;encoder_outputs&quot;) # 룩어헤드 마스크(첫번째 서브층) look_ahead_mask = tf.keras.Input( shape=(1, None, None), name=&quot;look_ahead_mask&quot;) # 패딩 마스크(두번째 서브층) padding_mask = tf.keras.Input(shape=(1, 1, None), name=&#x27;padding_mask&#x27;) # 멀티-헤드 어텐션 (첫번째 서브층 / 마스크드 셀프 어텐션) attention1 = MultiHeadAttention( d_model, num_heads, name=&quot;attention_1&quot;)(inputs=&#123; &#x27;query&#x27;: inputs, &#x27;key&#x27;: inputs, &#x27;value&#x27;: inputs, # Q = K = V &#x27;mask&#x27;: look_ahead_mask # 룩어헤드 마스크 &#125;) # 잔차 연결과 층 정규화 attention1 = tf.keras.layers.LayerNormalization( epsilon=1e-6)(attention1 + inputs) # 멀티-헤드 어텐션 (두번째 서브층 / 디코더-인코더 어텐션) attention2 = MultiHeadAttention( d_model, num_heads, name=&quot;attention_2&quot;)(inputs=&#123; &#x27;query&#x27;: attention1, &#x27;key&#x27;: enc_outputs, &#x27;value&#x27;: enc_outputs, # Q != K = V &#x27;mask&#x27;: padding_mask # 패딩 마스크 &#125;) # 드롭아웃 + 잔차 연결과 층 정규화 attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2) attention2 = tf.keras.layers.LayerNormalization( epsilon=1e-6)(attention2 + attention1) # 포지션 와이즈 피드 포워드 신경망 (세번째 서브층) outputs = tf.keras.layers.Dense(units=dff, activation=&#x27;relu&#x27;)(attention2) outputs = tf.keras.layers.Dense(units=d_model)(outputs) # 드롭아웃 + 잔차 연결과 층 정규화 outputs = tf.keras.layers.Dropout(rate=dropout)(outputs) outputs = tf.keras.layers.LayerNormalization( epsilon=1e-6)(outputs + attention2) return tf.keras.Model( inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask], outputs=outputs, name=name) 1234567891011121314151617tf.keras.backend.clear_session()# Hyper-parametersD_MODEL = 256NUM_LAYERS = 2NUM_HEADS = 8DFF = 512DROPOUT = 0.1model = transformer( vocab_size=VOCAB_SIZE, num_layers=NUM_LAYERS, dff=DFF, d_model=D_MODEL, num_heads=NUM_HEADS, dropout=DROPOUT) (1, 10025, 256) (1, 10025, 256) 123456789101112131415161718192021222324class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule): def __init__(self, d_model, warmup_steps=4000): super(CustomSchedule, self).__init__() self.d_model = d_model self.d_model = tf.cast(self.d_model, tf.float32) self.warmup_steps = warmup_steps def __call__(self, step): arg1 = tf.math.rsqrt(step) arg2 = step * (self.warmup_steps**-1.5) return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)def loss_function(y_true, y_pred): y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1)) loss = tf.keras.losses.SparseCategoricalCrossentropy( from_logits=True, reduction=&#x27;none&#x27;)(y_true, y_pred) mask = tf.cast(tf.not_equal(y_true, 0), tf.float32) loss = tf.multiply(loss, mask) return tf.reduce_mean(loss) 1234567891011learning_rate = CustomSchedule(D_MODEL)optimizer = tf.keras.optimizers.Adam( learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)def accuracy(y_true, y_pred): # 레이블의 크기는 (batch_size, MAX_LENGTH - 1) y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1)) return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy]) 123## 모델 학습EPOCHS = 50model.fit(dataset, epochs=EPOCHS) Epoch 1/50 104/104 [==============================] - 12s 54ms/step - loss: 1.2164 - accuracy: 0.0149 Epoch 2/50 104/104 [==============================] - 6s 54ms/step - loss: 1.0725 - accuracy: 0.0285 Epoch 3/50 104/104 [==============================] - 6s 54ms/step - loss: 0.9082 - accuracy: 0.0472 Epoch 4/50 104/104 [==============================] - 6s 54ms/step - loss: 0.7714 - accuracy: 0.0482 ... 104/104 [==============================] - 6s 54ms/step - loss: 0.0160 - accuracy: 0.1321 Epoch 46/50 104/104 [==============================] - 6s 56ms/step - loss: 0.0158 - accuracy: 0.1320 Epoch 47/50 104/104 [==============================] - 6s 55ms/step - loss: 0.0152 - accuracy: 0.1320 Epoch 48/50 104/104 [==============================] - 6s 54ms/step - loss: 0.0151 - accuracy: 0.1322 Epoch 49/50 104/104 [==============================] - 6s 55ms/step - loss: 0.0149 - accuracy: 0.1321 Epoch 50/50 104/104 [==============================] - 6s 53ms/step - loss: 0.0148 - accuracy: 0.1321 &lt;keras.callbacks.History at 0x7f794c0f0880&gt; 5. 챗봇 평가하기 학습시킨 챗봇에 새로운 문장을 넣어서 평가 12345678910111213141516171819202122232425262728293031323334# 새로운 문장도 인코더 입력 형식으로 변형하는 코드def preprocess_sentence(sentence): # 단어와 구두점 사이에 공백 추가. # ex) 12시 땡! -&gt; 12시 땡 ! sentence = re.sub(r&quot;([?.!,])&quot;, r&quot; \\1 &quot;, sentence) sentence = sentence.strip() return sentencedef evaluate(sentence): sentence = preprocess_sentence(sentence) sentence = tf.expand_dims( START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0) output = tf.expand_dims(START_TOKEN, 0) # 디코더의 예측 시작 for i in range(MAX_LENGTH): predictions = model(inputs=[sentence, output], training=False) # 현재(마지막) 시점의 예측 단어를 받아온다. predictions = predictions[:, -1:, :] predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32) # 만약 마지막 시점의 예측 단어가 종료 토큰이라면 예측을 중단 if tf.equal(predicted_id, END_TOKEN[0]): break # 마지막 시점의 예측 단어를 출력에 연결한다. # 이는 for문을 통해서 디코더의 입력으로 사용될 예정이다. output = tf.concat([output, predicted_id], axis=-1) return tf.squeeze(output, axis=0) 123456789101112def predict(sentence): prediction = evaluate(sentence) predicted_sentence = tokenizer.decode( [i for i in prediction if i &lt; tokenizer.vocab_size]) print(&#x27;Master: &#123;&#125;&#x27;.format(sentence)) # print(&#x27;Output: &#123;&#125;&#x27;.format(predicted_sentence)) print(&#x27;Chatbot: &#123;&#125;&#x27;.format(predicted_sentence)) return predicted_sentence 12#predict() 함수에 문장을 입력하면 해당 문장에 대한 결과가 출력됨output = predict(&quot;굿모닝&quot;) Input: 굿모닝 Output: 좋은 아침이에요 . 1output = predict(&quot;오늘 날씨&quot;) Input: 오늘 날씨 Output: 충분히 아름다워요 . 1output = predict(&quot;오늘 날씨 어때?&quot;) Input: 오늘 날씨 어때? Output: 오전엔 화창하지만 오후에는 비가 올 것입니다 . 1output = predict(&quot;집중력&quot;) Input: 집중력 Output: 병원 가보세요 . 1output = predict(&quot;퇴근&quot;) Input: 퇴근 Output: 인생은 채워나가는거죠 . 1output = predict(&quot;야근 싫어&quot;) Input: 야근 싫어 Output: 얼른 집에 가서 쉬시길 바랄게요 . 123output = str(input(&quot;오피스 챗봇입니다. 무엇을 도와드릴까요?:&quot;))output = predict(output) Master: 일하기 싫어 Chatbot: 저도요 ! ! Reference https:&#x2F;&#x2F;velog.io&#x2F;@seolini43&#x2F;파이썬Transformer로-오피스-챗봇-만들기-코드 **code: https://github.com/jmj3047/mj_chatbot_prac&#x2F;blob&#x2F;c5bec233b833b24345deeffe7391621415dc1dcb&#x2F;chatbot_backend.py","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Python","slug":"Python/Python","permalink":"https://jmj3047.github.io/categories/Python/Python/"}],"tags":[{"name":"Transformer","slug":"Transformer","permalink":"https://jmj3047.github.io/tags/Transformer/"},{"name":"Chatbot","slug":"Chatbot","permalink":"https://jmj3047.github.io/tags/Chatbot/"},{"name":"NLP","slug":"NLP","permalink":"https://jmj3047.github.io/tags/NLP/"}]},{"title":"MongoDB Update Operator","slug":"MongoDB_update","date":"2022-04-27T15:00:00.000Z","updated":"2022-05-02T00:59:39.000Z","comments":true,"path":"2022/04/28/MongoDB_update/","link":"","permalink":"https://jmj3047.github.io/2022/04/28/MongoDB_update/","excerpt":"","text":"$set: 필드값을 설정하고 필드가 존재하지 않으면 새 필드가 생성됨. 스키마를 갱신하거나 사용자 정의 키를 추가 할때 편리함. $unset: 키와 값을 모두 제거함 1234567891011121314&gt; db.users.insertOne(&#123;&quot;name&quot;:&quot;joe&quot;&#125;)&gt; db.users.updateOne(&#123;&quot;_id&quot; : ObjectId(&quot;6269e32b7b15b7097fbad433&quot;)&#125;,&#123;&quot;$set&quot;:&#123;&quot;favorite book&quot; : &quot;War and Peace&quot;&#125;&#125;)&#123; &quot;acknowledged&quot; : true, &quot;matchedCount&quot; : 1, &quot;modifiedCount&quot; : 1 &#125;&gt; db.users.findOne()&#123; &quot;_id&quot; : ObjectId(&quot;6269e32b7b15b7097fbad433&quot;), &quot;name&quot; : &quot;joe&quot;, &quot;favorite book&quot; : &quot;War and Peace&quot;&#125;&gt; db.users.updateOne(&#123;&quot;name&quot; : &quot;joe&quot;&#125;,&#123;&quot;$unset&quot;:&#123;&quot;favorite book&quot;:1&#125;&#125;)&#123; &quot;acknowledged&quot; : true, &quot;matchedCount&quot; : 1, &quot;modifiedCount&quot; : 1 &#125;&gt; db.users.find()&#123; &quot;_id&quot; : ObjectId(&quot;6269e32b7b15b7097fbad433&quot;), &quot;name&quot; : &quot;joe&quot; &#125; $inc: $set과 비슷하지만, 숫자를 증감하기 위해 사용. int, long, double, decimal 타입 값에만 사용 가능 12345678910111213141516171819202122&gt;db.games.insertOne(&#123;&quot;game&quot; :&quot;pinball&quot;,&quot;user&quot; :&quot;joe&quot;&#125;)&#123; &quot;acknowledged&quot; : true, &quot;insertedId&quot; : ObjectId(&quot;6269e4be7b15b7097fbad436&quot;)&#125;&gt; db.games.find()&#123; &quot;_id&quot; : ObjectId(&quot;6269e4be7b15b7097fbad436&quot;), &quot;game&quot; : &quot;pinball&quot;, &quot;user&quot; : &quot;joe&quot; &#125;&gt; db.games.updateOne(&#123;&quot;game&quot; :&quot;pinball&quot;,&quot;user&quot; :&quot;joe&quot;&#125;,&#123;&quot;$inc&quot;:&#123;&quot;score&quot;:50&#125;&#125;)&#123; &quot;acknowledged&quot; : true, &quot;matchedCount&quot; : 1, &quot;modifiedCount&quot; : 1 &#125;&gt; db.games.find()&#123; &quot;_id&quot; : ObjectId(&quot;6269e4be7b15b7097fbad436&quot;), &quot;game&quot; : &quot;pinball&quot;, &quot;user&quot; : &quot;joe&quot;, &quot;score&quot; : 50 &#125;&gt; db.games.updateOne(&#123;&quot;game&quot; :&quot;pinball&quot;,&quot;user&quot; :&quot;joe&quot;&#125;,&#123;&quot;$inc&quot;:&#123;&quot;score&quot;:10000&#125;&#125;)&#123; &quot;acknowledged&quot; : true, &quot;matchedCount&quot; : 1, &quot;modifiedCount&quot; : 1 &#125;&gt; db.games.find()&#123; &quot;_id&quot; : ObjectId(&quot;6269e4be7b15b7097fbad436&quot;), &quot;game&quot; : &quot;pinball&quot;, &quot;user&quot; : &quot;joe&quot;, &quot;score&quot; : 10050 &#125;&gt; $push: 배열이 이미 존재하지만 배열 끝에 요소를 추가하고, 존재하지 않으면 새로운 배열을 생성함. $each: $push에 $each제한자를 사용하면 작업 한 번으로 값을 여러개 추가할 수 있음. 12345678910111213141516171819202122&gt; db.blog.posts.findOne() &#123; &quot;_id&quot; : ObjectId(&quot;4b2d75476cc613d5ee930164&quot;), &quot;title&quot; : &quot;A blog post&quot;, &quot;content&quot; : &quot;...&quot;&#125; &gt; db.blog.posts.updateOne(&#123;&quot;title&quot; : &quot;A blog post&quot;&#125;, &#123;&quot;$push&quot; : &#123;&quot;comments&quot; : &#123;&quot;name&quot; : &quot;joe&quot;, &quot;email&quot; : &quot;joe@example.com&quot;, &quot;content&quot; : &quot;nice post.&quot;&#125;&#125;&#125;) &#123; &quot;acknowledged&quot; : true, &quot;matchedCount&quot; : 1, &quot;modifiedCount&quot; : 1 &#125; &gt; db.blog.posts.findOne() &#123; &quot;_id&quot; : ObjectId(&quot;4b2d75476cc613d5ee930164&quot;), &quot;title&quot; : &quot;A blog post&quot;, &quot;content&quot; : &quot;...&quot;, &quot;comments&quot; : [ &#123;&quot;name&quot; : &quot;joe&quot;, &quot;email&quot; : &quot;joe@example.com&quot;, &quot;content&quot; : &quot;nice post.&quot; &#125; ] &#125; $ne: 배열이 존재하지 않을 때 해당 값을 추가하면서 배열을 집합처럼 처리할 때 사용. $addToSet: 다른주소를 추가할 때 중복을 피할 수 있음 고유한 값을 여러개 추가하려면 $addToSet&#x2F;$each조합을 활용해야 함. $ne&#x2F;$push조합으로는 할 수 없음. 123456789101112&gt; db.users.updateOne(&#123;&quot;_id&quot; : ObjectId(&quot;4b2d75476cc613d5ee930164&quot;)&#125;, &#123;&quot;$addToSet&quot; : &#123;&quot;emails&quot; : &#123;&quot;$each&quot; : [&quot;joe@php.net&quot;, &quot;joe@example.com&quot;, &quot;joe@python.org&quot;]&#125;&#125;&#125;) &#123; &quot;acknowledged&quot; : true, &quot;matchedCount&quot; : 1, &quot;modifiedCount&quot; : 1 &#125; &gt; db.users.findOne(&#123;&quot;_id&quot; : ObjectId(&quot;4b2d75476cc613d5ee930164&quot;)&#125;) &#123; &quot;_id&quot; : ObjectId(&quot;4b2d75476cc613d5ee930164&quot;), &quot;username&quot; : &quot;joe&quot;, &quot;emails&quot; : [ &quot;joe@example.com&quot;, &quot;joe@gmail.com&quot;, &quot;joe@yahoo.com&quot;, &quot;joe@hotmail.com&quot; &quot;joe@php.net&quot; &quot;joe@python.org&quot; ] &#125; Reference 몽고DB 완벽 가이드: 실전 예제로 배우는 NoSQL 데이터베이스 기초부터 활용까지","categories":[{"name":"Data Base","slug":"Data-Base","permalink":"https://jmj3047.github.io/categories/Data-Base/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://jmj3047.github.io/tags/MongoDB/"}]},{"title":"MongoDB CRUD","slug":"MongoDB_CRUD","date":"2022-04-24T15:00:00.000Z","updated":"2022-10-15T07:08:48.489Z","comments":true,"path":"2022/04/25/MongoDB_CRUD/","link":"","permalink":"https://jmj3047.github.io/2022/04/25/MongoDB_CRUD/","excerpt":"","text":"Initial setting 12345678&gt;dbtest&gt;use videoswitched to db video # if video doesnt exist, created&gt;dbvideo&gt;db.movies # created movies collectionvideo.movies Create: insertOne 함수 1234567891011121314151617181920&gt;movie = &#123;&quot;title&quot; : &quot;Star Wars: Episode IV - A New Hope&quot;, &quot;director&quot; : &quot;George Lucas&quot;, &quot;year&quot; : 1977&#125;&#123; &quot;title&quot; : &quot;Star Wars: Episode IV - A New Hope&quot;, &quot;director&quot; : &quot;George Lucas&quot;, &quot;year&quot; : 1977&#125;&gt;db.movies.insertOne(movie) #영화가 데이터 베이스에 저장됨&#123; &quot;acknowledged&quot; : true, &quot;insertedId&quot; : ObjectId(&quot;6266057e4619361339f0c881&quot;)&#125;#Find 함수로 호출&gt;db.movies.find().pretty()&#123; &quot;_id&quot; : ObjectId(&quot;6266057e4619361339f0c881&quot;), &quot;title&quot; : &quot;Star Wars: Episode IV - A New Hope&quot;, &quot;director&quot; : &quot;George Lucas&quot;, &quot;year&quot; : 1977&#125; Read: find, findOne 함수 1234567&gt; db.movies.findOne(movie) &#123; &quot;_id&quot; : ObjectId(&quot;6266057e4619361339f0c881&quot;), &quot;title&quot; : &quot;Star Wars: Episode IV - A New Hope&quot;, &quot;director&quot; : &quot;George Lucas&quot;, &quot;year&quot; : 1977&#125; Update : updateOne 함수 1234567891011&gt; db.movies.updateOne(&#123;title : &quot;Star Wars: Episode IV - A New Hope&quot;&#125;, &#123;$set : &#123;reviews: []&#125;&#125;) &#123; &quot;acknowledged&quot; : true, &quot;matchedCount&quot; : 1, &quot;modifiedCount&quot; : 1 &#125;&gt; db.movies.find().pretty()&#123; &quot;_id&quot; : ObjectId(&quot;6266057e4619361339f0c881&quot;), &quot;title&quot; : &quot;Star Wars: Episode IV - A New Hope&quot;, &quot;director&quot; : &quot;George Lucas&quot;, &quot;year&quot; : 1977, &quot;reviews&quot; : [ ]&#125; Delete: deleteOne, deleteMany 함수 12&gt;db.movies.deleteOne(&#123;title : &quot;Star Wars: Episode IV - A New Hope&quot;&#125;)&#123; &quot;acknowledged&quot; : true, &quot;deletedCount&quot; : 1 &#125; Reference 몽고DB 완벽 가이드: 실전 예제로 배우는 NoSQL 데이터베이스 기초부터 활용까지","categories":[{"name":"Data Base","slug":"Data-Base","permalink":"https://jmj3047.github.io/categories/Data-Base/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://jmj3047.github.io/tags/MongoDB/"}]},{"title":"MongoDB Install & Basic Command","slug":"MongoDB_Install","date":"2022-04-24T15:00:00.000Z","updated":"2022-04-25T06:58:58.000Z","comments":true,"path":"2022/04/25/MongoDB_Install/","link":"","permalink":"https://jmj3047.github.io/2022/04/25/MongoDB_Install/","excerpt":"","text":"Link: www.mongodb.com/try/download/enterprise Download proper version of Mongodb Install이 완료된 후에는 MongoDB 환경변수 설정을 위해 시스템 환경 변수 편집을 진행하여 줍니다. 환경변수 편집을 위해 환경변수 &gt;시스템 변수 Path 설정을 선택하여 줍니다. 설치된 MongoDB의 bin폴더 경로를 입력하여 줍니다.(C:\\Program Files\\MongoDB\\Server\\5.0\\bin) 저장 후 cmd창에서 mongdb –version을 통해 정상 설치를 확인하여 줍니다. cmd 창에 mongodb 실행 1&gt;mongo 명령어 두줄로 잘 실행되는지 간단히 확인 12&gt; db.world.insert(&#123; &quot;speech&quot; : &quot;Hello World!&quot; &#125;);&gt; cur = db.world.find();x=cur.next();print(x[&quot;speech&quot;]); Basic Command사용 가능한 모든 데이터베이스 표시 : 1show dbs; 액세스 할 특정 데이터베이스를 선택 (Ex: mydb . 이미 존재하지 않으면 mydb 가 생성됩니다 : 1use mydb; 데이터베이스에 모든 콜렉션을 표시. 먼저 콜렉션을 선택하십시오 (위 참조). 1show collections; 데이터베이스와 함께 사용할 수있는 모든 기능 표시 : 1db.mydb.help(); 현재 선택한 데이터베이스를 확인 12&gt; dbmydb db.dropDatabase() 명령은 기존 데이터베이스를 삭제하는 데 사용됩니다. 1db.dropDatabase() Reference https://khj93.tistory.com/entry/MongoDB-Window에-MongoDB-설치하기 https://learntutorials.net/ko/mongodb/topic/691/mongodb-시작하기","categories":[{"name":"Data Base","slug":"Data-Base","permalink":"https://jmj3047.github.io/categories/Data-Base/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://jmj3047.github.io/tags/MongoDB/"}]},{"title":"BeautifulSoup Quick Start","slug":"BeautifulSoup_QuickStart","date":"2022-04-21T15:00:00.000Z","updated":"2022-04-22T02:53:21.000Z","comments":true,"path":"2022/04/22/BeautifulSoup_QuickStart/","link":"","permalink":"https://jmj3047.github.io/2022/04/22/BeautifulSoup_QuickStart/","excerpt":"","text":"Index_prac.html123456789101112131415161718192021222324252627&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;title&gt; The Dormouse&#x27;s story &lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class=&quot;title&quot;&gt;&lt;b&gt;The Dormouse&#x27;s story&lt;/b&gt;&lt;/p&gt; &lt;p class=&quot;story&quot;&gt; Once upon a time there were three little sisters; and their names were &lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;, &lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and &lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class=&quot;story&quot;&gt;To Be Continued...&lt;/p&gt; &lt;/body&gt;&lt;/html&gt; Quick Start.py123456789101112131415161718192021222324252627282930313233343536373839404142434445from bs4 import BeautifulSoupsoup = BeautifulSoup(open(&quot;index_prac.html&quot;), &#x27;html.parser&#x27;)# print allprint(soup.prettify())# navigate that data structureprint(soup.title)# &lt;title&gt;The Dormouse&#x27;s story&lt;/title&gt;print(soup.title.name)# u&#x27;title&#x27;print(soup.title.string)# u&#x27;The Dormouse&#x27;s story&#x27;print(soup.title.parent.name)# u&#x27;head&#x27;print(soup.p)# &lt;p class=&quot;title&quot;&gt;&lt;b&gt;The Dormouse&#x27;s story&lt;/b&gt;&lt;/p&gt;print(soup.p[&#x27;class&#x27;])# u&#x27;title&#x27;print(soup.a)# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;print(soup.find_all(&#x27;a&#x27;))# [&lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;,# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;,# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;]print(soup.find(id=&quot;link3&quot;))# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;## extracting all the URLsfor link in soup.find_all(&#x27;a&#x27;): print(link.get(&#x27;href&#x27;))# http://example.com/elsie# http://example.com/lacie# http://example.com/tillie## extracting all the text from a pageprint(soup.get_text()) Reference: https://www.crummy.com/software/BeautifulSoup/bs4/doc/#quick-start","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"HTML","slug":"Python/HTML","permalink":"https://jmj3047.github.io/categories/Python/HTML/"}],"tags":[{"name":"HTML","slug":"HTML","permalink":"https://jmj3047.github.io/tags/HTML/"},{"name":"Web Crawling","slug":"Web-Crawling","permalink":"https://jmj3047.github.io/tags/Web-Crawling/"},{"name":"BeautifulSoup","slug":"BeautifulSoup","permalink":"https://jmj3047.github.io/tags/BeautifulSoup/"}]},{"title":"Improved Training of Wasserstein GANs","slug":"WGAN","date":"2022-04-21T15:00:00.000Z","updated":"2022-10-15T08:05:10.371Z","comments":true,"path":"2022/04/22/WGAN/","link":"","permalink":"https://jmj3047.github.io/2022/04/22/WGAN/","excerpt":"","text":"Journal&#x2F;Conference: NIPSYear(published year): 2017Author: Ishaan Gulrajani, Faruk Ahmed, Martín Arjovsky, Vincent Dumoulin, Aaron C. CourvilleSubject: DCGAN, Generative Model Improved Training of Wasserstein GANs Summary 기존의 Wasserstein-GAN 모델의 weight clipping 을 대체할 수 있는 gradient penalty 방법을 제시 hyperparameter tuning 없이도 안정적인 학습이 가능해졌음을 제시 IntroductionGAN 모델을 안정적으로 학습하기 위한 많은 방법들이 존재해왔습니다. 특히, 가치함수가 수렴하는 성질을 분석하여 Discriminator(이후 Critic)가 1-Lipschitz function 공간에 있도록 하는 Wasserstein GAN(WGAN) 이 제시된 바 있습니다. 논문은 WGAN 의 단점을 개선한 WGAN-GP 모델을 제시합니다. Toy datasets에 대해 critic의 weight clipping이 undesired behavior를 유발할 수 있음을 증명 “Gradient penalty”(WGAN-GP) 기법으로 제안 다양한 GAN 구조에대해 안정적인 학습을 수행할 수 있고, 고품질 이미지 생성을 수행하며, 개별 샘플링이 필필요하지 않는 문자수준 언어 모델을 제시 BackgroundGenerative adversarial networks일반적인 GAN 구조에 대해 다시 한번 개념을 되짚습니다. Wasserstein GANsWGAN 은 GAN 의 목적함수인 JSD 가 parameter 에 연속적이지 않에 학습에 문제가 발생함을 지적하였습니다. 이에, Earth-Mover distance 로 모든 구간에서 연속적이고 대부분의 구간에서 미분 가능하게 하여 문제를 해결하였습니다. 이외에도 WGAN 의 특징에 대해 서술하며, 가장 중요한 특징으로 Lipschitz 조건을 만족하기 위해 시행하는 weight clipping 을 언급합니다. Properties of the optimal WGAN critic최적의 WGAN critic 을 가정했을 때, ****weight clipping이 WGAN critic에서 문제를 발생시킴을 언급하고 증명한 결과를 제시합니다. Difficulties with weight constraintsWGAN의 weight clipping이 최적화에 문제를 발생시킬 수 있고, 최적화가 잘 되더라도 critic이 pathological value surface 을 가질 수 있음을 증명하였던 내용을 확인하기 위한 실험을 진행합니다. 논문은 기존 WGAN 이 제시하였던 hard clipping 방식 이외에도, L2 norm clipping&#x2F;weight normalization&#x2F;L1 and L2 weight decay 등의 weight constraint 를 가정하였을 때 모두 비슷한 문제가 발생하였음을 언급합니다. Capacity underuse &amp; Exploding and vanishing gradientsk-Lipshitz 조건을 달성하기 위해 weight clipping 을 수행하였을 때, critic은 더욱 단순한 형태의 함수를 취하게 됩니다. 논문은 이를 증명하기 위해 Generator 의 분포를 toy distribution + unit-variance 가우시안-노이즈에 고정한뒤, weight clipping 과 함께 WGAN critic 을 학습한 결과를 제시합니다. 왼쪽 그림에서 Weight clipping 수행한 경우의 value surface 모양이 단순해졌음을 확인할 수 있습니다. 또한, 우측 그림과 같이, Gradient penalty 를 수행한 경우에 gradient vanishing 이나 exploding 이 발생하지 않았음을 제시합니다. Gradient penaltyWeight Clipping 을 사용하지 않고 Lipschitz constraint 를 유지할 수 있는 방법을 설명합니다. 입력에 대한 Critic 출력 gradient 의 크기를 직접 제약합니다. 이 때, tractability issue 를 피하기 위해 무작위로 추출한 샘플 $\\hat{x}$ 의 gradient norm 을 사용해 soft 한 제약을 줍니다. 이렇게 새롭게 정의되는 목적함수는 아래와 같습니다. Sampling distribution논문은 데이터 분포와 generator 분포에서 샘플링한 점의 쌍을 이은 뒤, 점을 잇는 선분을 따라 $\\hat{x}$ 를 샘플링하였고, 실험적으로 좋은 성능을 얻었음을 언급합니다. Penalty coefficientgradient penalty 를 가하는 정도를 경정하는 계수로, 논문에서는 모두 $\\lambda&#x3D;10$ 을 사용했음을 언급합니다. No critic batch normalization기존 GAN 모델은 batch normalization 을 모든 곳에서 사용했지만, 이는 discriminator의 단일 입력을 단일 출력으로 매핑하는 문제에서, 입력의 전체 배치로부터 출력의 배치로 매핑하는 문제로 변화시킵니다. 이 때문에 gradient penalty 를 수행하면 batch normalization 이 유효하지 않은 결과가 발생한다고 합니다. 따라서 논문은 critic 에 batch normalization 을 제거하였고 그럼에도 적절한 성능을 보였음을 언급합니다. Two-sided penaltygradient penalty 는 norm이 1 아래에 머무르지 않고(one-sided penalty), 1로 향하기(two-sided penalty)는 것을 촉진한다는 점을 제시합니다. ExperimentsTraining random architectures within a set 일반적인 DCGAN 구조에서 위의 표의 설정을 랜덤하게 설정하여 모델을 구성합니다. 이렇게 무작위로 200개의 모델을 구성한뒤, 32x32 ImageNet 에 대해 WGAN-GP, standard GAN을 합니다. 구성한 모델의 inception_score 가 min_score 보다 큰 경우 성공으로 분류합니다. WGAN-GP 는 많은 구조를 학습하는데 성공했다는 결과를 제시합니다. Training varied architectures on LSUN bedrooms아래와 같이 6개의 모델을 기본 모델로 사용합니다. 여기에 DCGAN, LSGAN, WGAN, WGAN-GP 를 각각 적용하였을 때의 성능을 비교합니다. 단, WGAN-GP는 discriminator 에서 Batch normalization 을 사용할 수 없기에 layer normalization 을 사용합니다. WGAN-GP 를 제외한 모든 모델에서 불안정하거나 mode collapse 에 빠진 모습을 보입니다. Improved performance over weight clippingWGAN-GP 가 weight clipping 에 비해 더 빠른 학습 속도와 샘플 효율을 보인다는 점을 증명하기 위한 실험 결과를 제시합니다. 이를 위해, WGAN 과 WGAN-GP 모델을 CIFAR-10 으로 학습하여 Inception Score 를 계산합니다. 왼쪽은 iteration에 따른 Inception Score이며, 오른쪽은 시간에 따른 Inception Score입니다. WGAN-GP는 weight clipping 보다 항상 더 좋은 성능을 보입니다. 이는 같은 optimizer 를 사용했을 때도 마찬가지이며, 비록 DCGAN 보다는 느리지만 수렴에 있어서 안정적인 점수를 보일 수 있음을 제시합니다. Sample quality on CIFAR-10 and LSUN bedrooms CIFAR-10으로 학습한 모델의 Inception score 를 계산하여 다양한 구조의 GAN을 비교한 표를 제시합니다. WGAN-GP 는 Supervised 의 경우 SGAN 을 제외했을 때 가장 좋은 성능을 보입니다. 또한, WGAN-GP 로 deep ResNet 모델을 사용하여 128X128 LSUN 침대 이미지를 생성하여 위와같은 결과를 제시합니다. Modeling discrete data with a continuous generator Generator 는 연속적인 분포의 함수를 가정합니다. 따라서언어 모델은 비연속적인 분포를 모델링 해야하므로 GAN 으로 학습하기에 부적절할 수 있습니다. 위는 Google Billion Word 데이터셋을 사용해 문자 수준 언어 모델을 WGAN-GP 로 학습한 결과입니다. 모델이 빈번하게 철자를 틀리지만, 언어의 통계에 대해서는 어느정도 학습을 수행하였음을 볼 수 있습니다. Meaningful loss curves and detecting overfitting기존의 weight clipping 은 loss 가 sample quality 와 연관되어 최소값으로 수렴할 수 있다는 점입니다. WGAN-GP 가 해당 특성을 유지하는지 확인하기 위한 테스크를 진행한 결과를 제시합니다. (a)에서 LSUN 침대 데이터셋을 학습하고 critic 의 negative loss 를 그렸을 때, Gnerator 가 학습됨에 따라 값이 줄어드는 것을 확인할 수 있습니다. 이 경우, WGAN-GP가 critic에서의 과적합을 완화했다고 볼 수 있습니다. 또한, MNIST 무작위 숫자 1000개로 학습한 결과는, 적은 데이터셋을 사용한 만큼 과적합이 발생하기 쉽습니다. 때문에, critic이 generator보다 더 빨리 과적합되어 training loss를 점차 증가시키고 validation loss를 감소시켰음을 확인할 수 있습니다. ConclusionWGAN에 Gradient penalty를 적용하여 기존의 weight clipping 을 적용함으로 인해 발생하는 문제를 해결할 수 있음을 제시하였습니다. Summarize GAN의 가장 큰 문제는 학습환경이 매우 불안정하다는 것이다. 생성자와 구분자 둘 중에 하나가 실력이 월등이 좋아진다면 밸런스가 붕괴되고 모델이 정확히 학습되지 않고 학습이 완료된 후에도 mode dropping 이 생기는데 이는 구분자가 그 역할을 충분히 하지 못해 모델이 최적점까지 학습이 안 된 것이다. 따라서 이 문제를 해결하기 위해 본 논문에서는 WGAN 방법을 도입했다. 간단히 설명하면 GAN의 discriminator보다 선생님 역할을 잘 할 수 있는 critic을 사용함으로써 gradient를 잘 전달시키고 critic과 generator를 최적점까지 학습할 수 있다는 것이다. 그렇다면 이를 적용하면 학습시킬 때 생성자와 구분자의 밸런스가 잘 맞는지 주의깊게 보지 않아도 되고 학습한 이후에 발생하는 mode droppin이 해결 가능하다. 식을 해석해보면 생성자가 Lipschitz 함수 조건을 만족하는가 하지않는가에 대한 기준이 하나 더 생기는것 이다. Link: Improved Training of Wasserstein GANs","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Generative Model","slug":"Paper/Generative-Model","permalink":"https://jmj3047.github.io/categories/Paper/Generative-Model/"}],"tags":[{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"},{"name":"Image Classification","slug":"Image-Classification","permalink":"https://jmj3047.github.io/tags/Image-Classification/"},{"name":"WGAN-GP","slug":"WGAN-GP","permalink":"https://jmj3047.github.io/tags/WGAN-GP/"},{"name":"WGAN","slug":"WGAN","permalink":"https://jmj3047.github.io/tags/WGAN/"}]},{"title":"Basic Web Crawling","slug":"Web_Crawling_Basic","date":"2022-04-21T15:00:00.000Z","updated":"2022-04-22T07:30:27.000Z","comments":true,"path":"2022/04/22/Web_Crawling_Basic/","link":"","permalink":"https://jmj3047.github.io/2022/04/22/Web_Crawling_Basic/","excerpt":"","text":"Crawling Tools -Beautifulsoup: 파이썬에서 가장 일반적인 수집도구(CSS 통해서 수집) -Scrapy (CSS, XPATH 형태로 수집) -Selenium (CSS, XPATH 통해서 데이터 수집 + Java Script) →자바 필요 + 몇개 설치 도구 필요 웹사이트 만드는 3대 조건 +1 :HTML, CSS, JavaScript, Ajax(비동기처리) 웹사이트 구동방식 :GET &#x2F; POST Create virtual env(git bash)123pip install virtualenvpython -m virtualenv venvsource venv/Scripts/activate Installing Library123pip install beautifulsoup4pip install numpy pandas matplotlib seabornpip install requests Reference: https://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-beautiful-soup","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"HTML","slug":"Python/HTML","permalink":"https://jmj3047.github.io/categories/Python/HTML/"}],"tags":[{"name":"HTML","slug":"HTML","permalink":"https://jmj3047.github.io/tags/HTML/"},{"name":"Web Crawling","slug":"Web-Crawling","permalink":"https://jmj3047.github.io/tags/Web-Crawling/"},{"name":"BeautifulSoup","slug":"BeautifulSoup","permalink":"https://jmj3047.github.io/tags/BeautifulSoup/"}]},{"title":"Web Crawling Practice","slug":"Web_Crawling_Headline","date":"2022-04-21T15:00:00.000Z","updated":"2022-04-22T08:17:11.000Z","comments":true,"path":"2022/04/22/Web_Crawling_Headline/","link":"","permalink":"https://jmj3047.github.io/2022/04/22/Web_Crawling_Headline/","excerpt":"","text":"1. Crawling Headline news from Naver12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import requestsfrom bs4 import BeautifulSoupimport pandas as pddef crawling(soup): div = soup.find(&quot;div&quot;, class_=&#x27;list_issue&#x27;) # print(type(div)) print(div.find_all(&#x27;a&#x27;)) #list형태 result = [] urls = [] for a in div.find_all(&quot;a&quot;): # print(a.get_text()) urls.append(a[&#x27;href&#x27;]) result.append(a.get_text()) # print(result) #save as csv file df = pd.DataFrame(&#123;&#x27;news_title&#x27;: result, &quot;url&quot;: urls&#125;) print(df) df.to_csv(&quot;newscrawling.csv&quot;) print(&quot;Crawling Success!&quot;) def main(): CUSTOM_HEADER = &#123; &#x27;referer&#x27; : &#x27;https://www.naver.com/&#x27;, &#x27;user-agent&#x27; : &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36&#x27; &#125; url = &#x27;https://www.naver.com/&#x27; req = requests.get(url = url, headers=CUSTOM_HEADER) print(req.status_code) #200: 정상적으로 사이트 돌아가고 있음 #404: 주소 오류 #503: 서버 내려진 상태 #print(req.text)이거를 beautifulsoup객체로 바꿔줌 soup = BeautifulSoup(req.text, &#x27;html.parser&#x27;) print(type(soup)) # print(soup.find(&quot;strong&quot;, class_=&#x27;new&#x27;)) # print(soup.find_all(&quot;span&quot;, class_=&#x27;blind&#x27;)) #print(soup.find(&quot;a&quot;, class_=&#x27;link_newsstand&#x27;)) crawling(soup)if __name__ == &quot;__main__&quot;: main() 2. Crawling Product List from ACBF1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import requestsfrom bs4 import BeautifulSoupimport pandas as pddef crawling(soup): div = soup.find_all(&quot;div&quot;, class_=&#x27;name&#x27;) print(type(div)) #&lt;class &#x27;bs4.element.ResultSet&#x27;&gt; # print(div) product_name = [] # urls =[] for a in div: # print(a.get_text()) # urls.append(a.get(&#x27;href&#x27;)) product_name.append(a.get_text()) print(product_name) # df = pd.DataFrame(&#123;&#x27;news_title&#x27;: product_name&#125;) # print(df) # df.to_csv(&quot;suit_product.csv&quot;) print(&quot;Crawling Success!&quot;) def main(): CUSTOM_HEADER = &#123; &#x27;referer&#x27; : &#x27;https://anyonecanbeafuse.com/category/%EC%88%98%ED%8A%B8/89/&#x27;, &#x27;user-agent&#x27; : &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36&#x27; &#125; url = &#x27;https://anyonecanbeafuse.com/category/%EC%88%98%ED%8A%B8/89/&#x27; req = requests.get(url = url, headers=CUSTOM_HEADER) print(req.status_code) #200: 정상적으로 사이트 돌아가고 있음 #404: 주소 오류 #503: 서버 내려진 상태 #print(req.text)이거를 beautifulsoup객체로 바꿔줌 soup = BeautifulSoup(req.text, &#x27;html.parser&#x27;) #print(type(soup)) # print(soup.find_all(&quot;div&quot;, class_=&#x27;name&#x27;)) crawling(soup)if __name__ == &quot;__main__&quot;: main() 3. Crawling Music Title from Chart123456789101112131415161718192021222324252627282930313233343536373839404142import requestsfrom bs4 import BeautifulSoupdef crawling(soup): tbody_df = soup.find(&quot;tbody&quot;) # print(tbody_df) result = [] for a in tbody_df.find_all(&#x27;p&#x27;, class_=&#x27;title&#x27;): # print(a.get_text()) # print(type(a.get_text())) result.append(a.get_text().strip(&quot;\\n&quot;)) print(result) print(&quot;Crawling Success!&quot;) def main(): CUSTOM_HEADER = &#123; &#x27;referer&#x27; : &#x27;https://music.bugs.co.kr/chart&#x27;, #필수 아님 &#x27;user-agent&#x27; : &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36&#x27; &#125; url = &#x27;https://music.bugs.co.kr/chart&#x27; req = requests.get(url = url, headers=CUSTOM_HEADER) print(req.status_code) #200: 정상적으로 사이트 돌아가고 있음 #404: 주소 오류 #503: 서버 내려진 상태 #print(req.text)이거를 beautifulsoup객체로 바꿔줌 soup = BeautifulSoup(req.text, &#x27;html.parser&#x27;) print(type(soup)) #&lt;class &#x27;bs4.BeautifulSoup&#x27;&gt; # print(soup.find_all(&quot;p&quot;, class_=&#x27;title&#x27;)) # print(soup.find_all(&quot;span&quot;, class_=&#x27;blind&#x27;)) #print(soup.find(&quot;a&quot;, class_=&#x27;link_newsstand&#x27;)) crawling(soup)if __name__ == &quot;__main__&quot;: main()","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"HTML","slug":"Python/HTML","permalink":"https://jmj3047.github.io/categories/Python/HTML/"}],"tags":[{"name":"HTML","slug":"HTML","permalink":"https://jmj3047.github.io/tags/HTML/"},{"name":"Web Crawling","slug":"Web-Crawling","permalink":"https://jmj3047.github.io/tags/Web-Crawling/"},{"name":"BeautifulSoup","slug":"BeautifulSoup","permalink":"https://jmj3047.github.io/tags/BeautifulSoup/"}]},{"title":"Unsupervised representation learning with deep convolutional generative adversarial networks","slug":"DCGAN","date":"2022-04-20T15:00:00.000Z","updated":"2022-10-15T08:03:41.524Z","comments":true,"path":"2022/04/21/DCGAN/","link":"","permalink":"https://jmj3047.github.io/2022/04/21/DCGAN/","excerpt":"","text":"Journal&#x2F;Conference: ICLRYear(published year): 2016Author: Alec Radford, Luke Metz, Soumith ChintalaSubject: DCGAN, Generative Model Unsupervised representation learning with deep convolutional generative adversarial networks Summary CNN 과 GAN framework 를 결합한 DCGAN 모델을 제시 Introduction논문 당시 GAN 은 불안정한 학습과 Generator 의 오작동으로 인해 제한적으로만 쓰였습니다. 이에 논문은 이미지 생성 모델을 만들기 위한 CNN 기반 GAN framework인 DCGAN(Deep Convolutional GANs) 을 제시합니다. 모델 구조에 제약을 가하여 대부분의 상황에서 안정적으로 학습할 수 있게 함 Discriminator 로 image classification 을 수행하였을 때 기타 비지도 학습 모델과 비슷한 성능을 보임 특정 필터가 특정 object를 그려낸다는 것을 시각화하여 제시함 Generator 에 입력하는 noise 를 제어하여 생성되는 샘플의 다양한 속성이 변화하는 것을 탐구함 Approach and Model Architecture논문 이전에도 GAN에 CNN을 써서 이미지 품질을 높이려는 시도가 있었으나 좋은 성과를 거두지 못하였다고 설명합니다. 이후, 다양한 데이터셋에서 안정적이고 높은 해상도의 이미지를 생성하기 위한 DCGAN 모델 설계 가이드라인을 제시합니다. Details of Adversarial Training3가지 데이터셋을 사용합니다. Large-scale Scene Understanding(LSUN) Imagenet-1k Faces 그외에 학습 디테일을 아래와 같이 제시합니다. pre-processing 제외 batch size 128 가중치는 N(0, 0.02) 로 초기화 Leaky ReLU의 기울기는 0.2로 설정함 AdamOptimizer, $\\beta_1 &#x3D;0.0002, \\beta_2&#x3D;0.9$ Generator 구조의 모식도는 위와 같습니다. LSUN 데이터셋으로 1 epoch 를 학습시킨 후 침실을 생성했을 때의 결과입니다. 이론적으로 모델이 훈련 예시를 기억했을 수도 있으나, 작은 학습률과 미니배치를 사용했음을 감안할 때 가능성이 낮다고 설명합니다. LSUN 데이터셋으로 5 epoch 학습 후 침실을 생성한 결과입니다. 침대 등의 근처에서 오히려 underfitting 이 발생했음을 확인할 수 있습니다. Empirical Validation of DCGANs CapabilitiesUnsupervised representation learning 알고리즘을 평가하는 일반적인 방법은 supervised 데이터셋으로 특징을 추출한 뒤 performance를 측정하는 것입니다. CIFAR-10 데이터셋에 대해 검증한 결과, 다른 방법들(K-means, Exemplar CNN 등)과 비교하여 결과에 큰 차이가 존재하지 않습니다. StreetView House Numbers dataset(SVHN) 데이터셋에서는 state-of-the-art 결과를 얻었음을 제시합니다. Investigating and Visualizing the Internals of the Networks가장 가까운 학습 데이터 이미지를 찾거나, 최근접 픽셀&#x2F;특징을 확인하거나 log-likelihood metric 으로 평가를 하는 방법은 모두 성능이 떨어지는 metric 이기에 사용하지 않았음을 언급합니다. 논문은 대신, 2개의 이미지를 생성할 때 사용한 noise 2개를 interpolation 하고, interpolated z 로 이미지를 생성한 결과를 제시합니다. 한 이미지에서 다른 이미지로 점진적으로 변해가는 모습을 관측할 수 있습니다. 또한 노이즈 벡터 z 의 산술 연산을 통해, vec(웃는 여자) −− vec(무표정 여자) ++ vec(무표정 남자) &#x3D;&#x3D; vec(웃는 남자) 같은 결과를 얻을 수 있었음을 제시합니다. 또한, 랜덤하게 생성한 필터와 학습된 필터의 activation 을 아래와 같이 시각화 하였습니다. 이해할 수 없는 feature 가 아닌 특정 object나 특징을 추출하고 있음을 확인할 수 있습니다. Conclusions and future work논문은 CNN 기반의 안정적인 이미지 생성모델인 DCGAN을 제안하였으며, image representation에 적합한 성능을 보임을 제시합니다. 그러나 여전히, 학습이 길어지는 경우 필터 일부가 요동치는 등의 현상을 관측하기도 하였음을 언급합니다. Link: Unsupervised representation learning with deep convolutional generative adversarial networks","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Generative Model","slug":"Paper/Generative-Model","permalink":"https://jmj3047.github.io/categories/Paper/Generative-Model/"}],"tags":[{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"},{"name":"DCGAN","slug":"DCGAN","permalink":"https://jmj3047.github.io/tags/DCGAN/"},{"name":"Convolutional Neural Networks","slug":"Convolutional-Neural-Networks","permalink":"https://jmj3047.github.io/tags/Convolutional-Neural-Networks/"},{"name":"Image Classification","slug":"Image-Classification","permalink":"https://jmj3047.github.io/tags/Image-Classification/"},{"name":"GAN","slug":"GAN","permalink":"https://jmj3047.github.io/tags/GAN/"}]},{"title":"Generative Adversarial Nets","slug":"Generative_Adversarial_Nets","date":"2022-04-20T15:00:00.000Z","updated":"2022-10-15T08:03:52.639Z","comments":true,"path":"2022/04/21/Generative_Adversarial_Nets/","link":"","permalink":"https://jmj3047.github.io/2022/04/21/Generative_Adversarial_Nets/","excerpt":"","text":"Journal&#x2F;Conference: NIPSYear(published year): 2014Author: I. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, Yoshua BengioSubject: GAN, Generative Model Generative Adversarial Nets Summary 적대적으로 동작하는 두개의 네트워크를 사용해 새로운 데이터를 생성할 수 있는 GAN(Generative Adversarial Nets) 구조를 제안 생성자(Generator) 와 감별자(Discriminator) 모두 마르코프 체인 등의 구조없이 back-propagation 으로 학습이 가능한 인공신경망 구조를 사용 이후 등장하는 수많은 GAN 기반 모델의 기원이 되는 논문 Introduction &amp; Related Works분류 문제에 제한되어 사용되던 딥러닝 모델의 용도를 새로운 데이터를 생성하는 문제에도 적용할 수 있는 적대적 생성 신경망(Generative Adversarial Nets)을 최초로 제시한 논문입니다. GAN은 아래와 같은 목표를 가진, 적대적인 두 모델을 학습합니다. 감별자(Discriminator) 모델 데이터가 원본 데이터셋에서 온것인지, 생성자가 만든 것인지를 판별 예시) 경찰이 지폐가 위조되었는지를 판별 생성자(Generator) 모델 감별자가 구분할 수 없는 가짜 데이터를 생성 예시) 위폐범이 경찰이 구분할 수 없는 위조 지폐를 제작함 논문은 해당 방법이 특별한 모델이나 학습 방법을 필요로 하지 않는 방법이라고 하며, MLP(multi-layer perception) 구조를 사용해 학습한 결과를 소개합니다. Adversarial nets적대적 신경망의 가장 직관적인 예시로 MLP 모델을 사용한 경우를 가정하여 설명합니다. 이 때 사용하는 표기법은 다음과 같습니다. $x\\sim p_{data}$ : 실제 데이터로부터 뽑은 샘플 $p_g$ : 생성자가 생성하는 데이터의 분포 $p_z(z)$ : 데이터를 생성하기 위해 사용하는 입력 노이즈 분포 $G(z;\\theta_g)$ : 생성자 모델 $\\theta_g$ : 생성자 모델 파라미터 $D(x;\\theta_d)$ : 감별자 모델 $\\theta_D$ : 감별자 모델 파라미터 D 는 실제 데이터와 생성된 데이터에 정확히 구분할 수 있는 확률을 최대화 하려고 합니다. G 는 D가 실제 데이터로 착각할 만한 데이터를 생성하는 것을 목적으로 $\\log(1-D(G(z))$ 를 최소화 하려고 합니다. 따라서 아래와 같이 가치함수 $V(G,D)$ 가 주어졌을 때, G 는 최소화, D는 최대화를 목적으로 경쟁합니다. 실제 계산에서 V 를 최대로 하는 D 를 구할 때 많은 계산이 필요하고, 데이터셋이 제한된 상황에서 과적합이 발생할 수도 있습니다. 따라서 실제 훈련에서는 D 를 k 번만 학습하고 G 를 학습합니다. 또한, 학습 초기에는 G가 생성하는 데이터의 품질이 낮으므로 D가 판별을 하기 쉬워, $\\log(1−D(G(z)))$ 항이 소실될수 있습니다. 따라서, $\\log D(G(z))$ 를 최대화 하는 문제로 변환하여 초기에 학습이 잘 이뤄질 수 있도록 합니다. 학습 과정의 모식도입니다. 파란 점선은 감별자 D의 분포, 검은 점은 원본 데이터 분포, 초록 실선은 생성자 G의 분포를 나타냅니다. 파란색점선: discriminator 검정색점선: real data에서나온sample 초록색실선: generator Z옆의검정색실선: domain from which z is sampled 화살표: 생성자가 noise를 real data와 얼마나 비슷하게 만들어주는지에 대한 지표 (a) 와 같이 학습이 완료되기 전의 상태에서 시작합니다.(model training 초기상태) (b) 와 같이 D 를 업데이트 할 때, 최적의 D( $D^{*}_G(x)$ ) 는 $\\frac{p_{data}(x)}{p_{data}(x) + p_g(x)}$ 로 수렴합니다.(내부의 알고리즘에 의해서 구분자가 train됨) (c) G를 업데이트하면, D 를 교란할 수 있도록 G 가 생성하는 분포가 실제 데이터 분포에 가까워집니다.(구분자가 학습한 걸 생성자에게 업데이트) (d) 학습 과정을 반복하면 생성자는 데이터 분포와 일치하는 데이터를 생성($p_g &#x3D; p_{data}$) 하며, 감별자는 어떠한 샘플도 구분할 수 없게 됩니다. ($D(x)&#x3D;\\frac{1}{2}$) (real data와f ake data가 같은 모습이 된 단계. 구분자는 fake와 real data를 구분할 수 없게 됨.) Fake data가왜noise인지? 명확한 이유는 명시되어 있지 않음. 대략적인 이유를 추론해보자면 생성자에 편향되지 않은 데이터가 들어가야 실험의 결과가 더 clear하기 때문. 생성자에 넣어서 만들어진 데이터가 m개라면, 그 데이터 m개가 만들어지려면 같은 숫자의 real data가 있어야 함. 따라서 총데이터는 2m개 구분자의 결과값은 fake data일때 0, real data 일 때 1인 하나의 스칼라 값 따라서 가장 이상적인 구분자가 될 때의 값은 0.5 GAN 모델은 markov 모델이 해야하는 훈련 과정과 overfitting에 문제점을 보완하기 위해 이를 한번에 하기 위해 만들어진 네트워크 Theoretical Results적대적 신경망 문제에서 생성자가 원본 데이터와 유사한 분포의 데이터를 생성할 수 있다는 증명을 제시합니다. 또한, 실제 적대적 신경망을 학습하기 위해 설계한 아래 알고리즘 또한 같은 결과에 수렴한다는 증명을 제시합니다. Global Optimality of $p_g &#x3D; p_{data}$먼저 임의의 G 가 주어졌을 때 최적의 D 를 계산하는 과정을 보입니다. 최적의 D 를 이용하여 Equation 1 을 G에 관한 수식으로 표현할 수 있습니다. 이 때 새롭게 정리한 가치함수가, G가 생성하는 데이터의 분포가 실제 분포를 따르는 경우에만 최소화된다는 것을 다음과 같이 증명합니다. Convergence of Algorithm 1G 와 D 가 $p_g$ 충분한 표현력을 갖고 있을 때, 제시한 알고리즘이 $p_g&#x3D;p_{data}$ 로 수렴함을 아래와 같이 증명합니다. 실제로 MLP 를 사용한 G 로는 모든 형태의 $p_g$ 를 표현할 수 없으므로 이론적인 최고 성능을 보장하기 어렵습니다. 논문은 그럼에도 불구하고 GAN이 실제 훈련결과에서 좋은 성능을 보임을 제시합니다. Experiments실험에 사용한 조건은 다음과 같습니다. Dataset : MNIST, Toronto Face Database(TFD), CIFAR-10 사용 Generator : ReLU&#x2F;sigmoid 활성함수를 혼합하여 사용 Discriminator : maxout 활성함수를 사용 D를 학습시킬 때만 Dropout을 사용 G에서 데이터를 생성하는 경우에만 noise를 input 으로 사용 GAN 은 데이터 분포 자체를 구하기 위한 tractable likelihood 를 가정하지 않습니다. 이러한 모델을 평가하기 위해 기존에 제안된 방법은 다음과 같습니다. Generator 에서 생성한 데이터를 Gaussian Parzen window 에 fitting fitting 한 분포가 주어졌을 때 log-likelihood 를 계산 Validation set 으로 교차 검증을 수행해 표준 편차를 계산 논문은 해당 방법의 분산이 크고 높은 차원의 데이터에서 잘 작동하지 않지만, GAN 이 기존 모델에 비해 상대적으로 좋은 결과를 보이고 있음을 제시합니다. 다음으로 GAN 모델로 생성한 데이터를 제시합니다. 가장 우측에는 원본 데이터셋 중 생성된 데이터에 가장 가까운 데이터를 배치하였습니다. 논문은 해당 모델이 기존의 생성 모델보다 낫다고 주장하기는 어렵지만, 비슷한 성과와 응용 가능성을 보여줄 수 있다는 의견을 제시합니다. 또한 위 그림과 같이, Generator 의 Input noise 를 점진적으로 변형시킬 때, 점점 interploation 되어가는 생성 데이터를 확인할 수 있습니다. Advantages and disadvantagesGAN 의 단점을 아래와 같이 정리합니다. Generator 가 생성하는 데이터의 분포가 명시적으로 존재하지 않습니다. Generator 와 Discriminator 의 균형이 깨지는 경우 학습이 원활이 이루어지지 않습니다. 또한, GAN 의 장점을 아래와 같이 정리합니다. 마르코프 체인 같은 구조 없이 역전파 만으로도 학습이 가능합니다. Generator 의 분포로 특별한 모델을 가정하지 않습니다. 더욱 복잡한 데이터 분포를 모사할 수 있어 선명한 데이터를 생성할 수 있습니다. Conclusions and future workGAN 프레임워크를 확장하고 개선할 수 있는 다양한 방법을 제시합니다. 주어진 조건에 따라 데이터를 생성하는 모델로 발전 가능 x가 주어졌을 때 z를 예측하는 보조 네트워크를 학습한다면 생성자의 데이터 분포를 예측할 수 있음 parameters를 공유하는 conditionals model를 학습함으로써 다른 conditionals models을 근사적으로 모델링할 수 있음 Semi-supervised learning에도 활용 가능 : 데이터가 제한된 경우 Discriminator 를 활용하여 classifier의 성능을 향상시킬 수 있음 효율성 개선: G와 D를 균형있게 학습할 수 있는 방법이나 새로운 z 분포를 제시하여 학습 속도 개선 가능 Summarize GAN 모델은 생성자(Generator)와 구분자(Discriminator) 둘의 적대적인 경쟁을 통해서 학습하는 딥러닝 네트워크 실제 우리가 학습시키려는 데이터와 생성자가 만든 Fake 데이터를 구분자에 모두 학습시켜서 구분을 더 잘 짓게 하는 방식으로이루어진네트워크이며, 생성자는랜덤노이즈를학습데이터와유사한패턴으로만들어주는네트워크구조를가진다. 이를 테스트하기 위해서 확인할 지표는 바이너리크로스엔트로피와 손실함수의 값이 구분자가 출력한 확률값이 정답에 가까우면 낮아지기 때문에 이것이 모델 학습의 목표가 된다. 구분자의 손실함수는 그래서 두가지 합인데 하나는 가짜이미지를 입력했을 때의 출력값과 1의차이, 그리고 가짜 이미지를 입력했을 때의 출력값과 0의 차이. 이 둘의 합이 구분자의 손실함수이며 이를 최소화하는 방향으로 구분자의 파라미터가 업데이트 된다. 이 업데이트는 최적화 함수를 통해 이루어진다. 데이터가 어떤 유형인지에 따라서 fake data를 어떤 것을 사용할지도 달라지는데 이 논문에서는 fake data를 데이터 분포를 통해서 샘플을 사용하며 이는 대체적으로 차원이 낮은 랜덤노이즈이다. 최악의 경우(max)를 가정했을 때 손실을 최소화(min)하는 것을 minimax게임이라고 하며 이것이 GAN 기저에 깔려있는 이론이라고 할 수 있다. GAN의 가장 큰문제는 학습환경이 매우 불안정하다는 것이다. 생성자와 구분자 둘 중에 하나가 실력이 월등이 좋아진다면 밸런스가 붕괴되고 모델이 정확히 학습되지 않고 학습이 완료된 후에도 mode dropping 이 생기는데 이는 구분자가 그 역할을 충분히 하지 못해 모델이 최적점까지 학습이 안 된 것이다. 이런 문제를 해결하기 위해 이후 논문에서 다양한 해결 방법이 제시된다. Link: Generative Adversarial Nets","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Generative Model","slug":"Paper/Generative-Model","permalink":"https://jmj3047.github.io/categories/Paper/Generative-Model/"}],"tags":[{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"},{"name":"GAN","slug":"GAN","permalink":"https://jmj3047.github.io/tags/GAN/"},{"name":"Generative Model","slug":"Generative-Model","permalink":"https://jmj3047.github.io/tags/Generative-Model/"}]},{"title":"ImageNet Classification with Deep Convolutional Neural Networks","slug":"ImageNet_Classification","date":"2022-04-20T15:00:00.000Z","updated":"2022-10-15T08:04:25.032Z","comments":true,"path":"2022/04/21/ImageNet_Classification/","link":"","permalink":"https://jmj3047.github.io/2022/04/21/ImageNet_Classification/","excerpt":"","text":"Journal&#x2F;Conference: NIPSYear(published year): 2012Author: Alex Krizhevsky, Ilya Sutskever, Geoffrey E. HintonSubject: AlexNet, Computer Vision ImageNet Classification with Deep Convolutional Neural Networks Summary 기존 머신러닝 모델을 제치고 딥러닝 모델이 더 우수한 성능을 보일 수 있음을 증명한 최초의 모델 ReLU 활성화 함수와 Dropout 의 유용함, Data Augmentation 기법을 제시 2012년 ImageNet 대회 ILSVRC 에서 우승을 차지한 모델 IntroductionAlexNet 이전의 객체 인식 모델은 대부분 고전적인 ML 모델로, 수만개 정도의 작은 데이터셋(NORB, Caltech-101&#x2F;256, CIFAR-10&#x2F;100)을 사용합니다. 그러나 이후, 수십만 개의 완전 분할 된 이미지로 구성된 LabelMe 와 1500 만 개 이상의 고해상도 이미지로 구성된 ImageNet 이 등장합니다. 이런 데이터셋을 처리하기 위해서는 높은 학습 역량을 가진 모델이 필요합니다. 또한, 학습과정에 사용되지 않은 수많은 데이터에 대해서도 추론을 할 수 있는 방대한 사전 지식을 담아내야합니다. 이에 논문은 컨볼루션 신경망(CNN) 모델을 기반으로 하는 AlexNet 을 제시합니다. CNN 은 FFNN(feed-forward NN)에 비해 더 적은 매개 변수를 가지므로 훈련이 용이합니다. 이를 통해 ILSVRC-2010, ILSVRC-2012 대회에 사용된 ImageNet subset에서 최고의 성능을 달성했습니다. 또한, 네트워크 성능 향상과 훈련시간 감소를 위한 여러가지 방법을 제시합니다. ALexNet 은 2개의 GTX 580 3GB GPU에서 5-6 일동안 훈련을 수행하였습니다. The Dataset지금은 대부분의 딥러닝 모델에서 기본적으로 사용하는 ImageNet 에 대한 소개입니다. 22,000 개 범주로 구분되는 1,500 만개 고해상도 이미지 웹에서 수집한 이미지를 Amazon 의 Mechanical Turk 크라우드 소싱 도구로 라벨링 2010 년부터 Pascal Visual Object Challenge의 일환으로 ImageNet 대규모 시각 인식 도전 (ILSVRC)이라는 연례 대회가 열렸습니다. ILSVRC는 1000 개의 카테고리 각각에 약 1000 개의 이미지가있는 ImageNet의 하위 집합을 사용합니다. 이는 약 120 만 개의 훈련 이미지, 50,000 개의 검증 이미지, 150,000 개의 테스트 이미지로 구성됩니다. 대부분의 실험 결과는 테스트 이미지가 공개된 ILSVRC-2010 를 사용합니다. 별도로, AlexNet 이 참가했던 ILSVRC-2012 실험 결과 또한 제시합니다. ImageNet 데이터셋 성능 지표로는 Top-1&#x2F;Top-5 Accuracy 를 사용합니다. 가변 해상도로 구성된 ImageNet 을 처리하기 위해 256 × 256의 고정 해상도로 다운 샘플링을 수행합니다. 직사각형 이미지는 scaling 후 중앙 256x256 패치를 잘라냅니다. 이외의 전처리는 수행하지 않습니다. The ArchitectureReLU Nonlinearity논문 발표 당시 일반적으로 사용된 perceptron 의 activation 함수는 tanh 혹은 sigmoid 입니다. 이들은 출력값이 무한대로 발산하지 않고 특정한 영역으로 제한되는 saturating 함수입니다. 반면 ReLU(Recitified Linear Unit) activation 은 출력값이 0 에서 무한대까지 발산할 수 있는 non-saturating 함수입니다. 논문은 4 layer CNN 으로 CIFAR-10 데이터셋을 사용하여 학습하였을 때, ReLU 가 6배 빠른 학습 속도를 보여주었음을 제시합니다. 이를 통해, non-saturating 한 함수가 gradient 를 더 빠르게 update 할 수 있음을 제시합니다. Training on Multiple GPUsGPU 메모리 제한과 느린 학습 속도를 개선할 수 있는 병렬학습 방법을 제시합니다. 기본 골자는 네트워크를 분할(커널, 뉴런 등)하여 서로 다른 GPU 에서 병렬적으로 연산을 수행하는 것입니다. 이 때, 메모리의 한계 및 병목 현상을 고려하여, 특정한 레이어에서만 연산 결과를 교환합니다. 논문은 이를 통해 half-size kernel 을 사용한 단일 GPU 모델보다 Top-1&#x2F;Top-5 accuracy 를 1.7% &#x2F; 1.2% 감소시켰음을 제시합니다. Local Response NormalizationReLU 활성 함수는 입력을 normalization 하지 않아도 saturation 이 발생하지 않습니다. 그러나 positive value 를 그대로 출력하는 ReLU 함수의 특성으로 인해 CNN 의 일부 구역에서 강한 신호가 생성될 수 있습니다. 이에 논문은 아래와 같은 local response normalization 방법을 제시합니다. 요약하면, CNN 에서 인접한 필터를 사용하여 normalization 을 진행한 것으로, 논문에서는 Top-1&#x2F;Top-5 Accuracy 를 1.4%, 1.2% 개선할 수 있었음을 제시합니다. 또한, CIFAR-10 으로 학습을 수행하였을 때도 2% 의 오차율 감소를 보였음을 제시합니다.(논문 당시에는 Batch Normalization 이 소개되지 않았습니다.) Overlapping PoolingCNN의 풀링 레이어는 같은 채널에 존재하는 인접한 뉴런의 출력을 요약해줍니다. 논문 이전에는 pooling 을 수행하는 영역이 겹치지 않도록 구성하여 사용하는 것이 일반적이었습니다. 논문은 풀링을 수행하는 영역이 이동하는 거리를 조절하여 풀링 영역이 겹치도록 한 결과, Top-1&#x2F;Top-5 Accuracy 를 0.4 %&#x2F;0.3 % 감소했다고 합니다. 또한 이를 통해 모델의 과적합 가능성을 줄일 수 있었다고 합니다. Overall Architecture AlexNet 의 전체 구조도 입니다. 2GPU 로 병렬학습을 수행하기 위해 두 갈래로 나뉘어 표현되어 있습니다. 총 5개의 convolution layer 와 3개의 max pooling layer, 3개의 dense layer 로 구성되어 있으며, 필요한 경우에만 GPU 연산 결과를 공유합니다. 또한 convolution&#x2F;dense layer 의 활성함수는 ReLU 를 사용합니다. Input : 224 x 224 x 3 &#x3D; 150,528 Convolution 1 : 11x11 kernel, 4 stride : 54x54x96 Max pooling 1 : 3x3 kernel, 2 stride : 26x26x96 Convolution 2 : 5x5 kernel, 2 padding : 26x26x256 Max pooling 2 : 3x3 kernel, 2 stride : 12x12x256 Convolution 3 : 3x3 kernel, 1 padding : 12x12x384 Convolution 4 : 3x3 kernel, 1 padding : 12x12x384 Convolution 5 : 3x3 kernel, 1 padding : 12x12x384 Max pooling 3 : 3x3 kernel, 2 stride : 5x5x256 Dense 1 : 4096 Dense 2 : 4096 Dense 3 : 1000 Reducing Overfitting6천만개의 파라미터로 구성된 모델의 과적합을 막기 위해 사용한 방법을 소개합니다. Data Augmentation학습 데이터를 인위적으로 변환하여 훈련 데이터를 증가시키는 방법입니다. 변환된 이미지를 저장하지 않고 GPU 학습시에 CPU에서 계산하도록 하여, 추가적인 계산 비용을 소모하지 않았다고 합니다. 주요 방법은 두가지로 요약됩니다. 256 × 256 이미지에서 224 × 224 패치를 추출하고, 수평 방향으로 뒤짚기 기존 데이터 셋의 2048 배 확장 가능 실제 : 5 개의 224 × 224 패치 (4 개의 코너 패치 및 중앙 패치)와 수평 반사를 수행한 10개의 패치 사용 RGB 채널 강도 조정 학습 데이터셋의 픽셀값으로 PCA 를 수행 PCA eigenvector 에 N(0,0.1) 인 정규분포에 추출한 랜덤값을 곱해 색상을 조정 Top-1 오차율을 1% 감소할 수 있었음 DropoutDense Layer 의 Output 에 Dropout rate 0.5 를 사용한 Dropout layer 를 추가합니다. 학습에 필요한 Epoch 를 2배 정도 늘렸으나, 과적합을 성공적으로 방지했음을 제시합니다. Details of learning모델 학습의 세부내용입니다. Batch size : 128 SGD (momentum 0.9, weight decay 0.0005) weight decay 가 모델을 정규화 할 뿐만 아니라 직접적으로 모델의 학습 오차를 줄였음을 제시합니다. 가중치 업데이트 과정은 아래와 같습니다. 가중치는 평균이 0, 표준 편차가 0.01인 정규 분포를 따르도록 초기화합니다. 2&#x2F;4&#x2F;5 번째 convolution 과 dense layer의 bias 는 1로 초기화하여, 학습을 가속할 수 있었음을 제시합니다. 학습률은 모든 layer 에 대해서 동일하되, 훈련을 수행하면서 메뉴얼하게 조정합니다. 학습률 0.01 에서 시작하여, 학습이 개선되지 않을 때 학습률을 10으로 나누는 방식으로 수행합니다. RESULT ILSVRC-2010 데이터에 대해서 기존 모델의 결과를 압도적으로 상회하는 결과를 제시하였습니다. AlexNet 이 직접 참가했던 ILSVRC-2012 에서도 다른 최고 성능의 모델에 비해 압도적인 결과를 보였음을 확인할 수 있습니다. 또한, CNN Layer 갯수를 추가할 때마다 성능이 상승함을 제시합니다. Qualitative Evaluations CNN kernel 을 시각화한 그림을 제시하면서, 각 커널이 이미지의 다양한 Feature 를 효과적으로 추출해냈음을 제시합니다. AlexNet 은 중앙을 벗어나는 데이터도 효과적으로 분류해냈습니다. 또한, Top-5 예측이 대부분 유사한 범주인 것으로 보아 합리적인 예측을 수행하고 있음을 제시합니다. 또한, 자세가 서로 다른 코끼리의 사례와 같이, Pixel 차원에서 완전히 다른 데이터임에도 유사한 범주로 분류할 수 있는 결과를 보여줍니다. Discussion“깊은” CNN 이 효과적으로 작동하였음을 제시합니다. Convolution layer 를 제거할 때마다 Top-1 Accuracy 가 2%씩 감소하는 점에 미루어, “깊이”의 중요성을 다시 한번 제시합니다. Link: ImageNet Classification with Deep Convolutional Neural Networks","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Computer Vision","slug":"Paper/Computer-Vision","permalink":"https://jmj3047.github.io/categories/Paper/Computer-Vision/"}],"tags":[{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"},{"name":"Convolutional Neural Networks","slug":"Convolutional-Neural-Networks","permalink":"https://jmj3047.github.io/tags/Convolutional-Neural-Networks/"},{"name":"Image Classification","slug":"Image-Classification","permalink":"https://jmj3047.github.io/tags/Image-Classification/"}]},{"title":"Hexo Hueman Tutorial","slug":"Hexo_Hueman_tutorial","date":"2022-04-20T15:00:00.000Z","updated":"2022-10-15T08:05:36.338Z","comments":true,"path":"2022/04/21/Hexo_Hueman_tutorial/","link":"","permalink":"https://jmj3047.github.io/2022/04/21/Hexo_Hueman_tutorial/","excerpt":"","text":"1.Starting Hexo Blog1234567891011121314151617181920212223242526username@LAPTOP-D1EUIRLS MINGW64 ~/Desktop$ hexo init your_blog_folderusername@LAPTOP-D1EUIRLS MINGW64 ~/Desktop$ cd your_blog_folder/username@LAPTOP-D1EUIRLS MINGW64 ~/Desktop/your_blog_folder$ echo &quot;# your_blog_folder&quot; &gt;&gt; README.mdgit initgit add README.mdgit commit -m &quot;first commit&quot;git branch -M mastergit remote add origin https://github.com/your_id/your_blog_folder.gitgit push -u origin masterusername@LAPTOP-D1EUIRLS MINGW64 ~/Desktop/your_blog_folder (master)$ git add .username@LAPTOP-D1EUIRLS MINGW64 ~/Desktop/your_blog_folder (master)$ git commit -m &quot;updated&quot;username@LAPTOP-D1EUIRLS MINGW64 ~/Desktop/your_blog_folder (master)$ git pushusername@LAPTOP-D1EUIRLS MINGW64 ~/Desktop/your_blog_folder (master)$ code . 2.Applying Hueman Theme3.Basic Hexo Tutorial4.Hexo Tag Plugins5.Add Math Formula(without changing from Notion) Creat File name mathjax.ejs on themes/hueman/layout folder 123456789101112MathJax.Hub.Config(&#123; jax: [&quot;input/TeX&quot;, &quot;output/HTML-CSS&quot;], # mathjax tex2jax: &#123; inlineMath: [ [&#x27;$&#x27;, &#x27;$&#x27;] ], displayMath: [ [&#x27;$$&#x27;, &#x27;$$&#x27;]], processEscapes: true, skipTags: [&#x27;script&#x27;, &#x27;noscript&#x27;, &#x27;style&#x27;, &#x27;textarea&#x27;, &#x27;pre&#x27;, &#x27;code&#x27;] &#125;, messageStyle: &quot;none&quot;, &quot;HTML-CSS&quot;: &#123; preferredFont: &quot;TeX&quot;, availableFonts: [&quot;STIX&quot;,&quot;TeX&quot;] &#125;&#125;); Check #Plugins in themes/hueman/_config.yml file and change mathjax: false to true Add mathjax:true at the header when you post Reference: Math Formula 6.Font Change7.Deleting Posts8.Error in Hueman ThemeTo Be Continued..","categories":[{"name":"Setting","slug":"Setting","permalink":"https://jmj3047.github.io/categories/Setting/"},{"name":"Hexo","slug":"Setting/Hexo","permalink":"https://jmj3047.github.io/categories/Setting/Hexo/"}],"tags":[{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Hexo","slug":"Hexo","permalink":"https://jmj3047.github.io/tags/Hexo/"},{"name":"Hueman","slug":"Hueman","permalink":"https://jmj3047.github.io/tags/Hueman/"}]},{"title":"K-Nearest Neighbor","slug":"KNN","date":"2022-04-20T15:00:00.000Z","updated":"2022-07-09T05:59:18.000Z","comments":true,"path":"2022/04/21/KNN/","link":"","permalink":"https://jmj3047.github.io/2022/04/21/KNN/","excerpt":"","text":"1. Classification 분류나 예측을 진행할때 나랑 가장 가까운 이웃 k개를 고려하겠다. 나랑 가까운 이웃 한명이 검정색이면 검정색으로 판단 파란색의 가장 가까운 이웃을 확인해본 결과 검정색 이므로 파란색도 검정색으로 분류되었다 K&#x3D;3 일 경우 형광색 친구를 분류한다고 하였을때 이웃중 파란색이 2개 검정색이 한개이기 때문에 파란색으로 분류된다. 분류를 원하는 관측치의 주변 N개의 데이터(근접 이웃)을 골라서, 주변대세를 확인 (다수결의 원칙으로) 2. Prediction 인접 K개의 데이터의 수치를 확인해줘서 그 데이터의 평균을 검은점의 예측치로 설정해준다. 3. How to find optimal k?k의 결정 k가 너무 큰 경우, KNN모델이 지나치게 일반화됨 K가 너무 작은 경우,KNN 모델의 예측 결과의 분산이 큼 주로 이것저것 해보고 error이 가장 작은 k를 설정하여준다. 거리 척도의 결정 상황에 맞는 거리척도를 사용하여야 한다. 거리척도의 종류:Minkowski distance , Euclidean distance, Citi block distance, Mahalanobis distance, Correlation distance 등 Reference: 한국공학대학교 강지훈교수님 강의","categories":[{"name":"Basic ML","slug":"Basic-ML","permalink":"https://jmj3047.github.io/categories/Basic-ML/"}],"tags":[{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"}]},{"title":"Pyspark Tutorial(2)","slug":"Pyspark_Tutorial_2","date":"2022-04-20T15:00:00.000Z","updated":"2022-04-22T07:33:14.000Z","comments":true,"path":"2022/04/21/Pyspark_Tutorial_2/","link":"","permalink":"https://jmj3047.github.io/2022/04/21/Pyspark_Tutorial_2/","excerpt":"","text":"Data cleansing01.pipeline.py123456789101112131415161718192021222324from pyspark.sql import SparkSessionfrom pyspark.sql import *from pyspark.sql import functions as F#Create Spark Sessionspark = SparkSession.builder.master(&quot;local[1]&quot;).appName(&quot;MLSampleTutorial&quot;).getOrCreate()#Load Datadf = spark.read.csv(&quot;data/AA_DFW_2015_Departures_Short.csv.gz&quot;, header = True)print(&quot;file loaded&quot;)print(df.show())#remove duration = 0df = df.filter(df[3] &gt; 0) #Actual elapsed time (Minutes) 여기 컬럼 값이 0보다 작은건 보여주지 않음# df.show()#ADD ID columndf = df.withColumn(&#x27;id&#x27;,F.monotonically_increasing_id()) #id값을 자동으로 넣어줌df.write.csv(&quot;data/output.csv&quot;, mode = &#x27;overwrite&#x27;)spark.stop() 02.total_spent.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# #라이브러리 불러오기# from pyspark import SparkConf, SparkContext# #사용자 정의 함수# #main함수# def main():# conf = SparkConf.setMaster(&quot;local&quot;).setAppName(&#x27;SpentbyCustomer&#x27;)# sc = SparkContext(conf= conf)# # 파이썬 코드# # 실행코드 작성# if __name__ == &quot;__main__&quot;:# main()########## 이게 spark 기본 세팅 ##########라이브러리 불러오기from pyspark import SparkConf, SparkContext#사용자 정의 함수def extractCusPrice(line): fields = line.split(&quot;,&quot;) return(int(fields[0]), float(fields[2]))#main함수def main(): #스파크 설정 conf = SparkConf().setMaster(&quot;local&quot;).setAppName(&#x27;SpentbyCustomer&#x27;) sc = SparkContext(conf= conf) #데이터 불러오기 input = sc.textFile(&#x27;data/customer-orders.csv&#x27;) #print(&#x27;is data?&#x27;) mappedInput = input.map(extractCusPrice) #튜플 형태로 나옴 totalByCustomer = mappedInput.reduceByKey(lambda x, y : x + y) #정렬 flipped = totalByCustomer.map(lambda x: (x[1], x[0])) totalByCustomerSorted = flipped.sortByKey() results = totalByCustomerSorted.collect() for result in results: print(result) #파이썬 코드 # 실행코드 작성if __name__ == &quot;__main__&quot;: main() 03.friends_by_age.py123456789101112131415161718from pyspark import SparkConf, SparkContextconf = SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;FriendsByAge&quot;)sc = SparkContext(conf = conf)def parseLine(line): fields = line.split(&#x27;,&#x27;) age = int(fields[2]) numFriends = int(fields[3]) return (age, numFriends)lines = sc.textFile(&quot;data/fakefriends.csv&quot;)rdd = lines.map(parseLine)totalsByAge = rdd.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))averagesByAge = totalsByAge.mapValues(lambda x: x[0] / x[1])results = averagesByAge.collect()for result in results: print(result) 04.min_temp.py123456789101112131415161718192021222324252627282930313233#온도를 측정하는 프로그램 만들기from dataclasses import fieldfrom pyspark import SparkConf, SparkContextconf = SparkConf().setMaster(&#x27;local&#x27;).setAppName(&#x27;MinTemperatures&#x27;) #마스터 노드에다가 올린다sc = SparkContext(conf = conf)print(&quot;Start&quot;)def parseLine(line): fields = line.split(&quot;,&quot;) #쉼표로 다 끊어줌 -&gt; 리스트로 반환됨 stationID = fields[0] entryType = fields[2] temperature = float(fields[3]) * 0.1 * (9.0/5.0) + 32.0 return (stationID, entryType, temperature)lines = sc.textFile(&#x27;data/1800.csv&#x27;)#print(lines)parseLines = lines.map(parseLine)#print(parseLine)minTemps = parseLines.filter(lambda x: &quot;TMIN&quot; in x[1])stationTemps = minTemps.map(lambda x: (x[0],x[2]))minTemps = stationTemps.map(lambda x, y: min(x, y))results = minTemps.collect()#print(results)for result in results: print(result[0]+ &quot;\\t&#123;:.2f&#125;F&quot;.format(result[1]))","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Pyspark","slug":"Python/Pyspark","permalink":"https://jmj3047.github.io/categories/Python/Pyspark/"}],"tags":[{"name":"pyspark","slug":"pyspark","permalink":"https://jmj3047.github.io/tags/pyspark/"}]},{"title":"Pyspark Tutorial(3)","slug":"Pyspark_Tutorial_3","date":"2022-04-20T15:00:00.000Z","updated":"2022-04-22T07:33:20.000Z","comments":true,"path":"2022/04/21/Pyspark_Tutorial_3/","link":"","permalink":"https://jmj3047.github.io/2022/04/21/Pyspark_Tutorial_3/","excerpt":"","text":"Machine Learning01.regression.py1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950from pyspark.ml.regression import DecisionTreeRegressorfrom pyspark.sql import SparkSessionfrom pyspark.ml.feature import VectorAssemblerprint(&quot;Starting Session&quot;)#세션 할당spark = SparkSession.builder.appName(&quot;DecisionTree&quot;).getOrCreate()#데이터 불러오기#StructType 과정 생략 가능data = spark.read.option(&quot;header&quot;, &quot;true&quot;).option(&quot;inferSchema&quot;, &quot;true&quot;).csv(&quot;data/realestate.csv&quot;)#print(data.show())#데이터 프레임을 행렬로 변환assembler = VectorAssembler().setInputCols([&quot;HouseAge&quot;, &quot;DistanceToMRT&quot;,&quot;NumberConvenienceStores&quot;]).setOutputCol(&quot;features&quot;) #데이터 컬럼 값 아무거나 넣어도 됨#타겟 데이터 설정df = assembler.transform(data).select(&quot;PriceofUnitArea&quot;,&quot;features&quot;)#데이터 분리trainTest = df.randomSplit([0.5,0.5])trainingDF = trainTest[0]testDF = trainTest[1]#Decision Tree 클래스 정의dtr = DecisionTreeRegressor().setFeaturesCol(&quot;features&quot;).setLabelCol(&quot;PriceofUnitArea&quot;)#모델 학습model = dtr.fit(trainingDF)#print(model)#모델 예측fullPredictions = model.transform(testDF).cache()#예측값과 label확인predictions = fullPredictions.select(&quot;prediction&quot;).rdd.map(lambda x: x[0])#실제데이터labels = fullPredictions.select(&quot;PriceofUnitArea&quot;).rdd.map(lambda x: x[0])#예측값과 label을 zip으로 묶어줌preds_label = predictions.zip(labels).collect()for prediction in preds_label: print(prediction)#세션 종료spark.stop() 02.logistic_regression.py1234567891011121314151617181920212223from pyspark.sql import SparkSessionfrom pyspark.ml.classification import LogisticRegression #Important# 세션 할당spark = SparkSession.builder.appName(&quot;AppName&quot;).getOrCreate()# load Datatraining = spark.read.format(&quot;libsvm&quot;).load(&quot;data/mllib/sample_libsvm_data.txt&quot;)print(&quot;Data loaded&quot;)# model# Scikit-Learn 문법과 비슷mlr = LogisticRegression() # Importantmlr_model = mlr.fit(training) # Important# 로지스텍 회귀, 선형 모델 .. 계수와 상수를 뽑아낼 수 있음print(&quot;Coefficients :&quot; + str(mlr_model.coefficients))print(&quot;Intercept :&quot; + str(mlr_model.intercept))spark.stop() 03.pipeline.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657from tokenize import Tokenfrom pyspark.ml import Pipelinefrom pyspark.ml.classification import LogisticRegressionfrom pyspark.ml.feature import HashingTF, Tokenizerfrom pyspark.sql import SparkSession# 세션 할당 spark = SparkSession.builder.appName(&quot;MLPipeline&quot;).getOrCreate()# 가상의 데이터 만들기training = spark.createDataFrame([ (0, &quot;a b c d e spark&quot;, 1.0), (1, &quot;b d&quot;, 0.0), (2, &quot;spark f g h&quot;, 1.0), (3, &quot;hadoop mapreduce&quot;, 0.0)], [&quot;id&quot;, &quot;text&quot;, &quot;label&quot;])# Feature Engineering# 1. Prparation# step01. 텍스트를 단어로 분리tokenizer = Tokenizer(inputCol=&#x27;text&#x27;, outputCol=&#x27;words&#x27;)# step02. 변환된 텍스트를 숫자로 변환hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=&quot;features&quot;)# step03. 모델을 가져옴lr = LogisticRegression(maxIter=5, regParam=0.01)# 2. Starting pipeplinepipeline = Pipeline(stages=[tokenizer, hashingTF, lr])# 3. Model Trainingmodel = pipeline.fit(training)# 4. Prepare test documents, which are unlabeled (id, text) tuples.test = spark.createDataFrame([ (4, &quot;spark i j k&quot;), (5, &quot;l m n&quot;), (6, &quot;spark hadoop spark&quot;), (7, &quot;apache hadoop&quot;)], [&quot;id&quot;, &quot;text&quot;])# 5. Predictionprediction = model.transform(test)selected = prediction.select(&quot;id&quot;, &quot;text&quot;, &quot;probability&quot;, &quot;prediction&quot;)for row in selected.collect(): row_id, text, prob, prediction = row #튜플 형태로 반환 print( # 문자열 포맷팅 &quot;(%d, %s) -------&gt; probability=%s, prediction=%f&quot; % (row_id, text, str(prob), prediction) )# training.show()# 세션 종료spark.stop() 04.randomforest.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960from cProfile import labelfrom pyspark.sql import SparkSession# 머신러닝 라이브러리from pyspark.ml import Pipelinefrom pyspark.ml.classification import RandomForestClassifierfrom pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexerfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator# 데이터 불러오기 spark = SparkSession.builder.appName(&quot;RandomForest&quot;).getOrCreate()data = spark.read.format(&quot;libsvm&quot;).load(&quot;data/mllib/sample_libsvm_data.txt&quot;)print(type(data))# Feature Engineering# label column labelIndexer = StringIndexer(inputCol=&#x27;label&#x27;, outputCol=&#x27;indexedLabel&#x27;).fit(data)# 범주형 데이터 체크, 인덱스화featureIndexer = VectorIndexer(inputCol=&#x27;features&#x27;, outputCol=&#x27;IndexedFeatures&#x27;, maxCategories=4).fit(data)# 데이터 분리(trainingData, testData) = data.randomSplit([0.7, 0.3])# 모델 rf = RandomForestClassifier(labelCol=&#x27;indexedLabel&#x27;, # 종속변수 featuresCol=&#x27;IndexedFeatures&#x27;, # 독립변수 numTrees=10)# outputCol=&#x27;indexedLabel&#x27; --&gt; original label로 변환labelConvereter = IndexToString(inputCol=&#x27;prediction&#x27;, outputCol=&#x27;predictedLabel&#x27;, labels=labelIndexer.labels)# 파이프라인 구축pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConvereter])# 모델 학습model = pipeline.fit(trainingData)# 모델 예측predictions = model.transform(testData)# 행에 표시할 것 추출 predictions.select(&quot;predictedLabel&quot;, &#x27;label&#x27;, &#x27;features&#x27;).show(5)# 모형 평가evaluator = MulticlassClassificationEvaluator( labelCol=&quot;indexedLabel&quot;, predictionCol=&quot;prediction&quot;, metricName=&quot;accuracy&quot;)accuracy = evaluator.evaluate(predictions)print(&quot;Test Error = %f &quot; % (1.0 - accuracy))spark.stop()","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Pyspark","slug":"Python/Pyspark","permalink":"https://jmj3047.github.io/categories/Python/Pyspark/"}],"tags":[{"name":"pyspark","slug":"pyspark","permalink":"https://jmj3047.github.io/tags/pyspark/"}]},{"title":"Pyspark Tutorial(1)","slug":"Pyspark_Tutorial_1","date":"2022-04-20T15:00:00.000Z","updated":"2022-04-22T07:33:10.000Z","comments":true,"path":"2022/04/21/Pyspark_Tutorial_1/","link":"","permalink":"https://jmj3047.github.io/2022/04/21/Pyspark_Tutorial_1/","excerpt":"","text":"Reference: https://spark.apache.org/docs/latest/quick-start.html Get Started01.basic.py1234567891011121314# -*- coding: utf-8 -*-import pysparkprint(pyspark.__version__)from pyspark.sql import SparkSession#스파크 세션 초기화 :spark session이 하나 만들어진것spark = SparkSession.builder.master(&#x27;local[1]&#x27;).appName(&#x27;SampleTutorial&#x27;).getOrCreate()rdd = spark.sparkContext.parallelize([1,2,3,4,5])print(&quot;rdd Count&quot;, rdd.count())spark.stop() 02.rating.py1234567891011121314151617181920212223242526272829303132333435#SparkContext#RDDfrom pyspark import SparkConf, SparkContextimport collectionsprint(&quot;Hello&quot;)def main(): # MasterNode = local # MapReduce conf = SparkConf().setMaster(&quot;local&quot;).setAppName(&#x27;RatingHistogram&#x27;) sc = SparkContext(conf = conf) lines = sc.textFile(&quot;ml-100k/u.logs&quot;) #print(lines) ratings = lines.map(lambda x: x.split()[2]) #print(&quot;ratings:&quot;,ratings) #rdd라는 객체가 만들어진것 result = ratings.countByValue() #print(&quot;result:&quot;,result) #정렬하기 sortedResults = collections.OrderedDict(sorted(result.items())) for key, value in sortedResults.items(): print(&quot;%s %i&quot; % (key, value)) if __name__ == &quot;__main__&quot;: main() ##spark를 쓰는 이유:로그 데이터를 가져와서 규칙을 대입해서 정렬한다음에 정형데이터로 치환하기위해#실제로 의미있는 로그라면 분석도 의미가 있다#분석과 로그데이터를 처리할 수 있는 환경을 지원해줌#과거에는 두개가 따로 있었음 03.dataloading.py1234567891011121314151617181920212223242526272829303132333435363738394041424344#Spark SQL 적용#Spark Sessionfrom pyspark.sql import SparkSession# #스파크 세션 생성# my_spark = SparkSession.builder.getOrCreate()# print(my_spark)# #테이블을 확인하는 코드# print(my_spark.catalog.listDatabases())# #show database# my_spark.sql(&quot;show databases&quot;).show()# #check current DB# my_spark.catalog.currentDatabase()# my_spark.stop()#loading csv filespark = SparkSession.builder.master(&#x27;local[1]&#x27;).appName(&quot;DBTutorial&quot;).getOrCreate()flights = spark.read.option(&#x27;header&#x27;,&#x27;true&#x27;).csv(&#x27;data/flight_small.csv&#x27;)#flights.show(4)#spark.catalog.currentDatabase()#flights 테이블을 default DB에 추가함flights.createOrReplaceTempView(&#x27;flights&#x27;)#print(spark.catalog.listTables(&#x27;default&#x27;))#spark.sql(&#x27;show tables from default&#x27;).show()#쿼리 통해서 데이터 저장query = &quot;FROM flights SELECT * LIMIT 10&quot;query2 = &quot;SELECT * FROM flights LIMIT 10&quot;# 스파크 세션 할당flights10 = spark.sql(query2)#flights10.show()#spark 데이터 프레임을 pandas data frame으로 변환import pandas as pdpd_flights10 = flights10.toPandas()print(pd_flights10.head()) 04.struct_type.py Reference: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.types.StructType.html 1234567891011121314151617181920212223242526272829303132from pyspark.sql import SparkSessionfrom pyspark.sql import functions as func #aliasfrom pyspark.sql.types import StructType, StructField, IntegerType, LongTypeprint(&quot;Hello&quot;)#세션 할당spark = SparkSession.builder.appName(&quot;PopularMovies&quot;).getOrCreate()#스키마 작성(u.logs 데이터)schema = StructType( [ StructField(&quot;userID&quot;,IntegerType(), True) , StructField(&quot;movieID&quot;,IntegerType(), True) , StructField(&quot;rating&quot;, IntegerType(), True) , StructField(&quot;timestamp&quot;, LongType(), True) ])print(&quot;Schema is done&quot;)#데이터 불러오기movies_df = spark.read.option(&quot;sep&quot;,&quot;\\t&quot;).schema(schema).csv(&quot;ml-100k/u.logs&quot;)#내림차순으로 인기 있는 영화 정렬#movieID로 그룹바이, count() 진행, orderbytopMovieIds = movies_df.groupby(&quot;movieID&quot;).count().orderBy(func.desc(&#x27;count&#x27;))print(topMovieIds.show(10))#세션 종료spark.stop() 05.advance_structtype.py1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162from pyspark.sql import SparkSessionfrom pyspark.sql import functions as func #aliasfrom pyspark.sql.types import StructType, StructField, IntegerType, LongTypeimport codecsprint(&quot;Starting Session&quot;)def loadMovieNames(): #u.item에서 영화 이름 가져옴 movieNames = &#123;&#125; with codecs.open(&quot;ml-100k/u.item&quot;,&quot;r&quot;, encoding=&quot;ISO-8859-1&quot;, errors =&quot;ignore&quot;) as f: for line in f: fields = line.split(&quot;|&quot;) movieNames[int(fields[0])] = fields[1] #데이터 추가하는 딕셔너리 문법 return movieNames #세션 할당spark = SparkSession.builder.appName(&quot;PopularMovies&quot;).getOrCreate()#파이썬 딕셔너리 객체를 spark 객체로 변환nameDict = spark.sparkContext.broadcast(loadMovieNames())#스키마 작성(u.logs 데이터)schema = StructType( [ StructField(&quot;userID&quot;,IntegerType(), True) , StructField(&quot;movieID&quot;,IntegerType(), True) , StructField(&quot;rating&quot;, IntegerType(), True) , StructField(&quot;timestamp&quot;, LongType(), True) ])print(&quot;Schema is done&quot;)#데이터 불러오기movies_df = spark.read.option(&quot;sep&quot;,&quot;\\t&quot;).schema(schema).csv(&quot;ml-100k/u.logs&quot;)#내림차순으로 인기 있는 영화 정렬할 필요 없음#movieID로 그룹바이, count() 진행, orderby#topMovieIds = movies_df.groupby(&quot;movieID&quot;).count().orderBy(func.desc(&#x27;count&#x27;))topMovieIds = movies_df.groupby(&quot;movieID&quot;).count()# 딕셔너리 # key-value# 키값을 알면 value자동으로 가져옴(movietitle)def lookupName(movieID): return nameDict.value[movieID]# 사용자 정의 함수 사용할 때 쓰는 spark 문법lookupNameUDF = func.udf(lookupName)# MovieTitle을 기존 topMovieIds 데이터에 추가#컬럼 추가moviesWithNames = topMovieIds.withColumn(&quot;movietitle&quot;,lookupNameUDF(func.col(&quot;movieID&quot;)))#정렬final_df = moviesWithNames.orderBy(func.desc(&quot;count&quot;))print(final_df.show(10))#세션 종료spark.stop()","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Pyspark","slug":"Python/Pyspark","permalink":"https://jmj3047.github.io/categories/Python/Pyspark/"}],"tags":[{"name":"pyspark","slug":"pyspark","permalink":"https://jmj3047.github.io/tags/pyspark/"}]},{"title":"How to install PySpark","slug":"install_PySpark","date":"2022-04-18T15:00:00.000Z","updated":"2022-04-22T02:53:55.000Z","comments":true,"path":"2022/04/19/install_PySpark/","link":"","permalink":"https://jmj3047.github.io/2022/04/19/install_PySpark/","excerpt":"","text":"Preparation installing spark need python3 if you are first using python, install anaconda Installing JAVA Installing file: Java SE 8 Archive Downloads (JDK 8u211 and later) Need to login Oracle Run the download file as admin → Click Next button → Changing the path on file (Space between words like Program Files can be problem during installation) Changing Path Same changes to folders in the JAVA runtime environment folder (Click ‘Change’ and modify) Create and save jre folder in the path right after the C dirve Installing Spark Installing site: https://spark.apache.org/downloads.html Download installation file After clicking Download Spark: [spark-3.2.0-bin-hadoop3.2.tgz](https://www.apache.org/dyn/closer.lua/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz), you can download it by clicking the HTTP 하단 page like picture below Installation URL: https://www.apache.org/dyn/closer.lua/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz (2022.01) Download WinRAR Program You need to install WinRAR, to unzip .tgz file. Installation file: https://www.rarlab.com/download.htm Install what fits your computer Create Spark folder and move files Moving files Copy all the file in spark-3.2.0-bin-hadoop3.2 folder After that, create spark folder below C drive and move all of them to it. Modify log4j.properties file • Open the fileconf - [log4j.properties](http://log4j.properties) Open the log file as notebook and change INFO → ERROR just like example below. During the process, all the output values can be removed. 1234567# Set everything to be logged to the console# log4j.rootCategory=INFO, consolelog4j.rootCategory=ERROR, consolelog4j.appender.console=org.apache.log4j.ConsoleAppenderlog4j.appender.console.target=System.errlog4j.appender.console.layout=org.apache.log4j.PatternLayoutlog4j.appender.console.layout.ConversionPattern=%d&#123;yy/MM/dd HH:mm:ss&#125; %p %c&#123;1&#125;: %m%n Installing winutils This time, we need program that makes local computer mistakes Sparks for Hadoop. Installing file: https://github.com/cdarlint/winutils Download winutils programs that fit installation version. I downloaded version 3.2.0 Create winutils&#x2F;bin folder on C drive and save the downloaded file. Ensure this file is authorized to be used so that it can be executed without errors whne running Spark This time, open CMD as admin and run the file If ChangeFileModeByMask error (3) occurs, create tmp\\hive folder below C drive. 12C:\\Windows\\system32&gt;cd c:\\winutils\\binc:\\winutils\\bin&gt; winutils.exe chmod 777 \\tmp\\hive Setting environment variables Set the system environment variable Click the 사용자 변수 - 새로 만들기 button on each user account Set SPARK_HOME variable Set JAVA_HOME variable Set HADOOP_HOME variable Edit PATH variable. Add the code below. Add code below %SPARK_HOME%\\bin %JAVA_HOME%\\bin Testing Spark Open CMD file, set the path as c:\\spark folder if the logo appears when input ‘spark’, success Check whether the code below works 1234&gt;&gt;&gt; rd = sc.textFile(&quot;README.md&quot;)&gt;&gt;&gt; rd.count()109&gt;&gt;&gt;","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Pyspark","slug":"Python/Pyspark","permalink":"https://jmj3047.github.io/categories/Python/Pyspark/"}],"tags":[{"name":"pyspark","slug":"pyspark","permalink":"https://jmj3047.github.io/tags/pyspark/"}]},{"title":"Adversarial Speaker Verification","slug":"Adversaria 2d6a8","date":"2022-04-16T15:00:00.000Z","updated":"2022-10-15T08:03:42.984Z","comments":true,"path":"2022/04/17/Adversaria 2d6a8/","link":"","permalink":"https://jmj3047.github.io/2022/04/17/Adversaria%202d6a8/","excerpt":"","text":"Journal&#x2F;Conference: ICASSP IEEEYear(published year): 2019Author: Zhong Meng, Yong Zhao, Jinyu Li, Yifan GongSubject: Speaker Verification Adversarial Speaker Verification GoalWith ASV, our goal is to learn a condition-invariant and speaker-discriminative deep hidden feature in the background DNN through adversarial multi-task learning such that a noise-robust deep embedding can be obtained from these deep features for an enrolled speaker or a test utterance. Data“Hey Cortana” from the Windows 10 desktop Cortana service logs.CHiME-3: buses (BUS), in cafes (CAF), in pedestrian areas (PED), at street junctions (STR))From the clean Cortana data, we select 6 utterances from each of the 3k speakers as the enrollment data (called “Enroll A”). We select 60k utterances from 3k target speakers and 3k impostors in Cortana dataset and mix them with CHiME-3 real noise to generate the noisy evaluation set. Result Why? In ASV, a speaker classification network and a condition identification network are jointly trained to minimize the speaker classification loss and to mini-maximize the condition loss through adversarial multitask learning.The target labels of the condition network can be categorical (environment types) and continuous (SNR values). With ASV, speaker-discriminative and condition-invariant deep embeddings can be extracted for both enrollment and test speech. 적대적 학습은 [22] 논문에서 먼저 적용되었는데 이 논문과의 차이점은, 두가지 소음 컨디션을 서로 다른 방법으로 막은 것(22 논문에서는 환경 개선 보다는 unlabeled 타겟 도메인 데이터를 훈련하여 적응 시키는 걸 목표로 함), 그리고 본 논문은 네트워크에 직접적으로 음성 피처를 인풋으로 넣어 훈련하는 반면, 22 논문은 i-벡터를 인풋으로 넣었고 이는 computational한 시간과 자원이 더 들어감. ** Train ASV model to adapt to noise ** Experiment Embeddings : 인공신경망에서 원래 차원보다 저차원의 벡터로 만드는 것을 의미원래 차원은 매우 많은 범주형 변수들로 구성되어 있고 이것들이 학습방식을 통해 저차원으로 대응됨. 수천 수만개의 고차원 변수들을 몇백개의 저차원 변수로 만들어 주고, 또한 변형된 저차원 공간에서도 충분히 카테고리형 의미를 내재함.출처: 인공신경망(딥러닝)의 Embedding 이란 무엇일까? - 임베딩의 의미(1&#x2F;3) 훈련단계에서 background DNN을 화자들을 구별하기 위해 훈련 시킴. F &#x3D; {f1 ,…, fT }, ft ∈ Rrf : deep hidden featuresX &#x3D; {x1 ,…, xT}, xt ∈ Rrx , t &#x3D; {1 ,…, T} : input speech frames from training set to intermediate deep hidden featuresΘf: parameters maps input speech framesMf: the hidden layers of the background DNN as a feature extractor network with parameters Θf P(a|ft;Θy), a ∈ A : Speaker posteriors, where A is the set of all speakers in the training setΘy: maps the deep features F to the speaker posteriors.My: the upper layers of the background DNN as a speaker classifier network with parameters Θy Θf and Θy are optimized by Minimizing cross entropy loss of speaker classification. Y &#x3D; {y1 ,…, yT }, yt ∈A : sequence of speaker labels aligned with X1[.]: indicator function equals to 1 if the condition in the bracket is satisfied and 0 other wise. Categorical Condition Classification Loss: to address the conditions that are characterized as a categorical variable additional condition classification network Mc: which predicts the condition posteriors p(b| ft;Θf ); b ∈ B given the deep features F from the training setB : the set of all conditions in the training set With a sequence of condition labels C &#x3D; {c1 ,…, cT} that is aligned with X, compute the condition classification loss through cross-entropy Continuous Condition Regression Loss: an additional condition regression network Mc to predict the frame-level condition value (SNR value) compute the condition regression loss through mean-square error Deep feature F 를 condition invariant 하게 만들려면, 소음들 각각의 환경에서 나오는 피처들의 차이가 최대한 적어야 함.따라서 Mf 와 Mc 는 같이 적대적으로 train 하게 되고, Θf 가 frame-level condition loss, Lcondition 을 최대화 시키고 Θc가 Lcondition을 최소화 시키는 방향으로 감.이 둘의 경쟁은 처음에 Mc에 대한 차별성을 높여주고, speaker invariance 의 deep feature 가 Mf에 의해 만들어짐.결국 Mf가 극단적으로 Mc가 구별하지 못하는 피처를 만드는 지점에 수렴.그와 동시에 논문에서는 화자 차별적인 deep feature 들을 Lspeaker(Eq3)의 speaker classification 손실함수를 최소화 하면서 만듦. 최적의 파라미터를 찾는 식: 여기서 λ가 speaker classification 손실함수와 condition 함수 사이의 균형을 통제.GRL은 forward propagation 에서 identity transform 역할을 하며 back propagation 에서 경사도를 – λ로 곱함. Link: ADVERSARIAL SPEAKER VERIFICATION","categories":[{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speaker Verification","slug":"Paper/Speaker-Verification","permalink":"https://jmj3047.github.io/categories/Paper/Speaker-Verification/"}],"tags":[{"name":"Adversarial Speaker Verification","slug":"Adversarial-Speaker-Verification","permalink":"https://jmj3047.github.io/tags/Adversarial-Speaker-Verification/"},{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"}]},{"title":"Definition of Distance","slug":"Definition_of_Distance","date":"2022-04-16T15:00:00.000Z","updated":"2022-07-09T05:53:29.000Z","comments":true,"path":"2022/04/17/Definition_of_Distance/","link":"","permalink":"https://jmj3047.github.io/2022/04/17/Definition_of_Distance/","excerpt":"","text":"1. Euclidean distance 가장 흔히 사용하는 거리측도 대응되는 x,y값 간 차이 제곱합의 제곱근으로써, 두 관측치 사이의 직선 거리를 의미함. 다차원 데이터에서도 마찬가지 이다. 2. Manhattan Distance 맨하탄은 블럭이 나누어져 있어 직선으로 갈 수가 없다. 직선거리가 아닌 격자거리. 격자:바둑판처럼 가로세로를 일정한 간격으로 직각이 되게 짠 구조나 물건. 각 좌표의 차이의 절댓값의 합 3. Mahalanobis Distance 변수 내 분산,변수 간 공분산을 모두 반영하여 x,y,간 거리를 계산하는 방식⇒변수간 상관관계를 고려한 거리지표이다. 데이터의 공분산 행렬이 단위행렬인 경우는 유클리디안 거리와 동일함 공분산 행렬의 역행렬을 취했다는 것 → 분산이 분모에 들어간다는 뜻 → 분산이 커지면 거리가 작아지고 , 분산이 작아지면 거리가 길어짐 마할라노비스거리가 제곱근이 취해져 있기 때문에 제곱근을 없앴다. 2차원 행렬로 비유를 했을시 , 쭈욱 대입하면 아래의 식으로 나타난다 y값에 0,0 을주고 대입하면 타원의 방정식이 나온다. 유클리디안 관점에서는 중앙점과 비교했을때, A가 더 멀다. 상관관계를 고려한 마할라노비스 거리로 보면 B가 더 멀다 Reference: 한국공학대학교 강지훈교수님 강의","categories":[{"name":"Basic ML","slug":"Basic-ML","permalink":"https://jmj3047.github.io/categories/Basic-ML/"}],"tags":[{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"}]}],"categories":[{"name":"Setting","slug":"Setting","permalink":"https://jmj3047.github.io/categories/Setting/"},{"name":"Hexo","slug":"Setting/Hexo","permalink":"https://jmj3047.github.io/categories/Setting/Hexo/"},{"name":"Basic ML","slug":"Basic-ML","permalink":"https://jmj3047.github.io/categories/Basic-ML/"},{"name":"Paper","slug":"Paper","permalink":"https://jmj3047.github.io/categories/Paper/"},{"name":"Speaker Verification","slug":"Paper/Speaker-Verification","permalink":"https://jmj3047.github.io/categories/Paper/Speaker-Verification/"},{"name":"NLP","slug":"Paper/NLP","permalink":"https://jmj3047.github.io/categories/Paper/NLP/"},{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Python","slug":"Python/Python","permalink":"https://jmj3047.github.io/categories/Python/Python/"},{"name":"Django","slug":"Python/Django","permalink":"https://jmj3047.github.io/categories/Python/Django/"},{"name":"Git","slug":"Setting/Git","permalink":"https://jmj3047.github.io/categories/Setting/Git/"},{"name":"HTML","slug":"Python/HTML","permalink":"https://jmj3047.github.io/categories/Python/HTML/"},{"name":"Data Base","slug":"Data-Base","permalink":"https://jmj3047.github.io/categories/Data-Base/"},{"name":"Generative Model","slug":"Paper/Generative-Model","permalink":"https://jmj3047.github.io/categories/Paper/Generative-Model/"},{"name":"Computer Vision","slug":"Paper/Computer-Vision","permalink":"https://jmj3047.github.io/categories/Paper/Computer-Vision/"},{"name":"Pyspark","slug":"Python/Pyspark","permalink":"https://jmj3047.github.io/categories/Python/Pyspark/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://jmj3047.github.io/tags/Hexo/"},{"name":"Clustering","slug":"Clustering","permalink":"https://jmj3047.github.io/tags/Clustering/"},{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Gaussian Mixture Model","slug":"Gaussian-Mixture-Model","permalink":"https://jmj3047.github.io/tags/Gaussian-Mixture-Model/"},{"name":"K-Means Clustering","slug":"K-Means-Clustering","permalink":"https://jmj3047.github.io/tags/K-Means-Clustering/"},{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/tags/Python/"},{"name":"Probabilistic sampling","slug":"Probabilistic-sampling","permalink":"https://jmj3047.github.io/tags/Probabilistic-sampling/"},{"name":"Nonprobability sampling","slug":"Nonprobability-sampling","permalink":"https://jmj3047.github.io/tags/Nonprobability-sampling/"},{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"},{"name":"TD-SV","slug":"TD-SV","permalink":"https://jmj3047.github.io/tags/TD-SV/"},{"name":"Transformer","slug":"Transformer","permalink":"https://jmj3047.github.io/tags/Transformer/"},{"name":"Attention","slug":"Attention","permalink":"https://jmj3047.github.io/tags/Attention/"},{"name":"Doc2vec","slug":"Doc2vec","permalink":"https://jmj3047.github.io/tags/Doc2vec/"},{"name":"Chatbot","slug":"Chatbot","permalink":"https://jmj3047.github.io/tags/Chatbot/"},{"name":"NLP","slug":"NLP","permalink":"https://jmj3047.github.io/tags/NLP/"},{"name":"Git","slug":"Git","permalink":"https://jmj3047.github.io/tags/Git/"},{"name":"Virtualenv","slug":"Virtualenv","permalink":"https://jmj3047.github.io/tags/Virtualenv/"},{"name":"Web Server","slug":"Web-Server","permalink":"https://jmj3047.github.io/tags/Web-Server/"},{"name":"CGI","slug":"CGI","permalink":"https://jmj3047.github.io/tags/CGI/"},{"name":"Flask","slug":"Flask","permalink":"https://jmj3047.github.io/tags/Flask/"},{"name":"Brython","slug":"Brython","permalink":"https://jmj3047.github.io/tags/Brython/"},{"name":"MongoDB","slug":"MongoDB","permalink":"https://jmj3047.github.io/tags/MongoDB/"},{"name":"HTML","slug":"HTML","permalink":"https://jmj3047.github.io/tags/HTML/"},{"name":"Web Crawling","slug":"Web-Crawling","permalink":"https://jmj3047.github.io/tags/Web-Crawling/"},{"name":"BeautifulSoup","slug":"BeautifulSoup","permalink":"https://jmj3047.github.io/tags/BeautifulSoup/"},{"name":"Image Classification","slug":"Image-Classification","permalink":"https://jmj3047.github.io/tags/Image-Classification/"},{"name":"WGAN-GP","slug":"WGAN-GP","permalink":"https://jmj3047.github.io/tags/WGAN-GP/"},{"name":"WGAN","slug":"WGAN","permalink":"https://jmj3047.github.io/tags/WGAN/"},{"name":"DCGAN","slug":"DCGAN","permalink":"https://jmj3047.github.io/tags/DCGAN/"},{"name":"Convolutional Neural Networks","slug":"Convolutional-Neural-Networks","permalink":"https://jmj3047.github.io/tags/Convolutional-Neural-Networks/"},{"name":"GAN","slug":"GAN","permalink":"https://jmj3047.github.io/tags/GAN/"},{"name":"Generative Model","slug":"Generative-Model","permalink":"https://jmj3047.github.io/tags/Generative-Model/"},{"name":"Hueman","slug":"Hueman","permalink":"https://jmj3047.github.io/tags/Hueman/"},{"name":"pyspark","slug":"pyspark","permalink":"https://jmj3047.github.io/tags/pyspark/"},{"name":"Adversarial Speaker Verification","slug":"Adversarial-Speaker-Verification","permalink":"https://jmj3047.github.io/tags/Adversarial-Speaker-Verification/"}]}