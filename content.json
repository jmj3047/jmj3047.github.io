{"meta":{"title":"Jang Minjee","subtitle":"","description":"","author":"Jang Minjee","url":"https://jmj3047.github.io","root":"/"},"pages":[],"posts":[{"title":"Hexo Blog ìƒì„± ë° ì¬ì—°ê²°","slug":"Hexo_Create","date":"2022-10-05T15:00:00.000Z","updated":"2022-10-06T23:44:25.025Z","comments":true,"path":"2022/10/06/Hexo_Create/","link":"","permalink":"https://jmj3047.github.io/2022/10/06/Hexo_Create/","excerpt":"","text":"Hexo Blog ìƒì„± ê°„ë‹¨í•˜ê²Œ Hexo ë¸”ë¡œê·¸ë¥¼ ë§Œë“¤ì–´ ë³¸ë‹¤. I. í•„ìˆ˜ íŒŒì¼ ì„¤ì¹˜ 1ë‹¨ê³„: nodejs.org ë‹¤ìš´ë¡œë“œ ì„¤ì¹˜ê°€ ì™„ë£Œ ë˜ì—ˆë‹¤ë©´ ê°„ë‹¨í•˜ê²Œ í™•ì¸í•´ë³¸ë‹¤. 1$ node -v 2ë‹¨ê³„: git-scm.com ë‹¤ìš´ë¡œë“œ ì„¤ì¹˜ê°€ ì™„ë£Œ ë˜ì—ˆë‹¤ë©´ ê°„ë‹¨í•˜ê²Œ í™•ì¸í•´ë³¸ë‹¤. 1$ git --version 3ë‹¨ê³„: hexo ì„¤ì¹˜ hexoëŠ” npmì„ í†µí•´ì„œ ì„¤ì¹˜ê°€ ê°€ëŠ¥í•˜ë‹¤. 1$ npm install -g hexo-cli II. ê¹ƒí—ˆë¸Œ ì„¤ì • ë‘ê°œì˜ ê¹ƒí—ˆë¸Œ Repoë¥¼ ìƒì„±í•œë‹¤. í¬ìŠ¤íŠ¸ ë²„ì „ê´€ë¦¬ (name: myblog) í¬ìŠ¤íŠ¸ ë°°í¬ìš© ê´€ë¦¬ (name: rain0430.github.io) rain0430 ëŒ€ì‹ ì— ê°ìì˜ usernameì„ ì…ë ¥í•˜ë©´ ëœë‹¤. ì´ ë•Œ, myblog repoë¥¼ git cloneì„ í†µí•´ ì ë‹¹í•œ ê²½ë¡œë¡œ ë‚´ë ¤ ë°›ëŠ”ë‹¤. $ git clone your_git_repo_address.git III. ë¸”ë¡œê·¸ ë§Œë“¤ê¸° (ì˜µì…˜) ì ë‹¹í•œ ê³³ì— ê²½ë¡œë¥¼ ì§€ì •í•œ ë‹¤ìŒ ë‹¤ìŒê³¼ ê°™ì´ í´ë”ë¥¼ ë§Œë“ ë‹¤. 12$ mkdir makeBlog # ë§Œì•½ Powershell ì´ë¼ë©´ mkdir ëŒ€ì‹ ì— mdë¥¼ ì“´ë‹¤. $ cd makeBlog ì„ì˜ì˜ ë¸”ë¡œê·¸ íŒŒì¼ëª…ì„ ë§Œë“ ë‹¤. 12345$ hexo init myblog$ cd myblog$ npm install$ npm install hexo-server --save$ npm install hexo-deployer-git --save ERROR Deployer not found: git hexo-deployer-gitì„ ì„¤ì¹˜ í•˜ì§€ ì•Šìœ¼ë©´ deployì‹œ ìœ„ì™€ ê°™ì€ ERRORê°€ ë°œìƒí•©ë‹ˆë‹¤. _config.yml íŒŒì¼ ì„¤ì • ì‹¸ì´íŠ¸ ì •ë³´ ìˆ˜ì • 1234title: ì œëª©ì„ ì§€ì–´ì£¼ì„¸ìš”subtitle: ë¶€ì œëª©ì„ ì§€ì–´ì£¼ì„¸ìš”description: descriptionì„ ì§€ì–´ì£¼ì„¸ìš”author: YourName ë¸”ë¡œê·¸ URL ì •ë³´ ì„¤ì • 1234url: https://rain0430.github.ioroot: /permalink: :year/:month/:day/:title/permalink_defaults: ê¹ƒí—ˆë¸Œ ì—°ë™ 12345# Deploymentdeploy: type: git repo: https://github.com/rain0430/rain0430.github.io.git branch: main IV. ê¹ƒí—ˆë¸Œì— ë°°í¬í•˜ê¸° ë°°í¬ ì „, í„°ë¯¸ë„ì—ì„œ localhost:4000 ì ‘ì†ì„ í†µí•´ í™”ë©´ì´ ëœ¨ëŠ”ì§€ í™•ì¸í•´ë³¸ë‹¤. 1234$ hexo generate$ hexo serverINFO Start processingINFO Hexo is running at http://localhost:4000 . Press Ctrl+C to stop. í™”ë©´ í™•ì¸ì´ ëœ ì´í›„ì—ëŠ” ê¹ƒí—ˆë¸Œì— ë°°í¬í•œë‹¤. ì‚¬ì „ì—, gitignore íŒŒì¼ì—ì„œ ì•„ë˜ì™€ ê°™ì´ ì„¤ì •ì„ ì§„í–‰í•œë‹¤. 1234567.DS_StoreThumbs.dbdb.json*.lognode_modules/public/.deploy*/ ìµœì¢…ì ìœ¼ë¡œ ë°°í¬ë¥¼ ì§„í–‰í•œë‹¤. 1$ hexo deploy ë°°í¬ê°€ ì™„ë£Œê°€ ë˜ë©´ ë¸Œë¼ìš°ì €ì—ì„œ USERNAME.github.ioë¡œ ì ‘ì†í•´ ì •ìƒì ìœ¼ë¡œ ë°°í¬ê°€ ë˜ì—ˆëŠ”ì§€ í™•ì¸í•œë‹¤. Hexo Blog ì¬ì—°ê²° ê¸°ì¡´ ë¸”ë¡œê·¸ í´ë” íŒŒì¼ ì••ì¶•í•´ì„œ ë°±ì—…í•œ í›„ ì§„í–‰í•´ì•¼ í•œë‹¤. â†’ theme ê°™ì€ ê²½ìš° ë°›ì•„ì˜¤ëŠ”ê±°ë¶€í„° ë‹¤ì‹œ í•´ì•¼ í•˜ê¸° ë•Œë¬¸ ì¬ì—°ê²°ë³´ë‹¤ëŠ” ì¬ìƒì„±ì´ë¼ê³  ë§í•˜ëŠ”ê²Œ ë” ì í•©í•˜ë‹¤. ë‹¤ë¥¸ ë¡œì»¬ì—ì„œ ë¸”ë¡œê·¸ë¥¼ ì¬ì—°ê²°í•´ì„œ ì‚¬ìš©í•  ê²½ìš° ì•„ë˜ì™€ ê°™ì´ ìˆœì°¨ì ìœ¼ë¡œ ì§„í–‰í•˜ë©´ ëœë‹¤. 1234$ hexo init your_blog_repo # ì—¬ê¸°ëŠ” ê°ì ì†ŒìŠ¤ ë ˆí¬ í™•ì¸$ cd myblog$ git init $ git remote add origin https://github.com/your_name/your_blog_repo.git # ê°ì ì†ŒìŠ¤ ë ˆí¬ ì£¼ì†Œ ì•„ë˜ ëª…ë ¹ì–´ì—ì„œ ì—ëŸ¬ê°€ ë°œìƒì´ ìˆë‹¤. 1$ git pull --set-upstream origin main # ì—ëŸ¬ ë°œìƒ ê·¸ëŸ° ê²½ìš°, ì•„ë˜ ëª…ë ¹ì–´ë¥¼ ì¶”ê°€í•œë‹¤. ê¸°ì¡´ì˜ ë””ë ‰í† ë¦¬ì™€ íŒŒì¼ì„ ëª¨ë‘ ì‚­ì œí•œë‹¤ëŠ” ëœ»ì´ë‹¤. 1$ git clean -d -f ê·¸ë¦¬ê³  ì—ëŸ¬ê°€ ë°œìƒí–ˆë˜ ëª…ë ¹ì–´ë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•œë‹¤. ì´ ë•Œì—ëŠ” ì´ì œ ì •ìƒì ìœ¼ë¡œ ì‹¤í–‰ë˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. 1$ git pull --set-upstream origin main # ì—ëŸ¬ ë°œìƒ ì•ˆí•¨ / ì†ŒìŠ¤ í™•ì¸ ì´ì œ ì •ìƒì ìœ¼ë¡œ í™˜ê²½ ì„¸íŒ…ì€ ëœ ê²ƒì´ë‹¤. ìˆœì°¨ì ìœ¼ë¡œ ì•„ë˜ì™€ ê°™ì´ ì§„í–‰í•˜ë„ë¡ í•œë‹¤. ì´ ë•Œ, theme í´ë”ì— ë³¸ì¸ì˜ í…Œë§ˆ ì†ŒìŠ¤ì½”ë“œê°€ ì˜ ìˆëŠ”ì§€ í™•ì¸ì„ í•˜ë„ë¡ í•œë‹¤. 1234$ npm install $ hexo clean$ hexo generate$ hexo server reference ìƒì„± ì¬ì—°ê²°","categories":[{"name":"Setting","slug":"Setting","permalink":"https://jmj3047.github.io/categories/Setting/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://jmj3047.github.io/tags/Hexo/"}]},{"title":"Clustering","slug":"Clustering","date":"2022-09-07T15:00:00.000Z","updated":"2022-10-10T02:08:17.035Z","comments":true,"path":"2022/09/08/Clustering/","link":"","permalink":"https://jmj3047.github.io/2022/09/08/Clustering/","excerpt":"","text":"Clustering is an example of unsupervised learning. Without any label, those with close distances in the data are classified into clusters. It is different from classification, which is supervised learning. In other words, it identifies patterns and groups hidden in the data and binds them together. Even if thereâ€™s label in data, there is a possibility that some data with same label can be grouped into different clusters. There are K-Means Clustering, Mean Shift, Gaussian Mixture Model, DBSCAN, Agglomerative Clustering in clustering algorithms and they will be covered in the next post. Korean reference","categories":[{"name":"Basic ML","slug":"Basic-ML","permalink":"https://jmj3047.github.io/categories/Basic-ML/"}],"tags":[{"name":"WP_edu","slug":"WP-edu","permalink":"https://jmj3047.github.io/tags/WP-edu/"},{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Clustering","slug":"Clustering","permalink":"https://jmj3047.github.io/tags/Clustering/"}]},{"title":"Data Sampling","slug":"Data_Sampling","date":"2022-09-06T15:00:00.000Z","updated":"2022-10-10T02:06:38.793Z","comments":true,"path":"2022/09/07/Data_Sampling/","link":"","permalink":"https://jmj3047.github.io/2022/09/07/Data_Sampling/","excerpt":"","text":"1. Reason why you need The more input data you have on machine learning, the slower the processing. Therefore, in order to speed up the processing speed of machine learning, acceleration of learning speed of data would be helpful, which can be done with optimization of machine learning with representative data. Then letâ€™s see how we can reduce the data so that we can only use the data we need. 2. What is data sampling The process of organizing the data and making it the best input data. For example, it can speed up the processing of machine learning by using sales of â€˜monthâ€™ rather than using sales of â€˜dayâ€™ units to analyze last yearâ€™s profits of a pizza house. This is the work of making optiml data. There are probabilistic sampling, nonprobability sampling in data sampling method. The probabilistic sampling method is a sampling method based on statistics, and the nonprobability sampling is a sampling method in which the subjectivity of a person is involved. Depending on each sampling method, you should select and use the sampling method that matches the data and the situation because there are advantages and disadvantages. 3. Probabilistic sampling Probabilistic sampling is a random sampling method that can be divided into simple random sampling, two-step sampling, stratified sampling, cluster&#x2F;collective sampling, and system sampling. Simple Random Sampling: A method of randomly extracting samples from the entire data. Two-Step Sampling: A sampling method that separates the entire n data into m subpopulations. Selects m subpopulations and provides simple random sampling of N data from m subpopulations. It is an accurate sampling method rather than simple random sampling. Stratified Sampling: By separating the population from several layers, it is a method of randomly extracting data from each layer by n. For example, it is a method of dividing the layers of Korean cities and province and extracting n data from each layer. cluster&#x2F;collective sampling: If the population is composed of multiple clusters, it is a method of selecting one or several clusters and using the entire data of the selected clusters. For example, it is a way to use all of the data by setting the Korea as itâ€™s city and province. system sampling: It is a method of extracting data one by one at a regular interval by numbering all data from 1 to n. This method is mainly used to sample representative values of the time series data 4. Nonprobability sampling This is a method of subjectively extracting the probability of being selected as a sample in advance. The advantage and disadvantage of this sampling is that the subjective intention of the sampling person is involved. The implicit population extracted by nonprobability sampling is a good sampling if it matches the ideal population, the most suitable population for the subject. Nonprobability sampling methods include convenience sampling, purpose sampling, and quota sampling methods. Convenience Sampling: A method of sampling by selecting a point or location where data is good for collecting. The sample surveyed by this sampling method has a disadvantage that it is less representative than the population. Itâ€™s not possible to go through the statistical inference process. Statistical inference is that we generalize the sample analysis results to speculation about populations. Purpose Sampling: This is how you select the object you think is the most suitable object for your purpose; you will sample the data that is subjectively appropriate for your purpose.The downside is that it has also low representative for the population. Quota Sampling: Divide the population into segments, assigning each segment a quartar that represents the number of samples. Within segments, the characteristics related to the topic must be similar, and the populations must be distributed differently between segments. This is methodologically similar to layer-by-layer sampling. But the difference is that the sample is not selected by probability but by subjective judgment. 5. Conclusion Comparing probabilistic sampling with nonprobability sampling, probabilistic sampling may look good when judged by just words. However, stochastic sampling is advantageous for data that can be analyzed based on statistics, and non-probability sampling is advantageous for data such as language and music. It is better to choose and use probabilistic and nonprobable sampling as appropriate. Korean Reference","categories":[{"name":"Basic ML","slug":"Basic-ML","permalink":"https://jmj3047.github.io/categories/Basic-ML/"}],"tags":[{"name":"WP_edu","slug":"WP-edu","permalink":"https://jmj3047.github.io/tags/WP-edu/"},{"name":"Probabilistic sampling","slug":"Probabilistic-sampling","permalink":"https://jmj3047.github.io/tags/Probabilistic-sampling/"},{"name":"Nonprobability sampling","slug":"Nonprobability-sampling","permalink":"https://jmj3047.github.io/tags/Nonprobability-sampling/"},{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"}]},{"title":"Growth Hacking, AARRR, Funnel, Retention","slug":"Growth_Hacking","date":"2022-08-11T15:00:00.000Z","updated":"2022-08-15T13:34:03.826Z","comments":true,"path":"2022/08/12/Growth_Hacking/","link":"","permalink":"https://jmj3047.github.io/2022/08/12/Growth_Hacking/","excerpt":"","text":"1. Growth Hacking ê·¸ë¡œìŠ¤í•´í‚¹(Growth Hacking)ì€ ì„±ì¥(Growth)ì„ ìœ„í•œ ëª¨ë“  ìˆ˜ë‹¨(Hacking)ì´ë€ ëœ»ìœ¼ë¡œ ê³µê²© ëŒ€ìƒì˜ ë¯¸ì„¸í•œ ë¹ˆí‹ˆì„ ì°¾ì•„ í•´í‚¹ì„ í•˜ë“¯ì´ ì„±ì¥ì„ ìœ„í•´ ê³ ê°ê³¼ ìœ í†µê³¼ì • ë“±ì˜ ê³µëµì§€ì ì„ ì°¾ì•„ë‚´ê³  ì´ë¥¼ ì ê·¹ì ìœ¼ë¡œ ê³µëµí•˜ëŠ” ë§ˆì¼€íŒ… ë°©ë²•ë¡  ë¸Œëœë“œ, ê¸°ì—…, ì œí’ˆ ë§¤ì¶œ ì¦ê°€ ë“±ì„ ìœ„í•œ ê°€ì„¤ì„ ìˆ˜ë¦½í•˜ê³  ì´ë¥¼ ë¹ ë¥´ê²Œ MVP ëª¨ë¸ë¡œ ì¶œì‹œí•˜ì—¬ ì‹œì¥ì˜ í‰ê°€ë¥¼ ë°›ì•„ ë³¸ í›„, ì†Œë¹„ìì™€ ì‹œì¥ì˜ ë°˜ì‘ì— ë”°ë¼ ì œí’ˆ ë˜ëŠ” ì„œë¹„ìŠ¤ê°€ ì‹œì¥ì—ì„œ ì›í•˜ëŠ” (ê³ ê°ë“¤ì´ ì›í•˜ëŠ”) ì™„ë²½í•œ ìƒí’ˆìœ¼ë¡œ ë„ë‹¬í•  ë•Œê¹Œì§€ ì‰¬ì§€ ì•Šê³  ê°œì„ í•´ ë‚˜ê°€ëŠ” ë°©ì‹ ì„±ì¥(Growth)ì„ ìœ„í•œ ëª¨ë“  ìˆ˜ë‹¨(Hacking)ì„ í†µí•´ íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•˜ê³  ì œí’ˆê³¼ ì„œë¹„ìŠ¤ë¥¼ ë¹ ë¥´ê²Œ ì„±ì¥ì‹œí‚¨ë‹¤ëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ ìˆ˜ë‹¨ì´ë€, ë°ì´í„° ê¸°ë°˜ì˜ ë¶„ì„ê³¼ ì‹œì¥ì˜ í”¼ë“œë°±ì„ ë°›ì•„ ì œí’ˆê³¼ ì„œë¹„ìŠ¤ë¥¼ ê°œì„ í•˜ê³  í™•ì¥í•´ ë‚˜ê°„ë‹¤ëŠ” ì˜ë¯¸ë¡œ, ìë³¸ê³¼ ë¦¬ì†ŒìŠ¤ê°€ í•œì •ì ì¸ ìŠ¤íƒ€íŠ¸ ê¸°ì—…ì—ê²Œì„œ íš¨ê³¼ì ì¸ ë§ˆì¼€íŒ… ì„±ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŒ êµ¬ë§¤ìì˜ í–‰ë™ íŒ¨í„´ì„ ë¶„ì„í•˜ê³  ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ê²½í—˜ì„ ìµœì í™”í•˜ëŠ” ë°©ë²•. ê·¸ë¡œìŠ¤í•´í‚¹ì€ ìƒì‚°ë¶€í„° ê´€ë¦¬ì— ì´ë¥´ê¸°ê¹Œì§€ ì†Œë¹„ìì˜ Wantsë¥¼ ì¶©ì¡±ì‹œí‚¤ëŠ” ìƒí’ˆì„ ë§Œë“œëŠ” ê²ƒ 2. AARRR Analysis Acquisition: ì–´ë–»ê²Œ ì²˜ìŒ ìš°ë¦¬ ì„œë¹„ìŠ¤ë¥¼ ì ‘í•˜ê²Œ ë˜ëŠ”ê°€(ì‚¬ìš©ì ìœ ì¹˜) Activation: ì‚¬ìš©ìê°€ ì²˜ìŒ ì„œë¹„ìŠ¤ë¥¼ ì ‘í•  ë•Œì— ê¸ì •ì ì¸ ê²½í—˜ì„ ì œê³µí•˜ê³  ìˆëŠ”ê°€(ì‚¬ìš©ì í™œì„±í™”) Retention: ì´í›„ì˜ ì„œë¹„ìŠ¤ë¥¼ ë‹¤ì‹œ ì‚¬ìš©í•˜ëŠ” ì •ë„ëŠ” ì–¼ë§ˆë‚˜ ë˜ëŠ”ê°€(ì‚¬ìš©ì ìœ ì§€) Revenue: ìµœì¢… ëª©ì (ê±°ì‹œì „í™˜)ìœ¼ë¡œ ì—°ê²°ë˜ê³  ìˆëŠ”ê°€(ë§¤ì¶œ) Referral: ì‚¬ìš©ìê°€ ìë°œì ìœ¼ë¡œ í™•ì‚°ì´ë‚˜ ê³µìœ ë¥¼ ì¼ìœ¼í‚¤ê³  ìˆëŠ”ê°€(ì¶”ì²œ) 3. Funnel Analysis í¼ë„ ë¶„ì„ì€ ì›¹ ì‚¬ì´íŠ¸ì—ì„œ íŠ¹ì • ê²°ê³¼ì— ë„ë‹¬í•˜ëŠ”ë° í•„ìš”í•œ ë‹¨ê³„ì™€, ê° ë‹¨ê³„ë¥¼ í†µê³¼í•˜ëŠ” ì‚¬ìš©ì ìˆ˜ë¥¼ íŒŒì•…í•˜ê¸° ìœ„í•œ ë°©ë²• í¼ë„ ë¶„ì„ì„ í†µí•´ ê¸°ì—…ì€ ë°©ë¬¸ìê°€ ì‚¬ì´íŠ¸ì— ê°€ì…í•˜ëŠ”ì§€, êµ¬ë§¤ìë¡œ ë³€í™˜ì´ ë˜ëŠ”ì§€ ë“±ì˜ ì •ë³´ë¥¼ íŠ¹ì •í•œ í¼ë„ì— ë§¤í•‘í•˜ê²Œ ë¨. ì´ë•Œ ì‚¬ìš©ìì˜ íë¦„ì„ ì‹œê°í™”í•˜ëŠ” ëª¨í˜•ì˜ ëª¨ìŠµì´ ë¶€ì—Œì´ë‚˜ ì°¨ê³ ì—ì„œ í”í•˜ê²Œ ì“°ì´ëŠ” ê¹”ë•Œê¸°ì™€ ìœ ì‚¬í•œ ëª¨ìŠµì„ ë ê³  ìˆì–´ â€˜í¼ë„ ë¶„ì„â€™ì´ë¼ëŠ” ì´ë¦„ì´ ë¶™ì—¬ì§ 4. Retention Analysis ë¦¬í…ì…˜ì€ ì•± ì„œë¹„ìŠ¤ ì„±ì¥ì— ìˆì–´ì„œ ë§¤ìš° ì¤‘ìš”í•œ ì§€í‘œ ë¦¬í…ì…˜ì´ë€ í•œë²ˆ íšë“í•œ ìœ ì €ë“¤ì´ ì„œë¹„ìŠ¤ë¥¼ ì´íƒˆí•˜ì§€ ì•Šê³  ê³„ì† ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•˜ëŠ” ê²ƒì„ ì˜ë¯¸ ë¦¬í…ì…˜ì´ ë†’ì€ ì„œë¹„ìŠ¤ëŠ” ë¦¬í…ì…˜ì´ ë‚®ì€ ì„œë¹„ìŠ¤ë³´ë‹¤ íšë“ë¹„ìš©ì— íˆ¬ìí•œ ë¹„ìš©ì„ ë¹ ë¥´ê²Œ íšŒìˆ˜í•  ìˆ˜ ìˆìœ¼ë©° ì´ë ‡ê²Œ íšŒìˆ˜í•œ ë¹„ìš©ìœ¼ë¡œ ë‹¤ì‹œê¸ˆ ë¹ ë¥´ê²Œ íšë“ì— íˆ¬ìí•  ìˆ˜ ìˆë„ë¡ í•˜ì—¬ ì„±ì¥ì„ ì´‰ì§„í•©ë‹ˆë‹¤. ë¦¬í…ì…˜ì´ ë‚®ë‹¤ëŠ” ê²ƒì€ íšë“ ì´í›„ ë‹¤ì‹œ ëŒì•„ì˜¤ì§€ ì•Šê³  ì´íƒˆí•˜ëŠ” ìœ ì €ê°€ ë§ì€ ê²ƒì¸ë°ìš”, ë¦¬í…ì…˜ì´ ë‚®ì€ ì„œë¹„ìŠ¤ëŠ” ì„±ì¥ì´ ë”ë”œ ë¿ë§Œ ì•„ë‹ˆë¼ í•œë²ˆ íšë“í–ˆë‹¤ê°€ ì´íƒˆí•œ ìœ ì €ëŠ” ë‹¤ì‹œ íšë“í•˜ê¸°ì—ë„ ë” ë§ì€ íšë“ ë¹„ìš©ê³¼ ì‹œê°„ì´ ì†Œìš”ë˜ë¯€ë¡œ ì•…ìˆœí™˜ì„ ë§Œë“¤ì–´ëƒ…ë‹ˆë‹¤. ë”°ë¼ì„œ ë¦¬í…ì…˜ì€ ì‚¬ì—… ì„±ì¥ì— ìˆì–´ì„œ ë°˜ë“œì‹œ ì§€ì¼œë³´ì•„ì•¼ í•  ì§€í‘œ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. ì”ì¡´ìœ¨(D+1ì§€í‘œ) Reference Retention Funnel How to measure Retention","categories":[{"name":"Basic ML","slug":"Basic-ML","permalink":"https://jmj3047.github.io/categories/Basic-ML/"}],"tags":[{"name":"WP_edu","slug":"WP-edu","permalink":"https://jmj3047.github.io/tags/WP-edu/"},{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"}]},{"title":"Deep Embedding Learning for Text-Dependent Speaker Verification","slug":"Deep_Embedding_Learning_for_Text-Dependent_Speaker_Verification","date":"2022-07-04T15:00:00.000Z","updated":"2022-07-09T04:36:26.760Z","comments":true,"path":"2022/07/05/Deep_Embedding_Learning_for_Text-Dependent_Speaker_Verification/","link":"","permalink":"https://jmj3047.github.io/2022/07/05/Deep_Embedding_Learning_for_Text-Dependent_Speaker_Verification/","excerpt":"","text":"Journal&#x2F;Conference: InterspeechYear(published year): 2020Author: Peng Zhang, Peng Hu, Xueliang ZhangSubject: Speaker Verification Deep Embedding Learning for Text-Dependent Speaker Verification Summary ì´ ë…¼ë¬¸ì€ í™”ì ê²€ì¦ ì‘ì—…ì„ ìœ„í•œ íš¨ê³¼ì ì¸ ë”¥ ì„ë² ë”© í•™ìŠµ ì•„í‚¤í…ì²˜ë¥¼ ì œì‹œí•œë‹¤. ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ì”ë¥˜ ì‹ ê²½ë§(ResNet) ë° ì‹œê°„ ì§€ì—° ì‹ ê²½ë§(TDNN) ê¸°ë°˜ ì•„í‚¤í…ì²˜ì™€ ë¹„êµí•˜ì—¬, ë‘ ê°€ì§€ ì£¼ìš” ê°œì„ ì´ ì œì•ˆëœë‹¤. ìš°ë¦¬ëŠ” í™”ìì˜ ë‹¨ê¸° ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ì¸ì½”ë”©í•˜ê¸° ìœ„í•´ ì¡°ë°€í•˜ê²Œ ì—°ê²°ëœ ì»¨ë³¼ë£¨ì…˜ ë„¤íŠ¸ì›Œí¬(DenseNet)ë¥¼ ì‚¬ìš©í•œë‹¤. ì–‘ë°©í–¥ ì£¼ì˜ í’€ë§ ì „ëµì´ ì œì•ˆëœë‹¤. ì¥ê¸°ì ì¸ ì‹œê°„ì  ë§¥ë½ì„ ëª¨ë¸ë§í•˜ê³  í™”ì ì •ì²´ì„±ì„ ë°˜ì˜í•˜ëŠ” ì¤‘ìš”í•œ í”„ë ˆì„ì„ ì§‘ê³„í•œë‹¤. ê²°ê³¼ëŠ” ì œì•ˆëœ ì•Œê³ ë¦¬ë“¬ì´ ê³¼ì œ 1ê³¼ ê³¼ì œ 3ì˜ í‰ê°€ ì„¸íŠ¸ì—ì„œ ê°ê° 8.06%, 19.70% minDCF ë° 9.26%, 16.16% EERs ìƒëŒ€ì  ê°ì†Œë¡œ FFSVC2020ì˜ ê³µì‹ ê¸°ì¤€ì„ ì„ ëŠ¥ê°€í•œë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤€ë‹¤. Introductionë”¥ ëŸ¬ë‹ ê¸°ë°˜ ë°©ë²•ì€ í™”ì ê°„ ì°¨ë³„ì— í•„ìˆ˜ì ì¸ ì •ë³´ë¥¼ í¬í•¨í•˜ëŠ” ë”¥ ìŠ¤í”¼ì»¤ ì„ë² ë”© ë˜ëŠ” ì§§ê²Œ ì„ë² ë”©ê³¼ ê°™ì€ ë°œí™” ìˆ˜ì¤€ í‘œí˜„ì„ ì–»ê¸° ìœ„í•´ ì§€ë°°ì ì´ì—ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” í™”ì ê²€ì¦ì„ ìœ„í•œ íš¨ê³¼ì ì¸ ë”¥ ì„ë² ë”© í•™ìŠµ ì•„í‚¤í…ì²˜ë¥¼ ì œì•ˆí•œë‹¤. ìµœê·¼ ì¡°ë°€í•˜ê²Œ ì—°ê²°ëœ ì»¨ë³¼ë£¨ì…˜ ë„¤íŠ¸ì›Œí¬ì˜ ì„±ê³µì— ìê·¹ë°›ì•„ (DenseNet) ì´ë¯¸ì§€ ë¶„ë¥˜ [15], ìŒì•… ì†ŒìŠ¤ ë¶„ë¦¬ [16], í™”ì ë¶„ë¦¬ [17] ë° í™”ìì¸ì‹ [18]ì—ì„œ, ìš°ë¦¬ëŠ” DenseNetì„ í”„ë ˆì„ ë ˆë²¨ í”¼ì²˜ ì¶”ì¶œê¸°ë¡œ ì±„íƒí•˜ì—¬ í›„ì† ë ˆì´ì–´ì˜ ì…ë ¥ì˜ ë³€í™”ì™€ í›ˆë ¨ íš¨ìœ¨ì„±ì„ ì¦ê°€ì‹œí‚¨ë‹¤. ê°œë…ì ìœ¼ë¡œ ê° ë°€ë„ ë¸”ë¡ì€ ì‘ì€ CNN ì‹œìŠ¤í…œ ì—­í• ì„ í•œë‹¤. ì°¨ì´ì ì€ ë ˆì´ì–´ì˜ ì¶œë ¥ì´ ì±„ë„ ì°¨ì›ì˜ ì¶œë ¥ ì—°ê²°ì— ì˜í•´ êµ¬í˜„ë˜ëŠ” í”¼ë“œ í¬ì›Œë“œ ë°©ì‹ìœ¼ë¡œ ì¡°ë°€í•˜ê²Œ ì—°ê²°ëœë‹¤ëŠ” ê²ƒì´ë‹¤. ë˜í•œ, ìš°ë¦¬ëŠ” ë°œí™” ìˆ˜ì¤€ ê¸°ëŠ¥ì˜ í‘œí˜„ë ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ì‹œê°„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì¶”ê°€ë¡œ ëª¨ë¸ë§í•˜ê¸° ìœ„í•œ ì–‘ë°©í–¥ ì£¼ì˜ í’€ë§ ê³„ì¸µì„ ì„¤ê³„í•œë‹¤. Model ArchitectureDenseNet(í”„ë ˆì„ ë ˆë²¨ ê¸°ëŠ¥ ì¶”ì¶œê¸°): ê°ê° 5ê°œì˜ CNN ë ˆì´ì–´ê°€ ìˆëŠ” 4ê°œì˜ ë°€ë„ ë¸”ë¡(DenseBlock)ìœ¼ë¡œ êµ¬ì„±. í”„ë ˆì„ ë ˆë²¨ í”¼ì²˜ ì¶”ì¶œ í›„, ì–‘ë°©í–¥ ì£¼ì˜ í’€ë§ ë ˆì´ì–´ëŠ” í”„ë ˆì„ ë ˆë²¨ í”¼ì²˜ë¥¼ ê³ ì • ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ë° ì‚¬ìš©ë˜ë©°, ì´ì–´ì„œ ì™„ì „íˆ ì—°ê²°ëœ ë‘ ê°œì˜ ìˆ¨ê²¨ì§„ ë ˆì´ì–´ê°€ ë°œí™” ë ˆë²¨ í”¼ì²˜ë¥¼ í˜•ì„±í•œë‹¤. ì¶œë ¥ì€ ì†Œí”„íŠ¸ë§¥ìŠ¤ ë¶„ë¥˜ê¸° ê³„ì¸µì´ë©°, ê° ë…¸ë“œëŠ” í™”ìIDì— ëŒ€ì‘í•©ë‹ˆë‹¤. Frame levelì˜ íŠ¹ì§• ì¶”ì¶œì„ í•˜ê³ , Inputê³¼ ë‹¤ìŒ ë ˆì´ì–´ì˜ ë³€í™”ì— ìµœì í™” ì‹œí‚¤ê³ , í›ˆë ¨ íš¨ìœ¨ì„ ë†’ì´ê¸°ìœ„í•´ì„œ ì‚¬ìš©í•¨.ê°œë…ì ìœ¼ë¡œ ê°ê°ì˜ dense blockë“¤ ì´ ì‘ì€ CNNì˜ ì—­í• ì„ ìˆ˜í–‰í•˜ê³  ìˆëŠ” ê²ƒ. ê°ê°ì˜ ë ˆì´ì–´ë“¤ì´ ë°”ë¡œ ì „ì— ìˆëŠ” ëª¨ë“  ë ˆì´ì–´ë“¤ê³¼ feature map(CNNì—ì„œì˜ í•©ì„±ê³±ê³¼ ê°™ì€ ì›ë¦¬)ì„ í†µí•´ì„œ ì—°ê²°ì´ ë˜ì–´ìˆëŠ” ë°©ì‹.ì´ëŸ¬í•œ ì—°ê²° íŒ¨í„´ì€ í›ˆë ¨ ì¤‘ ë ˆì´ì–´ë“¤ ì‚¬ì´ì—ì„œ ë” ë‚˜ì€ ê¸°ìš¸ê¸° flowì™€ ê° ë ˆì´ì–´ë“¤ì— ëŒ€í•œ ì ‘ê·¼ì„ ëª¨ë“  featureì—ê²Œ ì „ë‹¬í•¨ìœ¼ë¡œì¨ ì¼ì‹œì ì¸ ë¬¸ë§¥ ì •ë³´ë¥¼ capture ê°€ëŠ¥. í”„ë ˆì„ ë ˆë²¨ ë‹¨ê³„ì˜ ê²½ìš° ê° DenseBlockì€ 5ê°œì˜ ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´(Conv2D), ì§€ìˆ˜ ì„ í˜• ë‹¨ìœ„(ELU) ë° ì¸ìŠ¤í„´ìŠ¤ ì •ê·œí™”(IN)ë¡œ êµ¬ì„±ëœë‹¤. ê° ë°€ë„ ë¸”ë¡ ë’¤ì˜ í…ì„œ ëª¨ì–‘ì€ featureMaps Ã— timeSteps Ã— ì£¼íŒŒìˆ˜ ì±„ë„ í˜•ì‹ì´ë‹¤. ê° Conv2D ë° Conv2D+IN+ELUëŠ” kernelSize í˜•ì‹ìœ¼ë¡œ ì§€ì •ë©ë‹ˆë‹¤. ì‹œê°„ Ã— kernelSizeFreq, (ë³´í–‰)ì‹œê°„, ë³´í­Freq), (íŒ¨ë”©ì‹œê°„, íŒ¨ë”©Freq), ë§µì„ íŠ¹ì§•ìœ¼ë¡œ í•œë‹¤. ê° ë°€ë„ ë¸”ë¡(g)ì€ 5ê°œì˜ Conv2D+ì¦ê°€ìœ¨ì´ gì¸ IN+ELU ë¸”ë¡ì„ í¬í•¨í•©ë‹ˆë‹¤. ë°œí™” ìˆ˜ì¤€ ë‹¨ê³„ì˜ ê²½ìš° ìˆ«ìëŠ” ìš°ë¦¬ì˜ êµ¬í˜„ì—ì„œ ì¶œë ¥ ê¸°ëŠ¥ ë§µ ë˜ëŠ” ì„ë² ë”© ì°¨ì›ì˜ ì±„ë„ì„ ë‚˜íƒ€ë‚¸ë‹¤. DenseBlock[15]ì—ì„œ ì²˜ìŒ ì œì‹œëœ DenseBlock ì•„í‚¤í…ì²˜ì˜ ì£¼ìš” ì•„ì´ë””ì–´ëŠ” CNNì˜ ë„¤íŠ¸ì›Œí¬ êµ¬ì¶• ë¸”ë¡ì—ì„œ ê° ê³„ì¸µì—ì„œ ëª¨ë“  í›„ì† ê³„ì¸µì— ì§ì ‘ ì—°ê²°ì„ ë„ì…í•˜ëŠ” ê²ƒì´ë‹¤. ì‹œê°„ ë¹ˆë„ê°€ ê¸´ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ìº¡ì²˜í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆë‹¤. ê° ê³„ì¸µì€ í”¼ì³ ë§µ ì—°ê²°ì„ í†µí•´ ëª¨ë“  ë‹¤ìŒ ê³„ì¸µì— ì§ì ‘ ì—°ê²°ë©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì—°ê²° íŒ¨í„´ì€ í›ˆë ¨ ì¤‘ì— ê³„ì¸µ ê°„ì— ë” ë‚˜ì€ ê·¸ë ˆì´ë””ì–¸íŠ¸ íë¦„ì„ ìƒì„±í•˜ê³  ê° ê³„ì¸µì´ ì´ì „ ê³„ì¸µì˜ ëª¨ë“  íŠ¹ì§• í‘œí˜„ì— ì•¡ì„¸ìŠ¤í•  ìˆ˜ ìˆë„ë¡ í•˜ì—¬ ì‹œê°„ì  ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ìº¡ì²˜í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆë‹¤. Bidirectional Attentive Pooling ì–‘ë°©í–¥ ì£¼ì˜ í’€ë§(bidirectional attentive pooling, BAP)ì˜ ë„ì‹ì…ë‹ˆë‹¤. hiëŠ” BAP ì…ë ¥ì˜ ië²ˆì§¸ ë²¡í„°ë¥¼ ë‚˜íƒ€ë‚´ë©° wfì™€ wbëŠ” ê°ê° BGRUì˜ ì „ë°©ê³¼ í›„ë°© ìˆ¨ê²¨ì§„ ìƒíƒœë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. â†’U ë° â†UëŠ” BGRU ë ˆì´ì–´ì˜ ì–‘ë°©í–¥ ì¶œë ¥ì…ë‹ˆë‹¤. bidirectional gated recurrent unit (BGRU) layer + attentive pooling : utterance level feature ì¼ë°˜ì ì¸ average poolingì„ ì‚¬ìš©í•˜ì§€ ì•Šì€ ì´ìœ : í‰ê· í™” ëŒ€ì‹  ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜[10, 12]ì€ ìˆ¨ê²¨ì§„ í‘œí˜„ì„ ì ê·¹ì ìœ¼ë¡œ ì„ íƒí•˜ê³  í™”ì ì°¨ë³„ ì •ë³´ë¥¼ ê°•ì¡°í•˜ê¸° ìœ„í•œ ë” ë‚˜ì€ ëŒ€ì•ˆì´ë‹¤. ë³´ë‹¤ ì°¨ë³„ì ì¸ ê³ ì • ì°¨ì› ë°œí™” ìˆ˜ì¤€ í‘œí˜„ì„ ì–»ê³  ì¥ê¸° ì‹œí€€ìŠ¤ ì •ë³´ë¥¼ ìº¡ì²˜í•˜ê¸° ìœ„í•´ [19]ëŠ” CNN-BLSTM ëª¨ë¸ê³¼ ì£¼ì˜ ê¹Šì€ í’€ë§ ë ˆì´ì–´ë¥¼ í•¨ê»˜ ê²°í•©í•˜ëŠ” ì£¼ì˜ ê¸°ë°˜ CNN-BLSTM í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•˜ì˜€ë‹¤. BLSTMì„ ì£¼ì˜ ê¹Šì€ í’€ë§ ê³„ì¸µì— ì§ì ‘ ì—°ê²°í•˜ëŠ” [19]ì™€ëŠ” ë‹¬ë¦¬ ì£¼ì˜ ê¹Šì€ í’€ë§ì„ ì‚¬ìš©í•˜ì—¬ ì–‘ë°©í–¥ ë°˜ë³µ ì‹ ê²½ë§ì— ì˜í•´ ì¶œë ¥ë˜ëŠ” ì–‘ë°©í–¥ ì‹œê°„ ì •ë³´ë¥¼ ìº¡ì²˜í•œ ë‹¤ìŒ, ì–‘ë°©í–¥ ë°œí™” ìˆ˜ì¤€ ê¸°ëŠ¥ì„ ì—°ê²°í•œë‹¤. ì œì•ˆëœ í’€ë§ ë°©ë²•ì¸ ì–‘ë°©í–¥ ì£¼ì˜ í’€ë§(BAP)ì€ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„ë  ìˆ˜ ìˆë‹¤. BAP ê³„ì¸µì€ ì–‘ë°©í–¥ ìˆœì°¨ ëª¨ë¸ë§ê³¼ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì„ ëª¨ë‘ í™œìš©í•˜ì—¬ ì¥ê¸°ì ì¸ ì‹œê°„ì  ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ìº¡ì²˜í•œë‹¤. Result Dataset: FFSVC2020The first 30 utterances are of fixed content: â€˜ni hao mi yaâ€™ in Mandarin Chinese for TD-SV tasks. The remaining utterances are text-independent.In total, training data sets have nearly 1,139,671 utterances and the total duration approximately 950 hours with 374 speakers Conclusion í…ìŠ¤íŠ¸ ì˜ì¡´ì  ìŠ¤í”¼ì»¤ ê²€ì¦ì„ ìœ„í•œ ë”¥ ì„ë² ë”© í•™ìŠµ ì•„í‚¤í…ì²˜ë¥¼ ì œì•ˆ ì•„í‚¤í…ì²˜ëŠ” í”„ë ˆì„ ìˆ˜ì¤€ì—ì„œ ìŠ¤í”¼ì»¤ IDë¥¼ ìº¡ì²˜í•˜ê¸° ìœ„í•œ DenseBlockì˜ ìŠ¤íƒê³¼ ë°œí™” ìˆ˜ì¤€ì—ì„œ ìŠ¤í”¼ì»¤ ì„ë² ë”©ì„ í˜•ì„±í•˜ê¸° ìœ„í•œ ì–‘ë°©í–¥ ì£¼ì˜ í’€ë§ êµ¬ì¡°ë¡œ êµ¬ì„±ë¨ ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ì˜ ì¶œë ¥ì„ ì¡°ë°€í•˜ê²Œ ì—°ê²°í•¨ìœ¼ë¡œì¨, ì‹œê°„ ì£¼íŒŒìˆ˜ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ì˜ ë‹¤ì–‘í•œ ì¸¡ë©´ì„ ê°€ì§„ ë³´ë‹¤ ì˜ë¯¸ ìˆëŠ” í”„ë ˆì„ ë ˆë²¨ í‘œí˜„ì´ ìƒì„±ë¨ ì–‘ë°©í–¥ ì£¼ì˜ í’€ë§ ê³„ì¸µì€ BGRU ê³„ì¸µê³¼ ì£¼ì˜ í’€ë§ì˜ ì¡°í•©ìœ¼ë¡œ ì–‘ë°©í–¥ì—ì„œ ì‹œê°„ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ì¶”ê°€ë¡œ ìº¡ì²˜ FFSVC2020ì—ì„œ ì ìˆ˜ ì œì¶œì˜ ê²½ìš°, ìš°ë¦¬ê°€ ì œì•ˆí•œ ë°©ë²•ì€ í‰ê°€ ì„¸íŠ¸ì˜ ê³¼ì œ 1ê³¼ ê³¼ì œ 3ì— ëŒ€í•œ ìµœì†Œ DCFì™€ EERì—ì„œ ê°ê° 0.52ì™€ 4.72%, 0.14%ë¥¼ ë‹¬ì„±. ë˜í•œ ì´ ê²°ê³¼ëŠ” x-ë²¡í„° ë° ResNet ê¸°ì¤€ì„  ì‹œìŠ¤í…œì— ë¹„í•´ ìƒë‹¹í•œ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì—¬ì¤Œ Pdf: Deep Embedding Learning for Text-Dependent Speaker Verification","categories":[{"name":"Speaker Verification","slug":"Speaker-Verification","permalink":"https://jmj3047.github.io/categories/Speaker-Verification/"}],"tags":[{"name":"TD-SV","slug":"TD-SV","permalink":"https://jmj3047.github.io/tags/TD-SV/"},{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"}]},{"title":"Attention is all you need","slug":"Attention_is_all_you_need","date":"2022-05-09T15:00:00.000Z","updated":"2022-07-09T04:05:21.114Z","comments":true,"path":"2022/05/10/Attention_is_all_you_need/","link":"","permalink":"https://jmj3047.github.io/2022/05/10/Attention_is_all_you_need/","excerpt":"","text":"Journal&#x2F;Conference: NIPSYear(published year): 2017Author: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia PolosukhinSubject: NLP Attention is all you need Summary Attention ë§Œìœ¼ë¡œ ì‹œí€€ì…œ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ë³‘ë ¬í™”ì™€ ì—°ì‚° ì†ë„ í–¥ìƒì„ ê°€ëŠ¥í•˜ê²Œ í•œ ìƒˆë¡œìš´ ëª¨ë¸ ì œì‹œ Seq2Seq ê³¼ Attention ì„ ê²°í•©í•œ ëª¨ë¸(Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. CoRR, abs&#x2F;1409.0473, 2014.)ì—ì„œ í•œì¸µ ë” ë°œì „í•œ ëª¨ë¸ì…ë‹ˆë‹¤. Recurrent model(ì¬ê·€ êµ¬ì¡°)ì—†ì´ Self-attention ë§Œìœ¼ë¡œ êµ¬ì„±í•œ ì²«ë²ˆì§¸ ëª¨ë¸ì…ë‹ˆë‹¤. ì¬ê·€ êµ¬ì¡° ì œê±°ë¡œ ëª¨ë¸ì„ ë³‘ë ¬í™”(Parallelization)í•˜ì—¬ ìì—° ì–¸ì–´ ì²˜ë¦¬ í•™ìŠµ&#x2F;ì¶”ë¡  ì‹œê°„ì„ íšê¸°ì ìœ¼ë¡œ ë‹¨ì¶•ì‹œì¼°ìŠµë‹ˆë‹¤. Introductionê¸°ì¡´ì˜ ìì—°ì–¸ì–´ ì²˜ë¦¬ ëª¨ë¸ì€ RNN, LSTM, GLU ëª¨ë¸ë¡œ ëŒ€í‘œë˜ëŠ” ì¬ê·€ ëª¨ë¸(Recurrent Model)ì„ encoder-decoder êµ¬ì¡°ë¡œ ê²°í•©í•˜ëŠ” seq2seq ê³¼ ê°™ì€ ëª¨ë¸ì„ ì£¼ë¡œ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì¬ê·€ ëª¨ë¸ì€ ìˆœì°¨ ì²˜ë¦¬ë¡œ ì¸í•´ì„œ ë³‘ë ¬í™”ê°€ ì–´ë µë‹¤ëŠ” ì•½ì ì´ ìˆê³ , ë©”ëª¨ë¦¬ í¬ê¸°ê°€ ì œí•œë˜ì–´ ê¸´ ë¬¸ì¥ì„ ì²˜ë¦¬í•˜ê¸°ë„ ì–´ë µë‹¤ëŠ” ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ ë³´ì™„í•˜ê¸° ìœ„í•œ ì–´í…ì…˜(attention) ë§¤ì»¤ë‹ˆì¦˜ì´ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤. ì–´í…ì…˜ì€ ì¬ê·€ ê³¼ì •ì—ì„œ ì…ë ¥ì—ì„œ ì¶œë ¥ê¹Œì§€ì˜ ê±°ë¦¬ê°€ ê¸¸ì–´ì§€ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆì–´ ì…ì¶œë ¥ì˜ ì „ì—­ ì˜ì¡´ì„±ì„ ë†’ì—¬ì£¼ì—ˆì§€ë§Œ, ì¬ê·€ ëª¨ë¸ê³¼ ê²°í•©í•´ì„œë§Œ ì‚¬ìš©ë˜ì–´ ì™”ìŠµë‹ˆë‹¤. ì´ì— í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ”, ì–´í…ì…˜ë§Œìœ¼ë¡œ ëª¨ë¸ì„ êµ¬ì„±í•˜ì—¬ ì‰½ê²Œ ë³‘ë ¬í™” í•  ìˆ˜ ìˆê³  ìì—°ì–¸ì–´ì²˜ë¦¬ ê³¼ì œì˜ ì„±ëŠ¥ì„ ë†’ì¸ Transformer ëª¨ë¸ì„ ì œì‹œí•©ë‹ˆë‹¤. ì´ëŠ” ê¸°ì¡´ì˜ ì¬ê·€ ëª¨ë¸ê³¼ ë‹¤ë¥´ê²Œ 8ëŒ€ì˜ P100 GPUë¡œ 12ì‹œê°„ ì •ë„ë§Œ í•™ìŠµí–ˆìŒì—ë„ ë‹¹ì‹œ ê¸°ì¤€ SOTAë¥¼ ë‹¬ì„±í•˜ì˜€ìŠµë‹ˆë‹¤. WMT 2014 English-to-German Translation task -&gt; 28.4 BLEU WMT 2014 English-to-French Translation task -&gt; 41.0 BLEU Model ArchitectureTransformer ëª¨ë¸ì€ seq2seqìœ¼ë¡œ ëŒ€í‘œë˜ëŠ” ì¸ì½”ë”-ë””ì½”ë” êµ¬ì¡°ë¥¼ self-attention ìœ¼ë¡œ ìŒ“ì€ ë’¤, fully connected layer ë¡œ ì¶œë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤. Encoder and Decoder Stacks ì¸ì½”ë”(Encoder) $N$(&#x3D;6)ê°œì˜ ë™ì¼í•œ ë ˆì´ì–´ë¡œ êµ¬ì„± ê° ë ˆì´ì–´ëŠ” 2ê°œì˜ í•˜ìœ„ ë ˆì´ì–´ë¡œ êµ¬ì„± Multi-head self-attention position-wise fully connected feed-forward í•˜ìœ„ ë ˆì´ì–´ë¥¼ ê±°ì¹  ë•Œë§ˆë‹¤ Residual connection(Resnet) ê³¼ layer normalization ì„ ì‹¤í–‰ ê° ë ˆì´ì–´ ì¶œë ¥ì˜ í¬ê¸°ëŠ” $d_{model}$(&#x3D;512)ë¡œ ê³ ì • ë””ì½”ë”(Decoder) ì¸ì½”ë”ì™€ ê°™ì´ $N$(&#x3D;6)ê°œì˜ ë™ì¼í•œ ë ˆì´ì–´ë¡œ êµ¬ì„± ì¸ì½”ë”ì™€ ë™ì¼í•œ 2ê°œì˜ í•˜ìœ„ ë ˆì´ì–´ì— í•œê°€ì§€ë¥¼ ë” ì¶”ê°€í•˜ì—¬ 3ê°œì˜ í•˜ìœ„ ë ˆì´ì–´ë¡œ êµ¬ì„± Multi-head self-attention position-wise fully connected feed-forward ì¸ì½”ë”ì˜ ì¶œë ¥ìœ¼ë¡œ ì‹¤í–‰í•˜ëŠ” multi-head attention ìˆœì°¨ì ìœ¼ë¡œ ê²°ê³¼ë¥¼ ë§Œë“¤ì–´ ë‚¼ ìˆ˜ ìˆë„ë¡ Self-attention ë ˆì´ì–´ì— Masking ì„ì¶”ê°€ : $i$ ë²ˆì§¸ ì¶œë ¥ì„ ë§Œë“¤ ë•Œ, $i$ë²ˆì§¸ë³´ë‹¤ ì•ì„  ì¶œë ¥($i-1, i-2,\\dots$) ë§Œì„ ì°¸ê³ í•˜ë„ë¡ í•¨ Attentionattentionì€ queryì™€ key-value pairë“¤ì„ outputì— ë§µí•‘í•´ì£¼ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. ì¶œë ¥ì€ valuesë“¤ì˜ weighted sumìœ¼ë¡œ, valueì— í• ë‹¹ëœ weightëŠ” queryì™€ ëŒ€ì‘ë˜ëŠ” keyì˜ compatibility functionìœ¼ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤. Scaled Dot-Product Attention ì—¬ê¸°ì„œ ì‚¬ìš©í•˜ëŠ” attentionì€ Scaled Dot-Product Attention(SDPA)ë¼ ë¶€ë¥´ëŠ”ë°, inputì€ dimensionì´ $d_k$ì¸ queryì™€ key, dimensionì´ $d_v$ì¸ valueë“¤ë¡œ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤. ëª¨ë“  queryì™€ ëª¨ë“  keyë“¤ì— ëŒ€í•´ dot productë¡œ ê³„ì‚°ë˜ëŠ”ë° ê°ê°ì˜ ê²°ê³¼ì— dkë¡œ ë‚˜ëˆ„ì–´ì§„ë‹¤. ë‹¤ìŒ valueì˜ ê°€ì¤‘ì¹˜ë¥¼ ì–»ê¸° ìœ„í•´ softmax í•¨ìˆ˜ë¥¼ ì ìš©í•©ë‹ˆë‹¤. attention í•¨ìˆ˜ëŠ” additive attentionê³¼ dot-product attentionì´ ì‚¬ìš©ë©ë‹ˆë‹¤. additive attentionì€ single hidden layerì™€ í•¨ê»˜ feed-forward networkì— compatibility functionì„ ê³„ì‚°í•˜ëŠ”ë° ì‚¬ìš©ë©ë‹ˆë‹¤. dot-product attentionì´ ì¢€ ë” ë¹ ë¥´ê³  ì‹¤ì œë¡œ space-efficientí•©ë‹ˆë‹¤. ì™œëƒí•˜ë©´ optimized matrix multiplication codeë¥¼ ì‚¬ìš©í•´ì„œ êµ¬í˜„ë˜ì—ˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. $d_k$ê°€ ì‘ì€ ê²½ìš° additive attentionì´ dot product attentionë³´ë‹¤ ì„±ëŠ¥ì´ ì¢‹ìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ $d_k$ê°€ í° ê°’ì¸ ê²½ìš° softmax í•¨ìˆ˜ì—ì„œ ê¸°ìš¸ê¸° ë³€í™”ê°€ ê±°ì˜ ì—†ëŠ” ì˜ì—­ìœ¼ë¡œ ì´ë™í•˜ê¸° ë•Œë¬¸ì— dot productë¥¼ ì‚¬ìš©í•˜ë©´ì„œ $d_k$ìœ¼ë¡œ ë‚˜ëˆ„ì–´ scalingì„ ì ìš©í–ˆìŠµë‹ˆë‹¤. Multi-Head Attention $d_{model}$ dimensionì˜ query, key, value ë“¤ë¡œ í•˜ë‚˜ì˜ attentionì„ ìˆ˜í–‰í•˜ëŠ” ëŒ€ì‹ , query, key, valueë“¤ì— ê°ê° í•™ìŠµëœ linear projectionì„ $h$ë²ˆ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ ë” ì¢‹ìŠµë‹ˆë‹¤. ì¦‰, Q,K,Vì— ê°ê° ë‹¤ë¥¸ weightë¥¼ ê³±í•´ì£¼ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ë•Œ, projectionì´ë¼ í•˜ëŠ” ì´ìœ ëŠ” ê°ê°ì˜ ê°’ë“¤ì´ parameter matrixì™€ ê³±í•´ì¡Œì„ ë•Œ, $d_k,d_v,d_{model}$ì°¨ì›ìœ¼ë¡œ projectë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. query, key, valueë“¤ì„ ë³‘ë ¬ì ìœ¼ë¡œ attention functionì„ ê±°ì³ dimensionì´ $d_v$ì¸ output ê°’ìœ¼ë¡œ ë‚˜ì˜¤ê²Œ ë©ë‹ˆë‹¤. ì´í›„ ì—¬ëŸ¬ê°œì˜ headë¥¼ concatenateí•˜ê³  ë‹¤ì‹œ $W^O$ì™€ projectioní•˜ì—¬ dimensionì´ $d_{model}$ì¸ output ê°’ìœ¼ë¡œ ë‚˜ì˜¤ê²Œ ë©ë‹ˆë‹¤. Position-wise Feed-Forward Networksì¸ì½”ë”ì™€ ë””ì½”ë”ëŠ” fully connected feed-forward networkë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ë˜í•œ, ë‘ ë²ˆì˜ linear transformationsê³¼ activation function ReLUë¡œ êµ¬ì„±ë˜ì–´ì§‘ë‹ˆë‹¤. ê°ê°ì˜ positionë§ˆë‹¤ ê°™ì€ $W,b$ ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ layerê°€ ë‹¬ë¼ì§€ë©´ ë‹¤ë¥¸ parameterë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. Positional Encodingëª¨ë¸ì´ recurrenceì™€ convolutionì„ ì‚¬ìš©í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ë¬¸ì¥ì•ˆì— ìƒëŒ€ì ì¸ í˜¹ì€ ì ˆëŒ€ì ì¸ ìœ„ì¹˜ì˜ tokenë“¤ì— ëŒ€í•œ ì •ë³´ë¥¼ ì£¼ì…í•´ì•¼ë§Œ í–ˆìŠµë‹ˆë‹¤. ì´í›„ positional encodingì´ë¼ëŠ” ê²ƒì„ encoderì™€ decoder stack ë°‘ input embeddingì— ë”í•´ì¤¬ìŠµë‹ˆë‹¤. positional encodingì€ $d_{model}$ì¸ dimensionì„ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸ì— ë‘˜ì„ ë”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. $pos$ëŠ” position, $i$ëŠ” dimension $pos$ëŠ” sequenceì—ì„œ ë‹¨ì–´ì˜ ìœ„ì¹˜ì´ê³  í•´ë‹¹ ë‹¨ì–´ëŠ” $i$ì— 0ë¶€í„° $2d_{model}$ê¹Œì§€ ëŒ€ì…í•´ dimensionì´ $d_{model}$ì¸ positional encoding vectorë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Why Self-Attention Self-Attentionì„ ì‚¬ìš©í•˜ëŠ” ì²« ë²ˆì§¸ ì´ìœ ëŠ” layerë§ˆë‹¤ total computational complexityê°€ ì‘ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ë‘ ë²ˆì§¸ ì´ìœ ëŠ” computationì˜ ì–‘ì´ parallelizedí•˜ê¸°ë•Œë¬¸ì— sequential operationì˜ minimumìœ¼ë¡œ ì¸¡ì •ë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì„¸ ë²ˆì§¸ ì´ìœ ë¡œëŠ” ë„¤íŠ¸ì›Œí¬ì—ì„œì˜ long-range dependenciesì‚¬ì´ì˜ path lengthë•Œë¬¸ì…ë‹ˆë‹¤. long-range dependenciesë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì€ ë§ì€ ë¬¸ì¥ ë²ˆì—­ ë¶„ì•¼ì—ì„œì˜ key challengeê°€ ë©ë‹ˆë‹¤. input sequenceì™€ output sequenceì˜ ê¸¸ì´ê°€ ê¸¸ì–´ì§€ë©´ ë‘ positionê°„ì˜ ê±°ë¦¬ê°€ ë©€ì–´ì ¸ long-range dependenciesë¥¼ í•™ìŠµí•˜ëŠ”ë° ì–´ë ¤ì›Œì§‘ë‹ˆë‹¤. í…Œì´ë¸”ì„ ë³´ë©´ Recurrent layerì˜ ê²½ìš° Sequential operationì—ì„œ $O(n)$ì´ í•„ìš”í•˜ì§€ë§Œ Self-Attentionì˜ ê²½ìš° ìƒìˆ˜ì‹œê°„ì— ì‹¤í–‰ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ Self-Attentionì€ interpretable(ì„¤ëª…ê°€ëŠ¥í•œ) modelì¸ ê²ƒì´ ì´ì ì…ë‹ˆë‹¤. Traning OptimizerAdam optimizer ì— íŒŒë¼ë¯¸í„°ë¡œ $\\beta_1&#x3D;0.9, \\beta_2&#x3D;0.98, \\epsilon&#x3D;10^{-9}$ ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. í•™ìŠµë™ì•ˆ ì•„ë˜ì˜ ê³µì‹ì„ í†µí•´ learning rateë¥¼ ë³€í™”ì‹œì¼°ìŠµë‹ˆë‹¤. ì´ëŠ” warmup_stepì— ë”°ë¼ linearí•˜ê²Œ ì¦ê°€ì‹œí‚¤ê³  step numberì— ë”°ë¼ square rootí•œ ê°’ì„ í†µí•´ ì ì§„ì ìœ¼ë¡œ ì¤„ì—¬ê°”ìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  warmup_step &#x3D; 4000ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. Residual Dropoutê° sub-layerì—ì„œ inputì„ ë”í•˜ëŠ” ê²ƒê³¼ normalizationì„ í•˜ê¸°ì „ì— outputì— dropoutì„ ì„¤ì •í–ˆìŠµë‹ˆë‹¤. ë˜í•œ encoderì™€ decoder stacksì— embeddingì˜ í•©ê³„ì™€ positional encodingì—ë„ dropoutì„ ì„¤ì •í–ˆìŠµë‹ˆë‹¤. dropout rate $P_{drop}&#x3D;0.1$ ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. Label Smoothingí•™ìŠµí•˜ëŠ” ë™ì•ˆ label smoothing value $\\epsilon_{ls}&#x3D;0.1$ì„ ì ìš©í–ˆìŠµë‹ˆë‹¤. Result Machine Translation ì˜ì–´â†’ë…ì¼ì–´ ë²ˆì—­ì—ì„œëŠ” ê¸°ì¡´ ëª¨ë¸ë“¤ë³´ë‹¤ ë†’ì€ ì ìˆ˜ê°€ ë‚˜ì™”ê³  ì˜ì–´â†’í”„ë‘ìŠ¤ì–´ ë²ˆì—­ì—ì„œëŠ” single ëª¨ë¸ë³´ë‹¤ ì¢‹ê³  ensemble ëª¨ë¸ë“¤ê³¼ ë¹„ìŠ·í•œ ì„±ëŠ¥ì„ ë‚´ì£¼ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ ì¤‘ìš”í•œ ì ì€ Training Costì¸ë° ê¸°ì¡´ ëª¨ë¸ë“¤ë³´ë‹¤ í›¨ì”¬ ì ì€ Costê°€ ë“¤ì–´ê°€ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Model Variations (A)ë¥¼ ë³´ë©´ single-head attentionì€ head&#x3D;16ì¼ë•Œë³´ë‹¤ 0.9 BLEU ë‚®ê³  head&#x3D;32ë¡œ ëŠ˜ë ¸ì„ ë•Œë„ head&#x3D;16ì¼ë•Œë³´ë‹¤ BLEUê°€ ë‚®ìŠµë‹ˆë‹¤. (B)ë¥¼ ë³´ë©´ dkë¥¼ ë‚®ì¶”ëŠ” ê²ƒì´ model qualityë¥¼ ë‚®ì¶”ê²Œ í•©ë‹ˆë‹¤. (C), (D)ë¥¼ ë³´ë©´ ë” í° ëª¨ë¸ì¼ìˆ˜ë¡ ì¢‹ê³ , dropoutì´ overfittingì„ í”¼í•˜ëŠ”ë° ë„ì›€ì´ ë˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. (E)ë¥¼ ë³´ë©´ sinusoidal positionëŒ€ì‹  learned positional embeddingsë¥¼ ë„£ì—ˆì„ ë•Œì˜ ê²°ê³¼ê°€ base modelê³¼ ë™ì¼í•œ ê²°ê³¼ì¸ ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Conclusion ì¬ê·€ êµ¬ì¡° ì—†ì´ Multi-headed Self-attention ìœ¼ë¡œ ì¸ì½”ë”-ë””ì½”ë”ë¥¼ ëŒ€ì²´í•œ Transformer ëª¨ë¸ì„ ì œì‹œí•˜ì˜€ìŠµë‹ˆë‹¤. ì¬ê·€êµ¬ì¡°ê°€ ì—†ìœ¼ë¯€ë¡œ recurrent ë˜ëŠ” convolutional ë ˆì´ì–´ ê¸°ë°˜ ëª¨ë¸ë³´ë‹¤ ë¹ ë¥´ê²Œ í•™ìŠµì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•´ë‹¹ ëª¨ë¸ì€ WMT 2014 ì˜ì–´â†’ë…ì–´, ì˜ì–´â†’ë¶ˆì–´ ë²ˆì—­ ë¶„ì•¼ì—ì„œ ê¸°ì¡´ ëª¨ë“  ì•™ìƒë¸” ëª¨ë¸ë“¤ì„ ëŠ¥ê°€í•˜ëŠ” SOTAë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. Link: Attention is all you need","categories":[{"name":"NLP","slug":"NLP","permalink":"https://jmj3047.github.io/categories/NLP/"}],"tags":[{"name":"Transformer","slug":"Transformer","permalink":"https://jmj3047.github.io/tags/Transformer/"},{"name":"Attention","slug":"Attention","permalink":"https://jmj3047.github.io/tags/Attention/"},{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"}]},{"title":"Light Gradient Boosting Machine","slug":"LGBM","date":"2022-05-09T15:00:00.000Z","updated":"2022-07-09T05:59:36.015Z","comments":true,"path":"2022/05/10/LGBM/","link":"","permalink":"https://jmj3047.github.io/2022/05/10/LGBM/","excerpt":"","text":"1. DefinitionEnsembleâ†’ ì—¬ëŸ¬ ì˜ˆì¸¡ê¸°ë¥¼ ìˆ˜ì§‘í•´ì„œ ë‹¨ì¼ ì˜ˆì¸¡ê¸° ë³´ë‹¤ ë” ì¢‹ì€ ì˜ˆì¸¡ê¸°ë¥¼ ë§Œë“œëŠ” ê²ƒ. ì¼ë°˜ì ìœ¼ë¡œ ì•™ìƒë¸” ê¸°ë²•ì„ ì‚¬ìš©í•˜ë©´ , ì˜ˆì¸¡ê¸° í•˜ë‚˜ë¡œ í›ˆë ¨í•˜ì˜€ì„ë•Œ ë³´ë‹¤ , í¸í–¥ì€ ë¹„ìŠ·í•˜ì§€ë§Œ ë¶„ì‚°ì´ ì¤„ì–´ë“ ë‹¤ê³  ì•Œë ¤ì ¸ ìˆë‹¤. ë°°ê¹…(bagging) ì›ë°ì´í„° ì§‘í•©ìœ¼ë¡œë¶€í„° í¬ê¸°ê°€ ê°™ì€ í‘œë³¸ì„ ì—¬ëŸ¬ ë²ˆ ë‹¨ìˆœì„ì˜ ë³µì›ì¶”ì¶œí•˜ì—¬ ê° í‘œë³¸(ë¶“ìŠ¤íŠ¸ë© í‘œë³¸) ì— ëŒ€í•´ ë¶„ë¥˜ê¸°ë¥¼ ìƒì„±í•œ í›„ ê·¸ ê²°ê³¼ë¥¼ ì•™ìƒë¸”í•˜ëŠ” ë°©ë²• ë°˜ë³µì¶”ì¶œ ë°©ë²•ì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ê°™ì€ ë°ì´í„°ê°€ í•œ í‘œë³¸ì— ì—¬ëŸ¬ ë²ˆ ì¶”ì¶œë  ìˆ˜ë„ ìˆê³ , ì–´ë–¤ ë°ì´í„°ëŠ” ì¶”ì¶œë˜ì§€ ì•Šì„ìˆ˜ë„ ìˆë‹¤. ë¶€ìŠ¤íŒ…(boosting) ë°°ê¹…ì˜ ê³¼ì •ê³¼ ìœ ì‚¬í•˜ë‚˜ ë¶“ìŠ¤íŠ¸ë© í‘œë³¸ì„ êµ¬ì„±í•˜ëŠ” sampling ê³¼ì •ì—ì„œ ê° ìë£Œì— ë™ì¼í•œ í™•ìœ¨ì„ ë¶€ì—¬í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ë¶„ë¥˜ê°€ ì˜ëª»ëœ ë°ì´í„°ì—- ë” í° ê°€ì¤‘ì„ ì£¼ì–´ í‘œë³¸ì„ ì¶”ì¶œí•œë‹¤. Bagging ê³¼ Boosting ì˜ ì°¨ì´ Bagging ì€ ë…ë¦½ëœ ì˜ˆì¸¡ê¸°ë¥¼ í†µí•´ ë” ë‚˜ì€ ì˜ˆì¸¡ê¸°ë¥¼ ì–»ëŠ”ë‹¤ Boosting ì€ ì•ì˜ ì˜ˆì¸¡ê¸°ë¥¼ ë³´ì™„í•´ ë‚˜ê°€ë©´ì„œ ë” ë‚˜ì€ ì˜ˆì¸¡ê¸°ë¥¼ ì–»ëŠ”ë‹¤ ëœë¤í¬ë ˆìŠ¤íŠ¸(random forest) ë°°ê¹…ì— ëœë¤ ê³¼ì •ì„ ì¶”ê°€í•œ ë°©ë²•ì´ë‹¤. ê° ë…¸ë“œë§ˆë‹¤ ëª¨ë“  ì˜ˆì¸¡ë³€ìˆ˜ ì•ˆì—ì„œ ìµœì ì˜ ë¶„í• ì„ ì„ íƒí•˜ëŠ” ë°©ë²• ëŒ€ì‹  ì˜ˆì¸¡ë³€ìˆ˜ë“¤ì„ ì„ì˜ë¡œ ì¶”ì¶œí•˜ê³ , ì¶”ì¶œëœ ë³€ìˆ˜ ë‚´ì—ì„œ ìµœì ì˜ ë¶„í• ì„ ë§Œë“¤ì–´ ë‚˜ê°€ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•œë‹¤. ë¶€íŠ¸ìŠ¤íŠ¸ë©(Bootstrap) ë¶€íŠ¸ìŠ¤íŠ¸ë©ì€ í‰ê°€ë¥¼ ë°˜ë³µí•œë‹¤ëŠ” ì¸¡ë©´ì—ì„œ êµì°¨ê²€ì¦ê³¼ ìœ ì‚¬í•˜ë‚˜, í›ˆë ¨ìš© ìë£Œë¥¼ ë°˜ë³µ ì¬ì„ ì •í•œë‹¤ëŠ” ì ì—ì„œ ì°¨ì´ê°€ ìˆë‹¤. ì¦‰ ë¶€íŠ¸ìŠ¤íŠ¸ë©ì€ ê´€ì¸¡ì¹˜ë¥¼ í•œë²ˆ ì´ìƒ í›ˆë ¨ìš© ìë£Œë¡œ ì‚¬ìš©í•˜ëŠ” ë³µì›ì¶”ì¶œë²•ì— ê¸°ë°˜í•œë‹¤. ë¶€íŠ¸ìŠ¤íŠ¸ë©ì€ ì „ì²´ ë°ì´í„°ì˜ ì–‘ì´ í¬ì§€ì•Šì€ ê²½ìš°ì˜ ëª¨í˜•í‰ê°€ì— ê°€ì¥ ì í•©í•˜ë‹¤. 2. GBM Gradient Boosting Machine(GBM)ì€ Ensemble Learningì˜ ì¼í™˜ Gradient Boosting&#x3D;Gradient Descent+Boosting Gradient Descent ì²«ë²ˆì§¸ ë°ì´í„°ì—ì„œ ì˜ ëª» ë§ì¶˜ ë°ì´í„°ë“¤ì— ê°€ì¤‘ì¹˜ë¥¼ ì£¼ì–´, ë‘ë²ˆì§¸ ëª¨ë¸ ì—ì„œëŠ” ë” ë§ì€ì–‘ì„ ë§Œë“¤ì–´ ì¤€ë‹¤. ë˜, ë‘ë²ˆì§¸ ëª¨ë¸ì—ì„œ ì˜ëª» ë§¤ì¹­í•œ ë°ì´í„°ë“¤ ì—ê²Œ ê°€ì¤‘ì¹˜ë¥¼ ì£¼ì–´ì„œ, ì„¸ë²ˆì§¸ ëª¨ë¸ì„ ë§Œë“¤ì–´ì£¼ëŠ” ê·¸ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œ ëª¨ë¸ì„ ë°˜ë³µí•œë‹¤. Fit an addictive model(ensemble) in a foward stage-wise manner bagging ì²˜ëŸ¼ í•œë²ˆì— ë”± í•™ìŠµì„ ì‹œí‚¤ëŠ”ê²Œ ì•„ë‹ˆê³  ìœ„ì— ì–¸ê¸‰í•œ ê²ƒ ì²˜ëŸ¼ í•˜ë‚˜ì”© í•˜ë‚˜ì”© ë”í•´ê°€ë©´ì„œ ëª¨ë¸ì„ í•™ìŠµ ì‹œì¼œë‚˜ê°€ëŠ” ëª¨ë¸ Adaptive boosting ëª¨ë¸ì„ ê±°ë“­í• ìˆ˜ë¡, weak leanerê°€ ë§Œë“¤ì–´ ì§€ë©´ì„œ ì´ì „ì— ê°€ì¡Œë˜ ì˜¤ë¥˜ì— ëŒ€í•´ í•´ê²°í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì„ ë§Œë“¦ ì´ëŸ¬í•œ weak learner ë¥¼ ë§Œë“¤ê¸° ìœ„í•´ , ì•ì„œ ë³´ì˜€ë˜ ëª¨ë¸ì˜ shortcoming ì„ ë” ë§ì´ ìƒ˜í”Œë§ í•˜ë¼ëŠ” ëœ» Gradient Boosting ê·¸ë¼ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…ì€, Regression,Classification,Ranking ì„ ë‹¤ í•  ìˆ˜ ìˆë‹¤. ì´ ì…‹ì˜ ë‹¤ë¥´ê¸°ëŠ” loss function ì—ì„œ ì°¨ì´ê°€ ë‚  ë¿, concept ì€ ë™ì¼ ğŸ’¡ Regression ìœ¼ë¡œ ì„¤ëª…í•˜ê¸°ê°€ ê°€ì¥ ì§ê´€ì ì´ê¸° ë•Œë¬¸ì—, Regression ëª¨ë¸ë¡œì¨ concept ì„ ì„¤ëª…í•˜ê² ë‹¤. ì–˜ëŠ” adaptive boosting ê³¼ëŠ” ë‹¬ë¦¬, ìƒ˜í”Œë§ì„ ë”°ë¡œ ì‹œí–‰í•˜ì§€ëŠ” ì•ŠëŠ”ë‹¤. ì”ì°¨ë¥¼ ëª©í‘¯ê°’ (y) ìœ¼ë¡œ ë†“ê³  ê³„ì†í•´ì„œ ë°˜ë³µí•˜ë©´ì„œ ì”ì°¨ì— ëŒ€í•œ ì‹ì„ ë§Œë“¤ì–´ ë‚¸ë‹¤. ì”ì°¨ë¥¼ ëª©í‘œê°’ìœ¼ë¡œ ì¡ì•„ë‘ë©´, ì•ì„  ëª¨ë¸ì´ ë§ì¶”ì§€ ëª»í•œ ë§Œí¼ë§Œ ë§ì¶”ë ¤ê³  ë…¸ë ¥ì„ í•˜ê¸°ë•Œë¬¸ì—, ì•ì„  ëª¨ë¸ì˜ ê²°ê³¼ë¬¼ê³¼ ë’· ëª¨ë¸ì˜ ê²°ê³¼ë¬¼ì„ ë”í•˜ë©´ ì •ë‹µì´ ë‚˜ì˜¨ë‹¤. ê²½ì‚¬ë„ë¥¼ í†µí•´ì„œ Weak learner ë¥¼ boosting ì‹œí‚´ ì†ì‹¤í•¨ìˆ˜ì˜ gradient(ê²½ì‚¬ë„) ê°€ 0 ì— ê°€ê¹Œìš¸ë•Œ ê¹Œì§€ ë¯¸ë¶„ì„ í•´ì¤€ë‹¤. Gradient ê°€ 0ì´ ì•„ë‹ˆë¼ë©´ weight ë¥¼ gradinet ì˜ ë°˜ëŒ€ë°©í–¥ìœ¼ë¡œ ì›€ì§ì´ë˜ ì–¼ë§ˆë§Œí¼ ì›€ì§ì´ëƒì— ë”°ë¼ì„œ ë‹¬ë¼ì§€ë‹ˆ ì¡°ê¸ˆì”© ì›€ì§ì¸ë‹¤. ì²˜ìŒ, decision tree ë¡œ split point ë¥¼ ì¡ì•„, regression í•´ì¤€ ë¶€ë¶„ì˜ ì”ì°¨ë¥¼ ë³´ë©´ ë†’ì€ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. GBMì—ì„œëŠ” ì´ ì”ì°¨ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë˜ Split point ë¥¼ ì¡ì•„ì£¼ë©´ì„œ(ì´ëŸ¬í•œ ê³¼ì •ì—ì„œ ì†ì‹¤í•¨ìˆ˜ê°€ ë“¤ì–´ê°€ë©° ì†ì‹¤í•¨ìˆ˜ì˜ gradient ë¥¼ ì¤„ì—¬ë‚˜ê°€ëŠ” ê³¼ì •ì—ì„œ gradient descent ê°œë…ì´ ë“¤ì–´ê°€ëŠ” ê²ƒ) ì ì  ì”ì°¨ë¥¼ ì¤„ì—¬ ë‚˜ê°€ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ğŸ’¡ ì²˜ìŒì—ëŠ” íšŒê·€ì‹ì´ ì•ˆì¢‹ê²Œ ë‚˜ì˜¤ëŠ”ë° iteration ì´ ë°˜ë³µ ë ìˆ˜ë¡ íšŒê·€ì‹ì´ ì¢‹ì•„ì§€ëŠ”ê²ƒìœ¼ë¡œ ë³¼ ìˆ˜ ìˆë‹¤. Overfitting problem in GBM GBM ì˜ ê°€ì¥í° ë¬¸ì œì ì€ ì˜¤ì°¨ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ì„ í˜•ì„± í•˜ê¸°ë•Œë¬¸ì—(ì• ì´ˆì— ëª¨ë¸ ìì²´ê°€ ë°˜ë³µì„ ê±°ë“­ í• ìˆ˜ë¡, ì „ ëª¨ë¸ì˜ ì˜¤ì°¨ë¥¼ ì¤„ì—¬ë‚˜ê°€ëŠ” ëª¨ë¸ì´ê¸° ë•Œë¬¸ì— ì˜¤ë²„í”¼íŒ… ë¬¸ì œëŠ” í•„ì—°ì ) ìš°ë¦¬ê°€ ì–´ì°Œ í• ìˆ˜ ì—†ëŠ” ì˜¤ì°¨ê¹Œì§€ë„ ëª¨ë¸ì— í•™ìŠµì‹œí‚¤ì–´ì„œ ì˜¤ë²„í”¼íŒ… ë¬¸ì œë¥¼ ë¶ˆëŸ¬ ì¼ìœ¼í‚¨ë‹¤ ê³¼ì í•© í•´ê²°ë²• Subsamplingâ†’ê°ê°ì˜ ëª¨ë¸ì„ ë§Œë“¤ë•Œ ìƒ˜í”Œë§ì„ ëœë¤ìœ¼ë¡œ 80% ë§Œí•´ì„œ ëª¨ë¸ì„ ë§Œë“¤ì–´ì¤€ë‹¤ Shrinkage- original ì•Œê³ ë¦¬ì¦˜ë“¤ì€ ì „ì— ë§Œë“¤ì–´ì§„ ëª¨ë¸ë“¤ê³¼ ë’¤ë¡œ ê°ˆìˆ˜ë¡ ë§Œë“¤ì–´ì§€ëŠ” ëª¨ë¸ë“¤ì—, ì˜í–¥ë ¥ì´ ë™ë“±í–ˆëŠ”ë° , shrinkage ë¥¼ ì“°ë©´ , ë’¤ì— ë§Œë“¤ì–´ì§€ëŠ” ëª¨ë¸ë“¤ì— ëŒ€í•´ì„œ , ê°€ì¤‘ì¹˜ë¥¼ ì ê²Œ ë‘ì–´ ë§Œë“¤ì–´ì¤€ë‹¤. Early Stopping- validation error ê°€ ì¦ê°€ í•  ê²ƒ ê°™ìœ¼ë©´ ë¯¸ë¦¬ ì¤‘ì§€ë¥¼ ì‹œí‚¤ëŠ”ê²ƒ Information Gain:Split pointë¥¼ í†µí•´ì„œ ì–¼ë§ˆë‚˜ í˜¼ì¡ë„,ë¶ˆìˆœë„ê°€ ë‚®ì•„ì§€ëŠ”ê°€. Information Gain ì„ í†µí•´ ê·¸ ë³€ìˆ˜ì˜ ì˜í–¥ë„ë¥¼ ì²´í¬ í•  ìˆ˜ ìˆë‹¤. 3. LightGBMGOSS ëª¨ë“  í”¼ì³ë“¤ì„ ê²€ì‚¬í•˜ë©´ ì‹œê°„ì´ ë§ì´ ê±¸ë¦¬ê¸° ë•Œë¬¸ì— ì´ë¥¼ ë§‰ê¸°ìœ„í•´ì„œ, Gradient-based One-side Sampling (GOSS) ë¥¼ ì‚¬ìš©â†’ Large gradient ëŠ” keep í•˜ê³  small gradient ëŠ” ë“œë í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ, 1000ê°œ ë°ì´í„°ë¥¼ ëª¨ë‘ íƒìƒ‰í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ gradient ê°€ í° ê²ƒ ìœ„ì£¼ë¡œ íƒìƒ‰í•˜ëŠ” ë°©ì‹ â†’ íƒìƒ‰íšŸìˆ˜ë¥¼ ì¤„ì´ëŠ” ê²ƒ EFB (Exclusive Feature Bundleing) ëª¨ë“ í”¼ì³ë¥¼ íƒìƒ‰í•  í•„ìš”ë¥¼ ì—†ì• ëŠ” ê²ƒ Bundle ì„ ì°¾ëŠ” ë°©ë²• Graph coloring problem ìœ¼ë¡œ í•´ê²°ê°€ëŠ¥ ê°ê°ì˜ ë…¸ë“œëŠ” í”¼ì³ì´ê³ , edge ëŠ” í”¼ì³ë“¤ê°„ì˜ conflict â†’ conflict ê°€ ë§ì€ ì• ë“¤ì€ ì¤‘ë³µì´ ë§ì´ ë“¤ì–´ê°€ì„œ bundling ì´ ë˜ë©´ ì•ˆë¨ conflict ê°€ ì—†ëŠ” ì• ë“¤ ë¼ë¦¬ëŠ” bundling ì„ í•´ë„ ë¨ Greed bundling ê³„ì‚°ë²• edge ì˜ ê°•ë„: ë‘ ë³€ìˆ˜ì˜ conflict ê°•ë„ edge: ë™ì‹œì— 0ì´ì•„ë‹Œ ê°ì²´ì˜ ìˆ˜. Degree ì‹œì‘ì ì„ degreeì˜ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë¦¬ í•´ì¤€ ë‹¤ìŒ, degreeê°€ ë†’ì€ê²ƒ ë¶€í„° ì‹œì‘í•œë‹¤. cutoff ëŠ” hyperparameter ì¸ë°, cut-offê°€ 0.2ë¼ëŠ” ë§ì€, N&#x3D;10 ì´ê¸° ë•Œë¬¸ì— 2íšŒ ì´ìƒ Nonzero value ê°€ ê²¹ì¹˜ê²Œ ë˜ë©´, bundleingì´ ì•ˆë˜ëŠ” ê²ƒ. cut-off ê¸°ì¤€ì— ë§ì§€ ì•Šê¸° ë•Œë¬¸ì— x5ëŠ” ê³ ë¦½ì´ ëœë‹¤. feature merge ë¥¼ ì‰½ê²Œ í•´ì£¼ê¸° ìœ„í•´ featureì˜ ìœ„ì¹˜ë¥¼ ì‚´ì§ ì¡°ì • í•˜ì—¬ì¤€ë‹¤. feature ë¥¼ merge í•˜ëŠ”ë°©ë²•. Add. offset add offsetâ†’bundling ì„ í•˜ê¸°ìœ„í•œ ëŒ€ìƒì´ ë˜ëŠ” ë³€ìˆ˜ì—ë‹¤ê°€ ê¸°ì¤€ì´ ë˜ëŠ” ë³€ìˆ˜ê°€ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ìµœëŒ€ ê°’ì„ ë”í•´ì¤€ë‹¤. conflict ê°€ ì¼ì–´ë‚œ ë¶€ë¶„ì€ ê·¸ëŒ€ë¡œ ê¸°ì¤€ ë³€ìˆ˜ê°€ ê°€ì§€ëŠ” ê°’ì„ ë”í•´ì¤€ë‹¤. Reference: ê³ ë ¤ëŒ€í•™êµ ì‚°ì—…ê²½ì˜ê³µí•™ë¶€ DSBA ì—°êµ¬ì‹¤","categories":[{"name":"Basic ML","slug":"Basic-ML","permalink":"https://jmj3047.github.io/categories/Basic-ML/"}],"tags":[{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"}]},{"title":"Setting Git & Virtualenv","slug":"Setting_Git_&_Virtualenv","date":"2022-05-05T15:00:00.000Z","updated":"2022-05-06T06:09:11.917Z","comments":true,"path":"2022/05/06/Setting_Git_&_Virtualenv/","link":"","permalink":"https://jmj3047.github.io/2022/05/06/Setting_Git_&_Virtualenv/","excerpt":"","text":"Put Local folder into git repo Make folder â€˜exampleâ€™ and git repo â€˜example 123456789101112131415161718192021222324#in local cmd example foldergit init #add remote repogit remote add origin &#x27;repo https&#x27;#bring files in repo to localgit pull origin master #bring local files to git repogit add .git commit -m &#x27;updated&#x27;git push orgin master #check remotegit remote -v#check current statusgit status#error: failed to push some refs to &#x27;https://github.com/jmj3047/.git&#x27;#force to push git push -f origin master Setting virtual env in window&#x2F;linux1234567#****use virtual env no matter what****&gt;python -m venv env_name&gt;source env_name/Scripts/activate #window&gt;source env_name/bin/activate #linux#put all the version of modules in requirements.txt&gt;pip install -r requirements.txt","categories":[{"name":"Setting","slug":"Setting","permalink":"https://jmj3047.github.io/categories/Setting/"}],"tags":[{"name":"Git","slug":"Git","permalink":"https://jmj3047.github.io/tags/Git/"},{"name":"Virtualenv","slug":"Virtualenv","permalink":"https://jmj3047.github.io/tags/Virtualenv/"}]},{"title":"What is Transformer","slug":"What_is_Transformer","date":"2022-05-05T15:00:00.000Z","updated":"2022-07-09T04:06:34.113Z","comments":true,"path":"2022/05/06/What_is_Transformer/","link":"","permalink":"https://jmj3047.github.io/2022/05/06/What_is_Transformer/","excerpt":"","text":"Transformerë€?íŠ¸ëœìŠ¤í¬ë¨¸(Transformer)ëŠ” êµ¬ê¸€ì—ì„œ ë°œí‘œí•œ ë…¼ë¬¸ â€œAttention is all you needâ€ì— ë‚˜ì˜¤ëŠ” ëª¨ë¸ì´ë‹¤. ì•„ë˜ ê¸€ì€ ì´ ë…¼ë¬¸ abstractì˜ ì¼ë¶€ë¶„ì´ë‹¤. The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU â€¦ ì—¬ê¸°ì„œë„ ì•Œ ìˆ˜ ìˆë“¯ì´, íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ì–´í…ì…˜(Attention) mechanismì„ ê¸°ë°˜ìœ¼ë¡œ ì—¬ëŸ¬ê°œì˜ ì¸ì½”ë”ì™€ ë””ì½”ë”ë¥¼ ì—°ê²°í•œ êµ¬ì¡°ë¥¼ ê°–ê³  ìˆë‹¤. ë˜í•œ CNN, RNN, LSTM ë“±ì˜ êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì— í•™ìŠµ ì‹œê°„ì´ í›¨ì”¬ ê°ì†Œëœ ì„±ëŠ¥ì„ ë‚´ì—ˆë‹¤ê³  í•œë‹¤. ê·¸ë ‡ë‹¤ë©´ ê·¸ êµ¬ì¡°ê°€ ë¬´ì—‡ì¸ì§€ ë” ì•Œì•„ë³´ë„ë¡ í•˜ì. (1) íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì…ë ¥ë¨¼ì € íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì…ë ¥ë¶€í„° ì•Œì•„ë³´ì, ë‹¨ì–´ ë²¡í„° ë°ì´í„°ê°€ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°€ì§€ê²Œ ë˜ëŠ”ë° ì´ ë•Œ ê° ë‹¨ì–´ì˜ ìœ„ì¹˜ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì–´ì•¼ í•œë‹¤. ì™œëƒí•˜ë©´ íŠ¸ëœìŠ¤í¬ë¨¸ì— ë‹¨ì–´ê°€ ì…ë ¥ë  ë•Œ ìˆœì°¨ì ìœ¼ë¡œ ë°›ì•„ì§€ì§€ ì•Šê¸° ë•Œë¬¸ì´ë‹¤. ë”°ë¼ì„œ ìˆœì„œ ì •ë³´ë¥¼ ë”í•´ì£¼ì–´ì•¼ í•˜ê¸° ë•Œë¬¸ì— ìœ„ì¹˜ ì •ë³´ë¥¼ ê° ë‹¨ì–´ ë²¡í„°ë§ˆë‹¤ ë”í•´ì£¼ì–´ì•¼ í•˜ëŠ”ë°, ì´ ê³¼ì •ì„ í¬ì§€ì…”ë„ ì¸ì½”ë”©(positional encoding)ì´ë¼ê³  í•œë‹¤. í¬ì§€ì…”ë„ ì¸ì½”ë”© ê°’ì„ ë”í•´ì£¼ê¸° ìœ„í•´ì„œëŠ” ì‚¬ì¸í•¨ìˆ˜ì™€ ì½”ì‚¬ì¸í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œ ì•„ë˜ ë‘ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤. ìœ„ ì‹ì—ì„œ posëŠ” ì…ë ¥ëœ ë°ì´í„°ì˜ ì„ë² ë”© ë²¡í„°(ëª‡ë²ˆì§¸ ë‹¨ì–´ì¸ì§€)ë¥¼, iëŠ” ì„ë² ë”© ë²¡í„°ë‚´ì˜ ì°¨ì›ì˜ ì¸ë±ìŠ¤(0~512)ë¥¼ ëœ»í•œë‹¤. ì„ë² ë”© ë²¡í„°ë‚´ì˜ ì°¨ì›ì´ë€ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì˜ ì¸ì½”ë”ì™€ ë””ì½”ë”ì—ì„œ ì •í•´ì§„ ì…ë ¥ê³¼ ì¶œë ¥ì˜ í¬ê¸°ë¥¼ ë§í•œë‹¤. ë…¼ë¬¸ìƒì—ì„œ ì´ ì°¨ì›ì„ 512ë¡œ ì„¤ì •í–ˆìœ¼ë©´ ì´ ì°¨ì›ì€ ì¸ì½”ë”ì˜ ê°’ì„ ë””ì½”ë”ë¡œ ë³´ë‚¼ë•Œ ê°’ì„ ìœ ì§€í•˜ë„ë¡ í•œë‹¤. ë‹¤ì‹œ ëŒì•„ì™€ì„œ, ìœ„ í•¨ìˆ˜ì—ì„œ ì°¨ì›ì´ 2i(ì§ìˆ˜)ì¸ì§€ 2i+1(í™€ìˆ˜)ì¸ì§€ì— ë”°ë¼ì„œ ì‚¬ìš©í•˜ëŠ” í•¨ìˆ˜ê°€ ë‹¤ë¥´ë‹¤. ì§ìˆ˜ ì°¨ì›ì˜ ê²½ìš° ì‚¬ì¸í•¨ìˆ˜, í™€ìˆ˜ì°¨ì›ì˜ ê²½ìš° ì½”ì‚¬ì¸ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ê²Œëœë‹¤. (2)ì¸ì½”ë”(Encoder)ì˜ êµ¬ì¡° ìœ„ ì´ë¯¸ì§€ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ë…¼ë¬¸ì— í•¨ê»˜ ì‹¤ë ¤ ìˆëŠ” ì´ë¯¸ì§€ë¡œ, íŠ¸ëœìŠ¤í¬ë¨¸ì˜ êµ¬ì¡°ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.ì—¬ê¸°ì„œ ì™¼ìª½ ë¶€ë¶„ì´ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì¸ì½”ë” ë¶€ë¶„ì¸ë°, ì¸ì½”ë”ì˜ êµ¬ì¡°ëŠ” ì–´ë–»ê²Œ ì´ë£¨ì–´ì¡Œì„ê¹Œ? ë¨¼ì € ì´ë¯¸ì§€ ì™¼ìª½ ì•„ë˜ë¥¼ ë³´ì. Input dataê°€ ë“¤ì–´ê°€ê²Œ ë˜ë©´ Input Embeddingì„ ê±°ì¹˜ê²Œ ë˜ëŠ”ë°, ì—¬ê¸°ì„œëŠ” ë¬¸ìì—´ì¸ ë‹¨ì–´ ë°ì´í„°ë¥¼ ë²¡í„°í˜•íƒœë¡œ ë³€í™˜í•´ ì¤€ë‹¤(ë‹¨ì–´ ê¸¸ì´ X ë²¡í„°ì°¨ì›ì˜ í–‰ë ¬). ê·¸ë¦¬ê³  ë‚˜ì„œ ìœ„ì—ì„œ ì„¤ëª…í•œ í¬ì§€ì…”ë„ ì¸ì½”ë”©ì„ ìˆ˜í–‰í•´ì£¼ê²Œ ëœë‹¤. ê·¸ëŸ¬ê³  ë‚˜ì„œ ë°•ìŠ¤ë¡œ í‘œí˜„ëœ ì¸ì½”ë”ì— ë“¤ì–´ê°€ê²Œ ëœë‹¤. ì¸ì½”ë” ì•ˆì—ì„œëŠ” í¬ê²Œ Multi-Head Attentionê³¼ Feed Forwardê³¼ì •ì´ ìˆ˜í–‰ë˜ëŠ”ë°, Multi-Head Attentionì€ ì…€í”„ ì–´í…ì…˜ì´ ë³‘ë ¬ì ìœ¼ë¡œ ì‚¬ìš©ëœ ê²ƒì„ ë§í•˜ë©°, Feed Forwardë€ í”¼ë“œí¬ì›Œë“œ ì‹ ê²½ë§ êµ¬ì¡°ë¥¼ ì˜ë¯¸í•œë‹¤. í•œí¸, ìœ„ì—ì„œ ì ê¹ ì–¸ê¸‰í–ˆì§€ë§Œ íŠ¸ëœìŠ¤í¬ë¨¸ì—ì„œëŠ” ì—¬ëŸ¬ ê°œì˜ ì¸ì½”ë”ì™€ ë””ì½”ë”ë¥¼ ìŒ“ì€ êµ¬ì¡°ë¥¼ ê°–ê³  ìˆë‹¤. ì¦‰, ì¸ì½”ë”ê°€ 1ê°œê°€ ì•„ë‹ˆë¼ëŠ” ëœ»ì¸ë°, ë…¼ë¬¸ì—ì„œëŠ” 6ê°œì˜ ì¸ì½”ë” ì¸µì„ ì‚¬ìš©í–ˆë‹¤ê³  í•˜ë‹ˆ, 6ê°œë¼ê³  ì„¤ì •í•˜ë„ë¡ í•˜ê² ë‹¤. ì•„ë¬´íŠ¼, ì¸ì½”ë” ê³¼ì •ì„ ì´ 6ë²ˆ ë°˜ë³µí•œë‹¤ê³  ìƒê°í•˜ë©´ ëœë‹¤. *ì…€í”„ ì–´í…ì…˜ì´ë€?ì…€í”„ ì–´í…ì…˜ì´ë€ ìê¸° ìì‹ ì—ê²Œ ì–´í…ì…˜ í•¨ìˆ˜ë¥¼ ìˆ˜í–‰í•˜ëŠ” ê²ƒì„ ë§í•˜ëŠ”ë°, ê·¸ë ‡ë‹¤ë©´ ì–´í…ì…˜ì´ë€ ë¬´ì—‡ì¼ê¹Œ? ì–´í…ì…˜ì—ì„œë„ ë‹¤ì–‘í•œ ì¢…ë¥˜ê°€ ìˆëŠ”ë° ê°„ë‹¨íˆ ë§í•˜ìë©´, ì¿¼ë¦¬(Query)ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ì´ ì¿¼ë¦¬ì™€ ì—¬ëŸ¬ê°œì˜ í‚¤(Key)ì™€ì˜ ìœ ì‚¬ë„ë¥¼ ê°ê° êµ¬í•˜ê³ , êµ¬í•œ ìœ ì‚¬ë„ë¥¼ ê°€ì¤‘ì¹˜ë¡œ ì„¤ì •í•˜ì—¬ ê°ê°ì˜ ê°’(value)ì„ êµ¬í•œ ë’¤, ì´ ê°’(ìœ ì‚¬ë„ê°€ ë°˜ì˜ëœ ê°’)ë“¤ì„ ëª¨ë‘ ê°€ì¤‘í•©í•˜ì—¬ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë§í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´, í•œ í…ìŠ¤íŠ¸ ë¬¸ì¥ì´ ì¿¼ë¦¬ë¡œ ì…ë ¥ë  ë•Œ, ê° ë‹¨ì–´ ë²¡í„°ë“¤ê³¼ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•´ ì´ ìœ ì‚¬ë„ë¥¼ ê°€ì¤‘í•©í•˜ì—¬ ë°˜í™˜ëœ ê°’ì´ ê·¸ ë¬¸ì¥ì˜ ì–´í…ì…˜ ê°’ì´ ëœë‹¤. ê·¸ë ‡ë‹¤ë©´ ì…€í”„ ì–´í…ì…˜ ê°’ì„ êµ¬í•˜ê¸° ìœ„í•´ì„œ ì…ë ¥ëœ ë¬¸ì¥ì˜ ë‹¨ì–´ ë²¡í„°(ì¿¼ë¦¬)ì— ëŒ€í•´ ì¿¼ë¦¬(query),í‚¤(key), ê°’(value) ë²¡í„°ê°€ ì •ì˜ë˜ì–´ì•¼ í•  ê²ƒì´ë‹¤. ê·¸ ê³¼ì •ì€ ì•„ë˜ ì´ë¯¸ì§€ë¥¼ í†µí•´ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë‹¤. â€˜studentâ€™ë¼ëŠ” ë‹¨ì–´ ë²¡í„°ê°€ ì…ë ¥ë˜ì—ˆì„ ë•Œ, ê°ê° ì¿¼ë¦¬, í‚¤ ê°’ì˜ ê°€ì¤‘ì¹˜ í–‰ë ¬ì„ ê³±í•´ì£¼ì–´ ì¿¼ë¦¬, í‚¤, ê°’ ë²¡í„°ë¥¼ ì–»ì–´ë‚¸ë‹¤. ì´ë ‡ê²Œ ì¿¼ë¦¬ ë²¡í„°, í‚¤ ë²¡í„°, ê°’ ë²¡í„°ë¥¼ ì–»ì–´ëƒˆë‹¤ë©´ ì¿¼ë¦¬ ë²¡í„°ëŠ” ëª¨ë“  í‚¤ ë²¡í„°ì— ëŒ€í•´ ì–´í…ì…˜ ìŠ¤ì½”ì–´(attention score)ë¥¼ êµ¬í•˜ê²Œ ë˜ê³ , ì´ë¥¼ ì´ìš©í•˜ì—¬ ëª¨ë“  ê°’ ë²¡í„°ë¥¼ ê°€ì¤‘í•© í•˜ì—¬ ì–´í…ì…˜ ê°’ì„ êµ¬í•˜ê²Œ ëœë‹¤. í•œí¸, ì´ëŸ¬í•œ ì—°ì‚°ì€ ê° ë‹¨ì–´ë§ˆë‹¤ê°€ ì•„ë‹Œ ë¬¸ì¥ ì „ì²´ì— ëŒ€í•´ì„œ í–‰ë ¬ ì—°ì‚°ìœ¼ë¡œë„ ì¼ê´„ì ìœ¼ë¡œ ì—°ì‚°ì´ ê°€ëŠ¥í•œë°, ìœ„ì™€ ê°™ì´ ë¬¸ì¥ì— ëŒ€í•œ ì¿¼ë¦¬ ë²¡í„°, í‚¤ ë²¡í„°ì˜ ì—°ì‚°ì„ í†µí•´ ê°’ ë²¡í„° í–‰ë ¬ì„ êµ¬í•  ìˆ˜ ìˆê²Œ ëœë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ ì¿¼ë¦¬ë²¡í„°ì™€ í‚¤ ë²¡í„°ê°€ ì—°ì‚°ë˜ì–´ ë‚˜ì˜¨ í–‰ë ¬ì— ì „ì²´ì ìœ¼ë¡œ íŠ¹ì • ê°’(keyë²¡í„° ì°¨ì›ì˜ ì œê³±ê·¼ ê°’)ì„ ë‚˜ëˆ„ì–´ ì¤€ ë’¤, ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ë¥¼ ì ìš©í•´ì£¼ê³ , ê°€ì¤‘ì¹˜ê°€ ê³„ì‚°ëœ ê°’ ë²¡í„°ë¥¼ ê³±í•˜ê²Œ ë˜ë©´ ìµœì¢…ì ìœ¼ë¡œ ê° ë‹¨ì–´ì˜ ì–´í…ì…˜ ê°’ì„ ê°€ì§€ëŠ” ì–´í…ì…˜ ê°’ í–‰ë ¬ì´ ë„ì¶œëœë‹¤. ì¦‰, ìš”ì•½í•˜ìë©´ ì–´í…ì…˜ í•¨ìˆ˜ëŠ” ì¿¼ë¦¬(Query)ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ì´ ì¿¼ë¦¬ì™€ ì—¬ëŸ¬ê°œì˜ í‚¤(key)ì™€ì˜ ìœ ì‚¬ë„ë¥¼ ê°ê° êµ¬í•˜ê³ , êµ¬í•œ ìœ ì‚¬ë„ë¥¼ ê°€ì¤‘ì¹˜ë¡œ ì„¤ì •í•˜ì—¬ ê°ê°ì˜ ê°’(value)ì„ êµ¬í•œ ë’¤, ìœ ì‚¬ë„ê°€ ë°˜ì˜ëœ ê°’ë“¤ì„ ëª¨ë‘ ê°€ì¤‘í•© í•˜ì—¬ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë§í•œë‹¤. *ë©€í‹° í—¤ë“œ ì–´í…ì…˜(Multi-Head Attention)ì´ë€?ì•ì—ì„œ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì¸ì½”ë”ì—ì„œëŠ” ì–´í…ì…˜ì´ ë³‘ë ¬ì ìœ¼ë¡œ ìˆ˜í–‰ë˜ëŠ” ë©€í‹° í—¤ë“œ ì–´í…ì…˜ì´ ìˆ˜í–‰ëœë‹¤ê³  í–ˆë‹¤. ë…¼ë¬¸ì—ì„œëŠ” 512ì°¨ì›ì˜ ë²¡í„°ë¥¼ 8ë¡œ ë‚˜ëˆ„ì–´ 54ì°¨ì›ì˜ Query, Key, Value ë²¡í„°ë¡œ ë°”ê¾¸ì–´ì„œ ì–´í…ì…˜ í•¨ìˆ˜ë¥¼ ë³‘ë ¬ì ìœ¼ë¡œ ìˆ˜í–‰í•œ ê²ƒì¸ë°, ê·¸ë ‡ë‹¤ë©´ ì™œ ì´ë ‡ê²Œ ìˆ˜í–‰í•œ ê²ƒì¼ê¹Œ? ì¦‰, ì°¨ì›ì„ ë‚˜ëˆ„ì–´ì„œ ì–´í…ì…˜ í•¨ìˆ˜ë¥¼ ìˆ˜í–‰í•œ ë’¤, ê°€ì¤‘ì¹˜ í–‰ë ¬ì„ ê³±í•´ì£¼ê³  ì´ë¥¼ ë‹¤ì‹œ í•©ì¹˜ê²Œ ë˜ëŠ”ê±´ë°, ë…¼ë¬¸ì— ë”°ë¥´ë©´ single attention functionì„ í•˜ëŠ” ê²ƒë³´ë‹¤ ë³‘ë ¬ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ ëª¨ë¸ì´ í•™ìŠµí•˜ëŠ” ë°ì—ëŠ” ë” íš¨ê³¼ì ì´ì—ˆìœ¼ë©°, ëª¨ë¸ì´ ë‹¤ë¥¸ ì˜ì—­(ê³¼ê±°ì‹œì ê³¼ ë¯¸ë˜ì‹œì )ì— ìˆëŠ” ì •ë³´ë“¤ì„ ì°¸ì¡°í•  ìˆ˜ ìˆë‹¤ê³ í•œë‹¤. ë”°ë¼ì„œ ì¶œë ¥ëœ ê°’ë“¤ì€ ì¸ì½”ë”ì˜ ì…ë ¥ ê°’ì˜ ì°¨ì›ê³¼ ë™ì¼í•˜ê²Œ ìœ ì§€ëœë‹¤. *í”¼ë“œ í¬ì›Œë“œ ì‹ ê²½ë§ì´ë€?ì¸ì½”ë” ì•ˆì—ì„œ Multi-Head Attentionì´ ìˆ˜í–‰ë˜ê³  ë‚˜ë©´ Feed Forwardê°€ ìˆ˜í–‰ëœë‹¤ê³  í–ˆì—ˆëŠ”ë°, Feed ForwardëŠ” ë¬´ì—‡ì¼ê¹Œ? Feed ForwardëŠ” ì¼ì¢…ì˜ ì‹ ê²½ë§ìœ¼ë¡œ Feed Forward Neural Networkë¥¼ ì¤„ì—¬ì„œ FFNNì´ë¼ê³  í•œë‹¤. FFNNì˜ ì¢…ë¥˜ë„ ì—¬ëŸ¬ê°€ì§€ê°€ ìˆëŠ”ë°, íŠ¸ëœìŠ¤ í¬ë¨¸ì˜ ì¸ì½”ë” ì¸µì—ëŠ” í¬ì§€ì…˜ ì™€ì´ì¦ˆ(Position-wise) FFNNì„ ì‚¬ìš©í•œë‹¤. í¬ì§€ì…˜ ì™€ì´ì¦ˆ FFNNì€ Fully-connected FFNNê³¼ ê°™ì€ ê¸°ëŠ¥ì„ í•˜ëŠ”ë°, ì•„ë˜ì™€ ê°™ì€ ì—°ì‚°ì„ ìˆ˜í–‰í•œë‹¤. ìœ„ ì‹ì—ì„œ xì˜ ê°’ì€ Multi-Head Attentionì—ì„œ ì¶œë ¥ëœ í–‰ë ¬ ê°’ì´ë‹¤. ë°˜ë©´, ê°€ì¤‘ì¹˜ë¥¼ ì˜ë¯¸í•˜ëŠ” W1, W2, b1, b2ëŠ” ê°€ì¤‘ì¹˜ ê°’ìœ¼ë¡œ ì¸ì½”ë” ë§ˆë‹¤ ë‹¤ë¥¸ ê°’ì„ ê°€ì§€ì§€ë§Œ í•˜ë‚˜ì˜ ì¸ì½”ë” ì¸µ ì•ˆì—ì„œëŠ” ë¬¸ì¥ê³¼ ë‹¨ì–´ë“¤ë§ˆë‹¤ ë™ì¼í•˜ê²Œ ì‚¬ìš©ëœë‹¤ê³  í•œë‹¤. ì´ë ‡ê²Œ í”¼ë“œ í¬ì›Œë“œ ì‹ ê²½ë§ê¹Œì§€ ê±°ì¹˜ê²Œ ë˜ë©´ í•œ ì¸ì½”ë”ì˜ ì¶œë ¥ê°’ì´ ë„ì¶œ ë˜ê³ , ì´ ê°’ì€ ë‹¤ì‹œ ë‘ë²ˆì§¸ ì¸ì½”ë” ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°€ê²Œ ë˜ë©° ì´ ê³¼ì •ì´ ë°˜ë³µëœë‹¤. *Add &amp; Normí•œí¸ ì¸ì½”ë”ì˜ êµ¬ì¡°ë¥¼ ë³´ì—¬ì¤€ ì´ë°ë¥¼ ë‹¤ì‹œ ë³´ê³  ì˜¤ë©´ 2ê°œì˜ ì„œë¸Œì¸µì¸ Multi-Head Attentionê³¼ Feed Forwardê°€ ê°ê° ëë‚˜ê³  ë‚˜ë©´ Add &amp; Norm ì´ë¼ëŠ” ë‹¨ê³„ê°€ ìˆ˜í–‰ëœë‹¤. ì´ê²ƒì€ ë˜ ë¬´ì—‡ì¼ê¹Œ? ë…¼ë¬¸ì˜ ì¼ë¶€ë¶„ì„ ì½ì–´ë³´ë©´ Add &amp; Normì´ë€ ë°”ë¡œ ë‘ê°œì˜ ì„œë¸Œì¸µì„ residual connection í•´ì£¼ê³  layer normalizationì„ í•´ì£¼ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. Residual connectionê³¼ layer normalizationì— ëŒ€í•´ ì§§ê²Œ ìš”ì•½í•˜ìë©´, residual connectionì€ ì„œë¸Œì¸µì˜ ì…ë ¥ê³¼ ì¶œë ¥ì„ ë” í•˜ëŠ” ê²ƒì´ë‹¤. ì´ëŸ¬í•œ ì•Œê³ ë¦¬ì¦˜ì€ RNN, VGG êµ¬ì¡°ì—ì„œë„ ë³¼ ìˆ˜ ìˆê³ , ì´ëŸ¬í•œ ì—°ì‚°ì´ ê°€ëŠ¥í•œ ê²ƒì€ ì…ë ¥ ë°ì´í„°ì™€ ì¶œë ¥ ë°ì´í„°ê°€ ë™ì¼í•œ ì°¨ì›ì„ ê°–ê³  ìˆê¸° ë•Œë¬¸ì´ë¼ê³  í•œë‹¤. ë°˜ë©´, layer normalizationì€ ì •ê·œí™”ë¥¼ í•˜ëŠ” ê³¼ì •ìœ¼ë¡œ ì¶œë ¥ëœ ê°’ë“¤ì— ëŒ€í•´ì„œ í‰ê· ê³¼ ë¶„ì‚°ì„ êµ¬í•´ì„œ ì •ê·œí™”ë¥¼ í•˜ëŠ” ê²ƒì„ ë§í•œë‹¤. ì•ì—ì„œ ì…ë ¥ë°ì´í„°ì¸ 512ì°¨ì›ì˜ ë²¡í„°ë¥¼ 8ë¡œ ë‚˜ëˆ„ì–´ ì–´í…ì…˜ í•¨ìˆ˜ë¥¼ ë³‘ë ¬ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ì˜€ë‹¤ê³  í–ˆëŠ”ë°, ê·¸ë ‡ê²Œ ì¶œë ¥ëœ 8ê°œì˜ ê°’ë“¤ë¡œ layer normalizationì„ í•˜ëŠ” ê²ƒì´ë‹¤. (3)ë””ì½”ë”(Decoder)ì˜ êµ¬ì¡° ë‹¤ì‹œ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ êµ¬ì¡°ë¥¼ ì‚´í´ë³´ì. ì§€ê¸ˆê¹Œì§€ ì™¼ìª½ì— ìˆëŠ” ì¸ì½”ë”ì— ëŒ€í•´ ì‚´í´ ë³´ì•˜ê³ , ì´ì œ ì˜¤ë¥¸ìª½ì— ìˆëŠ” ë””ì½”ë”ì— ëŒ€í•´ ì‚´í´ë³´ë„ë¡ í•˜ê² ë‹¤. ë””ì½”ë”ëŠ” ì¸ì½”ë”ì—ì„œ ë„˜ê²¨ë°›ì€ ê°’ì— ëŒ€í•´ Multi-Head Attentionê³¼ Feed Forwardë¥¼ ìˆ˜í–‰í•˜ê¸° ì „ output dataì— ëŒ€í•´ ì„ë² ë”©ê³¼ í¬ì§€ì…”ë„ ì¸ì½”ë”©ì„ í•œ ê°’ì„ ì…ë ¥ ë°›ëŠ”ë‹¤. ê·¸ë¦¬ê³  ì¸ì½”ë”ì™€ëŠ” ë‹¤ë¥´ê²Œ Masked Multi-Head Attentionì´ë¼ëŠ” ê²ƒì„ í•´ì£¼ê²Œ ëœë‹¤. *Masked Multi-Head Attentionì´ë€?Masked Multi-Head Attentionì€ ë§ê·¸ëŒ€ë¡œ Multi-Head Attentionì—ì„œ Maskê¸°ëŠ¥ì´ ë“¤ì–´ê°„ ê²ƒì´ë‹¤. ì•ì—ì„œ Multi-Head Attentionì€ ì…€í”„ì–´í…ì…˜ì„ ë³‘ë ¬ì ìœ¼ë¡œ ìˆ˜í–‰í•œ ê²ƒì„ ì˜ë¯¸í–ˆì—ˆë‹¤. ë”°ë¼ì„œ ë‹¤ë¥¸ ì˜ì—­ì— ìˆëŠ”, ì¦‰ ë¯¸ë˜ì˜ ì‹œì ì— ìˆëŠ” ë‹¨ì–´ì˜ ì •ë³´ë„ ì•Œ ìˆ˜ ìˆê²Œ ëœë‹¤ê³  í–ˆì—ˆë‹¤. ì´ëŸ¬í•œ ì´ìœ ë¡œ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ë””ì½”ë”ì—ëŠ” í˜„ì¬ì‹œì ë³´ë‹¤ ë¯¸ë˜ì— ìˆëŠ” ë‹¨ì–´ë¥¼ ì°¸ê³ í•´ ì˜ˆì¸¡í•˜ì§€ ëª»í•˜ê³  ì´ì „ì‹œì ë“¤ì— ìˆëŠ” ë‹¨ì–´ë“¤ë§Œ ì°¸ê³ í•  ìˆ˜ ìˆë„ë¡ ë§ˆìŠ¤í‚¹í•´ì¤˜ì•¼ í•œë‹¤. ì•„ë§ˆ ë¯¸ë˜ì— ìˆëŠ” ë‹¨ì–´ë¥¼ ì°¸ê³ í•´ ì˜ˆì¸¡í•˜ë„ë¡ í•œë‹¤ë©´ í•™ìŠµí•˜ëŠ”ë° ë„ì›€ì´ ë˜ì§€ ì•ŠëŠ”ê°€ ë³´ë‹¤. ë‹µì§€ë³´ê³  ë² ë¼ëŠ” ëŠë‚Œì´ë„ê¹Œ? ì•„ë¬´íŠ¼ ë§ˆìŠ¤í‚¹ì„ í•˜ê¸° ìœ„í•´ lood-ahead maskë¼ëŠ” ê²ƒì„ í•´ì£¼ëŠ”ë°, Multi-Head Attentionì„ í†µí•´ ë‚˜ì˜¨ í–‰ë ¬ ê°’ì— ëŒ€í•´ ë§ˆìŠ¤í‚¹ì„ í•˜ê³ ì í•˜ëŠ” ê°’ì—ëŠ” 1, ë§ˆìŠ¤í‚¹ì„ í•˜ì§€ ì•ŠëŠ” ê°’ì—ëŠ” 0ì„ ë¦¬í„´í•˜ë„ë¡ í•œë‹¤. ê·¸ë¦¬ê³  ë‚˜ì„œ Add &amp; Norm ê³¼ì •ì„ ìˆ˜í–‰í•´ì¤€ë’¤ ë„ì¶œëœ ê²°ê³¼ë¥¼ ë‹¤ìŒ ê²°ê³¼ë¡œ ë³´ë‚´ì¤€ë‹¤. *ë””ì½”ë”ì˜ Multi-Head Attentionê³¼ Feed Forwardë””ì½”ë”ì—ì„œ Masked Multi-Head Attentionì´ ìˆ˜í–‰ë˜ê³  ë‚˜ë©´ ê·¸ ë‹¤ìŒë¶€í„°ëŠ” ì¸ì½”ë”ì™€ ë§ˆì°¬ê°€ì§€ë¡œ Multi-Head Attentionê³¼ Feed Forwardê°€ ìˆ˜í–‰ëœë‹¤. ê·¼ë° ì´ë•Œ Multi-Head Attentionì— ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°€ëŠ” ê°’ë“¤ì„ ì˜ ì‚´í´ ë´ì•¼ í•œë‹¤. ì¸ì½”ë”ì—ì„œ ì¶œë ¥ëœ ê°’ê³¼ ë””ì½”ë” ì²«ë²ˆì§¸ ì„œë¸Œì¸µì—ì„œ ì¶œë ¥ëœ ê°’ì´ ì¸í’‹ìœ¼ë¡œ ë“¤ì–´ê°€ê¸° ë•Œë¬¸ì´ë‹¤. ë‘ë²ˆì§¸ ì„œë¸Œì¸µì¸ Multi-Head Attentionì—ì„œëŠ” ë§ˆì°¬ê°€ì§€ë¡œ ì…€í”„ì–´í…ì…˜ í•¨ìˆ˜ë¥¼ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ Query, Key, Value ë²¡í„°ê°€ ì…ë ¥ë˜ì–´ì•¼ í•œë‹¤. ì´ë•Œ QueryëŠ” ë””ì½”ë” ì²«ë²ˆì§¸ ì„œë¸Œì¸µì—ì„œ ì¶œë ¥ëœ ê°’ì´ í•´ë‹¹ë˜ê³ , Key ë²¡í„°ì™€ Value ë²¡í„°ëŠ” ë§ˆì§€ë§‰ ì¸ì½”ë”ì—ì„œ ì¶œë ¥ëœ ê°’ìœ¼ë¡œ ì…ë ¥ëœë‹¤. ê·¸ë¦¬ê³  ë˜‘ê°™ì´ Multi-Head Attentionì„ ìˆ˜í–‰í•´ì£¼ê²Œ ëœë‹¤. ì´ë ‡ê²Œ 6ê°œì˜ ë””ì½”ë”ë§ˆë‹¤ Multi-Head Attentionì˜ Query ë²¡í„°ëŠ” ë””ì½”ë” ì²«ë²ˆì§¸ ì„œë¸Œì¸µì˜ output, Keyë²¡í„°ì™€ Valueë²¡í„°ëŠ” ì¸ì½”ë”ì˜ outputì´ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°€ê²Œ ëœë‹¤. Reference attention ë…¼ë¬¸: https://arxiv.org/abs/1409.0473 https://wikidocs.net/31379 https:&#x2F;&#x2F;velog.io&#x2F;@seolini43&#x2F;íŒŒì´ì¬Transformerë¡œ-ì˜¤í”¼ìŠ¤-ì±—ë´‡-ë§Œë“¤ê¸°-ì´ë¡ í¸","categories":[{"name":"NLP","slug":"NLP","permalink":"https://jmj3047.github.io/categories/NLP/"}],"tags":[{"name":"Transformer","slug":"Transformer","permalink":"https://jmj3047.github.io/tags/Transformer/"},{"name":"Attention","slug":"Attention","permalink":"https://jmj3047.github.io/tags/Attention/"},{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"}]},{"title":"Making Chatbot with doc2vec tutorial(1)","slug":"Making_Chatbot_with_doc2vec_tutorial(1)","date":"2022-05-05T15:00:00.000Z","updated":"2022-05-06T08:25:05.657Z","comments":true,"path":"2022/05/06/Making_Chatbot_with_doc2vec_tutorial(1)/","link":"","permalink":"https://jmj3047.github.io/2022/05/06/Making_Chatbot_with_doc2vec_tutorial(1)/","excerpt":"","text":"ëª¨ë¸ ë§Œë“¤ê¸°ë°ì´í„° ë§Œë“¤ê¸°doc2vecì„ ì´ìš©í•´ì„œ FAQë°ì´í„°ë“¤ì˜ ì§ˆë¬¸ë“¤ì„ ë²¡í„°í™”í•˜ëŠ” ëª¨ë¸ì„ ë§Œë“¤ì–´ ë³¸ë‹¤. word2vecì´ ë‹¨ì–´ë¥¼ ë²¡í„°í™” í•˜ëŠ” ê²ƒì´ë¼ë©´ doc2vecì€ ë‹¨ì–´ê°€ ì•„ë‹ˆë¼ ë¬¸ì„œë¥¼ ê¸°ì¤€ìœ¼ë¡œ (ì—¬ê¸°ì„œëŠ” ë¬¸ì¥)ë²¡í„°ë¥¼ ë§Œë“œëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì´ë‹¤. doc2vecì„ ì‚¬ìš©í•˜ë©´ ì„œë¡œ ë‹¤ë¥¸ ë¬¸ì„œë“¤ì´ ê°™ì€ ì°¨ì›ì˜ ë²¡í„°ê°’ì„ ê°–ê²Œ ëœë‹¤. ê° ë¬¸ì„œë¼ ê°–ëŠ” ë²¡í„°ê°’ì„ ë¹„êµí•´ ê°™ìœ¼ë©´ ê°™ì„ ìˆ˜ë¡ ìœ ì‚¬í•œ ë¬¸ì„œë¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ doc2vecì„ ì´ìš©í•´ FAQì˜ ì§ˆë¬¸ë“¤ì„ ë²¡í„°í™” í•œë‹¤ë©´ ì–´ë–¤ ì§ˆë¬¸ì´ ë“¤ì–´ì™”ì„ ë•Œ ë™ì¼ ëª¨ë¸ë¡œ ì§ˆë¬¸ì„ ë²¡í„°í™” í•œë‹¤ìŒ, ì €ì¥ë¼ ìˆëŠ” ì§ˆë¬¸ë“¤ì˜ ë²¡í„°ì™€ ë¹„êµí•´ì„œ ê°€ì¥ ìœ ì‚¬í•œ ì§ˆë¬¸ì„ ì°¾ì„ ìˆ˜ ìˆë‹¤. ê°€ì¥ ìœ ì‚¬í•œ ì§ˆë¬¸ì„ ì°¾ì€ ë‹¤ìŒ ê·¸ ì§ˆë¬¸ì˜ ë‹µì„ ì¶œë ¥í•˜ë©´ FAQì±—ë´‡ì„ ë§Œë“¤ ìˆ˜ ìˆë‹¤. **GPUì‚¬ìš© í•„ìˆ˜..! 1234567891011import osimport warningsfrom gensim.models import doc2vecfrom gensim.models.doc2vec import TaggedDocumentfaqs = [[&quot;1&quot;, &quot;ë‹¹í•´ë…„ë„ ë‚©ì…ì•¡ì€ ìˆ˜ì • ê°€ëŠ¥ í•œê°€ìš”?&quot;, &quot;ë„¤, ë‹¹í•´ë…„ë„ ë‚©ì…ì•¡ì€ 12464 í™”ë©´ ë“±ë¡ì „ê¹Œì§€ ìˆ˜ì • ê°€ëŠ¥í•©ë‹ˆë‹¤.&quot;], [&quot;2&quot;, &quot;ëŒ€ë¦¬ì¸í†µë³´ ëŒ€ìƒê³„ì¢Œ ê¸°ì¤€ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?&quot;, &quot;ëª¨ê³„ì¢Œ ê¸°ì¤€ ê°€ì¥ ìµœê·¼ì— ê°œì„¤ëœ ê³„ì¢Œì˜ ê´€ë¦¬ì ì—ì„œ ì¡°íšŒ ë©ë‹ˆë‹¤. ì˜ì›íì‡„ëœ ìê³„ì¢ŒëŠ” ì¡°íšŒëŒ€ìƒ ê³„ì¢Œì—ì„œ ì œì™¸ë©ë‹ˆë‹¤. ê³„ì¢Œì£¼ ê³„ì¢Œê°€ ì‚¬ì ˆì› ê³„ì¢Œê°€ ì•„ë‹Œ ê²½ìš°ë§Œ ì¡°íšŒë©ë‹ˆë‹¤&quot;], [&quot;3&quot;, &quot;ë“±ë¡ê°€ëŠ¥ ë‹¨ë§ê¸°ìˆ˜ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?&quot;, &quot;5ëŒ€ê¹Œì§€ ë“±ë¡ ê°€ëŠ¥ì…ë‹ˆë‹¤.&quot;], [&quot;4&quot;, &quot;ëª¨ë°”ì¼ê³„ì¢Œê°œì„¤ ê°€ëŠ¥í•œ ì‹œê°„ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?&quot;, &quot;08:00 ~ 20:00(ì˜ì—…ì¼ë§Œ ê°€ëŠ¥&quot;], [&quot;5&quot;, &quot;ë¯¸êµ­ì¸ì¼ë•Œ ë¯¸êµ­ë‚©ì„¸ìë“±ë¡ë²ˆí˜¸ ì‘ì„± ë°©ë²•ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?&quot;, &quot;ê³„ì¢Œì£¼ê°€ ë¯¸êµ­ì¸ì¼ ë•Œ ê³„ì¢Œì£¼ì˜ ë¯¸êµ­ë‚©ì„¸ìë“±ë¡ë²ˆí˜¸(ì‚¬íšŒë³´ì¥ë²ˆí˜¸(Social Security Number), ê³ ìš©ì£¼ì‹ë³„ë²ˆí˜¸(Employer Identification Number), ê°œì¸ë‚©ì„¸ìë²ˆí˜¸(Individual Taxpayer Identification Number))ë¥¼ ê¸°ì¬í•©ë‹ˆë‹¤..&quot;]] ìœ„ì™€ ê°™ì´ 5ê°œì˜ FAQë°ì´í„°ë¥¼ ì„ì˜ë¡œ ë§Œë“¤ì—ˆë‹¤. ì´ì œ ì—¬ê¸°ì— 5ê°œì˜ ì§ˆë¬¸ì„ ë²¡í„°í™” í• ê±´ë° ì‚¬ì‹¤ ë²¡í„°í™” í•  ë•Œ ë°ì´í„°ëŠ” ë§ì„ ìˆ˜ë¡ ì¢‹ë‹¤. ì ìœ¼ë©´ ì„œë¡œ ë¶„ê°„ì´ ì˜ ì•ˆë¨. í˜•íƒœì†Œ ë¶„ì„doc2vecìœ¼ë¡œ ë¬¸ì¥ì„ ë²¡í„°í™”í•˜ê¸° ì „ì— ì•½ê°„ì˜ ì „ì²˜ë¦¬ ê³¼ì •ì´ í•„ìš”í•˜ë‹¤. ê° ë¬¸ì¥ì„ tokenizeí•´ì•¼ í•œë‹¤. í† í°í™” í•˜ëŠ” ê³¼ì •ì´ ì˜ì–´ë‘ í•œêµ­ì–´ë‘ ì¡°ê¸ˆ ë‹¤ë¥¸ë° í•œêµ­ì–´ì˜ ê²½ìš° í˜•íƒœì†Œ ë¶„ì„(pos tagging)ì„ í†µí•´ í˜•íƒœì†Œ ë‹¨ìœ„ë¡œ ë‚˜ëˆˆë’¤ í† í°ìœ¼ë¡œ ì‚¬ìš©í•  í˜•íƒœì†Œë¥¼ ê²°ì •í•˜ê³  ë‚˜ëˆˆë‹¤. ì¦‰ ê° ë¬¸ì¥ì„ í˜•íƒœì†Œ ë‹¨ìœ„ì˜ ë°°ì—´ë¡œ ë§Œë“ ë‹¤. í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸°ëŠ” konlpyë¥¼ ì‚¬ìš©í•œë‹¤. 123456789101112#í˜•íƒœì†Œ ë¶„ì„import jpypefrom konlpy.tag import Kkmakkma = Kkma()def tokenize_kkma(doc): jpype.attachThreadToJVM() #ìë°”ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ì†ŒìŠ¤ ì½”ë“œ token_doc = [&#x27;/&#x27;.join(word) for word in kkma.pos(doc) ] #í˜•íƒœì†Œ ë¶„ì„í•œ ë‹¨ì–´ì™€ í˜•íƒœì†Œ ëª…ì„ &#x27;ë‹¨ì–´/í˜•íƒœì†Œ&#x27;í˜•íƒœë¡œ ì¶œë ¥í•˜ê¸° ìœ„í•œ ì½”ë“œ return token_doctokenize_kkma(faqs[0][1]) Kkmaë¥¼ import í•˜ê³  jpypeë„ import í•œë‹¤. jpypeëŠ” íŒŒì´ì¬ì—ì„œ ìë°”ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” íŒ¨í‚¤ì§€ì¸ë° ê¸°ë³¸ì ìœ¼ë¡œ kkmaê°€ ìë°” ë² ì´ìŠ¤ë¼ì„œ ê¼­ í•„ìš”í•˜ë‹¤. Kkma()ë¡œ í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ë¶ˆëŸ¬ì˜¨ë‹¤ìŒ kkma.pos(doc)ë¡œ í˜•íƒœì†Œ ë¶„ì„ì„ í•œë‹¤. 123456789101112ì¶œë ¥ ê²°ê³¼:[&#x27;ë‹¹í•´/NNG&#x27;, &#x27;ë…„ë„/NNM&#x27;, &#x27;ë‚©ì…/NNG&#x27;, &#x27;ì•¡/XSN&#x27;, &#x27;ì€/JX&#x27;, &#x27;ìˆ˜ì •/NNG&#x27;, &#x27;ê°€ëŠ¥/NNG&#x27;, &#x27;í•œ/MDN&#x27;, &#x27;ê°€ìš”/NNG&#x27;, &#x27;?/SF&#x27;] í˜•íƒœì†Œ ë¶„ì„ì„ í•˜ë©´ ë¬¸ì¥ì´ ë‹¨ì–´&#x2F;í˜•íƒœì†Œ í˜•íƒœì˜ ë°°ì—´ë¡œ ì¶œë ¥ëœë‹¤. 1ë²ˆ ë¬¸ì¥ì€ ì´ 10ê°œì˜ í˜•íƒœì†Œë¡œ ë‚˜ë‰˜ì—ˆë‹¤. í˜•íƒœì†Œ ë¶„ì„ê¸°ì¢…ë¥˜ì— ë”°ë¼ ê²°ê³¼ê°€ ì¡°ê¸ˆì”© ë‹¤ë¥¼ìˆ˜ ìˆë‹¤. Doc2Vec ëª¨ë¸ ë§Œë“¤ê¸°Doc2Vecì„ ì´ìš©í•´ ëª¨ë¸ì„ ë§Œë“¤ê¸° ìœ„í•´ì„œëŠ” í† í°í™” ëœ ë¦¬ìŠ¤íŠ¸ì™€ íƒœê·¸ ê°’ì´ í•„ìš”í•˜ë‹¤. ì—¬ê¸°ì„œ íƒœê·¸ëŠ” ë¬¸ì¥ ë²ˆí˜¸. [ë¬¸ì¥ì˜ ë²ˆí˜¸, ë¬¸ì¥ì„ í† í°í™”í•œ ë°°ì—´] ì´ë ‡ê²Œ ë‘ ê°œì˜ ê°’ì„ ê°€ì§„ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•´ doc2vec ëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ ìˆë‹¤. ì‹¤ì œë¡œ ëª¨ë¸ì„ ë§Œë“œëŠ”ë° ì‚¬ìš©í•˜ëŠ” ê±´ í† í° ê°’ì´ì§€ë§Œ ë¹„ìŠ·í•œ ë¬¸ì¥ì´ ë¬´ì—‡ì¸ì§€ ì°¾ê¸° ìœ„í•œ ì¸ë±ìŠ¤ë¡œ íƒœê·¸ ê°’ì„ ì‚¬ìš©í•˜ê²Œ ëœë‹¤. 1234567# ë¦¬ìŠ¤íŠ¸ì—ì„œ ê° ë¬¸ì¥ë¶€ë¶„ í† í°í™”token_faqs = [(tokenize_kkma(row[1]), row[0]) for row in faqs]# Doc2Vecì—ì„œ ì‚¬ìš©í•˜ëŠ” íƒœê·¸ë¬¸ì„œí˜•ìœ¼ë¡œ ë³€ê²½tagged_faqs = [TaggedDocument(d, [c]) for d, c in token_faqs]tagged_faqs 12345[TaggedDocument(words=[&#x27;ë‹¹í•´/NNG&#x27;, &#x27;ë…„ë„/NNM&#x27;, &#x27;ë‚©ì…/NNG&#x27;, &#x27;ì•¡/XSN&#x27;, &#x27;ì€/JX&#x27;, &#x27;ìˆ˜ì •/NNG&#x27;, &#x27;ê°€ëŠ¥/NNG&#x27;, &#x27;í•œ/MDN&#x27;, &#x27;ê°€ìš”/NNG&#x27;, &#x27;?/SF&#x27;], tags=[&#x27;1&#x27;]), TaggedDocument(words=[&#x27;ëŒ€ë¦¬ì¸/NNG&#x27;, &#x27;í†µë³´/NNG&#x27;, &#x27;ëŒ€ìƒ/NNG&#x27;, &#x27;ê³„ì¢Œ/NNG&#x27;, &#x27;ê¸°ì¤€/NNG&#x27;, &#x27;ì€/JX&#x27;, &#x27;ì–´ë–»/VA&#x27;, &#x27;ê²Œ/ECD&#x27;, &#x27;ë˜/VV&#x27;, &#x27;ë‚˜ìš”/EFQ&#x27;, &#x27;?/SF&#x27;], tags=[&#x27;2&#x27;]), TaggedDocument(words=[&#x27;ë“±ë¡/NNG&#x27;, &#x27;ê°€ëŠ¥/NNG&#x27;, &#x27;ë‹¨ë§/NNG&#x27;, &#x27;ê¸°ìˆ˜/NNG&#x27;, &#x27;ëŠ”/JX&#x27;, &#x27;ì–´ë–»/VA&#x27;, &#x27;ê²Œ/ECD&#x27;, &#x27;ë˜/VV&#x27;, &#x27;ë‚˜ìš”/EFQ&#x27;, &#x27;?/SF&#x27;], tags=[&#x27;3&#x27;]), TaggedDocument(words=[&#x27;ëª¨ë°”ì¼/NNG&#x27;, &#x27;ê³„ì¢Œ/NNG&#x27;, &#x27;ê°œì„¤/NNG&#x27;, &#x27;ê°€ëŠ¥/NNG&#x27;, &#x27;í•˜/XSV&#x27;, &#x27;ã„´/ETD&#x27;, &#x27;ì‹œê°„/NNG&#x27;, &#x27;ì€/JX&#x27;, &#x27;ì–´ë–»/VA&#x27;, &#x27;ê²Œ/ECD&#x27;, &#x27;ë˜/VV&#x27;, &#x27;ë‚˜ìš”/EFQ&#x27;, &#x27;?/SF&#x27;], tags=[&#x27;4&#x27;]), TaggedDocument(words=[&#x27;ë¯¸êµ­ì¸/NNG&#x27;, &#x27;ì¼/NNG&#x27;, &#x27;ë•Œ/NNG&#x27;, &#x27;ë¯¸êµ­/NNP&#x27;, &#x27;ë‚©ì„¸ì/NNG&#x27;, &#x27;ë“±ë¡/NNG&#x27;, &#x27;ë²ˆí˜¸/NNG&#x27;, &#x27;ì‘ì„±/NNG&#x27;, &#x27;ë°©ë²•/NNG&#x27;, &#x27;ì€/JX&#x27;, &#x27;ì–´ë–»/VA&#x27;, &#x27;ê²Œ/ECD&#x27;, &#x27;ë˜/VV&#x27;, &#x27;ë‚˜ìš”/EFQ&#x27;, &#x27;?/SF&#x27;], tags=[&#x27;5&#x27;])] TaggedDocument functionì„ ì‚¬ìš©í•˜ë©´ doc2vecì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” íƒœê·¸ ëœ ë¬¸ì„œ í˜•ì‹ìœ¼ë¡œ ë³€ê²½í•œë‹¤. ì¶œë ¥í•´ ë³´ë©´ wordsë°°ì—´ê³¼ tagsê°’ì„ ê°–ëŠ” Dicí˜•íƒœì˜ ìë£Œí˜•ì´ ë˜ì—ˆìŒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. 1234567891011121314151617181920212223# make model import multiprocessing cores = multiprocessing.cpu_count() d2v_faqs = doc2vec.Doc2Vec(vector_size=50, alpha=0.025, min_alpha=0.025, hs=1, negative=0, dm=0, dbow_words = 1, min_count = 1, workers = cores, seed=0) d2v_faqs.build_vocab(tagged_faqs) # train document vectors for epoch in range(10): d2v_faqs.train(tagged_faqs, total_examples = d2v_faqs.corpus_count, epochs = d2v_faqs.epochs ) d2v_faqs.alpha -= 0.0025 # decrease the learning rate d2v_faqs.min_alpha = d2v_faqs.alpha # fix the learning rate, no decay ëª¨ë¸ì„ ë§Œë“¤ê³  í•™ìŠµ ì‹œí‚¨ë‹¤. doc2vecëª¨ë¸ì„ ë§Œë“¤ ë•Œ íŒŒë¼ë¯¸í„°ëŠ” ì—¬ëŸ¬ê°€ì§€ê°€ ë“¤ì–´ê°€ëŠ”ë° ì—¬ê¸°ì„œëŠ” vector_sizeì™€ min_countì •ë„ë¥¼ ìˆ˜ì •í–ˆë‹¤. vector_sizeëŠ” ë§Œë“¤ì–´ì§€ëŠ” ë²¡í„° ì°¨ì›ì˜ í¬ê¸°ì´ê³ , min_countëŠ” ìµœì†Œ ëª‡ ë²ˆ ì´ìƒ ë‚˜ì˜¨ ë‹¨ì–´ì— ëŒ€í•´ í•™ìŠµí• ì§€ ì •í•˜ëŠ” íŒŒë¼ë¯¸í„°ì´ë‹¤. ì—¬ê¸°ì„œëŠ” ì¼ë‹¨ ì‚¬ì´ì¦ˆ 50ì— ìµœì†Œ íšŸìˆ˜ëŠ” 1íšŒë¡œ ì •í–ˆë‹¤. epochëŠ” 10ë²ˆìœ¼ë¡œ í•´ì„œ trainí–ˆë‹¤. ìœ ì‚¬ ë¬¸ì¥ ì°¾ê¸°ì´ì œ ì´ ëª¨ë¸ë¡œ ì–´ë–¤ ë¬¸ì¥ì´ ë“¤ì–´ì™”ì„ ë•Œ 1~5ë²ˆ ì¤‘ì— ë¬´ì—‡ê³¼ ë¹„ìŠ·í•œì§€ ì•Œì•„ë³´ì. ë¨¼ì € ì–´ë–¤ ë¬¸ì¥ì´ ë“¤ì–´ì˜¤ë©´ ê·¸ ë¬¸ì¥ì„ ë²¡í„°í™” í•˜ê³  ê·¸ ë²¡í„°ê°€ ì–´ë–¤ ë¬¸ì¥ê³¼ ë¹„ìŠ·í•œì§€ íƒœê·¸ ê°’ì„ ì°¾ì•„ë³¸ë‹¤. 12predict_vector = d2v_faqs.infer_vector([&quot;ë‹¹í•´ë…„ë„ ë‚©ì…ì•¡ì€ ìˆ˜ì • ê°€ëŠ¥ í•œê°€ìš”?&quot;])d2v_faqs.docvecs.most_similar([predict_vector], topn=2) 1[(&#x27;2&#x27;, 0.21605531871318817), (&#x27;3&#x27;, 0.10707802325487137)] ì œëŒ€ë¡œ ëëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•´ 1ë²ˆ ë¬¸ì¥ì„ ê·¸ëŒ€ë¡œ ë„£ì—ˆì§€ë§Œ ë‹µì€ 2, 3ë²ˆì´ ë‚˜ì™”ë‹¤. í•™ìŠµì´ ì œëŒ€ë¡œ ë˜ì§€ ì•Šì•„ì„œ ì´ëŸ° ê²°ê³¼ê°€ ë‚˜ì™”ë‹¤. í…ŒìŠ¤íŠ¸í•  ë¬¸ì¥ì„ ë²¡í„°í™” í•  ë•Œë„ í˜•íƒœì†Œ ë¶„ì„ì„ í•´ì¤˜ì•¼ í•œë‹¤. ì™œëƒí•˜ë©´ ëª¨ë¸ì„ í•™ìŠµ í•  ë•Œ ë¬¸ì¥ë“¤ì„ í˜•íƒœì†Œë¡œ ë¶„ì„í•´ì„œ ë„£ì–´ì¤¬ê¸° ë•Œë¬¸ì´ë‹¤. 123test_string = &quot;ëŒ€ë¦¬ì¸í†µë³´ ëŒ€ìƒê³„ì¢Œ ê¸°ì¤€ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?&quot;tokened_test_string = tokenize_kkma(test_string)tokened_test_string 1234567891011[&#x27;ëŒ€ë¦¬ì¸/NNG&#x27;, &#x27;í†µë³´/NNG&#x27;, &#x27;ëŒ€ìƒ/NNG&#x27;, &#x27;ê³„ì¢Œ/NNG&#x27;, &#x27;ê¸°ì¤€/NNG&#x27;, &#x27;ì€/JX&#x27;, &#x27;ì–´ë–»/VA&#x27;, &#x27;ê²Œ/ECD&#x27;, &#x27;ë˜/VV&#x27;, &#x27;ë‚˜ìš”/EFQ&#x27;, &#x27;?/SF&#x27;] 12test_vector = d2v_faqs.infer_vector(tokened_test_string)d2v_faqs.docvecs.most_similar([test_vector], topn=2) 1[(&#x27;1&#x27;, 0.1448383331298828), (&#x27;3&#x27;, 0.0218462273478508)] 2ë²ˆ ë¬¸ì¥ìœ¼ë¡œ í–ˆì„ ë•Œ ê²°ê³¼ì…ë‹ˆë‹¤. doc2vecì´ë¼ëŠ” ëª¨ë¸ì€ ë¬¸ì„œë‹¨ìœ„ë¡œ ë²¡í„°í™” í•˜ëŠ” ê²ƒì´ê¸° ë•Œë¬¸ì— ë¬¸ì„œê°€ ë§ì•„ì•¼ í•œë‹¤. ì—¬ê¸°ì„œëŠ” ë¬¸ì¥ì´ ë§ì•„ì•¼ í•œë‹¤. ë¬¸ì¥ì´ ë§ìœ¼ë©´ ë§ì„ ìˆ˜ë¡ ë¬¸ì¥ ê°„ì˜ ê±°ë¦¬ë¥¼ ê³„ì‚°í•´ì„œ ë” ì˜ êµ¬ë¶„í•´ì¤€ë‹¤. ê°„ë‹¨í•˜ê²Œ ìƒê°í•˜ë©´ ë¬¸ì¥ì´ ì ìœ¼ë©´ ì ì¤‘ë¥ ì´ ë†’ì„ ê²ƒ ê°™ì§€ë§Œ ì‚¬ì‹¤ì€ ê·¸ ë°˜ëŒ€ì´ë‹¤. ë°ì´í„°ê°€ ë§ì„ìˆ˜ë¡ ê·¸ ë°ì´í„°ê°„ì˜ ì°¨ì´ë¥¼ êµ¬ë¶„í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ë” ì˜ ì˜ˆì¸¡í•˜ê²Œ ëœë‹¤. Local path :C:\\Users\\jmj30\\Dropbox\\ì¹´ë©”ë¼ ì—…ë¡œë“œ\\Documentation\\2022\\2022 ìƒë°˜ê¸°\\íœ´ë¨¼êµìœ¡ì„¼í„°\\mj_chatbot_prac\\faq_chatbot Reference: https://cholol.tistory.com/466","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"}],"tags":[{"name":"Chatbot","slug":"Chatbot","permalink":"https://jmj3047.github.io/tags/Chatbot/"},{"name":"NLP","slug":"NLP","permalink":"https://jmj3047.github.io/tags/NLP/"},{"name":"Doc2vec","slug":"Doc2vec","permalink":"https://jmj3047.github.io/tags/Doc2vec/"}]},{"title":"Making English Chatbot with doc2vec(3)","slug":"Making_English_Chatbot_with_doc2vec(3)","date":"2022-05-05T15:00:00.000Z","updated":"2022-05-06T08:26:03.229Z","comments":true,"path":"2022/05/06/Making_English_Chatbot_with_doc2vec(3)/","link":"","permalink":"https://jmj3047.github.io/2022/05/06/Making_English_Chatbot_with_doc2vec(3)/","excerpt":"","text":"ë§ì€ ë°ì´í„°ë¡œ ì‹¤í—˜í•´ë³´ê¸°ë°ì´í„° ì‚´í´ë³´ê¸°ë” ë§ì€ í•™ìŠµ ë°ì´í„°ë¡œ ëª¨ë¸ì„ í•™ìŠµí•œë‹¤. ë°ì´í„° ì›ë³¸ ë§í¬: https://www.kaggle.com/jiriroz/qa-jokesì´ 3ë§Œ 8ì²œê°œì˜ ë¬¸ì¥ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°, ì „ì²˜ë¦¬12345678import osimport warningsfrom gensim.models import doc2vecfrom gensim.models.doc2vec import TaggedDocumentimport pandas as pdfaqs = pd.read_csv(os.path.join(&#x27;data&#x27;,&#x27;jokes.csv&#x27;))faqs í•œêµ­ì–´ì™€ ë‹¤ë¥´ê²Œ ì˜ì–´ëŠ” ë„ì–´ì“°ê¸°ë¡œ ë‹¨ì–´ê°€ ì˜ êµ¬ë¶„ë˜ê¸° ë•Œë¬¸ì— í˜•íƒœì†Œ ë¶„ì„ì€ ìƒëµí•œë‹¤. í˜•íƒœì†Œ ë¶„ì„ì„ í•˜ì§€ ì•Šì•„ë„ ë„ì–´ì“°ê¸°ë¡œ splití•˜ë©´ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ì˜ ì§¤ë¦¬ê¸° ë•Œë¬¸ì´ë‹¤. í•˜ì§€ë§Œ ì˜ì–´ ë‹¨ì–´ë¥¼ ì›í˜•ìœ¼ë¡œ ë§Œë“¤ì–´ ì£¼ëŠ” lemmatizationì´ë‚˜ theë‚˜ aê°™ì€ ê´€ì‚¬ë¥¼ ì œê±°í•˜ëŠ” stopword ì œê±°ëŠ” í•´ì¤€ë‹¤. 12345678910from nltk.tokenize import word_tokenizefrom nltk.stem import WordNetLemmatizerfrom nltk.corpus import stopwordsimport nltknltk.download(&#x27;punkt&#x27;)# í† í°í™”tokened_questions = [word_tokenize(question.lower()) for question in faqs[&#x27;Question&#x27;]]tokened_questions 123456789101112131415[[&#x27;did&#x27;, &#x27;you&#x27;, &#x27;hear&#x27;, &#x27;about&#x27;, &#x27;the&#x27;, &#x27;native&#x27;, &#x27;american&#x27;, &#x27;man&#x27;, &#x27;that&#x27;, &#x27;drank&#x27;, &#x27;200&#x27;, &#x27;cups&#x27;, &#x27;of&#x27;, &#x27;tea&#x27;, &#x27;?&#x27;], ëŒ€ë¬¸ìë¥¼ ëª¨ë‘ ì†Œë¬¸ìë¡œ ë°”ê¿”ì£¼ê³  í† í°í™”ë¥¼ í•œ ë‹¤ìŒ ë„ì–´ì“°ê¸°ë¡œ ì¶œë ¥í•´ì£¼ì—ˆë‹¤. 12345678910lemmatizer = WordNetLemmatizer()nltk.download(&#x27;wordnet&#x27;)# lemmatizationlemmed_questions = [[lemmatizer.lemmatize(word) for word in doc] for doc in tokened_questions]lemmed_questionsnltk.download(&#x27;stopwords&#x27;)# stopword ì œê±° ë¶ˆìš©ì–´ ì œê±°í•˜ê¸°stop_words = stopwords.words(&#x27;english&#x27;)questions = [[w for w in doc if not w in stop_words] for doc in lemmed_questions]questions 123456789101112131415**[[&#x27;hear&#x27;, &#x27;native&#x27;, &#x27;american&#x27;, &#x27;man&#x27;, &#x27;drank&#x27;, &#x27;200&#x27;, &#x27;cup&#x27;, &#x27;tea&#x27;, &#x27;?&#x27;], [&quot;&#x27;s&quot;, &#x27;best&#x27;, &#x27;anti&#x27;, &#x27;diarrheal&#x27;, &#x27;prescription&#x27;, &#x27;?&#x27;], [&#x27;call&#x27;, &#x27;person&#x27;, &#x27;outside&#x27;, &#x27;door&#x27;, &#x27;ha&#x27;, &#x27;arm&#x27;, &#x27;leg&#x27;, &#x27;?&#x27;], [&#x27;star&#x27;, &#x27;trek&#x27;, &#x27;character&#x27;, &#x27;member&#x27;, &#x27;magic&#x27;, &#x27;circle&#x27;, &#x27;?&#x27;], [&quot;&#x27;s&quot;, &#x27;difference&#x27;, &#x27;bullet&#x27;, &#x27;human&#x27;, &#x27;?&#x27;], [&#x27;wa&#x27;, &#x27;ethiopian&#x27;, &#x27;baby&#x27;, &#x27;cry&#x27;, &#x27;?&#x27;], [&quot;&#x27;s&quot;, &#x27;difference&#x27;, &#x27;corn&#x27;, &#x27;husker&#x27;, &#x27;epilepsy&#x27;, &#x27;hooker&#x27;, &#x27;dysentery&#x27;, &#x27;?&#x27;], [&#x27;2016&#x27;, &quot;&#x27;s&quot;, &#x27;biggest&#x27;, &#x27;sellout&#x27;, &#x27;?&#x27;],** lemmatizationí•˜ê³  ë¶ˆìš©ì–´ë¥¼ ì œê±°í•˜ê³  ë‚œ ë‹¤ìŒì˜ ê²°ê³¼ë¬¼. ì´ì œ ëª¨ë“  ì „ì²˜ë¦¬ê°€ ëë‚¬ìœ¼ë‹ˆ TaggedDocumentë¡œ ë³€í˜•ì‹œí‚¤ê³  ë‚˜ì„œ doc2vecì— ë„£ì–´ì¤€ë‹¤. 12345678# ë¦¬ìŠ¤íŠ¸ì—ì„œ ê° ë¬¸ì¥ë¶€ë¶„ í† í°í™”index_questions = []for i in range(len(faqs)): index_questions.append([questions[i], i ])# Doc2Vecì—ì„œ ì‚¬ìš©í•˜ëŠ” íƒœê·¸ë¬¸ì„œí˜•ìœ¼ë¡œ ë³€ê²½tagged_questions = [TaggedDocument(d, [int(c)]) for d, c in index_questions] doc2vec ëª¨ë¸í™”doc2vecì„ í›ˆë ¨í•˜ê¸° ì „ì— ëª¨ë¸ì— ë³€í˜•ì„ ì£¼ì—ˆë‹¤. forë¬¸ë„ ë¹¼ê³  íŒŒë¼ë¯¸í„°ë„ ë³€ê²½í•´ ì£¼ì—ˆë‹¤. 123456789101112131415161718192021222324252627# make modelimport multiprocessingcores = multiprocessing.cpu_count()d2v_faqs = doc2vec.Doc2Vec(vector_size=200, # alpha=0.025, # min_alpha=0.025, hs=1, negative=0, dm=0, dbow_words = 1, min_count = 5, workers = cores, seed=0, epochs=20)d2v_faqs.build_vocab(tagged_questions)d2v_faqs.train(tagged_questions, total_examples = d2v_faqs.corpus_count, epochs = d2v_faqs.epochs ) # # train document vectors # for epoch in range(50): # d2v_faqs.train(tagged_faqs, # total_examples = d2v_faqs.corpus_count, # epochs = d2v_faqs.epochs # ) # d2v_faqs.alpha -= 0.0025 # decrease the learning rate # d2v_faqs.min_alpha = d2v_faqs.alpha # fix the learning rate, no decay 1234567# í…ŒìŠ¤íŠ¸í•˜ëŠ” ë¬¸ì¥ë„ ê°™ì€ ì „ì²˜ë¦¬ë¥¼ í•´ì¤€ë‹¤.test_string = &quot;What&#x27;s the best anti diarrheal prescription?&quot;tokened_test_string = word_tokenize(test_string)lemmed_test_string = [lemmatizer.lemmatize(word) for word in tokened_test_string]test_string = [w for w in lemmed_test_string if not w in stop_words]test_string 12345678910111213141516# ì„±ëŠ¥ ì¸¡ì •raten = 5found = 0for i in range(len(faqs)): tstr = faqs[&#x27;Question&#x27;][i] tokened_test_string = word_tokenize(tstr) lemmed_test_string = [lemmatizer.lemmatize(word) for word in tokened_test_string] ttok = [w for w in lemmed_test_string if not w in stop_words] tvec = d2v_faqs.infer_vector(ttok) re = d2v_faqs.docvecs.most_similar([tvec], topn = raten) for j in range(raten): if i == re[j][0]: found = found + 1 breakprint(&quot;ì •í™•ë„ = &#123;&#125; % (&#123;&#125;/&#123;&#125; ) &quot;.format(found/len(faqs),found, len(faqs))) 1ì •í™•ë„ = 0.8626303274190598 % (33012/38269 ) Local path :C:\\Users\\jmj30\\Dropbox\\ì¹´ë©”ë¼ ì—…ë¡œë“œ\\Documentation\\2022\\2022 ìƒë°˜ê¸°\\íœ´ë¨¼êµìœ¡ì„¼í„°\\mj_chatbot_prac\\faq_chatbot Reference: https://cholol.tistory.com/466","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"}],"tags":[{"name":"Chatbot","slug":"Chatbot","permalink":"https://jmj3047.github.io/tags/Chatbot/"},{"name":"NLP","slug":"NLP","permalink":"https://jmj3047.github.io/tags/NLP/"},{"name":"Doc2vec","slug":"Doc2vec","permalink":"https://jmj3047.github.io/tags/Doc2vec/"}]},{"title":"Making English Chatbot with Django(4)","slug":"Making_English_Chatbot_with_Django(4)","date":"2022-05-05T15:00:00.000Z","updated":"2022-05-06T08:28:34.722Z","comments":true,"path":"2022/05/06/Making_English_Chatbot_with_Django(4)/","link":"","permalink":"https://jmj3047.github.io/2022/05/06/Making_English_Chatbot_with_Django(4)/","excerpt":"","text":"ì‹¤ì œ ì„œë¹„ìŠ¤ êµ¬í˜„í•´ë³´ê¸°**code: https://github.com/jmj3047/faq_chatbot_example.git vs codeë¡œ django ì„¤ì •í•˜ê¸°: https://integer-ji.tistory.com/81 ì±„íŒ…ì°½ ë§Œë“¤ê¸° html&#x2F;cssë¥¼ ì‚¬ìš©í•´ ê°„ë‹¨í•œ ì±„íŒ…í™”ë©´ì„ ë§Œë“¤ì—ˆë‹¤. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465&lt;!-- //templates/addresses/chat_test.html --&gt;&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;/static/jquery-3.2.1.min.js&quot;&gt;&lt;/script&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt; &lt;link rel=&quot;preconnect&quot; href=&quot;https://fonts.googleapis.com&quot;&gt; &lt;link rel=&quot;preconnect&quot; href=&quot;https://fonts.gstatic.com&quot; crossorigin&gt; &lt;link href=&quot;https://fonts.googleapis.com/css2?family=Inconsolata:wght@400;600&amp;display=swap&quot; rel=&quot;stylesheet&quot;&gt;&lt;/head&gt;&lt;style&gt;* &#123;font-family: &#x27;Inconsolata&#x27;, monospace;&#125;.chat_wrap &#123;display:none;width: 350px;height: 500px;position: fixed;bottom: 30px;right: 95px;background: #a9bdce;&#125;.chat_content &#123;font-size:16pt; position:relative; height: 600px;width: 500px;overflow-y:scroll;padding:10px 15px;background: cornflowerblue&#125;.chat_input &#123;border:solid 0.5px lightgray; padding:2px 5px;&#125;.chat_header &#123;padding: 10px 15px; width: 500px; border-bottom: 1px solid #95a6b4;&#125;.chat_header .close_btn &#123;border: none;background: lightgray;float: right;&#125;.send_btn &#123;border: none; background: #ffeb33;height: 100%; color: #0a0a0a;&#125;.msg_box:after &#123;content: &#x27;&#x27;;display: block;clear:both;&#125;.msg_box &gt; span &#123;padding: 3px 5px;word-break: break-all;display: block;max-width: 300px;margin-bottom: 10px;border-radius: 4px&#125;.msg_box.send &gt; span &#123;background:#ffeb33;float: right;&#125;.msg_box.receive &gt; span &#123;background:#fff;float: left;&#125;&lt;/style&gt;&lt;body&gt;&lt;div class=&quot;chat_header&quot;&gt; &lt;span style=&quot;font-size:20pt;&quot;&gt;EDITH&lt;/span&gt; &lt;button type=&quot;button&quot; id=&quot;close_chat_btn&quot; class=&quot;close_btn&quot;&gt;X&lt;/button&gt;&lt;/div&gt;&lt;div id=&quot;divbox&quot; class=&quot;chat_content&quot;&gt;&lt;/div&gt;&lt;form id=&quot;form&quot; style=&quot;display: inline&quot;&gt; &lt;input type=&quot;text&quot; placeholder=&quot;write message..&quot; name=&quot;input1&quot; class=&quot;chat_input&quot; id=&quot;input1&quot; size=&quot;74&quot; style=&quot;margin:-3px; display: inline; width: 468px; height: 32px; font-size: 16pt;&quot; /&gt; &lt;input type=&quot;button&quot; value=&quot;SEND&quot; id=&quot;btn_submit&quot; class=&quot;send_btn&quot; style=&quot;margin:-5px; display: inline; width: 53px; height: 38px; font-size: 14pt;&quot; /&gt;&lt;/form&gt;&lt;script&gt; $(&#x27;#btn_submit&#x27;).click(function () &#123; send(); &#125;); $(&#x27;#form&#x27;).on(&#x27;submit&#x27;, function(e)&#123; e.preventDefault(); send(); &#125;); $(&#x27;#close_chat_btn&#x27;).on(&#x27;click&#x27;, function()&#123; $(&#x27;#chat_wrap&#x27;).hide().empty(); &#125;); function send()&#123; $(&#x27;#divbox&#x27;).append(&#x27;&lt;div class=&quot;msg_box send&quot;&gt;&lt;span&gt;&#x27;+$(&#x27;#input1&#x27;).val()+&#x27;&lt;span&gt;&lt;/div&gt;&#x27;); $(&quot;#divbox&quot;).scrollTop($(&quot;#divbox&quot;)[0].scrollHeight); console.log(&quot;serial&quot;+$(&#x27;form&#x27;).serialize()) $.ajax(&#123; url: &#x27;http://127.0.0.1:8000/chat_service/&#x27;, //ì±—ë´‡ api url type: &#x27;post&#x27;, dataType: &#x27;json&#x27;, data: $(&#x27;form&#x27;).serialize(), success: function(data) &#123; &lt;!--$(&#x27;#reponse&#x27;).html(data.reponse);--&gt; $(&#x27;#divbox&#x27;).append(&#x27;&lt;div class=&quot;msg_box receive&quot;&gt;&lt;span&gt;&#x27;+ data.response +&#x27;&lt;span&gt;&lt;/div&gt;&#x27;); $(&quot;#divbox&quot;).scrollTop($(&quot;#divbox&quot;)[0].scrollHeight); &#125; &#125;); $(&#x27;#input1&#x27;).val(&#x27;&#x27;); &#125;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; ê°„ë‹¨í•˜ê²Œ ì„¤ëª…í•˜ë©´ Djangoë¡œ restfulAPIë¥¼ êµ¬í˜„í•˜ê¸° ìœ„í•œ ì†ŒìŠ¤ ìœ„ì— ì±—ë´‡ì„ ë¶™ì´ê¸° ìœ„í•œ í™”ë©´ê³¼ ëª¨ë¸ì´ ë“¤ì–´ê°€ ìˆëŠ” ë²„ì „ì´ë‹¤. ìœ„ì— ì†ŒìŠ¤ëŠ” í™”ë©´ ì—­í• ì„ í•˜ëŠ” chat_test.html íŒŒì¼ì´ë‹¤. jquery ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í–ˆê¸° ë•Œë¬¸ì— jqueryë¥¼ import í•´ì•¼ í•œë‹¤. jquery fileì´ static í´ë”ì— ìˆì–´ì•¼ í•œë‹¤. jqueryëŠ” ì†ŒìŠ¤ í•˜ë‹¨ë¶€ì— ìˆëŠ” scriptë¥¼ ìœ„í•´ í•„ìš”í•˜ë‹¤. ì±„íŒ…ì—ì„œ ì „ì†¡ ë²„íŠ¼ì„ ëˆ„ë¥´ê±°ë‚˜ ì—”í„°ë¥¼ ëˆ„ë¥´ë©´ send()ë¼ëŠ” í•¨ìˆ˜ê°€ ì‹¤í–‰ë˜ê³  ì´ í•¨ìˆ˜ëŠ” ajaxë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ë°›ì•„ì˜¤ëŠ” APIë¥¼ í˜¸ì¶œí•œë‹¤. ì—¬ê¸°ì„œëŠ” localhost&#x2F;chat_serviceë¥¼ í˜¸ì¶œí•œë‹¤. ì±„íŒ…ì„ ìœ„í•œ APIí™”ë©´ì´ ë§Œë“¤ì–´ì¡Œìœ¼ë©´ ì´ì œ ì§ˆë¬¸ì„ ë°›ì•„ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” APIë¥¼ ë§Œë“ ë‹¤. ì•„ì§ FAQë°ì´í„°ë¥¼ í•™ìŠµí•œ ëª¨ë¸ì€ ë„£ì§€ ì•Šì•˜ìœ¼ë‹ˆ ì¸í’‹ì´ ë“¤ì–´ì˜¤ë©´ ë”ë¯¸ë°ì´í„°(dummy)ë¥¼ ë¦¬í„´í•˜ëŠ” APIë¥¼ ë§Œë“ ë‹¤. ì´ëŸ° API ë™ì‘ë“¤ì€ view.pyì—ì„œ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤. 1234567891011#faq_chatbot_example/addresses/views.py@csrf_exemptdef chat_service(request): if request.method == &#x27;POST&#x27;: input1 = request.POST[&#x27;input1&#x27;] output = dict() output[&#x27;response&#x27;] = &quot;ì´ê±´ ì‘ë‹µ&quot; return HttpResponse(json.dumps(output), status=200) else: return render(request, &#x27;addresses/chat_test.html&#x27;) Django í”„ë¡œì íŠ¸ ì•ˆì— addresses ì•±ì— ìˆëŠ” views.pyë¥¼ ë³´ë©´ chat_service í•¨ìˆ˜ë¥¼ ë§Œë“¤ì—ˆë‹¤. POSTí˜•ì‹ìœ¼ë¡œ ì½œì´ ì˜¤ë©´ responseì— ì•„ì›ƒí’‹ ë©”ì„¸ì§€ë¥¼ ë‹´ì•„ì„œ jsoní˜•íƒœë¡œ ë¦¬í„´í•œë‹¤. views.pyì— í•¨ìˆ˜ë¥¼ ë§Œë“¤ê³  urlë¡œ ì—°ê²°í•˜ê¸° ìœ„í•´ì„œ urls.pyì— chat_serviceë¥¼ ì…ë ¥í•œë‹¤. 123456789101112###django 3.8.3 ë²„ì „ ë§ì¶°ì¤˜ì•¼ í•¨#/faq_chatbot_example/restfulapiserver/urls.py# from django.conf.urls import url, includefrom addresses import viewsfrom django.urls import path, re_path, includefrom django.contrib import adminurlpatterns = [ ... path(&#x27;chat_service/&#x27;, views.chat_service), ...] urls.pyì—ì„œ ~&#x2F;chat_serviceë¥¼ views.chat_serviceì— ì—°ê²°ì‹œí‚¨ë‹¤. ì´ì œ ~&#x2F;chat_serviceë¡œ ì½œí•˜ë©´ views.chat_serviceê°€ ì‹¤í–‰ëœë‹¤. ì•„ê¹Œ ìœ„ì—ì„œ ë§Œë“  ì±„íŒ…í˜ì´ì§€ì— ì „ì†¡ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ ajaxë¥¼ ì´ìš©í•´ chat_serviceë¥¼ í˜¸ì¶œí–ˆë‹¤. ì •ìƒì ìœ¼ë¡œ ë˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸ í•´ë³¸ë‹¤. FAQ ëª¨ë¸ ë„£ê¸°addresses ì•± ì•ˆì— ìƒˆë¡œìš´ py ëª¨ë¸ì„ ë§Œë“¤ì–´ì„œ ë„£ê¸° 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374#/faq_chatbot_example/addresses/faq_chatbot.pyfrom gensim.models import doc2vec, Doc2Vecfrom gensim.models.doc2vec import TaggedDocumentimport pandas as pdfrom nltk.tokenize import word_tokenizefrom nltk.stem import WordNetLemmatizerfrom nltk.corpus import stopwordsimport nltk# íŒŒì¼ë¡œë¶€í„° ëª¨ë¸ì„ ì½ëŠ”ë‹¤. ì—†ìœ¼ë©´ ìƒì„±í•œë‹¤.try: d2v_faqs = Doc2Vec.load(&#x27;d2v_faqs_size200_min5_epoch20_jokes.model&#x27;) lemmatizer = WordNetLemmatizer() stop_words = stopwords.words(&#x27;english&#x27;) faqs = pd.read_csv(&#x27;jokes.csv&#x27;)except: faqs = pd.read_csv(&#x27;jokes.csv&#x27;) nltk.download(&#x27;punkt&#x27;) # í† ê·¼í™” tokened_questions = [word_tokenize(question.lower()) for question in faqs[&#x27;Question&#x27;]] lemmatizer = WordNetLemmatizer() nltk.download(&#x27;wordnet&#x27;) # lemmatization lemmed_questions = [[lemmatizer.lemmatize(word) for word in doc] for doc in tokened_questions] nltk.download(&#x27;stopwords&#x27;) # stopword ì œê±° ë¶ˆìš©ì–´ ì œê±°í•˜ê¸° stop_words = stopwords.words(&#x27;english&#x27;) questions = [[w for w in doc if not w in stop_words] for doc in lemmed_questions] # ë¦¬ìŠ¤íŠ¸ì—ì„œ ê° ë¬¸ì¥ë¶€ë¶„ í† í°í™” index_questions = [] for i in range(len(faqs)): index_questions.append([questions[i], i ]) # Doc2Vecì—ì„œ ì‚¬ìš©í•˜ëŠ” íƒœê·¸ë¬¸ì„œí˜•ìœ¼ë¡œ ë³€ê²½ tagged_questions = [TaggedDocument(d, [int(c)]) for d, c in index_questions] # make model import multiprocessing cores = multiprocessing.cpu_count() d2v_faqs = doc2vec.Doc2Vec( vector_size=200, hs=1, negative=0, dm=0, dbow_words=1, min_count=5, workers=cores, seed=0, epochs=20 ) d2v_faqs.build_vocab(tagged_questions) d2v_faqs.train(tagged_questions, total_examples=d2v_faqs.corpus_count, epochs=d2v_faqs.epochs) d2v_faqs.save(&#x27;d2v_faqs_size200_min5_epoch20_jokes.model&#x27;)# FAQ ë‹µë³€def faq_answer(input): # í…ŒìŠ¤íŠ¸í•˜ëŠ” ë¬¸ì¥ë„ ê°™ì€ ì „ì²˜ë¦¬ë¥¼ í•´ì¤€ë‹¤. tokened_test_string = word_tokenize(input) lemmed_test_string = [lemmatizer.lemmatize(word) for word in tokened_test_string] test_string = [w for w in lemmed_test_string if not w in stop_words] topn = 5 test_vector = d2v_faqs.infer_vector(test_string) result = d2v_faqs.docvecs.most_similar([test_vector], topn=topn) print(result) for i in range(topn): print(&quot;&#123;&#125;ìœ„. &#123;&#125;, &#123;&#125; &#123;&#125; &#123;&#125;&quot;.format(i + 1, result[i][1], result[i][0], faqs[&#x27;Question&#x27;][result[i][0]], faqs[&#x27;Answer&#x27;][result[i][0]])) return faqs[&#x27;Answer&#x27;][result[0][0]]faq_answer(&quot;What do you call a person who is outside a door and has no arms nor legs?&quot;) ìœ„ ì†ŒìŠ¤ì—ì„œ ìƒë‹¨ì— ìˆëŠ” ëª¨ë¸ì„ ë§Œë“œëŠ” ì½”ë“œëŠ” API ì„œë²„ë¥¼ ì‹¤í–‰í•˜ëŠ” ì‹œì ì—ì„œ í˜¸ì¶œëœë‹¤. ë¬´ì¡°ê±´ í˜¸ì¶œí•˜ëŠ” ê±´ ì•„ë‹ˆê³  views.pyì—ì„œ importë¥¼ ì¨ ë„£ìœ¼ë©´ ìµœì´ˆ 1ë²ˆì€ ì‹¤í–‰ë˜ê²Œ ëœë‹¤. ì±„íŒ… ì›¹í˜ì´ì§€ë¡œë¶€í„° faq_chatbot.pyì— ìˆëŠ” faq_answerë¥¼ í˜¸ì¶œí•˜ëŠ” ê²ƒ ê¹Œì§€ flowë¥¼ ê·¸ë ¤ë³´ë©´ chat_test.htmlâ†’view.py(chat_service)â†’faq_chatbot.py(faq_answer) ìˆœì„œì´ë‹¤. ë”°ë¼ì„œ views.pyì—ì„œ faq_answerí•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ê¸° ìœ„í•´ importë¥¼ í•˜ê²Œ ë˜ëŠ”ë° djangoëŠ” ìµœì´ˆ ì‹¤í–‰ì‹œ views.pyë¥¼ í•œë²ˆ ì½ê¸° ë•Œë¬¸ì— faq_chatbot.pyì— ì ì–´ë†“ì€ ì†ŒìŠ¤ê°€ í•œë²ˆ ì‹¤í–‰ë˜ê²Œ ëœë‹¤. ë§¤ë²ˆì„œë²„ë¥¼ ì‹¤í–‰í•  ë•Œë§ˆë‹¤ ëª¨ë¸ì„ ìƒˆë¡œ ë§Œë“¤ê²Œ ë˜ë©´ ì„œë²„ ê¸°ë™ ì†ë„ê°€ ëŠë ¤ì§€ê³  ë¹„íš¨ìœ¨ì ì´ê¸° ë•Œë¬¸ì— ëª¨ë¸ì„ ë§Œë“¤ê³  ë‚˜ì„œ íŒ¨ì¼ë¡œ ì €ì¥í•˜ê³ , ë§Œë“¤ì–´ì§„ íŒŒì¼ì´ ì—†ë‹¤ë©´ ëª¨ë¸ì„ ìƒì„±í•˜ë„ë¡ try&#x2F;exceptë¥¼ ì‚¬ìš©í–ˆë‹¤. ì¶”ê°€ì ìœ¼ë¡œ í”„ë¡œì íŠ¸ìƒ ì†ŒìŠ¤ê°€ ì‹¤í–‰ë˜ê¸° ë•Œë¬¸ì— íŒŒì¼ê²½ë¡œëŠ” rootì´ë‹¤. jokes.csvê°€ ìˆì–´ì•¼ í•  ê³³ê³¼ ëª¨ë¸ì´ ìƒì„±ë˜ëŠ” ê³³ì˜ ê²½ë¡œëŠ” í”„ë¡œì íŠ¸ì˜ rootí´ë”ì´ë‹¤. ì ì´ì œ ì§ˆë¬¸ì˜ ë‹µì„ ì°¾ì•„ì£¼ëŠ” í•¨ìˆ˜ê°€ ë§Œë“¤ì–´ ì¡Œìœ¼ë‹ˆ ì•„ê¹Œ ë”ë¯¸ ë°ì´í„°ë¡œ ë¦¬í„´í•´ì£¼ë˜ views.pyì˜ í•¨ìˆ˜ë¥¼ ë°”ê¿”ë³´ì. 123456789101112#faq_chatbot_example/addresses/views.py@csrf_exemptdef chat_service(request): if request.method == &#x27;POST&#x27;: input1 = request.POST[&#x27;input1&#x27;] response = faq_answer(input1) output = dict() output[&#x27;response&#x27;] = response return HttpResponse(json.dumps(output), status=200) else: return render(request, &#x27;addresses/chat_test1.html&#x27;) ì´ì „ì—ëŠ” responseì— ë¬´ì¡°ê±´ ë”ë¯¸ ì‘ë‹µì„ ë³´ëƒˆëŠ”ë° ì´ì œëŠ” faq_answerí•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ í•´ë‹¹ ì§ˆë¬¸ì— ì•Œë§ì€ ì •ë‹µì„ ê°€ì ¸ì˜¨ë‹¤. faq_answerí•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì œì¼ ìƒë‹¨ì— from .faq_chatbot import faq_answerë¥¼ ì„ ì–¸í•´ì•¼ í•œë‹¤. ì‹¤í–‰ ê²°ê³¼(html íŒŒì¼ ìˆ˜ì •) **code: â€£https://github.com/jmj3047/faq_chatbot_example.git Reference: https://cholol.tistory.com/478","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Django","slug":"Python/Django","permalink":"https://jmj3047.github.io/categories/Python/Django/"}],"tags":[{"name":"Chatbot","slug":"Chatbot","permalink":"https://jmj3047.github.io/tags/Chatbot/"},{"name":"NLP","slug":"NLP","permalink":"https://jmj3047.github.io/tags/NLP/"},{"name":"Doc2vec","slug":"Doc2vec","permalink":"https://jmj3047.github.io/tags/Doc2vec/"}]},{"title":"Making Korean Chatbot with doc2vec(2)","slug":"Making_Korean_Chatbot_with_doc2vec(2)","date":"2022-05-05T15:00:00.000Z","updated":"2022-05-06T08:25:29.874Z","comments":true,"path":"2022/05/06/Making_Korean_Chatbot_with_doc2vec(2)/","link":"","permalink":"https://jmj3047.github.io/2022/05/06/Making_Korean_Chatbot_with_doc2vec(2)/","excerpt":"","text":"ëª¨ë¸ ë‹¤ë“¬ê¸°FAQë°ì´í„° ëŠ˜ë¦¬ê¸°ë” ë§ì€ í•™ìŠµ ë°ì´í„°ë¡œ ëª¨ë¸ì„ í•™ìŠµí•œë‹¤. ë°ì´í„° ì›ë³¸ ë§í¬: https://www.data.go.kr/dataset/3068685/fileData.do 123456789import osimport warningsfrom gensim.models import doc2vecfrom gensim.models.doc2vec import TaggedDocumentimport pandas as pdfaqs = pd.read_csv(os.path.join(&#x27;data&#x27;,&#x27;kor_elec_faq2.csv&#x27;), encoding=&#x27;CP949&#x27;)faqsfaqs[[&#x27;ìˆœë²ˆ&#x27;, &#x27;ì œëª©&#x27;, &#x27;ë‚´ìš©&#x27;]] pandasë¥¼ ì‚¬ìš©í•´ csvíŒŒì¼ì„ ë°”ë¡œ ì½ì–´ì¤€ë‹¤. utf-8 ì¸ì½”ë”© ë¬¸ì œë¡œ ì—ëŸ¬ê°€ ë‚˜ë©´ cp949ë¡œ ë„£ì–´ì¤€ë‹¤. ì „ì²´ í•„ë“œì—ì„œ í•„ìš”í•œ indexì™€ ì§ˆë¬¸(ì—¬ê¸°ì„œëŠ” ì œëª©), ë‹µë³€(ì—¬ê¸°ì„œëŠ” ë‚´ìš©)ë§Œ ë½‘ì•„ë‚¸ë‹¤. ì´ 351ê°œì˜ ì§ˆë¬¸ê³¼ ë‹µë³€ ë°ì´í„°ê°€ ìƒê²¼ìœ¼ë‹ˆ ì „ ê²Œì‹œë¬¼ê³¼ ë™ì¼í•œ ë°©ë²•ìœ¼ë¡œ ëª¨ë¸í•™ìŠµì„ ì‹œí‚¨ë‹¤. ì „ ë°ì´í„°ëŠ” pandasë°ì´í„°ê°€ ì•„ë‹ˆì—ˆê¸° ë•Œë¬¸ì— ìˆ˜ì •í•´ ì¤€ë‹¤. 1234567# ë¦¬ìŠ¤íŠ¸ì—ì„œ ê° ë¬¸ì¥ë¶€ë¶„ í† í°í™”token_faqs = []for i in range(len(faqs)): token_faqs.append([tokenize_kkma_noun(faqs[&#x27;ì œëª©&#x27;][i]), faqs[&#x27;ìˆœë²ˆ&#x27;][i]])# Doc2Vecì—ì„œ ì‚¬ìš©í•˜ëŠ” íƒœê·¸ë¬¸ì„œí˜•ìœ¼ë¡œ ë³€ê²½tagged_faqs = [TaggedDocument(d, [c]) for d, c in token_faqs] ì´ë ‡ê²Œ í•˜ê³  ëª¨ë¸ì„ ëŒë ¤ë„ ì„±ëŠ¥ì´ ì¢‹ì§€ ì•ŠìŒ. doc2vecëª¨ë¸ì˜ ê²½ìš° ìµœì†Œ ë§Œë‹¨ìœ„ì˜ ë¬¸ì¥ì´ ìˆì–´ì•¼ ì œëŒ€ë¡œ ë‚˜ì˜¨ë‹¤. íŠœë‹ ì‹œë„í•´ë³´ê¸°ì´ì „ê¹Œì§€ ë°ì´í„° ë‚´ì— ìˆëŠ” ìˆœë²ˆì„ ì¸ë±ìŠ¤ë¡œ ì‚¬ìš©í–ˆëŠ”ë° ì •ìƒì ìœ¼ë¡œ ì‘ë™í•˜ì§€ ì•Šì•„ ë‹¤ì‹œ ë§Œë“¤ì–´ì¤€ë‹¤. 123456789# ë¦¬ìŠ¤íŠ¸ì—ì„œ ê° ë¬¸ì¥ë¶€ë¶„ í† í°í™”token_faqs = []for i in range(len(faqs)): token_faqs.append([tokenize_kkma_noun(faqs[&#x27;ì œëª©&#x27;][i]), i ]) # token_faqs.append([tokenize_kkma_noun(faqs[&#x27;ì œëª©&#x27;][i]), faqs[&#x27;ìˆœë²ˆ&#x27;][i]])# Doc2Vecì—ì„œ ì‚¬ìš©í•˜ëŠ” íƒœê·¸ë¬¸ì„œí˜•ìœ¼ë¡œ ë³€ê²½tagged_faqs = [TaggedDocument(d, [int(c)]) for d, c in token_faqs]# tagged_faqs = [TaggedDocument(d, [c]) for d, c in token_faqs] ë¬¸ì„œ ì›ë³¸ì„ ìˆ˜ì •í•  í•„ìš”ëŠ” ì—†ê³  TaggedDocumentë§Œë“¤ ë•Œë§Œ ì˜ ë„£ì–´ì¤€ë©´ ëœë‹¤. ê¸°ì¡´ì—ëŠ” faqs[â€™ìˆœë²ˆâ€™][i]ë¥¼ íƒœê·¸ ê°’ìœ¼ë¡œ ë„£ì–´ì£¼ì—ˆëŠ”ë° ê·¸ëƒ¥ ië¥¼ ë„£ëŠ”ë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ì¢‹ì€ ì ì´ ì›ë³¸ì˜ indexì™€ íƒœê·¸ê°’ì´ ê°™ì•„ì§€ê¸° ë•Œë¬¸ì— ë‚˜ì¤‘ì— ì›ë¬¸ì§ˆë¬¸ì„ ë³µì›í•  ë•Œ faqs[â€™ì œëª©â€™][tag]ë¡œ ì¶œë ¥ì´ ê°€ëŠ¥í•˜ë‹¤. ê·¸ë¦¬ê³  ì´ì œ ê°€ì¥ ë¨¼ì € í•  ê±°ëŠ” ì „ì²˜ë¦¬ë¥¼ ì¡°ê¸ˆ ìˆ˜ì •í•˜ëŠ” ê²ƒì´ë‹¤. í˜•íƒœì†Œ ë¶„ì„ì„ í•  ë•Œ í•„ìš” ì—†ëŠ” ë°ì´í„°ë¥¼ ì œì™¸ì‹œí‚¤ëŠ” ë°©ë²•ì´ë‹¤. ë³´í†µ ë¬¸ì¥ì—ì„œëŠ” ëª…ì‚¬ì™€ ë™ì‚¬ê°€ ì¤‘ìš”í•˜ê¸° ë•Œë¬¸ì— ëª…ì‚¬ ë™ì‚¬ ë¹¼ê³ ëŠ” ë‹¤ ë‚ ë ¤ë³¸ë‹¤. 1234567891011121314151617#íŠœë‹:í˜•íƒœì†Œ í•„í„°ë§kkma = Kkma()filter_kkma = [&#x27;NNG&#x27;, #ë³´í†µëª…ì‚¬ &#x27;NNP&#x27;, #ê³ ìœ ëª…ì‚¬ &#x27;OL&#x27; , #ì™¸êµ­ì–´ &#x27;VA&#x27;,&#x27;VV&#x27;,&#x27;VXV&#x27; ]def tokenize_kkma(doc): jpype.attachThreadToJVM() token_doc = [&#x27;/&#x27;.join(word) for word in kkma.pos(doc)] return token_docdef tokenize_kkma_noun(doc): jpype.attachThreadToJVM() token_doc = [&#x27;/&#x27;.join(word) for word in kkma.pos(doc) if word[1] in filter_kkma] return token_doc ì´ëŸ°ì‹ìœ¼ë¡œ filter_kkmaë¦¬ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ ë§Œë“¤ì–´ í˜•íƒœì†Œë¥¼ ë¶„ì„í–ˆì„ ë•Œ ë‚˜ì˜¤ëŠ” í˜•íƒœì†Œê°€ filter_kkmaì— í¬í•¨ë˜ì–´ ìˆì„ ê²½ìš°ë§Œ í•™ìŠµ ëŒ€ìƒì— ì¶”ê°€í•œë‹¤. tokenize_kkmaë¥¼ ì“°ë©´ ì „ì²´ í˜•íƒœì†Œ ë¶„ì„, tokenize_kkmk_nounì„ ì“°ë©´ ë™ì‚¬ ëª…ì‚¬ë§Œ ì¶”ì¶œí•œë‹¤. ê°€ì¥ ê²°ê³¼ê°€ ì¢‹ê²Œ ë‚˜ì˜¨ ì¡°í•©ì€ ëª…ì‚¬ë§Œ ì¶”ì¶œ, for ë¬¸ 50ë²ˆì— epochs&#x3D;100ìœ¼ë¡œ í•œ ê²°ê³¼ê°’. 12345678910111213141516#íŠœë‹:ëª…ì‚¬ë§Œ ì¶”ì¶œkkma = Kkma()filter_kkma = [&#x27;NNG&#x27;, #ë³´í†µëª…ì‚¬ &#x27;NNP&#x27;, #ê³ ìœ ëª…ì‚¬ &#x27;OL&#x27; , #ì™¸êµ­ì–´ ]def tokenize_kkma(doc): jpype.attachThreadToJVM() token_doc = [&#x27;/&#x27;.join(word) for word in kkma.pos(doc)] return token_docdef tokenize_kkma_noun(doc): jpype.attachThreadToJVM() token_doc = [&#x27;/&#x27;.join(word) for word in kkma.pos(doc) if word[1] in filter_kkma] return token_doc 123456789101112131415161718192021222324252627# make model import multiprocessing import tensorflow as tf with tf.device(&#x27;/device:GPU:0&#x27;): cores = multiprocessing.cpu_count() d2v_faqs = doc2vec.Doc2Vec(vector_size=20, #100 alpha=0.025, min_alpha=0.025, hs=1, negative=0, dm=0, window=3, dbow_words = 1, min_count = 1, workers = cores, seed=0, epochs=100) d2v_faqs.build_vocab(tagged_faqs) # train document vectors for epoch in range(50): d2v_faqs.train(tagged_faqs, total_examples = d2v_faqs.corpus_count, epochs = d2v_faqs.epochs ) d2v_faqs.alpha -= 0.0025 # decrease the learning rate d2v_faqs.min_alpha = d2v_faqs.alpha # fix the learning rate, no decay 123test_string = &quot;ë³€ì••ê¸°ê³µë™ì´ìš©(ëª¨ìê±°ë˜)ì´ë€ ë¬´ì—‡ì´ë©°, ìš”ê¸ˆê³„ì‚°ì€ ì–´ë–»ê²Œ í•©ë‹ˆê¹Œ&quot;tokened_test_string = tokenize_kkma_noun(test_string)tokened_test_string 12345678910111213# ì„±ëŠ¥ ì¸¡ì •# raten = 5 #ì •í™•ë„ = 0.5128205128205128 % (180/351 )raten = 1 #ì •í™•ë„ = 0.24216524216524216 % (85/351 ) found = 0for i in range(len(faqs)): tstr = faqs[&#x27;ì œëª©&#x27;][i] ttok = tokenize_kkma_noun(tstr) tvec = d2v_faqs.infer_vector(ttok) re = d2v_faqs.docvecs.most_similar([tvec], topn = raten) for j in range(raten): if i == re[j][0]: found = found + 1print(&quot;ì •í™•ë„ = &#123;&#125; % (&#123;&#125;/&#123;&#125; ) &quot;.format(found/len(faqs),found, len(faqs)) ëª¨ë¸ ì €ì¥, ë¶ˆëŸ¬ì˜¤ê¸°1234567891011121314151617# ëª¨ë¸ ì €ì¥d2v_faqs.save(os.path.join(&#x27;data&#x27;,&#x27;/d2v_faqs_size100_min1_batch50_epoch100_nounonly_dm0.model&#x27;))# ëª¨ë¸ loadd2v_faqs_1 = doc2vec.Doc2Vec.load(os.path.join(&#x27;data&#x27;,&#x27;/d2v_faqs_size100_min1_batch50_epoch100_nounonly_dm0.model&#x27;))#testtest_string = &quot;ê±´ë¬¼ì„ ìƒˆë¡œ ì§€ì„ ë•Œ ì„ì‹œì „ë ¥ì€ ì–´ë–»ê²Œ ì‹ ì²­í•˜ë‚˜ìš”&quot;tokened_test_string = tokenize_kkma_noun(test_string)tokened_test_stringtopn = 5# ëª¨ë¸ ì¶”ì¸¡test_vector1 = d2v_faqs_1.infer_vector(tokened_test_string)result1 = d2v_faqs_1.docvecs.most_similar([test_vector1], topn=topn)for i in range(topn): print(&quot;ëª¨ë¸ 1 &#123;&#125;ìœ„. &#123;&#125;, &#123;&#125; &#123;&#125;&quot;.format(i+1, result1[i][1], result1[i][0],faqs[&#x27;ì œëª©&#x27;][result1[i][0]] )) Local path :C:\\Users\\jmj30\\Dropbox\\ì¹´ë©”ë¼ ì—…ë¡œë“œ\\Documentation\\2022\\2022 ìƒë°˜ê¸°\\íœ´ë¨¼êµìœ¡ì„¼í„°\\mj_chatbot_prac\\faq_chatbot Reference: https://cholol.tistory.com/466","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"}],"tags":[{"name":"Chatbot","slug":"Chatbot","permalink":"https://jmj3047.github.io/tags/Chatbot/"},{"name":"NLP","slug":"NLP","permalink":"https://jmj3047.github.io/tags/NLP/"},{"name":"Doc2vec","slug":"Doc2vec","permalink":"https://jmj3047.github.io/tags/Doc2vec/"}]},{"title":"Support Vector Machine","slug":"SVM","date":"2022-05-05T15:00:00.000Z","updated":"2022-07-09T05:59:13.910Z","comments":true,"path":"2022/05/06/SVM/","link":"","permalink":"https://jmj3047.github.io/2022/05/06/SVM/","excerpt":"","text":"1. ë¶„ë¥˜ì— ëŒ€í•œ ìˆ˜ì  í‘œí˜„ í•™ìŠµ ë°ì´í„° X(ë…ë¦½ë³€ìˆ˜),Y(ì¢…ì†ë³€ìˆ˜)ê°€ ìˆì„ ë•Œ (i&#x3D;1,2,3,4,5 â€¦.ë°ì´í„°ì˜ ê°¯ìˆ˜) Yâ‡’{-1,1} (ë‘ ê°œì˜ í´ë˜ìŠ¤ë¥¼ ì˜ë¯¸) â‡’ ê²½ìš°ì— ë”°ë¼ì„œ, í´ë˜ìŠ¤ë¥¼ 1ê³¼ -1 ë¡œ ë‚˜ëˆ” Y(ì •ë‹µ) * F(x)(ì˜ˆì¸¡í•œ ì •ë‹µ) &gt;0 ë¼ëŠ” ê²ƒì€ ì œëŒ€ë¡œ ë¶„ë¥˜ëœ í˜•íƒœ ( ê°™ì€ ë¶€í˜¸ë¼ë¦¬ ê³±í•˜ë©´ ì–‘ìˆ˜ì¸ ê²½ìš°ë‹ˆê¹Œ) 2. ì„ í˜• ë¶„í• (Linear Classifier) f(x)&#x3D; W transpose X + b (ì„ í˜•ì¡°í•©, ê°ê°ì˜ í•­ë“¤ì´ ë”í•˜ê¸°ë¡œ ì´ë£¨ì–´ì§„ ì¡°í•©.) ì„ í˜•ë¶„í• ì€ ì§ì„ ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒ (2ì°¨ì›ì´ê±´ 3ì°¨ì›ì´ê±´ ê·¸ ì´ìƒì´ê±´ ìƒê´€ ì—†ìŒ) b(bias) Y ì ˆí¸ì„ ì˜ë¯¸ WëŠ” ì§ì„ ì˜ ê¸°ìš¸ê¸° 3. ì´ˆí‰ë©´ ë¶„í•  ë” ë‚˜ì€(ìµœì ) ë¶„ë¥˜ë¥¼ ìœ„í•œ ì´ˆí‰ë©´(Hyperplane)â†’ì„  ë³´ë‹¤ ë” í° ì°¨ì› ì¢‹ì€ íŒë³„ì„ ì— ëŒ€í•œ ê¸°ì¤€ ìµœì í™”: ì¢‹ì€ ê²ƒ ì„ ê·¹ëŒ€í™” ì‹œí‚¤ê³  ë‚˜ìœ ê²ƒ ì„ ê·¹ì†Œí™” ì‹œí‚¤ëŠ” ê²ƒ ë¶„ë¥˜ì—ì„œì˜ ìµœì í™”: ì˜ ì•ˆë‚˜ë‰˜ëŠ”ê²ƒ , ì˜ ë‚˜ë‰˜ëŠ” ê²ƒ ë‚˜ì¤‘ì— Testing data ë¥¼ ëŒë ¸ì„ë•Œ, ê°€ì¥ ì¢‹ê²Œ ë‚˜ë‰œ ê²ƒì€ ë°˜ì ˆë¡œ ë‚˜ë‰œ ì§ì„ ì´ë‹¤. Test data ê°€ ì–´ë–»ê²Œ ë“¤ì–´ì˜¬ì§€ ëª¨ë¥´ëŠ” ê²ƒ ì´ê¸°ë•Œë¬¸ì— , ê³¼ì í•© ë˜ì–´ ìˆëŠ” ê²ƒë³´ë‹¤ í™•ì‹¤íˆ ì ˆë°˜ìœ¼ë¡œ ë‚˜ëˆ„ëŠ”ê²ƒì´ ì¢‹ë‹¤. ìµœì ì˜ ë¶„í•  ì´ˆí‰ë©´ ì°¾ê¸° Margin: cëŠ” ì„ í˜•ë¶„í• ì˜ ê° í´ë˜ìŠ¤ë³„ ê±°ë¦¬ ê° í´ë˜ìŠ¤ë³„ ê±°ë¦¬ë¥¼ í•©ì¹œ ê²ƒ Margin&#x3D;2cë¥¼ ìµœëŒ€í™” í•˜ëŠ”, w T x +b&#x3D;0 ì˜ ì§ì„ ì„ ì°¾ì•„ì•¼ í•˜ëŠ”ê²ƒ ì´ë‹¤. Marign ì„ ìµœëŒ€í™” ì‹œí‚¤ëŠ” ì´ˆí‰ë©´ì´ ìµœì  â€œLearning Theoryâ€ ì— ë”°ë¥´ë©´, Mariginì„ ìµœëŒ€í™” ì‹œí‚¤ëŠ” ì´ˆí‰ë©´ì´ ì¼ë°˜í™” ì˜¤ë¥˜ê°€ ê°€ì¥ ë‚®ê²Œ ë‚˜íƒ€ë‚¨(Test data ì—ì„œë„ ì¢‹ì€ ì ìˆ˜ê°€ ë‚˜ì˜¨ë‹¤) Margin:ì´ˆí‰ë©´ê³¼ ê°€ì¥ ê·¼ì ‘í•œ ê° í´ë˜ìŠ¤ ê´€ì¸¡ì¹˜ì™€ì˜ ê±°ë¦¬ì˜ í•©. Margin ìˆ˜ì‹ ìœ ë„ ì¼ë°˜ì ì¸ ë°©ë²• ì ê³¼ ì„  ì‚¬ì´ì˜ ê±°ë¦¬ ê±°ë¦¬ d ê°€ 2ê°œì´ë‹ˆê¹Œ 2&#x2F;||W|| Margin ìµœëŒ€í™” (ìµœì í™”) ||w|| ê°€ ë¶„ëª¨ì— ìˆê¸° ë•Œë¬¸ì— ê²°êµ­ ||w|| ë¥¼ ìµœì†Œí™” í•´ì£¼ëŠ”ê²ƒ ì´ 2&#x2F;||w|| ë¥¼ ìµœëŒ€í™” í•´ì£¼ëŠ”ê±°ë‘ ê°™ë‹¤ê³  í•  ìˆ˜ ìˆë‹¤ ìš°ë¦¬ëŠ” ê²°êµ­ w ê°’ì„ ìµœì†Œí™” ì‹œì¼œì£¼ëŠ”ê²ƒì´ ëª©ì ì´ê¸° ë•Œë¬¸ì— ì œê³±ì„ ì·¨í•´ì£¼ë“  ìƒìˆ˜ë¥¼ ê³±í•´ì£¼ëŠ” ìƒê´€ì´ ì—†ë‹¤ Lagrange Multiplier(ìˆ˜í•™ì  ê¸°ë²•) â‡’ ì œì•½ì¡°ê±´ì„ ìµœì í™” ì¡°ê±´ì— ë…¹ì—¬ë²„ë¦¬ëŠ” ê¸°ë²•. ğŸ’¡ ë¼ê·¸ë‘ì¥¬ë¥¼ ë‹¤ í’€ê³  ë‚˜ë©´ íŒë³„ì‹ì´ ë‚˜ì˜¨ë‹¤. Xi tranpose X ( í•™ìŠµë°ì´í„°ì™€ ë¶„ë¥˜í•  ë°ì´í„°ì˜ ë‚´ì ) 4. SVM(Support Vector Machine) íŒë³„ì‹ì— ì„œí¬íŠ¸ë²¡í„°ë§Œ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ì•„ì›ƒë¼ì´ì–´ì— ëŒ€í•œ ì˜í–¥ì„ ì•ˆ ë°›ìŒ(KKT ì¡°ê±´ìœ¼ë¡œ ê±¸ëŸ¬ëƒ„) KNN ë˜í•œ ì´ì›ƒì„ í™•ì¸í•˜ëŠ” ê°œìˆ˜ì¸ Kì˜ í•œê³„ê°€ ìˆì–´ì„œ ì–´ëŠ elbow point ë¥¼ ì§€ë‚˜ì¹˜ë©´ ì •í™•ë„ê°€ ë–¨ì–´ì§„ë‹¤. â†’ ë¹„ìŠ·í•œ ì›ë¦¬ â‡’ svm ë˜í•œ ë¶„ë¥˜ë¥¼ ìœ íš¨í•˜ê²Œ í•˜ê¸°ìœ„í•´ì„œ support verctor ë§Œ ì´ìš©í•´ì¤€ë‹¤. ì„ í˜•ìœ¼ë¡œ ì™„ë²½íˆ ë‚˜ëˆ ì§€ì§€ ì•ŠëŠ” ë°ì´í„°ë¼ë©´ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ê²ŒëŠ” ìœ„ì˜ ëª¨ë¸ ë³´ë‹¤ ì•„ë˜ ëª¨ë¸ì´ ë” ì¢‹ì„ ê²ƒ ìœ¼ë¡œ ë³´ì¸ë‹¤. í•˜ì§€ë§Œ SVM ì˜ ì œì•½ì¡°ê±´ì—ëŠ” íŠ¸ë ˆì¸ë°ì´í„°ê°€ ì™„ë²½í•˜ê²Œ ë‚˜ëˆ„ì–´ì ¸ì•¼ í•œë‹¤ëŠ” ì œì•½ ì¡°ê±´ì´ ê±¸ë ¤ìˆë‹¤. ì–´ë–»ê²Œ í•˜ë©´ ì¢‹ì„ê¹Œ? Slack Variable for â€œSoft Marginâ€ Soft Margin SVM Non-linear SVM Reference: í•œêµ­ê³µí•™ëŒ€í•™êµ ê²½ì˜í•™ê³¼ ê°•ì§€í›ˆ êµìˆ˜ë‹˜ ê°•ì˜","categories":[{"name":"Basic ML","slug":"Basic-ML","permalink":"https://jmj3047.github.io/categories/Basic-ML/"}],"tags":[{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"}]},{"title":"HTML with Python_CGI","slug":"HTML_with_Python_CGI","date":"2022-05-03T15:00:00.000Z","updated":"2022-05-04T08:38:19.036Z","comments":true,"path":"2022/05/04/HTML_with_Python_CGI/","link":"","permalink":"https://jmj3047.github.io/2022/05/04/HTML_with_Python_CGI/","excerpt":"","text":"** code: https://github.com/jmj3047/mj_chatbot_prac&#x2F;blob&#x2F;c5bec233b833b24345deeffe7391621415dc1dcb&#x2F;cgi_webpython.py Making Website with CGI Constructing Web Server: Download and Install Apche Official: Installing Apache in window(https://httpd.apache.org/docs/2.4/platform/windows.html) Beginner version: Install Bitnami Wamp Stack push 1 or 2 click this button below it takes some time to install it Constructing Web Server: Bitnami Wamp Stack Start Bitnami Wamp Stack Click Go to Application: If you can see the site like the picture below, success! Start or Stop the server: Click Manage Server If program below is shut down, go to the folder where â€˜bitnami wamp stackâ€™ installed and click â€˜manager-windows.exeâ€™ Using Python in Web(HTML): Setting Apache Install python and apache find folder where apach installed(â€˜D:\\wamp\\apache2\\confâ€™) &gt; â€˜confâ€™ folder &gt; httpd.conf open httpd.conf file and search: 1LoadModule cgid_module modules/mod_cgid.so and if â€˜#â€™ exists in front of code, delete it Find tag in httpd.conf and add some lines 1234567891011121314151617181920212223242526272829303132333435&lt;Directory &quot;/Applications/mampstack-8.0.5-0/apache2/htdocs&quot;&gt; # # Possible values for the Options directive are &quot;None&quot;, &quot;All&quot;, # or any combination of: # Indexes Includes FollowSymLinks SymLinksifOwnerMatch ExecCGI MultiViews # # Note that &quot;MultiViews&quot; must be named *explicitly* --- &quot;Options All&quot; # doesn&#x27;t give it to you. # # The Options directive is both complicated and important. Please see # http://httpd.apache.org/docs/2.4/mod/core.html#options # for more information. # Options Indexes FollowSymLinks # # AllowOverride controls what directives may be placed in .htaccess files. # It can be &quot;All&quot;, &quot;None&quot;, or any combination of the keywords: # AllowOverride FileInfo AuthConfig Limit # AllowOverride None # # Controls who can get stuff from this server. # Require all granted ********** add this code ********** &lt;Files *.py&gt; Options ExecCGI AddHandler cgi-script .py &lt;/Files&gt; *********************************** &lt;/Directory&gt; htdocs ë””ë ‰í† ë¦¬ ë‚´ í™•ì¥ìê°€ pyì¸ ëª¨ë“  íŒŒì¼ì€ CGIê¸°ëŠ¥ì„ í™œì„±ì‹œí‚¤ê³  CGIë¡œ ì‹¤í–‰í•˜ë¼ëŠ” ì˜ë¯¸ Restart Apache Web Server in manager-osx Python file setting index.py ê°€ ìˆëŠ” htdocs ë””ë ‰í† ë¦¬ì—ì„œ index.py ì‹¤í–‰ í›„ ì•„ë˜ê°™ì´ ì…ë ¥(ë‹¤ë¥¸ íŒŒì´ì¬ íŒŒì¼ì„ ë§Œë“¤ì–´ë„ ìƒê´€ ì—†ìŒ) 123#!/usr/local/bin/python3 &gt;&gt;&gt; python.exe ê²½ë¡œ í™˜ê²½ë³€ìˆ˜ì— ì €ì¥í•´ì¤¬ë‹¤ë©´ !Pythonë§Œ í•´ë„ ë¨ print(&quot;Content-Type: text/html&quot;) print() index.html ì˜ ì½”ë“œ ë„£ê¸°( ë‹¤ë¥¸ html íŒŒì¼ ì´ì–´ë„ ë¨): index.pyê°€ ì‹¤í–‰ë˜ì—ˆì„ ë•Œ index.html ì˜ ì½”ë“œê°€ ì¶œë ¥ë˜ê²Œ í•´ì£¼ëŠ” ì½”ë“œ 12345678910111213141516171819202122#!/usr/local/bin/python3print(&quot;Content-Type: text/html&quot;)print()print(&#x27;&#x27;&#x27;&lt;!doctype html&gt; # ---&gt; ì¤„ë°”ê¿ˆì„ ìœ„í•´ docsting (&#x27;&#x27;&#x27; &#x27;&#x27;&#x27;) ì‚¬ìš©&lt;html&gt;&lt;head&gt; &lt;title&gt;WEB1 - Welcome&lt;/title&gt; &lt;meta charset=&quot;utf-8&quot;&gt;&lt;/head&gt;&lt;body&gt; &lt;h1&gt;&lt;a href=&quot;index.html&quot;&gt;WEB&lt;/a&gt;&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;a href=&quot;qs-1.html&quot;&gt;HTML&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;qs-2.html&quot;&gt;CSS&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;qs-3.html&quot;&gt;JavaScript&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h2&gt;WEB&lt;/h2&gt; &lt;p&gt;The World Wide Web (abbreviated WWW or the Web) is an information space where documents and other web resources are identified by Uniform Resource Locators (URLs), interlinked by hypertext links, and can be accessed via the Internet.[1] English scientist Tim Berners-Lee invented the World Wide Web in 1989. He wrote the first web browser computer program in 1990 while employed at CERN in Switzerland.[2][3] The Web browser was released outside of CERN in 1991, first to other research institutions starting in January 1991 and to the general public on the Internet in August 1991. &lt;/p&gt;&lt;/body&gt;&lt;/html&gt;&#x27;&#x27;&#x27;) ì›¹ ë¸Œë¼ìš°ì € ì£¼ì†Œì°½ì— localhost:8080&#x2F;index.py ì…ë ¥í•˜ê³  ì ‘ì† index.html íŒŒì¼ì˜ ë‚´ìš©ì´ ì˜ ì¶œë ¥ëœë‹¤ë©´ êµ¬í˜„ ì„±ê³µ Internal Server Error ê°€ í™•ì¸ëœë‹¤ë©´ ì—ë””í„°ì—ì„œ apache2&#x2F;logs ë””ë ‰í† ë¦¬ ë‚´ error_log íŒŒì¼ì— ìˆëŠ” ì—ëŸ¬ ì½”ë“œ í™•ì¸ ë° êµ¬ê¸€ë§ EXAMPLE123456789101112131415161718192021222324252627282930#!C:\\Python310\\python.exe ---&gt;íŒŒì´ì¬ ê²½ë¡œ# í•œê¸€ì´ êº ì§€ì§€ ì•Šìœ¼ë ¤ë©´ ê¼­ ë„£ì–´ì•¼ í•¨# -*- coding:utf-8 -*-import sysimport codecssys.stdout =codecs.getwriter(&quot;utf-8&quot;)(sys.stdout.detach())import cgi# cgitbëŠ” CGI í”„ë¡œê·¸ë˜ë°ì‹œ ë””ë²„ê¹…ì„ ìœ„í•œ ëª¨ë“ˆë¡œ cgitb.enable()í•  ê²½ìš° ëŸ°íƒ€ì„ ì—ëŸ¬ë¥¼ ì›¹ë¸Œë¼ìš°ì €ë¡œ ì „ì†¡í•¨# cgitb.enable() í•˜ì§€ ì•Šì€ ìƒíƒœë¡œ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•œ ê²½ìš° ì›¹ì„œë²„ëŠ” í´ë¼ì´ì–¸íŠ¸ì—ê²Œ HTTPì‘ë‹µ ì½”ë“œ 500ì„ ì „ì†¡í•¨import cgitbcgitb.enable()# HTTP ê·œê²©ì—ì„œ í—¤ë” ì „ì†¡ ì´í›„ì—ëŠ” ë°˜ë“œì‹œ ì¤„ ë°”ê¿ˆì„ í•˜ê²Œë˜ì–´ ìˆìŒìœ¼ë¡œ ë§ˆì§€ë§‰ì— \\r\\nì„ ì „ì†¡# ë§ˆì§€ë§‰ì— \\r\\nì„ ì „ì†¡í•˜ì§€ ì•Šìœ¼ë©´ ë¸Œë¼ìš°ì € ì¸¡ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒprint(&quot;Content-type: text/html;charset=utf-8\\r\\n&quot;)print(&quot;&quot;&quot; &lt;!doctype html&gt; &lt;html&gt; &lt;head&gt; &lt;meta charset=&#x27;utf-8&#x27;&gt; &lt;h1&gt;ì•ˆë…•?&lt;/h1&gt; &lt;h2&gt;Thank you so much&lt;/h2&gt; &lt;h3&gt;This page is made by Python&lt;/h3&gt; &quot;&quot;&quot;)a = 3+4+5b = a/3print(&#x27;bëŠ” :&#x27;, b)print(&quot;&lt;/head&gt;&quot;)print(&quot;&lt;/html&gt;&quot;) Result Reference https://daekiry.tistory.com/4?category=928946 https://daekiry.tistory.com/5?category=928946 https://daekiry.tistory.com/6 https:&#x2F;&#x2F;velog.io&#x2F;@ssoulll&#x2F;python-ì›¹-í˜ì´ì§€ë¥¼-CGIë¡œ-êµ¬í˜„","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"HTML","slug":"Python/HTML","permalink":"https://jmj3047.github.io/categories/Python/HTML/"}],"tags":[{"name":"CGI","slug":"CGI","permalink":"https://jmj3047.github.io/tags/CGI/"},{"name":"Web Server","slug":"Web-Server","permalink":"https://jmj3047.github.io/tags/Web-Server/"}]},{"title":"HTML with Python_Flask & Brython","slug":"HTML_with_Python_Flask&Brython","date":"2022-05-03T15:00:00.000Z","updated":"2022-05-04T08:42:26.415Z","comments":true,"path":"2022/05/04/HTML_with_Python_Flask&Brython/","link":"","permalink":"https://jmj3047.github.io/2022/05/04/HTML_with_Python_Flask&Brython/","excerpt":"","text":"Flask íŒŒì´ì¬ ê¸°ë°˜ ë§ˆì´í¬ë¡œ ì›¹ ê°œë°œ í”„ë ˆì„ì›Œí¬ ì›¹ ê°œë°œì˜ í•µì‹¬ê¸°ëŠ¥ë§Œ ê°„ê²½í•˜ê²Œ ìœ ì§€ í•„ìš”í•œ ê¸°ëŠ¥ì€ ë‹¤ë¥¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë‚˜ í”„ë ˆì„ì›Œí¬ë¡œ ì†ì‰½ê²Œ í™•ì¥ ì‹ ì†í•˜ê²Œ ìµœì†Œí•œì˜ ë…¸ë ¥ìœ¼ë¡œ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ ê°€ëŠ¥ Installation start virtualenv pip install flask Error â†’ note: could not find a version that satisfies the requirement flask â†’ ë„¤íŠ¸ì›Œí¬ ë¬¸ì œë¡œ ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì €ì¥ì†Œì— ì ‘ê·¼í•˜ì§€ ëª»í•  ê²½ìš° ë‚˜ì˜¤ëŠ” ë¬¸ì œ, â†’ ì§ì ‘ https://github.com/mitsuhiko/flask ìœ„ì¹˜ë¡œ ê°€ì„œ ì†ŒìŠ¤ ë°›ì•„ ì„¤ì¹˜ í•´ì•¼ í•¨. Strat Flask**code: https://github.com/jmj3047/mj_chatbot_prac&#x2F;blob&#x2F;c5bec233b833b24345deeffe7391621415dc1dcb&#x2F;flask_prac&#x2F;0.flask_hello.py 12345678910from flask import Flaskapp = Flask(__name__)@app.route(&#x27;/&#x27;)def hello_world(): return &#x27;Hello World!&#x27; if __name__ == &#x27;__main__&#x27;: app.debug =True app.run() ì†ŒìŠ¤ë¥¼ ì‹¤í–‰í•˜ê³ , terminal ì—ì„œ â€˜python flask_test.pyâ€™ ì…ë ¥ í›„ http://127.0.0.1:5000 ìœ¼ë¡œ ì ‘ê·¼ Process for starting Flask Application íŠ¹ì • URL í˜¸ì¶œ(request) : http://127.0.0.1:5000/ ë˜ëŠ” http://localhost:5000 íŠ¹ì • URL ë§¤í•‘ ê²€ìƒ‰ : @app.route(â€˜&#x2F;â€˜) íŠ¹ì • URLì— ë§¤ì¹­ëœ í•¨ìˆ˜(def í•¨ìˆ˜) ì‹¤í–‰ : def hello_world() ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì‹¤í–‰ : result ê²°ê³¼ ì‘ë‹µìœ¼ë¡œ ì „ì†¡(response): return result HTML ë¡œ í™”ë©´ì— ì¶œë ¥ ì¿ í‚¤(Cookie), ì„¸ì…˜(Session), ë¡œê¹…(logging) ë“± ì œê³µ Routing**code: https://github.com/jmj3047/mj_chatbot_prac&#x2F;blob&#x2F;c5bec233b833b24345deeffe7391621415dc1dcb&#x2F;flask_prac&#x2F;1.flask_login.py URLì„ í†µí•´ ì²˜ë¦¬í•  í•¸ë“¤ëŸ¬ë¥¼ ì°¾ëŠ” ê²ƒ í”Œë¼ìŠ¤í¬ëŠ” ë³µì¡í•œ URIë¥¼ í•¨ìˆ˜ë¡œ ì—°ê²°í•˜ëŠ” ë°©ë²•ì„ ì œê³µ URI ë¥¼ ì—°ê²°í•˜ëŠ” route() ë°ì½”ë ˆì´í„° í•¨ìˆ˜ ì œê³µ &#x2F; ì ‘ì† ì‹œ root_world() ê°€ í˜¸ì¶œ ë¨ &#x2F;hello ì ‘ì† ì‹œ hello_world() ê°€ í˜¸ì¶œ ë¨ 12345678910111213141516from flask import Flask, redirect, url_forapp = Flask(__name__)@app.route(&#x27;/&#x27;) #127.0.0.1:5000ì— ê°€ë©´ í•¨ìˆ˜ ì‹¤í–‰def root_world(): result = &#x27;root world&#x27; return result@app.route(&#x27;/hello&#x27;) #127.0.0.1:5000/hello ë¥¼ ê°€ë©´ ì‹¤í–‰def hello_world(): result = &#x27;hello world&#x27; return resultif __name__ == &#x27;__main__&#x27;: app.debug =True app.run() app.debugëŠ” ê°œë°œì˜ í¸ì˜ë¥¼ ìœ„í•´ ì¡´ì¬ Trueê°’ì„ ê²½ìš° ì½”ë“œë¥¼ ë³€ê²½í•˜ë©´ ìë™ìœ¼ë¡œ ì„œë²„ê°€ ì¬ ì‹¤í–‰ ë¨ ë˜í•œ, ì›¹ìƒì—ì„œ íŒŒì´ì¬ ì½”ë“œë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆê²Œ ë˜ë¯€ë¡œ, ìš´ì˜í™˜ê²½ì—ì„œ ì‚¬ìš©ì„ ìœ ì˜í•´ì•¼ í•¨. í˜„ì¬ ì ‘ê·¼ì€ ê°œë°œ ì†ŒìŠ¤ê°€ ì¡´ì¬í•˜ëŠ” ë¡œì»¬ì—ì„œë§Œ ì ‘ê·¼ ê°€ëŠ¥ ì™¸ë¶€ì—ì„œë„ ì ‘ê·¼ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ë ¤ë©´ app.run(host&#x3D;â€™0.0.0.0â€™)ë¡œ ì„œë²„ ì‹¤í–‰ ë¶€ë¥¼ ë³€ê²½í•´ì•¼ í•¨ 1234567891011121314151617181920212223242526272829303132from flask import Flask, redirect, url_forapp = Flask(__name__)@app.route(&#x27;/users/&lt;user_id&gt;&#x27;) #ë™ì  ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ URI ì ‘ì†# &lt;ë™ì ë³€ìˆ˜&gt;ë¥¼ ë·°í•¨ìˆ˜ì˜ ì¸ìë¡œ ì‚¬ìš©# &lt;ë™ì  ë³€ìˆ˜&gt; ë‹¤ìŒì— /ë¥¼ ë„£ìœ¼ë©´ ì•ˆë¨def user_id(userid): result = &#x27;user_id = &#x27; + userid return result@app.route(&#x27;/admin&#x27;)def hello_admin(): return &#x27;Hello Admin&#x27;@app.route(&#x27;/guest/&lt;guest&gt;&#x27;)def hello_guest(guest): return &#x27;Hello %s as Guest&#x27; % guest@app.route(&#x27;/user/&lt;name&gt;&#x27;)def hello_user(name): if name == &#x27;admin&#x27;: return redirect(url_for(&#x27;hello_admin&#x27;)) else: return redirect(url_for(&#x27;hello_guest&#x27;, guest=name))# url_for(): í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ëŠ” URIë¥¼ ë°˜í™˜# redirect(): ë‹¤ë¥¸ route ê²½ë¡œ ì´ë™(ë‹¤ë¥¸ í˜ì´ì§€ ì´ë™)if __name__ == &#x27;__main__&#x27;: app.debug = True app.run() Flask GET ë°©ì‹ìœ¼ë¡œ ê°’ ì „ì†¡ ë° ì²˜ë¦¬**code: https://github.com/jmj3047/mj_chatbot_prac&#x2F;blob&#x2F;c5bec233b833b24345deeffe7391621415dc1dcb&#x2F;flask_prac&#x2F;2.flask_app.py https://github.com/jmj3047/mj_chatbot_prac&#x2F;blob&#x2F;c5bec233b833b24345deeffe7391621415dc1dcb&#x2F;flask_prac&#x2F;templates&#x2F;login&#x2F;login_form_get.html mkdir templates í´ë” ìƒì„± login_form_get.html íŒŒì¼ ì‘ì„± get ë°©ì‹ ì§€ì • : method&#x3D;â€getâ€ 1234567891011121314151617181920from flask import Flask, request, session, render_templateapp = Flask(__name__)@app.route(&#x27;/login_form_get&#x27;) def login_form_get(): return render_template(&#x27;login/login_form_get.html&#x27;)@app.route(&#x27;/login_get_proc&#x27;, methods=[&#x27;GET&#x27;]) def login_get_proc(): user_id = request.args.get(&#x27;user_id&#x27;) user_pwd = request.args.get(&#x27;user_pwd&#x27;) if len(user_id) == 0 or len(user_pwd) == 0: return &#x27;no &#123;&#125; or &#123;&#125;&#x27;.format(user_id, user_pwd) return &#x27;welcome &#123;&#125;&#x27;.format(user_id)if __name__ == &#x27;__main__&#x27;: app.debug = True app.run() 12345678910111213141516&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;meta charset = &quot;UTF-8&quot;&gt; &lt;title&gt;login_form_get.html&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt; &lt;form action=&quot;/login_get_proc&quot; method=&quot;get&quot;&gt; ID: &lt;input type=&quot;text&quot;, name=&quot;user_id&quot;&gt;&lt;br&gt; PW: &lt;input type=&quot;password&quot;, name=&quot;user_pwd&quot;&gt;&lt;br&gt; &lt;input type=&quot;submit&quot;, value=&quot;Click&quot;&gt; &lt;/form&gt; &lt;/h1&gt; &lt;/body&gt;&lt;/html&gt; Brython**code:https://github.com/jmj3047/mj_chatbot_prac&#x2F;blob&#x2F;c5bec233b833b24345deeffe7391621415dc1dcb&#x2F;flask_prac&#x2F;templates&#x2F;brython_test.html pythonì„ HTML ì½”ë“œì— ì‚½ì…í•´ì„œ ì‚¬ìš© 12345678910111213141516171819202122&lt;html&gt; &lt;head&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;/path/to/brython.js&quot;&gt;&lt;/script&gt; &lt;/head&gt; &lt;body onload=&quot;brython()&quot;&gt; &lt;script type=&quot;text/python&quot;&gt; from browser import document, alert def echo(event): alert(document[&quot;zone&quot;].value) document[&quot;mybutton&quot;].bind(&quot;click&quot;, echo) &lt;/script&gt; &lt;input id=&quot;zone&quot;&gt;&lt;button id=&quot;mybutton&quot;&gt;click !&lt;/button&gt; &lt;/body&gt;&lt;/html&gt; Reference: https://essim92.tistory.com/8 https://code-examples.net/ko/q/dc0356 https://github.com/brython-dev/brython #Test Brython online(DEMO)","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"HTML","slug":"Python/HTML","permalink":"https://jmj3047.github.io/categories/Python/HTML/"}],"tags":[{"name":"Web Server","slug":"Web-Server","permalink":"https://jmj3047.github.io/tags/Web-Server/"},{"name":"Flask","slug":"Flask","permalink":"https://jmj3047.github.io/tags/Flask/"},{"name":"Brython","slug":"Brython","permalink":"https://jmj3047.github.io/tags/Brython/"}]},{"title":"Making Office Chatbot with Transformer","slug":"Making_Office_Chatbot_with_Transformer","date":"2022-05-03T15:00:00.000Z","updated":"2022-05-06T06:05:45.364Z","comments":true,"path":"2022/05/04/Making_Office_Chatbot_with_Transformer/","link":"","permalink":"https://jmj3047.github.io/2022/05/04/Making_Office_Chatbot_with_Transformer/","excerpt":"","text":"ì¼ìƒ ëŒ€í™”ì™€ ì˜¤í”¼ìŠ¤ ëŒ€í™” ë°ì´í„°ë¥¼ Transformer ëª¨ë¸ë¡œ í•™ìŠµì‹œì¼œ, ì§ˆë¬¸ì— ëŒ€í•œ ì ì ˆí•œ ë‹µë³€ì„ í•˜ëŠ” ì±—ë´‡ Data: í•œêµ­ì–´ëŒ€í™”ë°ì´í„°ì…‹(ì˜¤í”¼ìŠ¤ë°ì´í„°) ì‚¬ìš© (AIHUBì— â€˜ê°œë°©ë°ì´í„°-ì¸ì‹ê¸°ìˆ  ì–¸ì–´ì§€ëŠ¥-í•œêµ­ì–´ëŒ€í™”ë°ì´í„°ì…‹â€™ì—ì„œ ë¡œê·¸ì¸ í›„ ë‹¤ìš´ë¡œë“œ) GPU ì‚¬ìš© ê·¸ ì™¸ í™˜ê²½ 123batch size = 64buffer size = 20000epochs = 50 1. Environments1!pip install tensorflow_datasets 12345678import pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport reimport urllib.requestimport timeimport tensorflow_datasets as tfdsimport tensorflow as tf 12import osos.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;3&quot; 1234567with tf.device(&#x27;/device:GPU:3&#x27;): # í…ì„œ ìƒì„± a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]) b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]) c = tf.matmul(a, b) print(c) tf.Tensor( [[22. 28.] [49. 64.]], shape=(2, 2), dtype=float32) 2. ë°ì´í„° ì „ì²˜ë¦¬12data = pd.read_csv(&#x27;./ChatbotData.csv&#x27;)data = data[0:5290] 1data[:10] 123456789f = open(r&#x27;./conversation_office.txt&#x27;,&quot;r&quot;)lines = f.readlines()Q = []A = []for i in range(len(lines)) : if i%2 == 0 : Q.append(lines[i][2:-1]) A.append(lines[i+1][2:-1]) 123456import pandas as pddf = pd.DataFrame()df[&#x27;Q&#x27;] = Qdf[&#x27;A&#x27;] = Adf[&#x27;label&#x27;] = 1 1df[:10] 1234#ë‘ ë°ì´í„°ë¥¼ concat() í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ í•©ì³ í•˜ë‚˜ì˜ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë‚˜íƒ€ë‚´ì£¼ë„ë¡ í•¨train_data = pd.concat([data, df],ignore_index=True)train_data = train_data.sample(frac=1).reset_index(drop=True) #ë°ì´í„°ë¥¼ ëœë¤ìœ¼ë¡œ ì„ì–´ì£¼ëŠ” ì½”ë“œ 3. ë‹¨ì–´ ì§‘í•© ìƒì„±123456789101112131415# ë¬¸ì¥ ê·¸ëŒ€ë¡œ í•™ìŠµ ëª¨ë¸ì— ë„£ìœ¼ë©´ ëª¨ë¸ì´ ì¸ì‹ì„ í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì— ë‹¨ì–´ ì§‘í•©ì„ ë§Œë“¤ì–´ ì¤˜ì•¼ í•¨.# ì •ìˆ˜ ì¸ì½”ë”©ê³¼ íŒ¨ë”©ì„ í•´ì£¼ëŠ” ì‘ì—…ì„ í•´ì£¼ì–´ì•¼ í•¨#íŠ¹ìˆ˜ê¸°í˜¸ ë„ì–´ì“°ê¸°questions = []for sentence in train_data[&#x27;Q&#x27;]: sentence = re.sub(r&quot;([?.!,])&quot;, r&quot; \\1 &quot;, sentence) sentence = sentence.strip() questions.append(sentence) answers = []for sentence in train_data[&#x27;A&#x27;]: sentence = re.sub(r&quot;([?.!,])&quot;, r&quot; \\1 &quot;, sentence) sentence = sentence.strip() answers.append(sentence) 123456789# ì„œë¸Œì›Œë“œí…ìŠ¤íŠ¸ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸, ë‹µë³€ ë°ì´í„°ë¡œë¶€í„° ë‹¨ì–´ ì§‘í•©(Vocabulary) ìƒì„±tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus( questions + answers, target_vocab_size=2**13) # ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì— ëŒ€í•œ ì •ìˆ˜ ë¶€ì—¬START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]# ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì„ ê³ ë ¤í•˜ì—¬ ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸°ë¥¼ + 2VOCAB_SIZE = tokenizer.vocab_size + 2 123456789101112131415161718192021222324252627#ì •ìˆ˜ ì¸ì½”ë”©ê³¼ íŒ¨ë”©# ì„œë¸Œì›Œë“œí…ìŠ¤íŠ¸ì¸ì½”ë” í† í¬ë‚˜ì´ì €ì˜ .encode()ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤ë¥¼ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜.print(&#x27;ì„ì˜ì˜ ì§ˆë¬¸ ìƒ˜í”Œì„ ì •ìˆ˜ ì¸ì½”ë”© : &#123;&#125;&#x27;.format(tokenizer.encode(questions[20])))#ì¶œë ¥ : ì„ì˜ì˜ ì§ˆë¬¸ ìƒ˜í”Œì„ ì •ìˆ˜ ì¸ì½”ë”© : [8656, 331]# ìµœëŒ€ ê¸¸ì´ë¥¼ 40ìœ¼ë¡œ ì •ì˜MAX_LENGTH = 40# í† í°í™” / ì •ìˆ˜ ì¸ì½”ë”© / ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í° ì¶”ê°€ / íŒ¨ë”©def tokenize_and_filter(inputs, outputs): tokenized_inputs, tokenized_outputs = [], [] for (sentence1, sentence2) in zip(inputs, outputs): # encode(í† í°í™” + ì •ìˆ˜ ì¸ì½”ë”©), ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í° ì¶”ê°€ sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN tokenized_inputs.append(sentence1) tokenized_outputs.append(sentence2) # íŒ¨ë”© tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences( tokenized_inputs, maxlen=MAX_LENGTH, padding=&#x27;post&#x27;) tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences( tokenized_outputs, maxlen=MAX_LENGTH, padding=&#x27;post&#x27;) return tokenized_inputs, tokenized_outputs ì„ì˜ì˜ ì§ˆë¬¸ ìƒ˜í”Œì„ ì •ìˆ˜ ì¸ì½”ë”© : [2704, 1081, 13, 542] 1questions, answers = tokenize_and_filter(questions, answers) 1234#sample# ì •ìˆ˜ ì¸ì½”ë”©ê³¼ íŒ¨ë”©ì´ ëœ ê²°ê³¼ê°€ ì¶œë ¥print(questions[0])print(answers[0]) [10023 31 121 4282 10024 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [10023 3607 213 13 21 1 10024 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] 12from tensorflow.python.client import device_libdevice_lib.list_local_devices() 1234567891011121314151617181920212223#ì¸ì½”ë”ì™€ ë””ì½”ë”ì˜ ì…ë ¥ ë°ì´í„°ê°€ ë˜ë„ë¡ ë°°ì¹˜ í¬ê¸°ë¡œ ë°ì´í„°ë¥¼ ë¬¶ì–´ì¤Œ with tf.device(&#x27;/device:GPU:3&#x27;): BATCH_SIZE = 64 BUFFER_SIZE = 20000 dataset = tf.data.Dataset.from_tensor_slices(( &#123; &#x27;inputs&#x27;: questions, &#x27;dec_inputs&#x27;: answers[:, :-1] # ë””ì½”ë”ì˜ ì…ë ¥ / ë§ˆì§€ë§‰ íŒ¨ë”© í† í° ì œê±° &#125;, &#123; &#x27;outputs&#x27;: answers[:, 1:] # ë§¨ ì²˜ìŒ í† í°ì´ ì œê±° = ì‹œì‘ í† í° ì œê±° &#125;, )) dataset = dataset.cache() dataset = dataset.shuffle(BUFFER_SIZE) dataset = dataset.batch(BATCH_SIZE) dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE) 4. íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ ë§Œë“¤ê¸°123456789101112131415161718192021222324252627282930313233343536373839def transformer(vocab_size, num_layers, dff, d_model, num_heads, dropout, name=&quot;transformer&quot;): # ì¸ì½”ë”ì˜ ì…ë ¥ inputs = tf.keras.Input(shape=(None,), name=&quot;inputs&quot;) # ë””ì½”ë”ì˜ ì…ë ¥ dec_inputs = tf.keras.Input(shape=(None,), name=&quot;dec_inputs&quot;) # ì¸ì½”ë”ì˜ íŒ¨ë”© ë§ˆìŠ¤í¬ enc_padding_mask = tf.keras.layers.Lambda( create_padding_mask, output_shape=(1, 1, None), name=&#x27;enc_padding_mask&#x27;)(inputs) # ë””ì½”ë”ì˜ ë£©ì–´í—¤ë“œ ë§ˆìŠ¤í¬(ì²«ë²ˆì§¸ ì„œë¸Œì¸µ) look_ahead_mask = tf.keras.layers.Lambda( create_look_ahead_mask, output_shape=(1, None, None), name=&#x27;look_ahead_mask&#x27;)(dec_inputs) # ë””ì½”ë”ì˜ íŒ¨ë”© ë§ˆìŠ¤í¬(ë‘ë²ˆì§¸ ì„œë¸Œì¸µ) dec_padding_mask = tf.keras.layers.Lambda( create_padding_mask, output_shape=(1, 1, None), name=&#x27;dec_padding_mask&#x27;)(inputs) # ì¸ì½”ë”ì˜ ì¶œë ¥ì€ enc_outputs. ë””ì½”ë”ë¡œ ì „ë‹¬ëœë‹¤. enc_outputs = encoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff, d_model=d_model, num_heads=num_heads, dropout=dropout, )(inputs=[inputs, enc_padding_mask]) # ì¸ì½”ë”ì˜ ì…ë ¥ì€ ì…ë ¥ ë¬¸ì¥ê³¼ íŒ¨ë”© ë§ˆìŠ¤í¬ # ë””ì½”ë”ì˜ ì¶œë ¥ì€ dec_outputs. ì¶œë ¥ì¸µìœ¼ë¡œ ì „ë‹¬ëœë‹¤. dec_outputs = decoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff, d_model=d_model, num_heads=num_heads, dropout=dropout, )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask]) # ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ì„ ìœ„í•œ ì¶œë ¥ì¸µ outputs = tf.keras.layers.Dense(units=vocab_size, name=&quot;outputs&quot;)(dec_outputs) return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name) 1234567891011121314151617181920212223242526272829303132class PositionalEncoding(tf.keras.layers.Layer): def __init__(self, position, d_model): super(PositionalEncoding, self).__init__() self.pos_encoding = self.positional_encoding(position, d_model) def get_angles(self, position, i, d_model): angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32)) return position * angles def positional_encoding(self, position, d_model): angle_rads = self.get_angles( position=tf.range(position, dtype=tf.float32)[:, tf.newaxis], i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :], d_model=d_model) # ë°°ì—´ì˜ ì§ìˆ˜ ì¸ë±ìŠ¤(2i)ì—ëŠ” ì‚¬ì¸ í•¨ìˆ˜ ì ìš© sines = tf.math.sin(angle_rads[:, 0::2]) # ë°°ì—´ì˜ í™€ìˆ˜ ì¸ë±ìŠ¤(2i+1)ì—ëŠ” ì½”ì‚¬ì¸ í•¨ìˆ˜ ì ìš© cosines = tf.math.cos(angle_rads[:, 1::2]) angle_rads = np.zeros(angle_rads.shape) angle_rads[:, 0::2] = sines angle_rads[:, 1::2] = cosines pos_encoding = tf.constant(angle_rads) pos_encoding = pos_encoding[tf.newaxis, ...] print(pos_encoding.shape) return tf.cast(pos_encoding, tf.float32) def call(self, inputs): return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :] 1234567891011121314151617181920212223242526272829303132333435def create_padding_mask(x): mask = tf.cast(tf.math.equal(x, 0), tf.float32) # (batch_size, 1, 1, keyì˜ ë¬¸ì¥ ê¸¸ì´) return mask[:, tf.newaxis, tf.newaxis, :]# ë””ì½”ë”ì˜ ì²«ë²ˆì§¸ ì„œë¸Œì¸µ(sublayer)ì—ì„œ ë¯¸ë˜ í† í°ì„ Maskí•˜ëŠ” í•¨ìˆ˜def create_look_ahead_mask(x): seq_len = tf.shape(x)[1] look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0) padding_mask = create_padding_mask(x) # íŒ¨ë”© ë§ˆìŠ¤í¬ë„ í¬í•¨ return tf.maximum(look_ahead_mask, padding_mask)#encoderdef encoder(vocab_size, num_layers, dff, d_model, num_heads, dropout, name=&quot;encoder&quot;): inputs = tf.keras.Input(shape=(None,), name=&quot;inputs&quot;) # ì¸ì½”ë”ëŠ” íŒ¨ë”© ë§ˆìŠ¤í¬ ì‚¬ìš© padding_mask = tf.keras.Input(shape=(1, 1, None), name=&quot;padding_mask&quot;) # í¬ì§€ì…”ë„ ì¸ì½”ë”© + ë“œë¡­ì•„ì›ƒ embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs) embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32)) embeddings = PositionalEncoding(vocab_size, d_model)(embeddings) outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings) # ì¸ì½”ë”ë¥¼ num_layersê°œ ìŒ“ê¸° for i in range(num_layers): outputs = encoder_layer(dff=dff, d_model=d_model, num_heads=num_heads, dropout=dropout, name=&quot;encoder_layer_&#123;&#125;&quot;.format(i), )([outputs, padding_mask]) return tf.keras.Model( inputs=[inputs, padding_mask], outputs=outputs, name=name) 1234567891011121314151617181920212223242526272829def encoder_layer(dff, d_model, num_heads, dropout, name=&quot;encoder_layer&quot;): inputs = tf.keras.Input(shape=(None, d_model), name=&quot;inputs&quot;) # ì¸ì½”ë”ëŠ” íŒ¨ë”© ë§ˆìŠ¤í¬ ì‚¬ìš© padding_mask = tf.keras.Input(shape=(1, 1, None), name=&quot;padding_mask&quot;) # ë©€í‹°-í—¤ë“œ ì–´í…ì…˜ (ì²«ë²ˆì§¸ ì„œë¸Œì¸µ / ì…€í”„ ì–´í…ì…˜) attention = MultiHeadAttention( d_model, num_heads, name=&quot;attention&quot;)(&#123; &#x27;query&#x27;: inputs, &#x27;key&#x27;: inputs, &#x27;value&#x27;: inputs, # Q = K = V &#x27;mask&#x27;: padding_mask # íŒ¨ë”© ë§ˆìŠ¤í¬ ì‚¬ìš© &#125;) # ë“œë¡­ì•„ì›ƒ + ì”ì°¨ ì—°ê²°ê³¼ ì¸µ ì •ê·œí™” attention = tf.keras.layers.Dropout(rate=dropout)(attention) attention = tf.keras.layers.LayerNormalization( epsilon=1e-6)(inputs + attention) # í¬ì§€ì…˜ ì™€ì´ì¦ˆ í”¼ë“œ í¬ì›Œë“œ ì‹ ê²½ë§ (ë‘ë²ˆì§¸ ì„œë¸Œì¸µ) outputs = tf.keras.layers.Dense(units=dff, activation=&#x27;relu&#x27;)(attention) outputs = tf.keras.layers.Dense(units=d_model)(outputs) # ë“œë¡­ì•„ì›ƒ + ì”ì°¨ ì—°ê²°ê³¼ ì¸µ ì •ê·œí™” outputs = tf.keras.layers.Dropout(rate=dropout)(outputs) outputs = tf.keras.layers.LayerNormalization( epsilon=1e-6)(attention + outputs) return tf.keras.Model( inputs=[inputs, padding_mask], outputs=outputs, name=name) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960class MultiHeadAttention(tf.keras.layers.Layer): def __init__(self, d_model, num_heads, name=&quot;multi_head_attention&quot;): super(MultiHeadAttention, self).__init__(name=name) self.num_heads = num_heads self.d_model = d_model assert d_model % self.num_heads == 0 # d_modelì„ num_headsë¡œ ë‚˜ëˆˆ ê°’. # ë…¼ë¬¸ ê¸°ì¤€ : 64 self.depth = d_model // self.num_heads # WQ, WK, WVì— í•´ë‹¹í•˜ëŠ” ë°€ì§‘ì¸µ ì •ì˜ self.query_dense = tf.keras.layers.Dense(units=d_model) self.key_dense = tf.keras.layers.Dense(units=d_model) self.value_dense = tf.keras.layers.Dense(units=d_model) # WOì— í•´ë‹¹í•˜ëŠ” ë°€ì§‘ì¸µ ì •ì˜ self.dense = tf.keras.layers.Dense(units=d_model) # num_heads ê°œìˆ˜ë§Œí¼ q, k, vë¥¼ splití•˜ëŠ” í•¨ìˆ˜ def split_heads(self, inputs, batch_size): inputs = tf.reshape( inputs, shape=(batch_size, -1, self.num_heads, self.depth)) return tf.transpose(inputs, perm=[0, 2, 1, 3]) def call(self, inputs): query, key, value, mask = inputs[&#x27;query&#x27;], inputs[&#x27;key&#x27;], inputs[ &#x27;value&#x27;], inputs[&#x27;mask&#x27;] batch_size = tf.shape(query)[0] query = self.query_dense(query) key = self.key_dense(key) value = self.value_dense(value) # 2. í—¤ë“œ ë‚˜ëˆ„ê¸° # q : (batch_size, num_heads, queryì˜ ë¬¸ì¥ ê¸¸ì´, d_model/num_heads) # k : (batch_size, num_heads, keyì˜ ë¬¸ì¥ ê¸¸ì´, d_model/num_heads) # v : (batch_size, num_heads, valueì˜ ë¬¸ì¥ ê¸¸ì´, d_model/num_heads) query = self.split_heads(query, batch_size) key = self.split_heads(key, batch_size) value = self.split_heads(value, batch_size) # 3. ìŠ¤ì¼€ì¼ë“œ ë‹· í”„ë¡œë•íŠ¸ ì–´í…ì…˜. ì•ì„œ êµ¬í˜„í•œ í•¨ìˆ˜ ì‚¬ìš©. # (batch_size, num_heads, queryì˜ ë¬¸ì¥ ê¸¸ì´, d_model/num_heads) scaled_attention, _ = scaled_dot_product_attention(query, key, value, mask) # (batch_size, queryì˜ ë¬¸ì¥ ê¸¸ì´, num_heads, d_model/num_heads) scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3]) # 4. í—¤ë“œ ì—°ê²°(concatenate)í•˜ê¸° # (batch_size, queryì˜ ë¬¸ì¥ ê¸¸ì´, d_model) concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model)) # 5. WOì— í•´ë‹¹í•˜ëŠ” ë°€ì§‘ì¸µ ì§€ë‚˜ê¸° # (batch_size, queryì˜ ë¬¸ì¥ ê¸¸ì´, d_model) outputs = self.dense(concat_attention) return outputs 123456789101112131415161718192021222324252627def scaled_dot_product_attention(query, key, value, mask): # query í¬ê¸° : (batch_size, num_heads, queryì˜ ë¬¸ì¥ ê¸¸ì´, d_model/num_heads) # key í¬ê¸° : (batch_size, num_heads, keyì˜ ë¬¸ì¥ ê¸¸ì´, d_model/num_heads) # value í¬ê¸° : (batch_size, num_heads, valueì˜ ë¬¸ì¥ ê¸¸ì´, d_model/num_heads) # padding_mask : (batch_size, 1, 1, keyì˜ ë¬¸ì¥ ê¸¸ì´) # Qì™€ Kì˜ ê³±. ì–´í…ì…˜ ìŠ¤ì½”ì–´ í–‰ë ¬. matmul_qk = tf.matmul(query, key, transpose_b=True) # ìŠ¤ì¼€ì¼ë§ # dkì˜ ë£¨íŠ¸ê°’ìœ¼ë¡œ ë‚˜ëˆ ì¤€ë‹¤. depth = tf.cast(tf.shape(key)[-1], tf.float32) logits = matmul_qk / tf.math.sqrt(depth) # ë§ˆìŠ¤í‚¹. ì–´í…ì…˜ ìŠ¤ì½”ì–´ í–‰ë ¬ì˜ ë§ˆìŠ¤í‚¹ í•  ìœ„ì¹˜ì— ë§¤ìš° ì‘ì€ ìŒìˆ˜ê°’ì„ ë„£ëŠ”ë‹¤. # ë§¤ìš° ì‘ì€ ê°’ì´ë¯€ë¡œ ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ë¥¼ ì§€ë‚˜ë©´ í–‰ë ¬ì˜ í•´ë‹¹ ìœ„ì¹˜ì˜ ê°’ì€ 0ì´ ëœë‹¤. if mask is not None: logits += (mask * -1e9) # ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ëŠ” ë§ˆì§€ë§‰ ì°¨ì›ì¸ keyì˜ ë¬¸ì¥ ê¸¸ì´ ë°©í–¥ìœ¼ë¡œ ìˆ˜í–‰ëœë‹¤. # attention weight : (batch_size, num_heads, queryì˜ ë¬¸ì¥ ê¸¸ì´, keyì˜ ë¬¸ì¥ ê¸¸ì´) attention_weights = tf.nn.softmax(logits, axis=-1) # output : (batch_size, num_heads, queryì˜ ë¬¸ì¥ ê¸¸ì´, d_model/num_heads) output = tf.matmul(attention_weights, value) return output, attention_weights 123456789101112131415161718192021222324252627def decoder(vocab_size, num_layers, dff, d_model, num_heads, dropout, name=&#x27;decoder&#x27;): inputs = tf.keras.Input(shape=(None,), name=&#x27;inputs&#x27;) enc_outputs = tf.keras.Input(shape=(None, d_model), name=&#x27;encoder_outputs&#x27;) # ë””ì½”ë”ëŠ” ë£©ì–´í—¤ë“œ ë§ˆìŠ¤í¬(ì²«ë²ˆì§¸ ì„œë¸Œì¸µ)ì™€ íŒ¨ë”© ë§ˆìŠ¤í¬(ë‘ë²ˆì§¸ ì„œë¸Œì¸µ) ë‘˜ ë‹¤ ì‚¬ìš©. look_ahead_mask = tf.keras.Input( shape=(1, None, None), name=&#x27;look_ahead_mask&#x27;) padding_mask = tf.keras.Input(shape=(1, 1, None), name=&#x27;padding_mask&#x27;) # í¬ì§€ì…”ë„ ì¸ì½”ë”© + ë“œë¡­ì•„ì›ƒ embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs) embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32)) embeddings = PositionalEncoding(vocab_size, d_model)(embeddings) outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings) # ë””ì½”ë”ë¥¼ num_layersê°œ ìŒ“ê¸° for i in range(num_layers): outputs = decoder_layer(dff=dff, d_model=d_model, num_heads=num_heads, dropout=dropout, name=&#x27;decoder_layer_&#123;&#125;&#x27;.format(i), )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask]) return tf.keras.Model( inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask], outputs=outputs, name=name) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def decoder_layer(dff, d_model, num_heads, dropout, name=&quot;decoder_layer&quot;): inputs = tf.keras.Input(shape=(None, d_model), name=&quot;inputs&quot;) enc_outputs = tf.keras.Input(shape=(None, d_model), name=&quot;encoder_outputs&quot;) # ë£©ì–´í—¤ë“œ ë§ˆìŠ¤í¬(ì²«ë²ˆì§¸ ì„œë¸Œì¸µ) look_ahead_mask = tf.keras.Input( shape=(1, None, None), name=&quot;look_ahead_mask&quot;) # íŒ¨ë”© ë§ˆìŠ¤í¬(ë‘ë²ˆì§¸ ì„œë¸Œì¸µ) padding_mask = tf.keras.Input(shape=(1, 1, None), name=&#x27;padding_mask&#x27;) # ë©€í‹°-í—¤ë“œ ì–´í…ì…˜ (ì²«ë²ˆì§¸ ì„œë¸Œì¸µ / ë§ˆìŠ¤í¬ë“œ ì…€í”„ ì–´í…ì…˜) attention1 = MultiHeadAttention( d_model, num_heads, name=&quot;attention_1&quot;)(inputs=&#123; &#x27;query&#x27;: inputs, &#x27;key&#x27;: inputs, &#x27;value&#x27;: inputs, # Q = K = V &#x27;mask&#x27;: look_ahead_mask # ë£©ì–´í—¤ë“œ ë§ˆìŠ¤í¬ &#125;) # ì”ì°¨ ì—°ê²°ê³¼ ì¸µ ì •ê·œí™” attention1 = tf.keras.layers.LayerNormalization( epsilon=1e-6)(attention1 + inputs) # ë©€í‹°-í—¤ë“œ ì–´í…ì…˜ (ë‘ë²ˆì§¸ ì„œë¸Œì¸µ / ë””ì½”ë”-ì¸ì½”ë” ì–´í…ì…˜) attention2 = MultiHeadAttention( d_model, num_heads, name=&quot;attention_2&quot;)(inputs=&#123; &#x27;query&#x27;: attention1, &#x27;key&#x27;: enc_outputs, &#x27;value&#x27;: enc_outputs, # Q != K = V &#x27;mask&#x27;: padding_mask # íŒ¨ë”© ë§ˆìŠ¤í¬ &#125;) # ë“œë¡­ì•„ì›ƒ + ì”ì°¨ ì—°ê²°ê³¼ ì¸µ ì •ê·œí™” attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2) attention2 = tf.keras.layers.LayerNormalization( epsilon=1e-6)(attention2 + attention1) # í¬ì§€ì…˜ ì™€ì´ì¦ˆ í”¼ë“œ í¬ì›Œë“œ ì‹ ê²½ë§ (ì„¸ë²ˆì§¸ ì„œë¸Œì¸µ) outputs = tf.keras.layers.Dense(units=dff, activation=&#x27;relu&#x27;)(attention2) outputs = tf.keras.layers.Dense(units=d_model)(outputs) # ë“œë¡­ì•„ì›ƒ + ì”ì°¨ ì—°ê²°ê³¼ ì¸µ ì •ê·œí™” outputs = tf.keras.layers.Dropout(rate=dropout)(outputs) outputs = tf.keras.layers.LayerNormalization( epsilon=1e-6)(outputs + attention2) return tf.keras.Model( inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask], outputs=outputs, name=name) 1234567891011121314151617tf.keras.backend.clear_session()# Hyper-parametersD_MODEL = 256NUM_LAYERS = 2NUM_HEADS = 8DFF = 512DROPOUT = 0.1model = transformer( vocab_size=VOCAB_SIZE, num_layers=NUM_LAYERS, dff=DFF, d_model=D_MODEL, num_heads=NUM_HEADS, dropout=DROPOUT) (1, 10025, 256) (1, 10025, 256) 123456789101112131415161718192021222324class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule): def __init__(self, d_model, warmup_steps=4000): super(CustomSchedule, self).__init__() self.d_model = d_model self.d_model = tf.cast(self.d_model, tf.float32) self.warmup_steps = warmup_steps def __call__(self, step): arg1 = tf.math.rsqrt(step) arg2 = step * (self.warmup_steps**-1.5) return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)def loss_function(y_true, y_pred): y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1)) loss = tf.keras.losses.SparseCategoricalCrossentropy( from_logits=True, reduction=&#x27;none&#x27;)(y_true, y_pred) mask = tf.cast(tf.not_equal(y_true, 0), tf.float32) loss = tf.multiply(loss, mask) return tf.reduce_mean(loss) 1234567891011learning_rate = CustomSchedule(D_MODEL)optimizer = tf.keras.optimizers.Adam( learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)def accuracy(y_true, y_pred): # ë ˆì´ë¸”ì˜ í¬ê¸°ëŠ” (batch_size, MAX_LENGTH - 1) y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1)) return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy]) 123## ëª¨ë¸ í•™ìŠµEPOCHS = 50model.fit(dataset, epochs=EPOCHS) Epoch 1/50 104/104 [==============================] - 12s 54ms/step - loss: 1.2164 - accuracy: 0.0149 Epoch 2/50 104/104 [==============================] - 6s 54ms/step - loss: 1.0725 - accuracy: 0.0285 Epoch 3/50 104/104 [==============================] - 6s 54ms/step - loss: 0.9082 - accuracy: 0.0472 Epoch 4/50 104/104 [==============================] - 6s 54ms/step - loss: 0.7714 - accuracy: 0.0482 ... 104/104 [==============================] - 6s 54ms/step - loss: 0.0160 - accuracy: 0.1321 Epoch 46/50 104/104 [==============================] - 6s 56ms/step - loss: 0.0158 - accuracy: 0.1320 Epoch 47/50 104/104 [==============================] - 6s 55ms/step - loss: 0.0152 - accuracy: 0.1320 Epoch 48/50 104/104 [==============================] - 6s 54ms/step - loss: 0.0151 - accuracy: 0.1322 Epoch 49/50 104/104 [==============================] - 6s 55ms/step - loss: 0.0149 - accuracy: 0.1321 Epoch 50/50 104/104 [==============================] - 6s 53ms/step - loss: 0.0148 - accuracy: 0.1321 &lt;keras.callbacks.History at 0x7f794c0f0880&gt; 5. ì±—ë´‡ í‰ê°€í•˜ê¸° í•™ìŠµì‹œí‚¨ ì±—ë´‡ì— ìƒˆë¡œìš´ ë¬¸ì¥ì„ ë„£ì–´ì„œ í‰ê°€ 12345678910111213141516171819202122232425262728293031323334# ìƒˆë¡œìš´ ë¬¸ì¥ë„ ì¸ì½”ë” ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ë³€í˜•í•˜ëŠ” ì½”ë“œdef preprocess_sentence(sentence): # ë‹¨ì–´ì™€ êµ¬ë‘ì  ì‚¬ì´ì— ê³µë°± ì¶”ê°€. # ex) 12ì‹œ ë•¡! -&gt; 12ì‹œ ë•¡ ! sentence = re.sub(r&quot;([?.!,])&quot;, r&quot; \\1 &quot;, sentence) sentence = sentence.strip() return sentencedef evaluate(sentence): sentence = preprocess_sentence(sentence) sentence = tf.expand_dims( START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0) output = tf.expand_dims(START_TOKEN, 0) # ë””ì½”ë”ì˜ ì˜ˆì¸¡ ì‹œì‘ for i in range(MAX_LENGTH): predictions = model(inputs=[sentence, output], training=False) # í˜„ì¬(ë§ˆì§€ë§‰) ì‹œì ì˜ ì˜ˆì¸¡ ë‹¨ì–´ë¥¼ ë°›ì•„ì˜¨ë‹¤. predictions = predictions[:, -1:, :] predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32) # ë§Œì•½ ë§ˆì§€ë§‰ ì‹œì ì˜ ì˜ˆì¸¡ ë‹¨ì–´ê°€ ì¢…ë£Œ í† í°ì´ë¼ë©´ ì˜ˆì¸¡ì„ ì¤‘ë‹¨ if tf.equal(predicted_id, END_TOKEN[0]): break # ë§ˆì§€ë§‰ ì‹œì ì˜ ì˜ˆì¸¡ ë‹¨ì–´ë¥¼ ì¶œë ¥ì— ì—°ê²°í•œë‹¤. # ì´ëŠ” forë¬¸ì„ í†µí•´ì„œ ë””ì½”ë”ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©ë  ì˜ˆì •ì´ë‹¤. output = tf.concat([output, predicted_id], axis=-1) return tf.squeeze(output, axis=0) 123456789101112def predict(sentence): prediction = evaluate(sentence) predicted_sentence = tokenizer.decode( [i for i in prediction if i &lt; tokenizer.vocab_size]) print(&#x27;Master: &#123;&#125;&#x27;.format(sentence)) # print(&#x27;Output: &#123;&#125;&#x27;.format(predicted_sentence)) print(&#x27;Chatbot: &#123;&#125;&#x27;.format(predicted_sentence)) return predicted_sentence 12#predict() í•¨ìˆ˜ì— ë¬¸ì¥ì„ ì…ë ¥í•˜ë©´ í•´ë‹¹ ë¬¸ì¥ì— ëŒ€í•œ ê²°ê³¼ê°€ ì¶œë ¥ë¨output = predict(&quot;êµ¿ëª¨ë‹&quot;) Input: êµ¿ëª¨ë‹ Output: ì¢‹ì€ ì•„ì¹¨ì´ì—ìš” . 1output = predict(&quot;ì˜¤ëŠ˜ ë‚ ì”¨&quot;) Input: ì˜¤ëŠ˜ ë‚ ì”¨ Output: ì¶©ë¶„íˆ ì•„ë¦„ë‹¤ì›Œìš” . 1output = predict(&quot;ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?&quot;) Input: ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ? Output: ì˜¤ì „ì—” í™”ì°½í•˜ì§€ë§Œ ì˜¤í›„ì—ëŠ” ë¹„ê°€ ì˜¬ ê²ƒì…ë‹ˆë‹¤ . 1output = predict(&quot;ì§‘ì¤‘ë ¥&quot;) Input: ì§‘ì¤‘ë ¥ Output: ë³‘ì› ê°€ë³´ì„¸ìš” . 1output = predict(&quot;í‡´ê·¼&quot;) Input: í‡´ê·¼ Output: ì¸ìƒì€ ì±„ì›Œë‚˜ê°€ëŠ”ê±°ì£  . 1output = predict(&quot;ì•¼ê·¼ ì‹«ì–´&quot;) Input: ì•¼ê·¼ ì‹«ì–´ Output: ì–¼ë¥¸ ì§‘ì— ê°€ì„œ ì‰¬ì‹œê¸¸ ë°”ë„ê²Œìš” . 123output = str(input(&quot;ì˜¤í”¼ìŠ¤ ì±—ë´‡ì…ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?:&quot;))output = predict(output) Master: ì¼í•˜ê¸° ì‹«ì–´ Chatbot: ì €ë„ìš” ! ! Reference https:&#x2F;&#x2F;velog.io&#x2F;@seolini43&#x2F;íŒŒì´ì¬Transformerë¡œ-ì˜¤í”¼ìŠ¤-ì±—ë´‡-ë§Œë“¤ê¸°-ì½”ë“œ **code: https://github.com/jmj3047/mj_chatbot_prac&#x2F;blob&#x2F;c5bec233b833b24345deeffe7391621415dc1dcb&#x2F;chatbot_backend.py","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"}],"tags":[{"name":"Transformer","slug":"Transformer","permalink":"https://jmj3047.github.io/tags/Transformer/"},{"name":"Chatbot","slug":"Chatbot","permalink":"https://jmj3047.github.io/tags/Chatbot/"},{"name":"NLP","slug":"NLP","permalink":"https://jmj3047.github.io/tags/NLP/"}]},{"title":"MongoDB Update Operator","slug":"MongoDB_update","date":"2022-04-27T15:00:00.000Z","updated":"2022-05-02T00:59:39.551Z","comments":true,"path":"2022/04/28/MongoDB_update/","link":"","permalink":"https://jmj3047.github.io/2022/04/28/MongoDB_update/","excerpt":"","text":"$set: í•„ë“œê°’ì„ ì„¤ì •í•˜ê³  í•„ë“œê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒˆ í•„ë“œê°€ ìƒì„±ë¨. ìŠ¤í‚¤ë§ˆë¥¼ ê°±ì‹ í•˜ê±°ë‚˜ ì‚¬ìš©ì ì •ì˜ í‚¤ë¥¼ ì¶”ê°€ í• ë•Œ í¸ë¦¬í•¨. $unset: í‚¤ì™€ ê°’ì„ ëª¨ë‘ ì œê±°í•¨ 1234567891011121314&gt; db.users.insertOne(&#123;&quot;name&quot;:&quot;joe&quot;&#125;)&gt; db.users.updateOne(&#123;&quot;_id&quot; : ObjectId(&quot;6269e32b7b15b7097fbad433&quot;)&#125;,&#123;&quot;$set&quot;:&#123;&quot;favorite book&quot; : &quot;War and Peace&quot;&#125;&#125;)&#123; &quot;acknowledged&quot; : true, &quot;matchedCount&quot; : 1, &quot;modifiedCount&quot; : 1 &#125;&gt; db.users.findOne()&#123; &quot;_id&quot; : ObjectId(&quot;6269e32b7b15b7097fbad433&quot;), &quot;name&quot; : &quot;joe&quot;, &quot;favorite book&quot; : &quot;War and Peace&quot;&#125;&gt; db.users.updateOne(&#123;&quot;name&quot; : &quot;joe&quot;&#125;,&#123;&quot;$unset&quot;:&#123;&quot;favorite book&quot;:1&#125;&#125;)&#123; &quot;acknowledged&quot; : true, &quot;matchedCount&quot; : 1, &quot;modifiedCount&quot; : 1 &#125;&gt; db.users.find()&#123; &quot;_id&quot; : ObjectId(&quot;6269e32b7b15b7097fbad433&quot;), &quot;name&quot; : &quot;joe&quot; &#125; $inc: $setê³¼ ë¹„ìŠ·í•˜ì§€ë§Œ, ìˆ«ìë¥¼ ì¦ê°í•˜ê¸° ìœ„í•´ ì‚¬ìš©. int, long, double, decimal íƒ€ì… ê°’ì—ë§Œ ì‚¬ìš© ê°€ëŠ¥ 12345678910111213141516171819202122&gt;db.games.insertOne(&#123;&quot;game&quot; :&quot;pinball&quot;,&quot;user&quot; :&quot;joe&quot;&#125;)&#123; &quot;acknowledged&quot; : true, &quot;insertedId&quot; : ObjectId(&quot;6269e4be7b15b7097fbad436&quot;)&#125;&gt; db.games.find()&#123; &quot;_id&quot; : ObjectId(&quot;6269e4be7b15b7097fbad436&quot;), &quot;game&quot; : &quot;pinball&quot;, &quot;user&quot; : &quot;joe&quot; &#125;&gt; db.games.updateOne(&#123;&quot;game&quot; :&quot;pinball&quot;,&quot;user&quot; :&quot;joe&quot;&#125;,&#123;&quot;$inc&quot;:&#123;&quot;score&quot;:50&#125;&#125;)&#123; &quot;acknowledged&quot; : true, &quot;matchedCount&quot; : 1, &quot;modifiedCount&quot; : 1 &#125;&gt; db.games.find()&#123; &quot;_id&quot; : ObjectId(&quot;6269e4be7b15b7097fbad436&quot;), &quot;game&quot; : &quot;pinball&quot;, &quot;user&quot; : &quot;joe&quot;, &quot;score&quot; : 50 &#125;&gt; db.games.updateOne(&#123;&quot;game&quot; :&quot;pinball&quot;,&quot;user&quot; :&quot;joe&quot;&#125;,&#123;&quot;$inc&quot;:&#123;&quot;score&quot;:10000&#125;&#125;)&#123; &quot;acknowledged&quot; : true, &quot;matchedCount&quot; : 1, &quot;modifiedCount&quot; : 1 &#125;&gt; db.games.find()&#123; &quot;_id&quot; : ObjectId(&quot;6269e4be7b15b7097fbad436&quot;), &quot;game&quot; : &quot;pinball&quot;, &quot;user&quot; : &quot;joe&quot;, &quot;score&quot; : 10050 &#125;&gt; $push: ë°°ì—´ì´ ì´ë¯¸ ì¡´ì¬í•˜ì§€ë§Œ ë°°ì—´ ëì— ìš”ì†Œë¥¼ ì¶”ê°€í•˜ê³ , ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒˆë¡œìš´ ë°°ì—´ì„ ìƒì„±í•¨. $each: $pushì— $eachì œí•œìë¥¼ ì‚¬ìš©í•˜ë©´ ì‘ì—… í•œ ë²ˆìœ¼ë¡œ ê°’ì„ ì—¬ëŸ¬ê°œ ì¶”ê°€í•  ìˆ˜ ìˆìŒ. 12345678910111213141516171819202122&gt; db.blog.posts.findOne() &#123; &quot;_id&quot; : ObjectId(&quot;4b2d75476cc613d5ee930164&quot;), &quot;title&quot; : &quot;A blog post&quot;, &quot;content&quot; : &quot;...&quot;&#125; &gt; db.blog.posts.updateOne(&#123;&quot;title&quot; : &quot;A blog post&quot;&#125;, &#123;&quot;$push&quot; : &#123;&quot;comments&quot; : &#123;&quot;name&quot; : &quot;joe&quot;, &quot;email&quot; : &quot;joe@example.com&quot;, &quot;content&quot; : &quot;nice post.&quot;&#125;&#125;&#125;) &#123; &quot;acknowledged&quot; : true, &quot;matchedCount&quot; : 1, &quot;modifiedCount&quot; : 1 &#125; &gt; db.blog.posts.findOne() &#123; &quot;_id&quot; : ObjectId(&quot;4b2d75476cc613d5ee930164&quot;), &quot;title&quot; : &quot;A blog post&quot;, &quot;content&quot; : &quot;...&quot;, &quot;comments&quot; : [ &#123;&quot;name&quot; : &quot;joe&quot;, &quot;email&quot; : &quot;joe@example.com&quot;, &quot;content&quot; : &quot;nice post.&quot; &#125; ] &#125; $ne: ë°°ì—´ì´ ì¡´ì¬í•˜ì§€ ì•Šì„ ë•Œ í•´ë‹¹ ê°’ì„ ì¶”ê°€í•˜ë©´ì„œ ë°°ì—´ì„ ì§‘í•©ì²˜ëŸ¼ ì²˜ë¦¬í•  ë•Œ ì‚¬ìš©. $addToSet: ë‹¤ë¥¸ì£¼ì†Œë¥¼ ì¶”ê°€í•  ë•Œ ì¤‘ë³µì„ í”¼í•  ìˆ˜ ìˆìŒ ê³ ìœ í•œ ê°’ì„ ì—¬ëŸ¬ê°œ ì¶”ê°€í•˜ë ¤ë©´ $addToSet&#x2F;$eachì¡°í•©ì„ í™œìš©í•´ì•¼ í•¨. $ne&#x2F;$pushì¡°í•©ìœ¼ë¡œëŠ” í•  ìˆ˜ ì—†ìŒ. 123456789101112&gt; db.users.updateOne(&#123;&quot;_id&quot; : ObjectId(&quot;4b2d75476cc613d5ee930164&quot;)&#125;, &#123;&quot;$addToSet&quot; : &#123;&quot;emails&quot; : &#123;&quot;$each&quot; : [&quot;joe@php.net&quot;, &quot;joe@example.com&quot;, &quot;joe@python.org&quot;]&#125;&#125;&#125;) &#123; &quot;acknowledged&quot; : true, &quot;matchedCount&quot; : 1, &quot;modifiedCount&quot; : 1 &#125; &gt; db.users.findOne(&#123;&quot;_id&quot; : ObjectId(&quot;4b2d75476cc613d5ee930164&quot;)&#125;) &#123; &quot;_id&quot; : ObjectId(&quot;4b2d75476cc613d5ee930164&quot;), &quot;username&quot; : &quot;joe&quot;, &quot;emails&quot; : [ &quot;joe@example.com&quot;, &quot;joe@gmail.com&quot;, &quot;joe@yahoo.com&quot;, &quot;joe@hotmail.com&quot; &quot;joe@php.net&quot; &quot;joe@python.org&quot; ] &#125; Reference ëª½ê³ DB ì™„ë²½ ê°€ì´ë“œ: ì‹¤ì „ ì˜ˆì œë¡œ ë°°ìš°ëŠ” NoSQL ë°ì´í„°ë² ì´ìŠ¤ ê¸°ì´ˆë¶€í„° í™œìš©ê¹Œì§€","categories":[{"name":"Data Base","slug":"Data-Base","permalink":"https://jmj3047.github.io/categories/Data-Base/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://jmj3047.github.io/tags/MongoDB/"}]},{"title":"MongoDB Install & Basic Command","slug":"MongoDB_Install","date":"2022-04-24T15:00:00.000Z","updated":"2022-04-25T06:58:58.861Z","comments":true,"path":"2022/04/25/MongoDB_Install/","link":"","permalink":"https://jmj3047.github.io/2022/04/25/MongoDB_Install/","excerpt":"","text":"Link: www.mongodb.com/try/download/enterprise Download proper version of Mongodb Installì´ ì™„ë£Œëœ í›„ì—ëŠ” MongoDB í™˜ê²½ë³€ìˆ˜ ì„¤ì •ì„ ìœ„í•´ ì‹œìŠ¤í…œ í™˜ê²½ ë³€ìˆ˜ í¸ì§‘ì„ ì§„í–‰í•˜ì—¬ ì¤ë‹ˆë‹¤. í™˜ê²½ë³€ìˆ˜ í¸ì§‘ì„ ìœ„í•´ í™˜ê²½ë³€ìˆ˜ &gt;ì‹œìŠ¤í…œ ë³€ìˆ˜ Path ì„¤ì •ì„ ì„ íƒí•˜ì—¬ ì¤ë‹ˆë‹¤. ì„¤ì¹˜ëœ MongoDBì˜ biní´ë” ê²½ë¡œë¥¼ ì…ë ¥í•˜ì—¬ ì¤ë‹ˆë‹¤.(C:\\Program Files\\MongoDB\\Server\\5.0\\bin) ì €ì¥ í›„ cmdì°½ì—ì„œ mongdb â€“versionì„ í†µí•´ ì •ìƒ ì„¤ì¹˜ë¥¼ í™•ì¸í•˜ì—¬ ì¤ë‹ˆë‹¤. cmd ì°½ì— mongodb ì‹¤í–‰ 1&gt;mongo ëª…ë ¹ì–´ ë‘ì¤„ë¡œ ì˜ ì‹¤í–‰ë˜ëŠ”ì§€ ê°„ë‹¨íˆ í™•ì¸ 12&gt; db.world.insert(&#123; &quot;speech&quot; : &quot;Hello World!&quot; &#125;);&gt; cur = db.world.find();x=cur.next();print(x[&quot;speech&quot;]); Basic Commandì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  ë°ì´í„°ë² ì´ìŠ¤ í‘œì‹œ : 1show dbs; ì•¡ì„¸ìŠ¤ í•  íŠ¹ì • ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì„ íƒ (Ex: mydb . ì´ë¯¸ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ mydb ê°€ ìƒì„±ë©ë‹ˆë‹¤ : 1use mydb; ë°ì´í„°ë² ì´ìŠ¤ì— ëª¨ë“  ì½œë ‰ì…˜ì„ í‘œì‹œ. ë¨¼ì € ì½œë ‰ì…˜ì„ ì„ íƒí•˜ì‹­ì‹œì˜¤ (ìœ„ ì°¸ì¡°). 1show collections; ë°ì´í„°ë² ì´ìŠ¤ì™€ í•¨ê»˜ ì‚¬ìš©í•  ìˆ˜ìˆëŠ” ëª¨ë“  ê¸°ëŠ¥ í‘œì‹œ : 1db.mydb.help(); í˜„ì¬ ì„ íƒí•œ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ í™•ì¸ 12&gt; dbmydb db.dropDatabase() ëª…ë ¹ì€ ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì‚­ì œí•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. 1db.dropDatabase() Reference https://khj93.tistory.com/entry/MongoDB-Windowì—-MongoDB-ì„¤ì¹˜í•˜ê¸° https://learntutorials.net/ko/mongodb/topic/691/mongodb-ì‹œì‘í•˜ê¸°","categories":[{"name":"Data Base","slug":"Data-Base","permalink":"https://jmj3047.github.io/categories/Data-Base/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://jmj3047.github.io/tags/MongoDB/"}]},{"title":"MongoDB CRUD","slug":"MongoDB_CRUD","date":"2022-04-24T15:00:00.000Z","updated":"2022-05-02T01:47:30.455Z","comments":true,"path":"2022/04/25/MongoDB_CRUD/","link":"","permalink":"https://jmj3047.github.io/2022/04/25/MongoDB_CRUD/","excerpt":"","text":"Initial setting 12345678&gt;dbtest&gt;use videoswitched to db video # if video doesnt exist, created&gt;dbvideo&gt;db.movies # created movies collectionvideo.movies Create: insertOne í•¨ìˆ˜ 1234567891011121314151617181920&gt;movie = &#123;&quot;title&quot; : &quot;Star Wars: Episode IV - A New Hope&quot;, &quot;director&quot; : &quot;George Lucas&quot;, &quot;year&quot; : 1977&#125;&#123; &quot;title&quot; : &quot;Star Wars: Episode IV - A New Hope&quot;, &quot;director&quot; : &quot;George Lucas&quot;, &quot;year&quot; : 1977&#125;&gt;db.movies.insertOne(movie) #ì˜í™”ê°€ ë°ì´í„° ë² ì´ìŠ¤ì— ì €ì¥ë¨&#123; &quot;acknowledged&quot; : true, &quot;insertedId&quot; : ObjectId(&quot;6266057e4619361339f0c881&quot;)&#125;#Find í•¨ìˆ˜ë¡œ í˜¸ì¶œ&gt;db.movies.find().pretty()&#123; &quot;_id&quot; : ObjectId(&quot;6266057e4619361339f0c881&quot;), &quot;title&quot; : &quot;Star Wars: Episode IV - A New Hope&quot;, &quot;director&quot; : &quot;George Lucas&quot;, &quot;year&quot; : 1977&#125; Read: find, findOne í•¨ìˆ˜ 1234567&gt; db.movies.findOne(movie) &#123; &quot;_id&quot; : ObjectId(&quot;6266057e4619361339f0c881&quot;), &quot;title&quot; : &quot;Star Wars: Episode IV - A New Hope&quot;, &quot;director&quot; : &quot;George Lucas&quot;, &quot;year&quot; : 1977&#125; Update : updateOne í•¨ìˆ˜ 1234567891011&gt; db.movies.updateOne(&#123;title : &quot;Star Wars: Episode IV - A New Hope&quot;&#125;, &#123;$set : &#123;reviews: []&#125;&#125;) &#123; &quot;acknowledged&quot; : true, &quot;matchedCount&quot; : 1, &quot;modifiedCount&quot; : 1 &#125;&gt; db.movies.find().pretty()&#123; &quot;_id&quot; : ObjectId(&quot;6266057e4619361339f0c881&quot;), &quot;title&quot; : &quot;Star Wars: Episode IV - A New Hope&quot;, &quot;director&quot; : &quot;George Lucas&quot;, &quot;year&quot; : 1977, &quot;reviews&quot; : [ ]&#125; Delete: deleteOne, deleteMany í•¨ìˆ˜ 12&gt;db.movies.deleteOne(&#123;title : &quot;Star Wars: Episode IV - A New Hope&quot;&#125;)&#123; &quot;acknowledged&quot; : true, &quot;deletedCount&quot; : 1 &#125; Reference ëª½ê³ DB ì™„ë²½ ê°€ì´ë“œ: ì‹¤ì „ ì˜ˆì œë¡œ ë°°ìš°ëŠ” NoSQL ë°ì´í„°ë² ì´ìŠ¤ ê¸°ì´ˆë¶€í„° í™œìš©ê¹Œì§€","categories":[{"name":"Data Base","slug":"Data-Base","permalink":"https://jmj3047.github.io/categories/Data-Base/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://jmj3047.github.io/tags/MongoDB/"}]},{"title":"BeautifulSoup Quick Start","slug":"BeautifulSoup_QuickStart","date":"2022-04-21T15:00:00.000Z","updated":"2022-04-22T02:53:21.057Z","comments":true,"path":"2022/04/22/BeautifulSoup_QuickStart/","link":"","permalink":"https://jmj3047.github.io/2022/04/22/BeautifulSoup_QuickStart/","excerpt":"","text":"Index_prac.html123456789101112131415161718192021222324252627&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;title&gt; The Dormouse&#x27;s story &lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class=&quot;title&quot;&gt;&lt;b&gt;The Dormouse&#x27;s story&lt;/b&gt;&lt;/p&gt; &lt;p class=&quot;story&quot;&gt; Once upon a time there were three little sisters; and their names were &lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;, &lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and &lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class=&quot;story&quot;&gt;To Be Continued...&lt;/p&gt; &lt;/body&gt;&lt;/html&gt; Quick Start.py123456789101112131415161718192021222324252627282930313233343536373839404142434445from bs4 import BeautifulSoupsoup = BeautifulSoup(open(&quot;index_prac.html&quot;), &#x27;html.parser&#x27;)# print allprint(soup.prettify())# navigate that data structureprint(soup.title)# &lt;title&gt;The Dormouse&#x27;s story&lt;/title&gt;print(soup.title.name)# u&#x27;title&#x27;print(soup.title.string)# u&#x27;The Dormouse&#x27;s story&#x27;print(soup.title.parent.name)# u&#x27;head&#x27;print(soup.p)# &lt;p class=&quot;title&quot;&gt;&lt;b&gt;The Dormouse&#x27;s story&lt;/b&gt;&lt;/p&gt;print(soup.p[&#x27;class&#x27;])# u&#x27;title&#x27;print(soup.a)# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;print(soup.find_all(&#x27;a&#x27;))# [&lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;,# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;,# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;]print(soup.find(id=&quot;link3&quot;))# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;## extracting all the URLsfor link in soup.find_all(&#x27;a&#x27;): print(link.get(&#x27;href&#x27;))# http://example.com/elsie# http://example.com/lacie# http://example.com/tillie## extracting all the text from a pageprint(soup.get_text()) Reference: https://www.crummy.com/software/BeautifulSoup/bs4/doc/#quick-start","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"HTML","slug":"Python/HTML","permalink":"https://jmj3047.github.io/categories/Python/HTML/"}],"tags":[{"name":"HTML","slug":"HTML","permalink":"https://jmj3047.github.io/tags/HTML/"},{"name":"Web Crawling","slug":"Web-Crawling","permalink":"https://jmj3047.github.io/tags/Web-Crawling/"},{"name":"BeautifulSoup","slug":"BeautifulSoup","permalink":"https://jmj3047.github.io/tags/BeautifulSoup/"}]},{"title":"Basic Web Crawling","slug":"Web_Crawling_Basic","date":"2022-04-21T15:00:00.000Z","updated":"2022-04-22T07:30:27.797Z","comments":true,"path":"2022/04/22/Web_Crawling_Basic/","link":"","permalink":"https://jmj3047.github.io/2022/04/22/Web_Crawling_Basic/","excerpt":"","text":"Crawling Tools -Beautifulsoup: íŒŒì´ì¬ì—ì„œ ê°€ì¥ ì¼ë°˜ì ì¸ ìˆ˜ì§‘ë„êµ¬(CSS í†µí•´ì„œ ìˆ˜ì§‘) -Scrapy (CSS, XPATH í˜•íƒœë¡œ ìˆ˜ì§‘) -Selenium (CSS, XPATH í†µí•´ì„œ ë°ì´í„° ìˆ˜ì§‘ + Java Script) â†’ìë°” í•„ìš” + ëª‡ê°œ ì„¤ì¹˜ ë„êµ¬ í•„ìš” ì›¹ì‚¬ì´íŠ¸ ë§Œë“œëŠ” 3ëŒ€ ì¡°ê±´ +1 :HTML, CSS, JavaScript, Ajax(ë¹„ë™ê¸°ì²˜ë¦¬) ì›¹ì‚¬ì´íŠ¸ êµ¬ë™ë°©ì‹ :GET &#x2F; POST Create virtual env(git bash)123pip install virtualenvpython -m virtualenv venvsource venv/Scripts/activate Installing Library123pip install beautifulsoup4pip install numpy pandas matplotlib seabornpip install requests Reference: https://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-beautiful-soup","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"HTML","slug":"Python/HTML","permalink":"https://jmj3047.github.io/categories/Python/HTML/"}],"tags":[{"name":"HTML","slug":"HTML","permalink":"https://jmj3047.github.io/tags/HTML/"},{"name":"Web Crawling","slug":"Web-Crawling","permalink":"https://jmj3047.github.io/tags/Web-Crawling/"},{"name":"BeautifulSoup","slug":"BeautifulSoup","permalink":"https://jmj3047.github.io/tags/BeautifulSoup/"}]},{"title":"Web Crawling Practice","slug":"Web_Crawling_Headline","date":"2022-04-21T15:00:00.000Z","updated":"2022-04-22T08:17:11.127Z","comments":true,"path":"2022/04/22/Web_Crawling_Headline/","link":"","permalink":"https://jmj3047.github.io/2022/04/22/Web_Crawling_Headline/","excerpt":"","text":"1. Crawling Headline news from Naver12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import requestsfrom bs4 import BeautifulSoupimport pandas as pddef crawling(soup): div = soup.find(&quot;div&quot;, class_=&#x27;list_issue&#x27;) # print(type(div)) print(div.find_all(&#x27;a&#x27;)) #listí˜•íƒœ result = [] urls = [] for a in div.find_all(&quot;a&quot;): # print(a.get_text()) urls.append(a[&#x27;href&#x27;]) result.append(a.get_text()) # print(result) #save as csv file df = pd.DataFrame(&#123;&#x27;news_title&#x27;: result, &quot;url&quot;: urls&#125;) print(df) df.to_csv(&quot;newscrawling.csv&quot;) print(&quot;Crawling Success!&quot;) def main(): CUSTOM_HEADER = &#123; &#x27;referer&#x27; : &#x27;https://www.naver.com/&#x27;, &#x27;user-agent&#x27; : &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36&#x27; &#125; url = &#x27;https://www.naver.com/&#x27; req = requests.get(url = url, headers=CUSTOM_HEADER) print(req.status_code) #200: ì •ìƒì ìœ¼ë¡œ ì‚¬ì´íŠ¸ ëŒì•„ê°€ê³  ìˆìŒ #404: ì£¼ì†Œ ì˜¤ë¥˜ #503: ì„œë²„ ë‚´ë ¤ì§„ ìƒíƒœ #print(req.text)ì´ê±°ë¥¼ beautifulsoupê°ì²´ë¡œ ë°”ê¿”ì¤Œ soup = BeautifulSoup(req.text, &#x27;html.parser&#x27;) print(type(soup)) # print(soup.find(&quot;strong&quot;, class_=&#x27;new&#x27;)) # print(soup.find_all(&quot;span&quot;, class_=&#x27;blind&#x27;)) #print(soup.find(&quot;a&quot;, class_=&#x27;link_newsstand&#x27;)) crawling(soup)if __name__ == &quot;__main__&quot;: main() 2. Crawling Product List from ACBF1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import requestsfrom bs4 import BeautifulSoupimport pandas as pddef crawling(soup): div = soup.find_all(&quot;div&quot;, class_=&#x27;name&#x27;) print(type(div)) #&lt;class &#x27;bs4.element.ResultSet&#x27;&gt; # print(div) product_name = [] # urls =[] for a in div: # print(a.get_text()) # urls.append(a.get(&#x27;href&#x27;)) product_name.append(a.get_text()) print(product_name) # df = pd.DataFrame(&#123;&#x27;news_title&#x27;: product_name&#125;) # print(df) # df.to_csv(&quot;suit_product.csv&quot;) print(&quot;Crawling Success!&quot;) def main(): CUSTOM_HEADER = &#123; &#x27;referer&#x27; : &#x27;https://anyonecanbeafuse.com/category/%EC%88%98%ED%8A%B8/89/&#x27;, &#x27;user-agent&#x27; : &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36&#x27; &#125; url = &#x27;https://anyonecanbeafuse.com/category/%EC%88%98%ED%8A%B8/89/&#x27; req = requests.get(url = url, headers=CUSTOM_HEADER) print(req.status_code) #200: ì •ìƒì ìœ¼ë¡œ ì‚¬ì´íŠ¸ ëŒì•„ê°€ê³  ìˆìŒ #404: ì£¼ì†Œ ì˜¤ë¥˜ #503: ì„œë²„ ë‚´ë ¤ì§„ ìƒíƒœ #print(req.text)ì´ê±°ë¥¼ beautifulsoupê°ì²´ë¡œ ë°”ê¿”ì¤Œ soup = BeautifulSoup(req.text, &#x27;html.parser&#x27;) #print(type(soup)) # print(soup.find_all(&quot;div&quot;, class_=&#x27;name&#x27;)) crawling(soup)if __name__ == &quot;__main__&quot;: main() 3. Crawling Music Title from Chart123456789101112131415161718192021222324252627282930313233343536373839404142import requestsfrom bs4 import BeautifulSoupdef crawling(soup): tbody_df = soup.find(&quot;tbody&quot;) # print(tbody_df) result = [] for a in tbody_df.find_all(&#x27;p&#x27;, class_=&#x27;title&#x27;): # print(a.get_text()) # print(type(a.get_text())) result.append(a.get_text().strip(&quot;\\n&quot;)) print(result) print(&quot;Crawling Success!&quot;) def main(): CUSTOM_HEADER = &#123; &#x27;referer&#x27; : &#x27;https://music.bugs.co.kr/chart&#x27;, #í•„ìˆ˜ ì•„ë‹˜ &#x27;user-agent&#x27; : &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36&#x27; &#125; url = &#x27;https://music.bugs.co.kr/chart&#x27; req = requests.get(url = url, headers=CUSTOM_HEADER) print(req.status_code) #200: ì •ìƒì ìœ¼ë¡œ ì‚¬ì´íŠ¸ ëŒì•„ê°€ê³  ìˆìŒ #404: ì£¼ì†Œ ì˜¤ë¥˜ #503: ì„œë²„ ë‚´ë ¤ì§„ ìƒíƒœ #print(req.text)ì´ê±°ë¥¼ beautifulsoupê°ì²´ë¡œ ë°”ê¿”ì¤Œ soup = BeautifulSoup(req.text, &#x27;html.parser&#x27;) print(type(soup)) #&lt;class &#x27;bs4.BeautifulSoup&#x27;&gt; # print(soup.find_all(&quot;p&quot;, class_=&#x27;title&#x27;)) # print(soup.find_all(&quot;span&quot;, class_=&#x27;blind&#x27;)) #print(soup.find(&quot;a&quot;, class_=&#x27;link_newsstand&#x27;)) crawling(soup)if __name__ == &quot;__main__&quot;: main()","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"HTML","slug":"Python/HTML","permalink":"https://jmj3047.github.io/categories/Python/HTML/"}],"tags":[{"name":"HTML","slug":"HTML","permalink":"https://jmj3047.github.io/tags/HTML/"},{"name":"Web Crawling","slug":"Web-Crawling","permalink":"https://jmj3047.github.io/tags/Web-Crawling/"},{"name":"BeautifulSoup","slug":"BeautifulSoup","permalink":"https://jmj3047.github.io/tags/BeautifulSoup/"}]},{"title":"Improved Training of Wasserstein GANs","slug":"WGAN","date":"2022-04-21T15:00:00.000Z","updated":"2022-07-09T04:06:27.918Z","comments":true,"path":"2022/04/22/WGAN/","link":"","permalink":"https://jmj3047.github.io/2022/04/22/WGAN/","excerpt":"","text":"Journal&#x2F;Conference: NIPSYear(published year): 2017Author: Ishaan Gulrajani, Faruk Ahmed, MartÃ­n Arjovsky, Vincent Dumoulin, Aaron C. CourvilleSubject: DCGAN, Generative Model Improved Training of Wasserstein GANs Summary ê¸°ì¡´ì˜ Wasserstein-GAN ëª¨ë¸ì˜ weight clipping ì„ ëŒ€ì²´í•  ìˆ˜ ìˆëŠ” gradient penalty ë°©ë²•ì„ ì œì‹œ hyperparameter tuning ì—†ì´ë„ ì•ˆì •ì ì¸ í•™ìŠµì´ ê°€ëŠ¥í•´ì¡ŒìŒì„ ì œì‹œ IntroductionGAN ëª¨ë¸ì„ ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµí•˜ê¸° ìœ„í•œ ë§ì€ ë°©ë²•ë“¤ì´ ì¡´ì¬í•´ì™”ìŠµë‹ˆë‹¤. íŠ¹íˆ, ê°€ì¹˜í•¨ìˆ˜ê°€ ìˆ˜ë ´í•˜ëŠ” ì„±ì§ˆì„ ë¶„ì„í•˜ì—¬ Discriminator(ì´í›„ Critic)ê°€ 1-Lipschitz function ê³µê°„ì— ìˆë„ë¡ í•˜ëŠ” Wasserstein GAN(WGAN) ì´ ì œì‹œëœ ë°” ìˆìŠµë‹ˆë‹¤. ë…¼ë¬¸ì€ WGAN ì˜ ë‹¨ì ì„ ê°œì„ í•œ WGAN-GP ëª¨ë¸ì„ ì œì‹œí•©ë‹ˆë‹¤. Toy datasetsì— ëŒ€í•´ criticì˜ weight clippingì´ undesired behaviorë¥¼ ìœ ë°œí•  ìˆ˜ ìˆìŒì„ ì¦ëª… â€œGradient penaltyâ€(WGAN-GP) ê¸°ë²•ìœ¼ë¡œ ì œì•ˆ ë‹¤ì–‘í•œ GAN êµ¬ì¡°ì—ëŒ€í•´ ì•ˆì •ì ì¸ í•™ìŠµì„ ìˆ˜í–‰í•  ìˆ˜ ìˆê³ , ê³ í’ˆì§ˆ ì´ë¯¸ì§€ ìƒì„±ì„ ìˆ˜í–‰í•˜ë©°, ê°œë³„ ìƒ˜í”Œë§ì´ í•„í•„ìš”í•˜ì§€ ì•ŠëŠ” ë¬¸ììˆ˜ì¤€ ì–¸ì–´ ëª¨ë¸ì„ ì œì‹œ BackgroundGenerative adversarial networksì¼ë°˜ì ì¸ GAN êµ¬ì¡°ì— ëŒ€í•´ ë‹¤ì‹œ í•œë²ˆ ê°œë…ì„ ë˜ì§šìŠµë‹ˆë‹¤. Wasserstein GANsWGAN ì€ GAN ì˜ ëª©ì í•¨ìˆ˜ì¸ JSD ê°€ parameter ì— ì—°ì†ì ì´ì§€ ì•Šì— í•™ìŠµì— ë¬¸ì œê°€ ë°œìƒí•¨ì„ ì§€ì í•˜ì˜€ìŠµë‹ˆë‹¤. ì´ì—, Earth-Mover distance ë¡œ ëª¨ë“  êµ¬ê°„ì—ì„œ ì—°ì†ì ì´ê³  ëŒ€ë¶€ë¶„ì˜ êµ¬ê°„ì—ì„œ ë¯¸ë¶„ ê°€ëŠ¥í•˜ê²Œ í•˜ì—¬ ë¬¸ì œë¥¼ í•´ê²°í•˜ì˜€ìŠµë‹ˆë‹¤. ì´ì™¸ì—ë„ WGAN ì˜ íŠ¹ì§•ì— ëŒ€í•´ ì„œìˆ í•˜ë©°, ê°€ì¥ ì¤‘ìš”í•œ íŠ¹ì§•ìœ¼ë¡œ Lipschitz ì¡°ê±´ì„ ë§Œì¡±í•˜ê¸° ìœ„í•´ ì‹œí–‰í•˜ëŠ” weight clipping ì„ ì–¸ê¸‰í•©ë‹ˆë‹¤. Properties of the optimal WGAN criticìµœì ì˜ WGAN critic ì„ ê°€ì •í–ˆì„ ë•Œ, ****weight clippingì´ WGAN criticì—ì„œ ë¬¸ì œë¥¼ ë°œìƒì‹œí‚´ì„ ì–¸ê¸‰í•˜ê³  ì¦ëª…í•œ ê²°ê³¼ë¥¼ ì œì‹œí•©ë‹ˆë‹¤. Difficulties with weight constraintsWGANì˜ weight clippingì´ ìµœì í™”ì— ë¬¸ì œë¥¼ ë°œìƒì‹œí‚¬ ìˆ˜ ìˆê³ , ìµœì í™”ê°€ ì˜ ë˜ë”ë¼ë„ criticì´ pathological value surface ì„ ê°€ì§ˆ ìˆ˜ ìˆìŒì„ ì¦ëª…í•˜ì˜€ë˜ ë‚´ìš©ì„ í™•ì¸í•˜ê¸° ìœ„í•œ ì‹¤í—˜ì„ ì§„í–‰í•©ë‹ˆë‹¤. ë…¼ë¬¸ì€ ê¸°ì¡´ WGAN ì´ ì œì‹œí•˜ì˜€ë˜ hard clipping ë°©ì‹ ì´ì™¸ì—ë„, L2 norm clipping&#x2F;weight normalization&#x2F;L1 and L2 weight decay ë“±ì˜ weight constraint ë¥¼ ê°€ì •í•˜ì˜€ì„ ë•Œ ëª¨ë‘ ë¹„ìŠ·í•œ ë¬¸ì œê°€ ë°œìƒí•˜ì˜€ìŒì„ ì–¸ê¸‰í•©ë‹ˆë‹¤. Capacity underuse &amp; Exploding and vanishing gradientsk-Lipshitz ì¡°ê±´ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•´ weight clipping ì„ ìˆ˜í–‰í•˜ì˜€ì„ ë•Œ, criticì€ ë”ìš± ë‹¨ìˆœí•œ í˜•íƒœì˜ í•¨ìˆ˜ë¥¼ ì·¨í•˜ê²Œ ë©ë‹ˆë‹¤. ë…¼ë¬¸ì€ ì´ë¥¼ ì¦ëª…í•˜ê¸° ìœ„í•´ Generator ì˜ ë¶„í¬ë¥¼ toy distribution + unit-variance ê°€ìš°ì‹œì•ˆ-ë…¸ì´ì¦ˆì— ê³ ì •í•œë’¤, weight clipping ê³¼ í•¨ê»˜ WGAN critic ì„ í•™ìŠµí•œ ê²°ê³¼ë¥¼ ì œì‹œí•©ë‹ˆë‹¤. ì™¼ìª½ ê·¸ë¦¼ì—ì„œ Weight clipping ìˆ˜í–‰í•œ ê²½ìš°ì˜ value surface ëª¨ì–‘ì´ ë‹¨ìˆœí•´ì¡ŒìŒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, ìš°ì¸¡ ê·¸ë¦¼ê³¼ ê°™ì´, Gradient penalty ë¥¼ ìˆ˜í–‰í•œ ê²½ìš°ì— gradient vanishing ì´ë‚˜ exploding ì´ ë°œìƒí•˜ì§€ ì•Šì•˜ìŒì„ ì œì‹œí•©ë‹ˆë‹¤. Gradient penaltyWeight Clipping ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  Lipschitz constraint ë¥¼ ìœ ì§€í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤. ì…ë ¥ì— ëŒ€í•œ Critic ì¶œë ¥ gradient ì˜ í¬ê¸°ë¥¼ ì§ì ‘ ì œì•½í•©ë‹ˆë‹¤. ì´ ë•Œ, tractability issue ë¥¼ í”¼í•˜ê¸° ìœ„í•´ ë¬´ì‘ìœ„ë¡œ ì¶”ì¶œí•œ ìƒ˜í”Œ $\\hat{x}$ ì˜ gradient norm ì„ ì‚¬ìš©í•´ soft í•œ ì œì•½ì„ ì¤ë‹ˆë‹¤. ì´ë ‡ê²Œ ìƒˆë¡­ê²Œ ì •ì˜ë˜ëŠ” ëª©ì í•¨ìˆ˜ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. Sampling distributionë…¼ë¬¸ì€ ë°ì´í„° ë¶„í¬ì™€ generator ë¶„í¬ì—ì„œ ìƒ˜í”Œë§í•œ ì ì˜ ìŒì„ ì´ì€ ë’¤, ì ì„ ì‡ëŠ” ì„ ë¶„ì„ ë”°ë¼ $\\hat{x}$ ë¥¼ ìƒ˜í”Œë§í•˜ì˜€ê³ , ì‹¤í—˜ì ìœ¼ë¡œ ì¢‹ì€ ì„±ëŠ¥ì„ ì–»ì—ˆìŒì„ ì–¸ê¸‰í•©ë‹ˆë‹¤. Penalty coefficientgradient penalty ë¥¼ ê°€í•˜ëŠ” ì •ë„ë¥¼ ê²½ì •í•˜ëŠ” ê³„ìˆ˜ë¡œ, ë…¼ë¬¸ì—ì„œëŠ” ëª¨ë‘ $\\lambda&#x3D;10$ ì„ ì‚¬ìš©í–ˆìŒì„ ì–¸ê¸‰í•©ë‹ˆë‹¤. No critic batch normalizationê¸°ì¡´ GAN ëª¨ë¸ì€ batch normalization ì„ ëª¨ë“  ê³³ì—ì„œ ì‚¬ìš©í–ˆì§€ë§Œ, ì´ëŠ” discriminatorì˜ ë‹¨ì¼ ì…ë ¥ì„ ë‹¨ì¼ ì¶œë ¥ìœ¼ë¡œ ë§¤í•‘í•˜ëŠ” ë¬¸ì œì—ì„œ, ì…ë ¥ì˜ ì „ì²´ ë°°ì¹˜ë¡œë¶€í„° ì¶œë ¥ì˜ ë°°ì¹˜ë¡œ ë§¤í•‘í•˜ëŠ” ë¬¸ì œë¡œ ë³€í™”ì‹œí‚µë‹ˆë‹¤. ì´ ë•Œë¬¸ì— gradient penalty ë¥¼ ìˆ˜í–‰í•˜ë©´ batch normalization ì´ ìœ íš¨í•˜ì§€ ì•Šì€ ê²°ê³¼ê°€ ë°œìƒí•œë‹¤ê³  í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ë…¼ë¬¸ì€ critic ì— batch normalization ì„ ì œê±°í•˜ì˜€ê³  ê·¸ëŸ¼ì—ë„ ì ì ˆí•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŒì„ ì–¸ê¸‰í•©ë‹ˆë‹¤. Two-sided penaltygradient penalty ëŠ” normì´ 1 ì•„ë˜ì— ë¨¸ë¬´ë¥´ì§€ ì•Šê³ (one-sided penalty), 1ë¡œ í–¥í•˜ê¸°(two-sided penalty)ëŠ” ê²ƒì„ ì´‰ì§„í•œë‹¤ëŠ” ì ì„ ì œì‹œí•©ë‹ˆë‹¤. ExperimentsTraining random architectures within a set ì¼ë°˜ì ì¸ DCGAN êµ¬ì¡°ì—ì„œ ìœ„ì˜ í‘œì˜ ì„¤ì •ì„ ëœë¤í•˜ê²Œ ì„¤ì •í•˜ì—¬ ëª¨ë¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ ë¬´ì‘ìœ„ë¡œ 200ê°œì˜ ëª¨ë¸ì„ êµ¬ì„±í•œë’¤, 32x32 ImageNet ì— ëŒ€í•´ WGAN-GP, standard GANì„ í•©ë‹ˆë‹¤. êµ¬ì„±í•œ ëª¨ë¸ì˜ inception_score ê°€ min_score ë³´ë‹¤ í° ê²½ìš° ì„±ê³µìœ¼ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤. WGAN-GP ëŠ” ë§ì€ êµ¬ì¡°ë¥¼ í•™ìŠµí•˜ëŠ”ë° ì„±ê³µí–ˆë‹¤ëŠ” ê²°ê³¼ë¥¼ ì œì‹œí•©ë‹ˆë‹¤. Training varied architectures on LSUN bedroomsì•„ë˜ì™€ ê°™ì´ 6ê°œì˜ ëª¨ë¸ì„ ê¸°ë³¸ ëª¨ë¸ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. ì—¬ê¸°ì— DCGAN, LSGAN, WGAN, WGAN-GP ë¥¼ ê°ê° ì ìš©í•˜ì˜€ì„ ë•Œì˜ ì„±ëŠ¥ì„ ë¹„êµí•©ë‹ˆë‹¤. ë‹¨, WGAN-GPëŠ” discriminator ì—ì„œ Batch normalization ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ê¸°ì— layer normalization ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. WGAN-GP ë¥¼ ì œì™¸í•œ ëª¨ë“  ëª¨ë¸ì—ì„œ ë¶ˆì•ˆì •í•˜ê±°ë‚˜ mode collapse ì— ë¹ ì§„ ëª¨ìŠµì„ ë³´ì…ë‹ˆë‹¤. Improved performance over weight clippingWGAN-GP ê°€ weight clipping ì— ë¹„í•´ ë” ë¹ ë¥¸ í•™ìŠµ ì†ë„ì™€ ìƒ˜í”Œ íš¨ìœ¨ì„ ë³´ì¸ë‹¤ëŠ” ì ì„ ì¦ëª…í•˜ê¸° ìœ„í•œ ì‹¤í—˜ ê²°ê³¼ë¥¼ ì œì‹œí•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´, WGAN ê³¼ WGAN-GP ëª¨ë¸ì„ CIFAR-10 ìœ¼ë¡œ í•™ìŠµí•˜ì—¬ Inception Score ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. ì™¼ìª½ì€ iterationì— ë”°ë¥¸ Inception Scoreì´ë©°, ì˜¤ë¥¸ìª½ì€ ì‹œê°„ì— ë”°ë¥¸ Inception Scoreì…ë‹ˆë‹¤. WGAN-GPëŠ” weight clipping ë³´ë‹¤ í•­ìƒ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. ì´ëŠ” ê°™ì€ optimizer ë¥¼ ì‚¬ìš©í–ˆì„ ë•Œë„ ë§ˆì°¬ê°€ì§€ì´ë©°, ë¹„ë¡ DCGAN ë³´ë‹¤ëŠ” ëŠë¦¬ì§€ë§Œ ìˆ˜ë ´ì— ìˆì–´ì„œ ì•ˆì •ì ì¸ ì ìˆ˜ë¥¼ ë³´ì¼ ìˆ˜ ìˆìŒì„ ì œì‹œí•©ë‹ˆë‹¤. Sample quality on CIFAR-10 and LSUN bedrooms CIFAR-10ìœ¼ë¡œ í•™ìŠµí•œ ëª¨ë¸ì˜ Inception score ë¥¼ ê³„ì‚°í•˜ì—¬ ë‹¤ì–‘í•œ êµ¬ì¡°ì˜ GANì„ ë¹„êµí•œ í‘œë¥¼ ì œì‹œí•©ë‹ˆë‹¤. WGAN-GP ëŠ” Supervised ì˜ ê²½ìš° SGAN ì„ ì œì™¸í–ˆì„ ë•Œ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. ë˜í•œ, WGAN-GP ë¡œ deep ResNet ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ 128X128 LSUN ì¹¨ëŒ€ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ì—¬ ìœ„ì™€ê°™ì€ ê²°ê³¼ë¥¼ ì œì‹œí•©ë‹ˆë‹¤. Modeling discrete data with a continuous generator Generator ëŠ” ì—°ì†ì ì¸ ë¶„í¬ì˜ í•¨ìˆ˜ë¥¼ ê°€ì •í•©ë‹ˆë‹¤. ë”°ë¼ì„œì–¸ì–´ ëª¨ë¸ì€ ë¹„ì—°ì†ì ì¸ ë¶„í¬ë¥¼ ëª¨ë¸ë§ í•´ì•¼í•˜ë¯€ë¡œ GAN ìœ¼ë¡œ í•™ìŠµí•˜ê¸°ì— ë¶€ì ì ˆí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìœ„ëŠ” Google Billion Word ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•´ ë¬¸ì ìˆ˜ì¤€ ì–¸ì–´ ëª¨ë¸ì„ WGAN-GP ë¡œ í•™ìŠµí•œ ê²°ê³¼ì…ë‹ˆë‹¤. ëª¨ë¸ì´ ë¹ˆë²ˆí•˜ê²Œ ì² ìë¥¼ í‹€ë¦¬ì§€ë§Œ, ì–¸ì–´ì˜ í†µê³„ì— ëŒ€í•´ì„œëŠ” ì–´ëŠì •ë„ í•™ìŠµì„ ìˆ˜í–‰í•˜ì˜€ìŒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Meaningful loss curves and detecting overfittingê¸°ì¡´ì˜ weight clipping ì€ loss ê°€ sample quality ì™€ ì—°ê´€ë˜ì–´ ìµœì†Œê°’ìœ¼ë¡œ ìˆ˜ë ´í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì…ë‹ˆë‹¤. WGAN-GP ê°€ í•´ë‹¹ íŠ¹ì„±ì„ ìœ ì§€í•˜ëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•œ í…ŒìŠ¤í¬ë¥¼ ì§„í–‰í•œ ê²°ê³¼ë¥¼ ì œì‹œí•©ë‹ˆë‹¤. (a)ì—ì„œ LSUN ì¹¨ëŒ€ ë°ì´í„°ì…‹ì„ í•™ìŠµí•˜ê³  critic ì˜ negative loss ë¥¼ ê·¸ë ¸ì„ ë•Œ, Gnerator ê°€ í•™ìŠµë¨ì— ë”°ë¼ ê°’ì´ ì¤„ì–´ë“œëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ê²½ìš°, WGAN-GPê°€ criticì—ì„œì˜ ê³¼ì í•©ì„ ì™„í™”í–ˆë‹¤ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, MNIST ë¬´ì‘ìœ„ ìˆ«ì 1000ê°œë¡œ í•™ìŠµí•œ ê²°ê³¼ëŠ”, ì ì€ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•œ ë§Œí¼ ê³¼ì í•©ì´ ë°œìƒí•˜ê¸° ì‰½ìŠµë‹ˆë‹¤. ë•Œë¬¸ì—, criticì´ generatorë³´ë‹¤ ë” ë¹¨ë¦¬ ê³¼ì í•©ë˜ì–´ training lossë¥¼ ì ì°¨ ì¦ê°€ì‹œí‚¤ê³  validation lossë¥¼ ê°ì†Œì‹œì¼°ìŒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ConclusionWGANì— Gradient penaltyë¥¼ ì ìš©í•˜ì—¬ ê¸°ì¡´ì˜ weight clipping ì„ ì ìš©í•¨ìœ¼ë¡œ ì¸í•´ ë°œìƒí•˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆìŒì„ ì œì‹œí•˜ì˜€ìŠµë‹ˆë‹¤. Summarize GANì˜ ê°€ì¥ í° ë¬¸ì œëŠ” í•™ìŠµí™˜ê²½ì´ ë§¤ìš° ë¶ˆì•ˆì •í•˜ë‹¤ëŠ” ê²ƒì´ë‹¤. ìƒì„±ìì™€ êµ¬ë¶„ì ë‘˜ ì¤‘ì— í•˜ë‚˜ê°€ ì‹¤ë ¥ì´ ì›”ë“±ì´ ì¢‹ì•„ì§„ë‹¤ë©´ ë°¸ëŸ°ìŠ¤ê°€ ë¶•ê´´ë˜ê³  ëª¨ë¸ì´ ì •í™•íˆ í•™ìŠµë˜ì§€ ì•Šê³  í•™ìŠµì´ ì™„ë£Œëœ í›„ì—ë„ mode dropping ì´ ìƒê¸°ëŠ”ë° ì´ëŠ” êµ¬ë¶„ìê°€ ê·¸ ì—­í• ì„ ì¶©ë¶„íˆ í•˜ì§€ ëª»í•´ ëª¨ë¸ì´ ìµœì ì ê¹Œì§€ í•™ìŠµì´ ì•ˆ ëœ ê²ƒì´ë‹¤. ë”°ë¼ì„œ ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë³¸ ë…¼ë¬¸ì—ì„œëŠ” WGAN ë°©ë²•ì„ ë„ì…í–ˆë‹¤. ê°„ë‹¨íˆ ì„¤ëª…í•˜ë©´ GANì˜ discriminatorë³´ë‹¤ ì„ ìƒë‹˜ ì—­í• ì„ ì˜ í•  ìˆ˜ ìˆëŠ” criticì„ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ gradientë¥¼ ì˜ ì „ë‹¬ì‹œí‚¤ê³  criticê³¼ generatorë¥¼ ìµœì ì ê¹Œì§€ í•™ìŠµí•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤. ê·¸ë ‡ë‹¤ë©´ ì´ë¥¼ ì ìš©í•˜ë©´ í•™ìŠµì‹œí‚¬ ë•Œ ìƒì„±ìì™€ êµ¬ë¶„ìì˜ ë°¸ëŸ°ìŠ¤ê°€ ì˜ ë§ëŠ”ì§€ ì£¼ì˜ê¹Šê²Œ ë³´ì§€ ì•Šì•„ë„ ë˜ê³  í•™ìŠµí•œ ì´í›„ì— ë°œìƒí•˜ëŠ” mode droppinì´ í•´ê²° ê°€ëŠ¥í•˜ë‹¤. ì‹ì„ í•´ì„í•´ë³´ë©´ ìƒì„±ìê°€ Lipschitz í•¨ìˆ˜ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ”ê°€ í•˜ì§€ì•ŠëŠ”ê°€ì— ëŒ€í•œ ê¸°ì¤€ì´ í•˜ë‚˜ ë” ìƒê¸°ëŠ”ê²ƒ ì´ë‹¤. Link: Improved Training of Wasserstein GANs","categories":[{"name":"Generative Model","slug":"Generative-Model","permalink":"https://jmj3047.github.io/categories/Generative-Model/"}],"tags":[{"name":"Image Classification","slug":"Image-Classification","permalink":"https://jmj3047.github.io/tags/Image-Classification/"},{"name":"WGAN-GP","slug":"WGAN-GP","permalink":"https://jmj3047.github.io/tags/WGAN-GP/"},{"name":"WGAN","slug":"WGAN","permalink":"https://jmj3047.github.io/tags/WGAN/"},{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"}]},{"title":"Generative Adversarial Nets","slug":"Generative_Adversarial_Nets","date":"2022-04-20T15:00:00.000Z","updated":"2022-07-09T04:06:11.647Z","comments":true,"path":"2022/04/21/Generative_Adversarial_Nets/","link":"","permalink":"https://jmj3047.github.io/2022/04/21/Generative_Adversarial_Nets/","excerpt":"","text":"Journal&#x2F;Conference: NIPSYear(published year): 2014Author: I. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, Yoshua BengioSubject: GAN, Generative Model Generative Adversarial Nets Summary ì ëŒ€ì ìœ¼ë¡œ ë™ì‘í•˜ëŠ” ë‘ê°œì˜ ë„¤íŠ¸ì›Œí¬ë¥¼ ì‚¬ìš©í•´ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ìƒì„±í•  ìˆ˜ ìˆëŠ” GAN(Generative Adversarial Nets) êµ¬ì¡°ë¥¼ ì œì•ˆ ìƒì„±ì(Generator) ì™€ ê°ë³„ì(Discriminator) ëª¨ë‘ ë§ˆë¥´ì½”í”„ ì²´ì¸ ë“±ì˜ êµ¬ì¡°ì—†ì´ back-propagation ìœ¼ë¡œ í•™ìŠµì´ ê°€ëŠ¥í•œ ì¸ê³µì‹ ê²½ë§ êµ¬ì¡°ë¥¼ ì‚¬ìš© ì´í›„ ë“±ì¥í•˜ëŠ” ìˆ˜ë§ì€ GAN ê¸°ë°˜ ëª¨ë¸ì˜ ê¸°ì›ì´ ë˜ëŠ” ë…¼ë¬¸ Introduction &amp; Related Worksë¶„ë¥˜ ë¬¸ì œì— ì œí•œë˜ì–´ ì‚¬ìš©ë˜ë˜ ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ìš©ë„ë¥¼ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ë¬¸ì œì—ë„ ì ìš©í•  ìˆ˜ ìˆëŠ” ì ëŒ€ì  ìƒì„± ì‹ ê²½ë§(Generative Adversarial Nets)ì„ ìµœì´ˆë¡œ ì œì‹œí•œ ë…¼ë¬¸ì…ë‹ˆë‹¤. GANì€ ì•„ë˜ì™€ ê°™ì€ ëª©í‘œë¥¼ ê°€ì§„, ì ëŒ€ì ì¸ ë‘ ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤. ê°ë³„ì(Discriminator) ëª¨ë¸ ë°ì´í„°ê°€ ì›ë³¸ ë°ì´í„°ì…‹ì—ì„œ ì˜¨ê²ƒì¸ì§€, ìƒì„±ìê°€ ë§Œë“  ê²ƒì¸ì§€ë¥¼ íŒë³„ ì˜ˆì‹œ) ê²½ì°°ì´ ì§€íê°€ ìœ„ì¡°ë˜ì—ˆëŠ”ì§€ë¥¼ íŒë³„ ìƒì„±ì(Generator) ëª¨ë¸ ê°ë³„ìê°€ êµ¬ë¶„í•  ìˆ˜ ì—†ëŠ” ê°€ì§œ ë°ì´í„°ë¥¼ ìƒì„± ì˜ˆì‹œ) ìœ„íë²”ì´ ê²½ì°°ì´ êµ¬ë¶„í•  ìˆ˜ ì—†ëŠ” ìœ„ì¡° ì§€íë¥¼ ì œì‘í•¨ ë…¼ë¬¸ì€ í•´ë‹¹ ë°©ë²•ì´ íŠ¹ë³„í•œ ëª¨ë¸ì´ë‚˜ í•™ìŠµ ë°©ë²•ì„ í•„ìš”ë¡œ í•˜ì§€ ì•ŠëŠ” ë°©ë²•ì´ë¼ê³  í•˜ë©°, MLP(multi-layer perception) êµ¬ì¡°ë¥¼ ì‚¬ìš©í•´ í•™ìŠµí•œ ê²°ê³¼ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. Adversarial netsì ëŒ€ì  ì‹ ê²½ë§ì˜ ê°€ì¥ ì§ê´€ì ì¸ ì˜ˆì‹œë¡œ MLP ëª¨ë¸ì„ ì‚¬ìš©í•œ ê²½ìš°ë¥¼ ê°€ì •í•˜ì—¬ ì„¤ëª…í•©ë‹ˆë‹¤. ì´ ë•Œ ì‚¬ìš©í•˜ëŠ” í‘œê¸°ë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. $x\\sim p_{data}$ : ì‹¤ì œ ë°ì´í„°ë¡œë¶€í„° ë½‘ì€ ìƒ˜í”Œ $p_g$ : ìƒì„±ìê°€ ìƒì„±í•˜ëŠ” ë°ì´í„°ì˜ ë¶„í¬ $p_z(z)$ : ë°ì´í„°ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•˜ëŠ” ì…ë ¥ ë…¸ì´ì¦ˆ ë¶„í¬ $G(z;\\theta_g)$ : ìƒì„±ì ëª¨ë¸ $\\theta_g$ : ìƒì„±ì ëª¨ë¸ íŒŒë¼ë¯¸í„° $D(x;\\theta_d)$ : ê°ë³„ì ëª¨ë¸ $\\theta_D$ : ê°ë³„ì ëª¨ë¸ íŒŒë¼ë¯¸í„° D ëŠ” ì‹¤ì œ ë°ì´í„°ì™€ ìƒì„±ëœ ë°ì´í„°ì— ì •í™•íˆ êµ¬ë¶„í•  ìˆ˜ ìˆëŠ” í™•ë¥ ì„ ìµœëŒ€í™” í•˜ë ¤ê³  í•©ë‹ˆë‹¤. G ëŠ” Dê°€ ì‹¤ì œ ë°ì´í„°ë¡œ ì°©ê°í•  ë§Œí•œ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì„ ëª©ì ìœ¼ë¡œ $\\log(1-D(G(z))$ ë¥¼ ìµœì†Œí™” í•˜ë ¤ê³  í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ì•„ë˜ì™€ ê°™ì´ ê°€ì¹˜í•¨ìˆ˜ $V(G,D)$ ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, G ëŠ” ìµœì†Œí™”, DëŠ” ìµœëŒ€í™”ë¥¼ ëª©ì ìœ¼ë¡œ ê²½ìŸí•©ë‹ˆë‹¤. ì‹¤ì œ ê³„ì‚°ì—ì„œ V ë¥¼ ìµœëŒ€ë¡œ í•˜ëŠ” D ë¥¼ êµ¬í•  ë•Œ ë§ì€ ê³„ì‚°ì´ í•„ìš”í•˜ê³ , ë°ì´í„°ì…‹ì´ ì œí•œëœ ìƒí™©ì—ì„œ ê³¼ì í•©ì´ ë°œìƒí•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì‹¤ì œ í›ˆë ¨ì—ì„œëŠ” D ë¥¼ k ë²ˆë§Œ í•™ìŠµí•˜ê³  G ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤. ë˜í•œ, í•™ìŠµ ì´ˆê¸°ì—ëŠ” Gê°€ ìƒì„±í•˜ëŠ” ë°ì´í„°ì˜ í’ˆì§ˆì´ ë‚®ìœ¼ë¯€ë¡œ Dê°€ íŒë³„ì„ í•˜ê¸° ì‰¬ì›Œ, $\\log(1âˆ’D(G(z)))$ í•­ì´ ì†Œì‹¤ë ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ, $\\log D(G(z))$ ë¥¼ ìµœëŒ€í™” í•˜ëŠ” ë¬¸ì œë¡œ ë³€í™˜í•˜ì—¬ ì´ˆê¸°ì— í•™ìŠµì´ ì˜ ì´ë¤„ì§ˆ ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. í•™ìŠµ ê³¼ì •ì˜ ëª¨ì‹ë„ì…ë‹ˆë‹¤. íŒŒë€ ì ì„ ì€ ê°ë³„ì Dì˜ ë¶„í¬, ê²€ì€ ì ì€ ì›ë³¸ ë°ì´í„° ë¶„í¬, ì´ˆë¡ ì‹¤ì„ ì€ ìƒì„±ì Gì˜ ë¶„í¬ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. íŒŒë€ìƒ‰ì ì„ : discriminator ê²€ì •ìƒ‰ì ì„ : real dataì—ì„œë‚˜ì˜¨sample ì´ˆë¡ìƒ‰ì‹¤ì„ : generator Zì˜†ì˜ê²€ì •ìƒ‰ì‹¤ì„ : domain from which z is sampled í™”ì‚´í‘œ: ìƒì„±ìê°€ noiseë¥¼ real dataì™€ ì–¼ë§ˆë‚˜ ë¹„ìŠ·í•˜ê²Œ ë§Œë“¤ì–´ì£¼ëŠ”ì§€ì— ëŒ€í•œ ì§€í‘œ (a) ì™€ ê°™ì´ í•™ìŠµì´ ì™„ë£Œë˜ê¸° ì „ì˜ ìƒíƒœì—ì„œ ì‹œì‘í•©ë‹ˆë‹¤.(model training ì´ˆê¸°ìƒíƒœ) (b) ì™€ ê°™ì´ D ë¥¼ ì—…ë°ì´íŠ¸ í•  ë•Œ, ìµœì ì˜ D( $D^{*}_G(x)$ ) ëŠ” $\\frac{p_{data}(x)}{p_{data}(x) + p_g(x)}$ ë¡œ ìˆ˜ë ´í•©ë‹ˆë‹¤.(ë‚´ë¶€ì˜ ì•Œê³ ë¦¬ì¦˜ì— ì˜í•´ì„œ êµ¬ë¶„ìê°€ trainë¨) (c) Gë¥¼ ì—…ë°ì´íŠ¸í•˜ë©´, D ë¥¼ êµë€í•  ìˆ˜ ìˆë„ë¡ G ê°€ ìƒì„±í•˜ëŠ” ë¶„í¬ê°€ ì‹¤ì œ ë°ì´í„° ë¶„í¬ì— ê°€ê¹Œì›Œì§‘ë‹ˆë‹¤.(êµ¬ë¶„ìê°€ í•™ìŠµí•œ ê±¸ ìƒì„±ìì—ê²Œ ì—…ë°ì´íŠ¸) (d) í•™ìŠµ ê³¼ì •ì„ ë°˜ë³µí•˜ë©´ ìƒì„±ìëŠ” ë°ì´í„° ë¶„í¬ì™€ ì¼ì¹˜í•˜ëŠ” ë°ì´í„°ë¥¼ ìƒì„±($p_g &#x3D; p_{data}$) í•˜ë©°, ê°ë³„ìëŠ” ì–´ë– í•œ ìƒ˜í”Œë„ êµ¬ë¶„í•  ìˆ˜ ì—†ê²Œ ë©ë‹ˆë‹¤. ($D(x)&#x3D;\\frac{1}{2}$) (real dataì™€f ake dataê°€ ê°™ì€ ëª¨ìŠµì´ ëœ ë‹¨ê³„. êµ¬ë¶„ìëŠ” fakeì™€ real dataë¥¼ êµ¬ë¶„í•  ìˆ˜ ì—†ê²Œ ë¨.) Fake dataê°€ì™œnoiseì¸ì§€? ëª…í™•í•œ ì´ìœ ëŠ” ëª…ì‹œë˜ì–´ ìˆì§€ ì•ŠìŒ. ëŒ€ëµì ì¸ ì´ìœ ë¥¼ ì¶”ë¡ í•´ë³´ìë©´ ìƒì„±ìì— í¸í–¥ë˜ì§€ ì•Šì€ ë°ì´í„°ê°€ ë“¤ì–´ê°€ì•¼ ì‹¤í—˜ì˜ ê²°ê³¼ê°€ ë” clearí•˜ê¸° ë•Œë¬¸. ìƒì„±ìì— ë„£ì–´ì„œ ë§Œë“¤ì–´ì§„ ë°ì´í„°ê°€ mê°œë¼ë©´, ê·¸ ë°ì´í„° mê°œê°€ ë§Œë“¤ì–´ì§€ë ¤ë©´ ê°™ì€ ìˆ«ìì˜ real dataê°€ ìˆì–´ì•¼ í•¨. ë”°ë¼ì„œ ì´ë°ì´í„°ëŠ” 2mê°œ êµ¬ë¶„ìì˜ ê²°ê³¼ê°’ì€ fake dataì¼ë•Œ 0, real data ì¼ ë•Œ 1ì¸ í•˜ë‚˜ì˜ ìŠ¤ì¹¼ë¼ ê°’ ë”°ë¼ì„œ ê°€ì¥ ì´ìƒì ì¸ êµ¬ë¶„ìê°€ ë  ë•Œì˜ ê°’ì€ 0.5 GAN ëª¨ë¸ì€ markov ëª¨ë¸ì´ í•´ì•¼í•˜ëŠ” í›ˆë ¨ ê³¼ì •ê³¼ overfittingì— ë¬¸ì œì ì„ ë³´ì™„í•˜ê¸° ìœ„í•´ ì´ë¥¼ í•œë²ˆì— í•˜ê¸° ìœ„í•´ ë§Œë“¤ì–´ì§„ ë„¤íŠ¸ì›Œí¬ Theoretical Resultsì ëŒ€ì  ì‹ ê²½ë§ ë¬¸ì œì—ì„œ ìƒì„±ìê°€ ì›ë³¸ ë°ì´í„°ì™€ ìœ ì‚¬í•œ ë¶„í¬ì˜ ë°ì´í„°ë¥¼ ìƒì„±í•  ìˆ˜ ìˆë‹¤ëŠ” ì¦ëª…ì„ ì œì‹œí•©ë‹ˆë‹¤. ë˜í•œ, ì‹¤ì œ ì ëŒ€ì  ì‹ ê²½ë§ì„ í•™ìŠµí•˜ê¸° ìœ„í•´ ì„¤ê³„í•œ ì•„ë˜ ì•Œê³ ë¦¬ì¦˜ ë˜í•œ ê°™ì€ ê²°ê³¼ì— ìˆ˜ë ´í•œë‹¤ëŠ” ì¦ëª…ì„ ì œì‹œí•©ë‹ˆë‹¤. Global Optimality of $p_g &#x3D; p_{data}$ë¨¼ì € ì„ì˜ì˜ G ê°€ ì£¼ì–´ì¡Œì„ ë•Œ ìµœì ì˜ D ë¥¼ ê³„ì‚°í•˜ëŠ” ê³¼ì •ì„ ë³´ì…ë‹ˆë‹¤. ìµœì ì˜ D ë¥¼ ì´ìš©í•˜ì—¬ Equation 1 ì„ Gì— ê´€í•œ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ë•Œ ìƒˆë¡­ê²Œ ì •ë¦¬í•œ ê°€ì¹˜í•¨ìˆ˜ê°€, Gê°€ ìƒì„±í•˜ëŠ” ë°ì´í„°ì˜ ë¶„í¬ê°€ ì‹¤ì œ ë¶„í¬ë¥¼ ë”°ë¥´ëŠ” ê²½ìš°ì—ë§Œ ìµœì†Œí™”ëœë‹¤ëŠ” ê²ƒì„ ë‹¤ìŒê³¼ ê°™ì´ ì¦ëª…í•©ë‹ˆë‹¤. Convergence of Algorithm 1G ì™€ D ê°€ $p_g$ ì¶©ë¶„í•œ í‘œí˜„ë ¥ì„ ê°–ê³  ìˆì„ ë•Œ, ì œì‹œí•œ ì•Œê³ ë¦¬ì¦˜ì´ $p_g&#x3D;p_{data}$ ë¡œ ìˆ˜ë ´í•¨ì„ ì•„ë˜ì™€ ê°™ì´ ì¦ëª…í•©ë‹ˆë‹¤. ì‹¤ì œë¡œ MLP ë¥¼ ì‚¬ìš©í•œ G ë¡œëŠ” ëª¨ë“  í˜•íƒœì˜ $p_g$ ë¥¼ í‘œí˜„í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ì´ë¡ ì ì¸ ìµœê³  ì„±ëŠ¥ì„ ë³´ì¥í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤. ë…¼ë¬¸ì€ ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³  GANì´ ì‹¤ì œ í›ˆë ¨ê²°ê³¼ì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì„ì„ ì œì‹œí•©ë‹ˆë‹¤. Experimentsì‹¤í—˜ì— ì‚¬ìš©í•œ ì¡°ê±´ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. Dataset : MNIST, Toronto Face Database(TFD), CIFAR-10 ì‚¬ìš© Generator : ReLU&#x2F;sigmoid í™œì„±í•¨ìˆ˜ë¥¼ í˜¼í•©í•˜ì—¬ ì‚¬ìš© Discriminator : maxout í™œì„±í•¨ìˆ˜ë¥¼ ì‚¬ìš© Dë¥¼ í•™ìŠµì‹œí‚¬ ë•Œë§Œ Dropoutì„ ì‚¬ìš© Gì—ì„œ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ê²½ìš°ì—ë§Œ noiseë¥¼ input ìœ¼ë¡œ ì‚¬ìš© GAN ì€ ë°ì´í„° ë¶„í¬ ìì²´ë¥¼ êµ¬í•˜ê¸° ìœ„í•œ tractable likelihood ë¥¼ ê°€ì •í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ëª¨ë¸ì„ í‰ê°€í•˜ê¸° ìœ„í•´ ê¸°ì¡´ì— ì œì•ˆëœ ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. Generator ì—ì„œ ìƒì„±í•œ ë°ì´í„°ë¥¼ Gaussian Parzen window ì— fitting fitting í•œ ë¶„í¬ê°€ ì£¼ì–´ì¡Œì„ ë•Œ log-likelihood ë¥¼ ê³„ì‚° Validation set ìœ¼ë¡œ êµì°¨ ê²€ì¦ì„ ìˆ˜í–‰í•´ í‘œì¤€ í¸ì°¨ë¥¼ ê³„ì‚° ë…¼ë¬¸ì€ í•´ë‹¹ ë°©ë²•ì˜ ë¶„ì‚°ì´ í¬ê³  ë†’ì€ ì°¨ì›ì˜ ë°ì´í„°ì—ì„œ ì˜ ì‘ë™í•˜ì§€ ì•Šì§€ë§Œ, GAN ì´ ê¸°ì¡´ ëª¨ë¸ì— ë¹„í•´ ìƒëŒ€ì ìœ¼ë¡œ ì¢‹ì€ ê²°ê³¼ë¥¼ ë³´ì´ê³  ìˆìŒì„ ì œì‹œí•©ë‹ˆë‹¤. ë‹¤ìŒìœ¼ë¡œ GAN ëª¨ë¸ë¡œ ìƒì„±í•œ ë°ì´í„°ë¥¼ ì œì‹œí•©ë‹ˆë‹¤. ê°€ì¥ ìš°ì¸¡ì—ëŠ” ì›ë³¸ ë°ì´í„°ì…‹ ì¤‘ ìƒì„±ëœ ë°ì´í„°ì— ê°€ì¥ ê°€ê¹Œìš´ ë°ì´í„°ë¥¼ ë°°ì¹˜í•˜ì˜€ìŠµë‹ˆë‹¤. ë…¼ë¬¸ì€ í•´ë‹¹ ëª¨ë¸ì´ ê¸°ì¡´ì˜ ìƒì„± ëª¨ë¸ë³´ë‹¤ ë‚«ë‹¤ê³  ì£¼ì¥í•˜ê¸°ëŠ” ì–´ë µì§€ë§Œ, ë¹„ìŠ·í•œ ì„±ê³¼ì™€ ì‘ìš© ê°€ëŠ¥ì„±ì„ ë³´ì—¬ì¤„ ìˆ˜ ìˆë‹¤ëŠ” ì˜ê²¬ì„ ì œì‹œí•©ë‹ˆë‹¤. ë˜í•œ ìœ„ ê·¸ë¦¼ê³¼ ê°™ì´, Generator ì˜ Input noise ë¥¼ ì ì§„ì ìœ¼ë¡œ ë³€í˜•ì‹œí‚¬ ë•Œ, ì ì  interploation ë˜ì–´ê°€ëŠ” ìƒì„± ë°ì´í„°ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Advantages and disadvantagesGAN ì˜ ë‹¨ì ì„ ì•„ë˜ì™€ ê°™ì´ ì •ë¦¬í•©ë‹ˆë‹¤. Generator ê°€ ìƒì„±í•˜ëŠ” ë°ì´í„°ì˜ ë¶„í¬ê°€ ëª…ì‹œì ìœ¼ë¡œ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. Generator ì™€ Discriminator ì˜ ê· í˜•ì´ ê¹¨ì§€ëŠ” ê²½ìš° í•™ìŠµì´ ì›í™œì´ ì´ë£¨ì–´ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤. ë˜í•œ, GAN ì˜ ì¥ì ì„ ì•„ë˜ì™€ ê°™ì´ ì •ë¦¬í•©ë‹ˆë‹¤. ë§ˆë¥´ì½”í”„ ì²´ì¸ ê°™ì€ êµ¬ì¡° ì—†ì´ ì—­ì „íŒŒ ë§Œìœ¼ë¡œë„ í•™ìŠµì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. Generator ì˜ ë¶„í¬ë¡œ íŠ¹ë³„í•œ ëª¨ë¸ì„ ê°€ì •í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë”ìš± ë³µì¡í•œ ë°ì´í„° ë¶„í¬ë¥¼ ëª¨ì‚¬í•  ìˆ˜ ìˆì–´ ì„ ëª…í•œ ë°ì´í„°ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Conclusions and future workGAN í”„ë ˆì„ì›Œí¬ë¥¼ í™•ì¥í•˜ê³  ê°œì„ í•  ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤. ì£¼ì–´ì§„ ì¡°ê±´ì— ë”°ë¼ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ëª¨ë¸ë¡œ ë°œì „ ê°€ëŠ¥ xê°€ ì£¼ì–´ì¡Œì„ ë•Œ zë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë³´ì¡° ë„¤íŠ¸ì›Œí¬ë¥¼ í•™ìŠµí•œë‹¤ë©´ ìƒì„±ìì˜ ë°ì´í„° ë¶„í¬ë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆìŒ parametersë¥¼ ê³µìœ í•˜ëŠ” conditionals modelë¥¼ í•™ìŠµí•¨ìœ¼ë¡œì¨ ë‹¤ë¥¸ conditionals modelsì„ ê·¼ì‚¬ì ìœ¼ë¡œ ëª¨ë¸ë§í•  ìˆ˜ ìˆìŒ Semi-supervised learningì—ë„ í™œìš© ê°€ëŠ¥ : ë°ì´í„°ê°€ ì œí•œëœ ê²½ìš° Discriminator ë¥¼ í™œìš©í•˜ì—¬ classifierì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŒ íš¨ìœ¨ì„± ê°œì„ : Gì™€ Dë¥¼ ê· í˜•ìˆê²Œ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ë°©ë²•ì´ë‚˜ ìƒˆë¡œìš´ z ë¶„í¬ë¥¼ ì œì‹œí•˜ì—¬ í•™ìŠµ ì†ë„ ê°œì„  ê°€ëŠ¥ Summarize GAN ëª¨ë¸ì€ ìƒì„±ì(Generator)ì™€ êµ¬ë¶„ì(Discriminator) ë‘˜ì˜ ì ëŒ€ì ì¸ ê²½ìŸì„ í†µí•´ì„œ í•™ìŠµí•˜ëŠ” ë”¥ëŸ¬ë‹ ë„¤íŠ¸ì›Œí¬ ì‹¤ì œ ìš°ë¦¬ê°€ í•™ìŠµì‹œí‚¤ë ¤ëŠ” ë°ì´í„°ì™€ ìƒì„±ìê°€ ë§Œë“  Fake ë°ì´í„°ë¥¼ êµ¬ë¶„ìì— ëª¨ë‘ í•™ìŠµì‹œì¼œì„œ êµ¬ë¶„ì„ ë” ì˜ ì§“ê²Œ í•˜ëŠ” ë°©ì‹ìœ¼ë¡œì´ë£¨ì–´ì§„ë„¤íŠ¸ì›Œí¬ì´ë©°, ìƒì„±ìëŠ”ëœë¤ë…¸ì´ì¦ˆë¥¼í•™ìŠµë°ì´í„°ì™€ìœ ì‚¬í•œíŒ¨í„´ìœ¼ë¡œë§Œë“¤ì–´ì£¼ëŠ”ë„¤íŠ¸ì›Œí¬êµ¬ì¡°ë¥¼ê°€ì§„ë‹¤. ì´ë¥¼ í…ŒìŠ¤íŠ¸í•˜ê¸° ìœ„í•´ì„œ í™•ì¸í•  ì§€í‘œëŠ” ë°”ì´ë„ˆë¦¬í¬ë¡œìŠ¤ì—”íŠ¸ë¡œí”¼ì™€ ì†ì‹¤í•¨ìˆ˜ì˜ ê°’ì´ êµ¬ë¶„ìê°€ ì¶œë ¥í•œ í™•ë¥ ê°’ì´ ì •ë‹µì— ê°€ê¹Œìš°ë©´ ë‚®ì•„ì§€ê¸° ë•Œë¬¸ì— ì´ê²ƒì´ ëª¨ë¸ í•™ìŠµì˜ ëª©í‘œê°€ ëœë‹¤. êµ¬ë¶„ìì˜ ì†ì‹¤í•¨ìˆ˜ëŠ” ê·¸ë˜ì„œ ë‘ê°€ì§€ í•©ì¸ë° í•˜ë‚˜ëŠ” ê°€ì§œì´ë¯¸ì§€ë¥¼ ì…ë ¥í–ˆì„ ë•Œì˜ ì¶œë ¥ê°’ê³¼ 1ì˜ì°¨ì´, ê·¸ë¦¬ê³  ê°€ì§œ ì´ë¯¸ì§€ë¥¼ ì…ë ¥í–ˆì„ ë•Œì˜ ì¶œë ¥ê°’ê³¼ 0ì˜ ì°¨ì´. ì´ ë‘˜ì˜ í•©ì´ êµ¬ë¶„ìì˜ ì†ì‹¤í•¨ìˆ˜ì´ë©° ì´ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ êµ¬ë¶„ìì˜ íŒŒë¼ë¯¸í„°ê°€ ì—…ë°ì´íŠ¸ ëœë‹¤. ì´ ì—…ë°ì´íŠ¸ëŠ” ìµœì í™” í•¨ìˆ˜ë¥¼ í†µí•´ ì´ë£¨ì–´ì§„ë‹¤. ë°ì´í„°ê°€ ì–´ë–¤ ìœ í˜•ì¸ì§€ì— ë”°ë¼ì„œ fake dataë¥¼ ì–´ë–¤ ê²ƒì„ ì‚¬ìš©í• ì§€ë„ ë‹¬ë¼ì§€ëŠ”ë° ì´ ë…¼ë¬¸ì—ì„œëŠ” fake dataë¥¼ ë°ì´í„° ë¶„í¬ë¥¼ í†µí•´ì„œ ìƒ˜í”Œì„ ì‚¬ìš©í•˜ë©° ì´ëŠ” ëŒ€ì²´ì ìœ¼ë¡œ ì°¨ì›ì´ ë‚®ì€ ëœë¤ë…¸ì´ì¦ˆì´ë‹¤. ìµœì•…ì˜ ê²½ìš°(max)ë¥¼ ê°€ì •í–ˆì„ ë•Œ ì†ì‹¤ì„ ìµœì†Œí™”(min)í•˜ëŠ” ê²ƒì„ minimaxê²Œì„ì´ë¼ê³  í•˜ë©° ì´ê²ƒì´ GAN ê¸°ì €ì— ê¹”ë ¤ìˆëŠ” ì´ë¡ ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. GANì˜ ê°€ì¥ í°ë¬¸ì œëŠ” í•™ìŠµí™˜ê²½ì´ ë§¤ìš° ë¶ˆì•ˆì •í•˜ë‹¤ëŠ” ê²ƒì´ë‹¤. ìƒì„±ìì™€ êµ¬ë¶„ì ë‘˜ ì¤‘ì— í•˜ë‚˜ê°€ ì‹¤ë ¥ì´ ì›”ë“±ì´ ì¢‹ì•„ì§„ë‹¤ë©´ ë°¸ëŸ°ìŠ¤ê°€ ë¶•ê´´ë˜ê³  ëª¨ë¸ì´ ì •í™•íˆ í•™ìŠµë˜ì§€ ì•Šê³  í•™ìŠµì´ ì™„ë£Œëœ í›„ì—ë„ mode dropping ì´ ìƒê¸°ëŠ”ë° ì´ëŠ” êµ¬ë¶„ìê°€ ê·¸ ì—­í• ì„ ì¶©ë¶„íˆ í•˜ì§€ ëª»í•´ ëª¨ë¸ì´ ìµœì ì ê¹Œì§€ í•™ìŠµì´ ì•ˆ ëœ ê²ƒì´ë‹¤. ì´ëŸ° ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì´í›„ ë…¼ë¬¸ì—ì„œ ë‹¤ì–‘í•œ í•´ê²° ë°©ë²•ì´ ì œì‹œëœë‹¤. Link: Generative Adversarial Nets","categories":[{"name":"Generative Model","slug":"Generative-Model","permalink":"https://jmj3047.github.io/categories/Generative-Model/"}],"tags":[{"name":"GAN","slug":"GAN","permalink":"https://jmj3047.github.io/tags/GAN/"},{"name":"Generative Model","slug":"Generative-Model","permalink":"https://jmj3047.github.io/tags/Generative-Model/"},{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"}]},{"title":"Hexo Hueman Tutorial","slug":"Hexo_Hueman_tutorial","date":"2022-04-20T15:00:00.000Z","updated":"2022-10-05T11:24:44.460Z","comments":true,"path":"2022/04/21/Hexo_Hueman_tutorial/","link":"","permalink":"https://jmj3047.github.io/2022/04/21/Hexo_Hueman_tutorial/","excerpt":"","text":"1.Starting Hexo Blog1234567891011121314151617181920212223242526username@LAPTOP-D1EUIRLS MINGW64 ~/Desktop$ hexo init your_blog_folderusername@LAPTOP-D1EUIRLS MINGW64 ~/Desktop$ cd your_blog_folder/username@LAPTOP-D1EUIRLS MINGW64 ~/Desktop/your_blog_folder$ echo &quot;# your_blog_folder&quot; &gt;&gt; README.mdgit initgit add README.mdgit commit -m &quot;first commit&quot;git branch -M mastergit remote add origin https://github.com/your_id/your_blog_folder.gitgit push -u origin masterusername@LAPTOP-D1EUIRLS MINGW64 ~/Desktop/your_blog_folder (master)$ git add .username@LAPTOP-D1EUIRLS MINGW64 ~/Desktop/your_blog_folder (master)$ git commit -m &quot;updated&quot;username@LAPTOP-D1EUIRLS MINGW64 ~/Desktop/your_blog_folder (master)$ git pushusername@LAPTOP-D1EUIRLS MINGW64 ~/Desktop/your_blog_folder (master)$ code . 2.Applying Hueman Theme3.Basic Hexo Tutorial4.Hexo Tag Plugins5.Add Math Formula(without changing from Notion) Creat File name mathjax.ejs on themes/hueman/layout folder 123456789101112MathJax.Hub.Config(&#123; jax: [&quot;input/TeX&quot;, &quot;output/HTML-CSS&quot;], # mathjax tex2jax: &#123; inlineMath: [ [&#x27;$&#x27;, &#x27;$&#x27;] ], displayMath: [ [&#x27;$$&#x27;, &#x27;$$&#x27;]], processEscapes: true, skipTags: [&#x27;script&#x27;, &#x27;noscript&#x27;, &#x27;style&#x27;, &#x27;textarea&#x27;, &#x27;pre&#x27;, &#x27;code&#x27;] &#125;, messageStyle: &quot;none&quot;, &quot;HTML-CSS&quot;: &#123; preferredFont: &quot;TeX&quot;, availableFonts: [&quot;STIX&quot;,&quot;TeX&quot;] &#125;&#125;); Check #Plugins in themes/hueman/_config.yml file and change mathjax: false to true Add mathjax:true at the header when you post Reference: Math Formula 6.Font Change7.Deleting Posts8.Error in Hueman ThemeTo Be Continued..","categories":[{"name":"Setting","slug":"Setting","permalink":"https://jmj3047.github.io/categories/Setting/"}],"tags":[{"name":"Hueman","slug":"Hueman","permalink":"https://jmj3047.github.io/tags/Hueman/"},{"name":"Hexo","slug":"Hexo","permalink":"https://jmj3047.github.io/tags/Hexo/"},{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"}]},{"title":"ImageNet Classification with Deep Convolutional Neural Networks","slug":"ImageNet_Classification","date":"2022-04-20T15:00:00.000Z","updated":"2022-07-09T04:06:09.161Z","comments":true,"path":"2022/04/21/ImageNet_Classification/","link":"","permalink":"https://jmj3047.github.io/2022/04/21/ImageNet_Classification/","excerpt":"","text":"Journal&#x2F;Conference: NIPSYear(published year): 2012Author: Alex Krizhevsky, Ilya Sutskever, Geoffrey E. HintonSubject: AlexNet, Computer Vision ImageNet Classification with Deep Convolutional Neural Networks Summary ê¸°ì¡´ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ ì œì¹˜ê³  ë”¥ëŸ¬ë‹ ëª¨ë¸ì´ ë” ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì¼ ìˆ˜ ìˆìŒì„ ì¦ëª…í•œ ìµœì´ˆì˜ ëª¨ë¸ ReLU í™œì„±í™” í•¨ìˆ˜ì™€ Dropout ì˜ ìœ ìš©í•¨, Data Augmentation ê¸°ë²•ì„ ì œì‹œ 2012ë…„ ImageNet ëŒ€íšŒ ILSVRC ì—ì„œ ìš°ìŠ¹ì„ ì°¨ì§€í•œ ëª¨ë¸ IntroductionAlexNet ì´ì „ì˜ ê°ì²´ ì¸ì‹ ëª¨ë¸ì€ ëŒ€ë¶€ë¶„ ê³ ì „ì ì¸ ML ëª¨ë¸ë¡œ, ìˆ˜ë§Œê°œ ì •ë„ì˜ ì‘ì€ ë°ì´í„°ì…‹(NORB, Caltech-101&#x2F;256, CIFAR-10&#x2F;100)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´í›„, ìˆ˜ì‹­ë§Œ ê°œì˜ ì™„ì „ ë¶„í•  ëœ ì´ë¯¸ì§€ë¡œ êµ¬ì„±ëœ LabelMe ì™€ 1500 ë§Œ ê°œ ì´ìƒì˜ ê³ í•´ìƒë„ ì´ë¯¸ì§€ë¡œ êµ¬ì„±ëœ ImageNet ì´ ë“±ì¥í•©ë‹ˆë‹¤. ì´ëŸ° ë°ì´í„°ì…‹ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ì„œëŠ” ë†’ì€ í•™ìŠµ ì—­ëŸ‰ì„ ê°€ì§„ ëª¨ë¸ì´ í•„ìš”í•©ë‹ˆë‹¤. ë˜í•œ, í•™ìŠµê³¼ì •ì— ì‚¬ìš©ë˜ì§€ ì•Šì€ ìˆ˜ë§ì€ ë°ì´í„°ì— ëŒ€í•´ì„œë„ ì¶”ë¡ ì„ í•  ìˆ˜ ìˆëŠ” ë°©ëŒ€í•œ ì‚¬ì „ ì§€ì‹ì„ ë‹´ì•„ë‚´ì•¼í•©ë‹ˆë‹¤. ì´ì— ë…¼ë¬¸ì€ ì»¨ë³¼ë£¨ì…˜ ì‹ ê²½ë§(CNN) ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” AlexNet ì„ ì œì‹œí•©ë‹ˆë‹¤. CNN ì€ FFNN(feed-forward NN)ì— ë¹„í•´ ë” ì ì€ ë§¤ê°œ ë³€ìˆ˜ë¥¼ ê°€ì§€ë¯€ë¡œ í›ˆë ¨ì´ ìš©ì´í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ILSVRC-2010, ILSVRC-2012 ëŒ€íšŒì— ì‚¬ìš©ëœ ImageNet subsetì—ì„œ ìµœê³ ì˜ ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, ë„¤íŠ¸ì›Œí¬ ì„±ëŠ¥ í–¥ìƒê³¼ í›ˆë ¨ì‹œê°„ ê°ì†Œë¥¼ ìœ„í•œ ì—¬ëŸ¬ê°€ì§€ ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤. ALexNet ì€ 2ê°œì˜ GTX 580 3GB GPUì—ì„œ 5-6 ì¼ë™ì•ˆ í›ˆë ¨ì„ ìˆ˜í–‰í•˜ì˜€ìŠµë‹ˆë‹¤. The Datasetì§€ê¸ˆì€ ëŒ€ë¶€ë¶„ì˜ ë”¥ëŸ¬ë‹ ëª¨ë¸ì—ì„œ ê¸°ë³¸ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ImageNet ì— ëŒ€í•œ ì†Œê°œì…ë‹ˆë‹¤. 22,000 ê°œ ë²”ì£¼ë¡œ êµ¬ë¶„ë˜ëŠ” 1,500 ë§Œê°œ ê³ í•´ìƒë„ ì´ë¯¸ì§€ ì›¹ì—ì„œ ìˆ˜ì§‘í•œ ì´ë¯¸ì§€ë¥¼ Amazon ì˜ Mechanical Turk í¬ë¼ìš°ë“œ ì†Œì‹± ë„êµ¬ë¡œ ë¼ë²¨ë§ 2010 ë…„ë¶€í„° Pascal Visual Object Challengeì˜ ì¼í™˜ìœ¼ë¡œ ImageNet ëŒ€ê·œëª¨ ì‹œê° ì¸ì‹ ë„ì „ (ILSVRC)ì´ë¼ëŠ” ì—°ë¡€ ëŒ€íšŒê°€ ì—´ë ¸ìŠµë‹ˆë‹¤. ILSVRCëŠ” 1000 ê°œì˜ ì¹´í…Œê³ ë¦¬ ê°ê°ì— ì•½ 1000 ê°œì˜ ì´ë¯¸ì§€ê°€ìˆëŠ” ImageNetì˜ í•˜ìœ„ ì§‘í•©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ëŠ” ì•½ 120 ë§Œ ê°œì˜ í›ˆë ¨ ì´ë¯¸ì§€, 50,000 ê°œì˜ ê²€ì¦ ì´ë¯¸ì§€, 150,000 ê°œì˜ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. ëŒ€ë¶€ë¶„ì˜ ì‹¤í—˜ ê²°ê³¼ëŠ” í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ê°€ ê³µê°œëœ ILSVRC-2010 ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ë³„ë„ë¡œ, AlexNet ì´ ì°¸ê°€í–ˆë˜ ILSVRC-2012 ì‹¤í—˜ ê²°ê³¼ ë˜í•œ ì œì‹œí•©ë‹ˆë‹¤. ImageNet ë°ì´í„°ì…‹ ì„±ëŠ¥ ì§€í‘œë¡œëŠ” Top-1&#x2F;Top-5 Accuracy ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ê°€ë³€ í•´ìƒë„ë¡œ êµ¬ì„±ëœ ImageNet ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ 256 Ã— 256ì˜ ê³ ì • í•´ìƒë„ë¡œ ë‹¤ìš´ ìƒ˜í”Œë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì§ì‚¬ê°í˜• ì´ë¯¸ì§€ëŠ” scaling í›„ ì¤‘ì•™ 256x256 íŒ¨ì¹˜ë¥¼ ì˜ë¼ëƒ…ë‹ˆë‹¤. ì´ì™¸ì˜ ì „ì²˜ë¦¬ëŠ” ìˆ˜í–‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. The ArchitectureReLU Nonlinearityë…¼ë¬¸ ë°œí‘œ ë‹¹ì‹œ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ëœ perceptron ì˜ activation í•¨ìˆ˜ëŠ” tanh í˜¹ì€ sigmoid ì…ë‹ˆë‹¤. ì´ë“¤ì€ ì¶œë ¥ê°’ì´ ë¬´í•œëŒ€ë¡œ ë°œì‚°í•˜ì§€ ì•Šê³  íŠ¹ì •í•œ ì˜ì—­ìœ¼ë¡œ ì œí•œë˜ëŠ” saturating í•¨ìˆ˜ì…ë‹ˆë‹¤. ë°˜ë©´ ReLU(Recitified Linear Unit) activation ì€ ì¶œë ¥ê°’ì´ 0 ì—ì„œ ë¬´í•œëŒ€ê¹Œì§€ ë°œì‚°í•  ìˆ˜ ìˆëŠ” non-saturating í•¨ìˆ˜ì…ë‹ˆë‹¤. ë…¼ë¬¸ì€ 4 layer CNN ìœ¼ë¡œ CIFAR-10 ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµí•˜ì˜€ì„ ë•Œ, ReLU ê°€ 6ë°° ë¹ ë¥¸ í•™ìŠµ ì†ë„ë¥¼ ë³´ì—¬ì£¼ì—ˆìŒì„ ì œì‹œí•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´, non-saturating í•œ í•¨ìˆ˜ê°€ gradient ë¥¼ ë” ë¹ ë¥´ê²Œ update í•  ìˆ˜ ìˆìŒì„ ì œì‹œí•©ë‹ˆë‹¤. Training on Multiple GPUsGPU ë©”ëª¨ë¦¬ ì œí•œê³¼ ëŠë¦° í•™ìŠµ ì†ë„ë¥¼ ê°œì„ í•  ìˆ˜ ìˆëŠ” ë³‘ë ¬í•™ìŠµ ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤. ê¸°ë³¸ ê³¨ìëŠ” ë„¤íŠ¸ì›Œí¬ë¥¼ ë¶„í• (ì»¤ë„, ë‰´ëŸ° ë“±)í•˜ì—¬ ì„œë¡œ ë‹¤ë¥¸ GPU ì—ì„œ ë³‘ë ¬ì ìœ¼ë¡œ ì—°ì‚°ì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ ë•Œ, ë©”ëª¨ë¦¬ì˜ í•œê³„ ë° ë³‘ëª© í˜„ìƒì„ ê³ ë ¤í•˜ì—¬, íŠ¹ì •í•œ ë ˆì´ì–´ì—ì„œë§Œ ì—°ì‚° ê²°ê³¼ë¥¼ êµí™˜í•©ë‹ˆë‹¤. ë…¼ë¬¸ì€ ì´ë¥¼ í†µí•´ half-size kernel ì„ ì‚¬ìš©í•œ ë‹¨ì¼ GPU ëª¨ë¸ë³´ë‹¤ Top-1&#x2F;Top-5 accuracy ë¥¼ 1.7% &#x2F; 1.2% ê°ì†Œì‹œì¼°ìŒì„ ì œì‹œí•©ë‹ˆë‹¤. Local Response NormalizationReLU í™œì„± í•¨ìˆ˜ëŠ” ì…ë ¥ì„ normalization í•˜ì§€ ì•Šì•„ë„ saturation ì´ ë°œìƒí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ positive value ë¥¼ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ëŠ” ReLU í•¨ìˆ˜ì˜ íŠ¹ì„±ìœ¼ë¡œ ì¸í•´ CNN ì˜ ì¼ë¶€ êµ¬ì—­ì—ì„œ ê°•í•œ ì‹ í˜¸ê°€ ìƒì„±ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ì— ë…¼ë¬¸ì€ ì•„ë˜ì™€ ê°™ì€ local response normalization ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤. ìš”ì•½í•˜ë©´, CNN ì—ì„œ ì¸ì ‘í•œ í•„í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ normalization ì„ ì§„í–‰í•œ ê²ƒìœ¼ë¡œ, ë…¼ë¬¸ì—ì„œëŠ” Top-1&#x2F;Top-5 Accuracy ë¥¼ 1.4%, 1.2% ê°œì„ í•  ìˆ˜ ìˆì—ˆìŒì„ ì œì‹œí•©ë‹ˆë‹¤. ë˜í•œ, CIFAR-10 ìœ¼ë¡œ í•™ìŠµì„ ìˆ˜í–‰í•˜ì˜€ì„ ë•Œë„ 2% ì˜ ì˜¤ì°¨ìœ¨ ê°ì†Œë¥¼ ë³´ì˜€ìŒì„ ì œì‹œí•©ë‹ˆë‹¤.(ë…¼ë¬¸ ë‹¹ì‹œì—ëŠ” Batch Normalization ì´ ì†Œê°œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.) Overlapping PoolingCNNì˜ í’€ë§ ë ˆì´ì–´ëŠ” ê°™ì€ ì±„ë„ì— ì¡´ì¬í•˜ëŠ” ì¸ì ‘í•œ ë‰´ëŸ°ì˜ ì¶œë ¥ì„ ìš”ì•½í•´ì¤ë‹ˆë‹¤. ë…¼ë¬¸ ì´ì „ì—ëŠ” pooling ì„ ìˆ˜í–‰í•˜ëŠ” ì˜ì—­ì´ ê²¹ì¹˜ì§€ ì•Šë„ë¡ êµ¬ì„±í•˜ì—¬ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ì—ˆìŠµë‹ˆë‹¤. ë…¼ë¬¸ì€ í’€ë§ì„ ìˆ˜í–‰í•˜ëŠ” ì˜ì—­ì´ ì´ë™í•˜ëŠ” ê±°ë¦¬ë¥¼ ì¡°ì ˆí•˜ì—¬ í’€ë§ ì˜ì—­ì´ ê²¹ì¹˜ë„ë¡ í•œ ê²°ê³¼, Top-1&#x2F;Top-5 Accuracy ë¥¼ 0.4 %&#x2F;0.3 % ê°ì†Œí–ˆë‹¤ê³  í•©ë‹ˆë‹¤. ë˜í•œ ì´ë¥¼ í†µí•´ ëª¨ë¸ì˜ ê³¼ì í•© ê°€ëŠ¥ì„±ì„ ì¤„ì¼ ìˆ˜ ìˆì—ˆë‹¤ê³  í•©ë‹ˆë‹¤. Overall Architecture AlexNet ì˜ ì „ì²´ êµ¬ì¡°ë„ ì…ë‹ˆë‹¤. 2GPU ë¡œ ë³‘ë ¬í•™ìŠµì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ ë‘ ê°ˆë˜ë¡œ ë‚˜ë‰˜ì–´ í‘œí˜„ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ 5ê°œì˜ convolution layer ì™€ 3ê°œì˜ max pooling layer, 3ê°œì˜ dense layer ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, í•„ìš”í•œ ê²½ìš°ì—ë§Œ GPU ì—°ì‚° ê²°ê³¼ë¥¼ ê³µìœ í•©ë‹ˆë‹¤. ë˜í•œ convolution&#x2F;dense layer ì˜ í™œì„±í•¨ìˆ˜ëŠ” ReLU ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. Input : 224 x 224 x 3 &#x3D; 150,528 Convolution 1 : 11x11 kernel, 4 stride : 54x54x96 Max pooling 1 : 3x3 kernel, 2 stride : 26x26x96 Convolution 2 : 5x5 kernel, 2 padding : 26x26x256 Max pooling 2 : 3x3 kernel, 2 stride : 12x12x256 Convolution 3 : 3x3 kernel, 1 padding : 12x12x384 Convolution 4 : 3x3 kernel, 1 padding : 12x12x384 Convolution 5 : 3x3 kernel, 1 padding : 12x12x384 Max pooling 3 : 3x3 kernel, 2 stride : 5x5x256 Dense 1 : 4096 Dense 2 : 4096 Dense 3 : 1000 Reducing Overfitting6ì²œë§Œê°œì˜ íŒŒë¼ë¯¸í„°ë¡œ êµ¬ì„±ëœ ëª¨ë¸ì˜ ê³¼ì í•©ì„ ë§‰ê¸° ìœ„í•´ ì‚¬ìš©í•œ ë°©ë²•ì„ ì†Œê°œí•©ë‹ˆë‹¤. Data Augmentationí•™ìŠµ ë°ì´í„°ë¥¼ ì¸ìœ„ì ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ í›ˆë ¨ ë°ì´í„°ë¥¼ ì¦ê°€ì‹œí‚¤ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ë³€í™˜ëœ ì´ë¯¸ì§€ë¥¼ ì €ì¥í•˜ì§€ ì•Šê³  GPU í•™ìŠµì‹œì— CPUì—ì„œ ê³„ì‚°í•˜ë„ë¡ í•˜ì—¬, ì¶”ê°€ì ì¸ ê³„ì‚° ë¹„ìš©ì„ ì†Œëª¨í•˜ì§€ ì•Šì•˜ë‹¤ê³  í•©ë‹ˆë‹¤. ì£¼ìš” ë°©ë²•ì€ ë‘ê°€ì§€ë¡œ ìš”ì•½ë©ë‹ˆë‹¤. 256 Ã— 256 ì´ë¯¸ì§€ì—ì„œ 224 Ã— 224 íŒ¨ì¹˜ë¥¼ ì¶”ì¶œí•˜ê³ , ìˆ˜í‰ ë°©í–¥ìœ¼ë¡œ ë’¤ì§šê¸° ê¸°ì¡´ ë°ì´í„° ì…‹ì˜ 2048 ë°° í™•ì¥ ê°€ëŠ¥ ì‹¤ì œ : 5 ê°œì˜ 224 Ã— 224 íŒ¨ì¹˜ (4 ê°œì˜ ì½”ë„ˆ íŒ¨ì¹˜ ë° ì¤‘ì•™ íŒ¨ì¹˜)ì™€ ìˆ˜í‰ ë°˜ì‚¬ë¥¼ ìˆ˜í–‰í•œ 10ê°œì˜ íŒ¨ì¹˜ ì‚¬ìš© RGB ì±„ë„ ê°•ë„ ì¡°ì • í•™ìŠµ ë°ì´í„°ì…‹ì˜ í”½ì…€ê°’ìœ¼ë¡œ PCA ë¥¼ ìˆ˜í–‰ PCA eigenvector ì— N(0,0.1) ì¸ ì •ê·œë¶„í¬ì— ì¶”ì¶œí•œ ëœë¤ê°’ì„ ê³±í•´ ìƒ‰ìƒì„ ì¡°ì • Top-1 ì˜¤ì°¨ìœ¨ì„ 1% ê°ì†Œí•  ìˆ˜ ìˆì—ˆìŒ DropoutDense Layer ì˜ Output ì— Dropout rate 0.5 ë¥¼ ì‚¬ìš©í•œ Dropout layer ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤. í•™ìŠµì— í•„ìš”í•œ Epoch ë¥¼ 2ë°° ì •ë„ ëŠ˜ë ¸ìœ¼ë‚˜, ê³¼ì í•©ì„ ì„±ê³µì ìœ¼ë¡œ ë°©ì§€í–ˆìŒì„ ì œì‹œí•©ë‹ˆë‹¤. Details of learningëª¨ë¸ í•™ìŠµì˜ ì„¸ë¶€ë‚´ìš©ì…ë‹ˆë‹¤. Batch size : 128 SGD (momentum 0.9, weight decay 0.0005) weight decay ê°€ ëª¨ë¸ì„ ì •ê·œí™” í•  ë¿ë§Œ ì•„ë‹ˆë¼ ì§ì ‘ì ìœ¼ë¡œ ëª¨ë¸ì˜ í•™ìŠµ ì˜¤ì°¨ë¥¼ ì¤„ì˜€ìŒì„ ì œì‹œí•©ë‹ˆë‹¤. ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ê³¼ì •ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. ê°€ì¤‘ì¹˜ëŠ” í‰ê· ì´ 0, í‘œì¤€ í¸ì°¨ê°€ 0.01ì¸ ì •ê·œ ë¶„í¬ë¥¼ ë”°ë¥´ë„ë¡ ì´ˆê¸°í™”í•©ë‹ˆë‹¤. 2&#x2F;4&#x2F;5 ë²ˆì§¸ convolution ê³¼ dense layerì˜ bias ëŠ” 1ë¡œ ì´ˆê¸°í™”í•˜ì—¬, í•™ìŠµì„ ê°€ì†í•  ìˆ˜ ìˆì—ˆìŒì„ ì œì‹œí•©ë‹ˆë‹¤. í•™ìŠµë¥ ì€ ëª¨ë“  layer ì— ëŒ€í•´ì„œ ë™ì¼í•˜ë˜, í›ˆë ¨ì„ ìˆ˜í–‰í•˜ë©´ì„œ ë©”ë‰´ì–¼í•˜ê²Œ ì¡°ì •í•©ë‹ˆë‹¤. í•™ìŠµë¥  0.01 ì—ì„œ ì‹œì‘í•˜ì—¬, í•™ìŠµì´ ê°œì„ ë˜ì§€ ì•Šì„ ë•Œ í•™ìŠµë¥ ì„ 10ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ë°©ì‹ìœ¼ë¡œ ìˆ˜í–‰í•©ë‹ˆë‹¤. RESULT ILSVRC-2010 ë°ì´í„°ì— ëŒ€í•´ì„œ ê¸°ì¡´ ëª¨ë¸ì˜ ê²°ê³¼ë¥¼ ì••ë„ì ìœ¼ë¡œ ìƒíšŒí•˜ëŠ” ê²°ê³¼ë¥¼ ì œì‹œí•˜ì˜€ìŠµë‹ˆë‹¤. AlexNet ì´ ì§ì ‘ ì°¸ê°€í–ˆë˜ ILSVRC-2012 ì—ì„œë„ ë‹¤ë¥¸ ìµœê³  ì„±ëŠ¥ì˜ ëª¨ë¸ì— ë¹„í•´ ì••ë„ì ì¸ ê²°ê³¼ë¥¼ ë³´ì˜€ìŒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, CNN Layer ê°¯ìˆ˜ë¥¼ ì¶”ê°€í•  ë•Œë§ˆë‹¤ ì„±ëŠ¥ì´ ìƒìŠ¹í•¨ì„ ì œì‹œí•©ë‹ˆë‹¤. Qualitative Evaluations CNN kernel ì„ ì‹œê°í™”í•œ ê·¸ë¦¼ì„ ì œì‹œí•˜ë©´ì„œ, ê° ì»¤ë„ì´ ì´ë¯¸ì§€ì˜ ë‹¤ì–‘í•œ Feature ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì¶”ì¶œí•´ëƒˆìŒì„ ì œì‹œí•©ë‹ˆë‹¤. AlexNet ì€ ì¤‘ì•™ì„ ë²—ì–´ë‚˜ëŠ” ë°ì´í„°ë„ íš¨ê³¼ì ìœ¼ë¡œ ë¶„ë¥˜í•´ëƒˆìŠµë‹ˆë‹¤. ë˜í•œ, Top-5 ì˜ˆì¸¡ì´ ëŒ€ë¶€ë¶„ ìœ ì‚¬í•œ ë²”ì£¼ì¸ ê²ƒìœ¼ë¡œ ë³´ì•„ í•©ë¦¬ì ì¸ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ê³  ìˆìŒì„ ì œì‹œí•©ë‹ˆë‹¤. ë˜í•œ, ìì„¸ê°€ ì„œë¡œ ë‹¤ë¥¸ ì½”ë¼ë¦¬ì˜ ì‚¬ë¡€ì™€ ê°™ì´, Pixel ì°¨ì›ì—ì„œ ì™„ì „íˆ ë‹¤ë¥¸ ë°ì´í„°ì„ì—ë„ ìœ ì‚¬í•œ ë²”ì£¼ë¡œ ë¶„ë¥˜í•  ìˆ˜ ìˆëŠ” ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. Discussionâ€œê¹Šì€â€ CNN ì´ íš¨ê³¼ì ìœ¼ë¡œ ì‘ë™í•˜ì˜€ìŒì„ ì œì‹œí•©ë‹ˆë‹¤. Convolution layer ë¥¼ ì œê±°í•  ë•Œë§ˆë‹¤ Top-1 Accuracy ê°€ 2%ì”© ê°ì†Œí•˜ëŠ” ì ì— ë¯¸ë£¨ì–´, â€œê¹Šì´â€ì˜ ì¤‘ìš”ì„±ì„ ë‹¤ì‹œ í•œë²ˆ ì œì‹œí•©ë‹ˆë‹¤. Link: ImageNet Classification with Deep Convolutional Neural Networks","categories":[{"name":"Computer Vision","slug":"Computer-Vision","permalink":"https://jmj3047.github.io/categories/Computer-Vision/"}],"tags":[{"name":"Convolutional Neural Networks","slug":"Convolutional-Neural-Networks","permalink":"https://jmj3047.github.io/tags/Convolutional-Neural-Networks/"},{"name":"Image Classification","slug":"Image-Classification","permalink":"https://jmj3047.github.io/tags/Image-Classification/"},{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"}]},{"title":"Pyspark Tutorial(1)","slug":"Pyspark_Tutorial_1","date":"2022-04-20T15:00:00.000Z","updated":"2022-04-22T07:33:10.084Z","comments":true,"path":"2022/04/21/Pyspark_Tutorial_1/","link":"","permalink":"https://jmj3047.github.io/2022/04/21/Pyspark_Tutorial_1/","excerpt":"","text":"Reference: https://spark.apache.org/docs/latest/quick-start.html Get Started01.basic.py1234567891011121314# -*- coding: utf-8 -*-import pysparkprint(pyspark.__version__)from pyspark.sql import SparkSession#ìŠ¤íŒŒí¬ ì„¸ì…˜ ì´ˆê¸°í™” :spark sessionì´ í•˜ë‚˜ ë§Œë“¤ì–´ì§„ê²ƒspark = SparkSession.builder.master(&#x27;local[1]&#x27;).appName(&#x27;SampleTutorial&#x27;).getOrCreate()rdd = spark.sparkContext.parallelize([1,2,3,4,5])print(&quot;rdd Count&quot;, rdd.count())spark.stop() 02.rating.py1234567891011121314151617181920212223242526272829303132333435#SparkContext#RDDfrom pyspark import SparkConf, SparkContextimport collectionsprint(&quot;Hello&quot;)def main(): # MasterNode = local # MapReduce conf = SparkConf().setMaster(&quot;local&quot;).setAppName(&#x27;RatingHistogram&#x27;) sc = SparkContext(conf = conf) lines = sc.textFile(&quot;ml-100k/u.logs&quot;) #print(lines) ratings = lines.map(lambda x: x.split()[2]) #print(&quot;ratings:&quot;,ratings) #rddë¼ëŠ” ê°ì²´ê°€ ë§Œë“¤ì–´ì§„ê²ƒ result = ratings.countByValue() #print(&quot;result:&quot;,result) #ì •ë ¬í•˜ê¸° sortedResults = collections.OrderedDict(sorted(result.items())) for key, value in sortedResults.items(): print(&quot;%s %i&quot; % (key, value)) if __name__ == &quot;__main__&quot;: main() ##sparkë¥¼ ì“°ëŠ” ì´ìœ :ë¡œê·¸ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ ê·œì¹™ì„ ëŒ€ì…í•´ì„œ ì •ë ¬í•œë‹¤ìŒì— ì •í˜•ë°ì´í„°ë¡œ ì¹˜í™˜í•˜ê¸°ìœ„í•´#ì‹¤ì œë¡œ ì˜ë¯¸ìˆëŠ” ë¡œê·¸ë¼ë©´ ë¶„ì„ë„ ì˜ë¯¸ê°€ ìˆë‹¤#ë¶„ì„ê³¼ ë¡œê·¸ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” í™˜ê²½ì„ ì§€ì›í•´ì¤Œ#ê³¼ê±°ì—ëŠ” ë‘ê°œê°€ ë”°ë¡œ ìˆì—ˆìŒ 03.dataloading.py1234567891011121314151617181920212223242526272829303132333435363738394041424344#Spark SQL ì ìš©#Spark Sessionfrom pyspark.sql import SparkSession# #ìŠ¤íŒŒí¬ ì„¸ì…˜ ìƒì„±# my_spark = SparkSession.builder.getOrCreate()# print(my_spark)# #í…Œì´ë¸”ì„ í™•ì¸í•˜ëŠ” ì½”ë“œ# print(my_spark.catalog.listDatabases())# #show database# my_spark.sql(&quot;show databases&quot;).show()# #check current DB# my_spark.catalog.currentDatabase()# my_spark.stop()#loading csv filespark = SparkSession.builder.master(&#x27;local[1]&#x27;).appName(&quot;DBTutorial&quot;).getOrCreate()flights = spark.read.option(&#x27;header&#x27;,&#x27;true&#x27;).csv(&#x27;data/flight_small.csv&#x27;)#flights.show(4)#spark.catalog.currentDatabase()#flights í…Œì´ë¸”ì„ default DBì— ì¶”ê°€í•¨flights.createOrReplaceTempView(&#x27;flights&#x27;)#print(spark.catalog.listTables(&#x27;default&#x27;))#spark.sql(&#x27;show tables from default&#x27;).show()#ì¿¼ë¦¬ í†µí•´ì„œ ë°ì´í„° ì €ì¥query = &quot;FROM flights SELECT * LIMIT 10&quot;query2 = &quot;SELECT * FROM flights LIMIT 10&quot;# ìŠ¤íŒŒí¬ ì„¸ì…˜ í• ë‹¹flights10 = spark.sql(query2)#flights10.show()#spark ë°ì´í„° í”„ë ˆì„ì„ pandas data frameìœ¼ë¡œ ë³€í™˜import pandas as pdpd_flights10 = flights10.toPandas()print(pd_flights10.head()) 04.struct_type.py Reference: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.types.StructType.html 1234567891011121314151617181920212223242526272829303132from pyspark.sql import SparkSessionfrom pyspark.sql import functions as func #aliasfrom pyspark.sql.types import StructType, StructField, IntegerType, LongTypeprint(&quot;Hello&quot;)#ì„¸ì…˜ í• ë‹¹spark = SparkSession.builder.appName(&quot;PopularMovies&quot;).getOrCreate()#ìŠ¤í‚¤ë§ˆ ì‘ì„±(u.logs ë°ì´í„°)schema = StructType( [ StructField(&quot;userID&quot;,IntegerType(), True) , StructField(&quot;movieID&quot;,IntegerType(), True) , StructField(&quot;rating&quot;, IntegerType(), True) , StructField(&quot;timestamp&quot;, LongType(), True) ])print(&quot;Schema is done&quot;)#ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°movies_df = spark.read.option(&quot;sep&quot;,&quot;\\t&quot;).schema(schema).csv(&quot;ml-100k/u.logs&quot;)#ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì¸ê¸° ìˆëŠ” ì˜í™” ì •ë ¬#movieIDë¡œ ê·¸ë£¹ë°”ì´, count() ì§„í–‰, orderbytopMovieIds = movies_df.groupby(&quot;movieID&quot;).count().orderBy(func.desc(&#x27;count&#x27;))print(topMovieIds.show(10))#ì„¸ì…˜ ì¢…ë£Œspark.stop() 05.advance_structtype.py1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162from pyspark.sql import SparkSessionfrom pyspark.sql import functions as func #aliasfrom pyspark.sql.types import StructType, StructField, IntegerType, LongTypeimport codecsprint(&quot;Starting Session&quot;)def loadMovieNames(): #u.itemì—ì„œ ì˜í™” ì´ë¦„ ê°€ì ¸ì˜´ movieNames = &#123;&#125; with codecs.open(&quot;ml-100k/u.item&quot;,&quot;r&quot;, encoding=&quot;ISO-8859-1&quot;, errors =&quot;ignore&quot;) as f: for line in f: fields = line.split(&quot;|&quot;) movieNames[int(fields[0])] = fields[1] #ë°ì´í„° ì¶”ê°€í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ ë¬¸ë²• return movieNames #ì„¸ì…˜ í• ë‹¹spark = SparkSession.builder.appName(&quot;PopularMovies&quot;).getOrCreate()#íŒŒì´ì¬ ë”•ì…”ë„ˆë¦¬ ê°ì²´ë¥¼ spark ê°ì²´ë¡œ ë³€í™˜nameDict = spark.sparkContext.broadcast(loadMovieNames())#ìŠ¤í‚¤ë§ˆ ì‘ì„±(u.logs ë°ì´í„°)schema = StructType( [ StructField(&quot;userID&quot;,IntegerType(), True) , StructField(&quot;movieID&quot;,IntegerType(), True) , StructField(&quot;rating&quot;, IntegerType(), True) , StructField(&quot;timestamp&quot;, LongType(), True) ])print(&quot;Schema is done&quot;)#ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°movies_df = spark.read.option(&quot;sep&quot;,&quot;\\t&quot;).schema(schema).csv(&quot;ml-100k/u.logs&quot;)#ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì¸ê¸° ìˆëŠ” ì˜í™” ì •ë ¬í•  í•„ìš” ì—†ìŒ#movieIDë¡œ ê·¸ë£¹ë°”ì´, count() ì§„í–‰, orderby#topMovieIds = movies_df.groupby(&quot;movieID&quot;).count().orderBy(func.desc(&#x27;count&#x27;))topMovieIds = movies_df.groupby(&quot;movieID&quot;).count()# ë”•ì…”ë„ˆë¦¬ # key-value# í‚¤ê°’ì„ ì•Œë©´ valueìë™ìœ¼ë¡œ ê°€ì ¸ì˜´(movietitle)def lookupName(movieID): return nameDict.value[movieID]# ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜ ì‚¬ìš©í•  ë•Œ ì“°ëŠ” spark ë¬¸ë²•lookupNameUDF = func.udf(lookupName)# MovieTitleì„ ê¸°ì¡´ topMovieIds ë°ì´í„°ì— ì¶”ê°€#ì»¬ëŸ¼ ì¶”ê°€moviesWithNames = topMovieIds.withColumn(&quot;movietitle&quot;,lookupNameUDF(func.col(&quot;movieID&quot;)))#ì •ë ¬final_df = moviesWithNames.orderBy(func.desc(&quot;count&quot;))print(final_df.show(10))#ì„¸ì…˜ ì¢…ë£Œspark.stop()","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Pyspark","slug":"Python/Pyspark","permalink":"https://jmj3047.github.io/categories/Python/Pyspark/"}],"tags":[{"name":"pyspark","slug":"pyspark","permalink":"https://jmj3047.github.io/tags/pyspark/"}]},{"title":"Pyspark Tutorial(2)","slug":"Pyspark_Tutorial_2","date":"2022-04-20T15:00:00.000Z","updated":"2022-04-22T07:33:14.911Z","comments":true,"path":"2022/04/21/Pyspark_Tutorial_2/","link":"","permalink":"https://jmj3047.github.io/2022/04/21/Pyspark_Tutorial_2/","excerpt":"","text":"Data cleansing01.pipeline.py123456789101112131415161718192021222324from pyspark.sql import SparkSessionfrom pyspark.sql import *from pyspark.sql import functions as F#Create Spark Sessionspark = SparkSession.builder.master(&quot;local[1]&quot;).appName(&quot;MLSampleTutorial&quot;).getOrCreate()#Load Datadf = spark.read.csv(&quot;data/AA_DFW_2015_Departures_Short.csv.gz&quot;, header = True)print(&quot;file loaded&quot;)print(df.show())#remove duration = 0df = df.filter(df[3] &gt; 0) #Actual elapsed time (Minutes) ì—¬ê¸° ì»¬ëŸ¼ ê°’ì´ 0ë³´ë‹¤ ì‘ì€ê±´ ë³´ì—¬ì£¼ì§€ ì•ŠìŒ# df.show()#ADD ID columndf = df.withColumn(&#x27;id&#x27;,F.monotonically_increasing_id()) #idê°’ì„ ìë™ìœ¼ë¡œ ë„£ì–´ì¤Œdf.write.csv(&quot;data/output.csv&quot;, mode = &#x27;overwrite&#x27;)spark.stop() 02.total_spent.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# #ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°# from pyspark import SparkConf, SparkContext# #ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜# #mainí•¨ìˆ˜# def main():# conf = SparkConf.setMaster(&quot;local&quot;).setAppName(&#x27;SpentbyCustomer&#x27;)# sc = SparkContext(conf= conf)# # íŒŒì´ì¬ ì½”ë“œ# # ì‹¤í–‰ì½”ë“œ ì‘ì„±# if __name__ == &quot;__main__&quot;:# main()########## ì´ê²Œ spark ê¸°ë³¸ ì„¸íŒ… ##########ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°from pyspark import SparkConf, SparkContext#ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜def extractCusPrice(line): fields = line.split(&quot;,&quot;) return(int(fields[0]), float(fields[2]))#mainí•¨ìˆ˜def main(): #ìŠ¤íŒŒí¬ ì„¤ì • conf = SparkConf().setMaster(&quot;local&quot;).setAppName(&#x27;SpentbyCustomer&#x27;) sc = SparkContext(conf= conf) #ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° input = sc.textFile(&#x27;data/customer-orders.csv&#x27;) #print(&#x27;is data?&#x27;) mappedInput = input.map(extractCusPrice) #íŠœí”Œ í˜•íƒœë¡œ ë‚˜ì˜´ totalByCustomer = mappedInput.reduceByKey(lambda x, y : x + y) #ì •ë ¬ flipped = totalByCustomer.map(lambda x: (x[1], x[0])) totalByCustomerSorted = flipped.sortByKey() results = totalByCustomerSorted.collect() for result in results: print(result) #íŒŒì´ì¬ ì½”ë“œ # ì‹¤í–‰ì½”ë“œ ì‘ì„±if __name__ == &quot;__main__&quot;: main() 03.friends_by_age.py123456789101112131415161718from pyspark import SparkConf, SparkContextconf = SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;FriendsByAge&quot;)sc = SparkContext(conf = conf)def parseLine(line): fields = line.split(&#x27;,&#x27;) age = int(fields[2]) numFriends = int(fields[3]) return (age, numFriends)lines = sc.textFile(&quot;data/fakefriends.csv&quot;)rdd = lines.map(parseLine)totalsByAge = rdd.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))averagesByAge = totalsByAge.mapValues(lambda x: x[0] / x[1])results = averagesByAge.collect()for result in results: print(result) 04.min_temp.py123456789101112131415161718192021222324252627282930313233#ì˜¨ë„ë¥¼ ì¸¡ì •í•˜ëŠ” í”„ë¡œê·¸ë¨ ë§Œë“¤ê¸°from dataclasses import fieldfrom pyspark import SparkConf, SparkContextconf = SparkConf().setMaster(&#x27;local&#x27;).setAppName(&#x27;MinTemperatures&#x27;) #ë§ˆìŠ¤í„° ë…¸ë“œì—ë‹¤ê°€ ì˜¬ë¦°ë‹¤sc = SparkContext(conf = conf)print(&quot;Start&quot;)def parseLine(line): fields = line.split(&quot;,&quot;) #ì‰¼í‘œë¡œ ë‹¤ ëŠì–´ì¤Œ -&gt; ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜ë¨ stationID = fields[0] entryType = fields[2] temperature = float(fields[3]) * 0.1 * (9.0/5.0) + 32.0 return (stationID, entryType, temperature)lines = sc.textFile(&#x27;data/1800.csv&#x27;)#print(lines)parseLines = lines.map(parseLine)#print(parseLine)minTemps = parseLines.filter(lambda x: &quot;TMIN&quot; in x[1])stationTemps = minTemps.map(lambda x: (x[0],x[2]))minTemps = stationTemps.map(lambda x, y: min(x, y))results = minTemps.collect()#print(results)for result in results: print(result[0]+ &quot;\\t&#123;:.2f&#125;F&quot;.format(result[1]))","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Pyspark","slug":"Python/Pyspark","permalink":"https://jmj3047.github.io/categories/Python/Pyspark/"}],"tags":[{"name":"pyspark","slug":"pyspark","permalink":"https://jmj3047.github.io/tags/pyspark/"}]},{"title":"Pyspark Tutorial(3)","slug":"Pyspark_Tutorial_3","date":"2022-04-20T15:00:00.000Z","updated":"2022-04-22T07:33:20.475Z","comments":true,"path":"2022/04/21/Pyspark_Tutorial_3/","link":"","permalink":"https://jmj3047.github.io/2022/04/21/Pyspark_Tutorial_3/","excerpt":"","text":"Machine Learning01.regression.py1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950from pyspark.ml.regression import DecisionTreeRegressorfrom pyspark.sql import SparkSessionfrom pyspark.ml.feature import VectorAssemblerprint(&quot;Starting Session&quot;)#ì„¸ì…˜ í• ë‹¹spark = SparkSession.builder.appName(&quot;DecisionTree&quot;).getOrCreate()#ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°#StructType ê³¼ì • ìƒëµ ê°€ëŠ¥data = spark.read.option(&quot;header&quot;, &quot;true&quot;).option(&quot;inferSchema&quot;, &quot;true&quot;).csv(&quot;data/realestate.csv&quot;)#print(data.show())#ë°ì´í„° í”„ë ˆì„ì„ í–‰ë ¬ë¡œ ë³€í™˜assembler = VectorAssembler().setInputCols([&quot;HouseAge&quot;, &quot;DistanceToMRT&quot;,&quot;NumberConvenienceStores&quot;]).setOutputCol(&quot;features&quot;) #ë°ì´í„° ì»¬ëŸ¼ ê°’ ì•„ë¬´ê±°ë‚˜ ë„£ì–´ë„ ë¨#íƒ€ê²Ÿ ë°ì´í„° ì„¤ì •df = assembler.transform(data).select(&quot;PriceofUnitArea&quot;,&quot;features&quot;)#ë°ì´í„° ë¶„ë¦¬trainTest = df.randomSplit([0.5,0.5])trainingDF = trainTest[0]testDF = trainTest[1]#Decision Tree í´ë˜ìŠ¤ ì •ì˜dtr = DecisionTreeRegressor().setFeaturesCol(&quot;features&quot;).setLabelCol(&quot;PriceofUnitArea&quot;)#ëª¨ë¸ í•™ìŠµmodel = dtr.fit(trainingDF)#print(model)#ëª¨ë¸ ì˜ˆì¸¡fullPredictions = model.transform(testDF).cache()#ì˜ˆì¸¡ê°’ê³¼ labelí™•ì¸predictions = fullPredictions.select(&quot;prediction&quot;).rdd.map(lambda x: x[0])#ì‹¤ì œë°ì´í„°labels = fullPredictions.select(&quot;PriceofUnitArea&quot;).rdd.map(lambda x: x[0])#ì˜ˆì¸¡ê°’ê³¼ labelì„ zipìœ¼ë¡œ ë¬¶ì–´ì¤Œpreds_label = predictions.zip(labels).collect()for prediction in preds_label: print(prediction)#ì„¸ì…˜ ì¢…ë£Œspark.stop() 02.logistic_regression.py1234567891011121314151617181920212223from pyspark.sql import SparkSessionfrom pyspark.ml.classification import LogisticRegression #Important# ì„¸ì…˜ í• ë‹¹spark = SparkSession.builder.appName(&quot;AppName&quot;).getOrCreate()# load Datatraining = spark.read.format(&quot;libsvm&quot;).load(&quot;data/mllib/sample_libsvm_data.txt&quot;)print(&quot;Data loaded&quot;)# model# Scikit-Learn ë¬¸ë²•ê³¼ ë¹„ìŠ·mlr = LogisticRegression() # Importantmlr_model = mlr.fit(training) # Important# ë¡œì§€ìŠ¤í… íšŒê·€, ì„ í˜• ëª¨ë¸ .. ê³„ìˆ˜ì™€ ìƒìˆ˜ë¥¼ ë½‘ì•„ë‚¼ ìˆ˜ ìˆìŒprint(&quot;Coefficients :&quot; + str(mlr_model.coefficients))print(&quot;Intercept :&quot; + str(mlr_model.intercept))spark.stop() 03.pipeline.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657from tokenize import Tokenfrom pyspark.ml import Pipelinefrom pyspark.ml.classification import LogisticRegressionfrom pyspark.ml.feature import HashingTF, Tokenizerfrom pyspark.sql import SparkSession# ì„¸ì…˜ í• ë‹¹ spark = SparkSession.builder.appName(&quot;MLPipeline&quot;).getOrCreate()# ê°€ìƒì˜ ë°ì´í„° ë§Œë“¤ê¸°training = spark.createDataFrame([ (0, &quot;a b c d e spark&quot;, 1.0), (1, &quot;b d&quot;, 0.0), (2, &quot;spark f g h&quot;, 1.0), (3, &quot;hadoop mapreduce&quot;, 0.0)], [&quot;id&quot;, &quot;text&quot;, &quot;label&quot;])# Feature Engineering# 1. Prparation# step01. í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì–´ë¡œ ë¶„ë¦¬tokenizer = Tokenizer(inputCol=&#x27;text&#x27;, outputCol=&#x27;words&#x27;)# step02. ë³€í™˜ëœ í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ ë³€í™˜hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=&quot;features&quot;)# step03. ëª¨ë¸ì„ ê°€ì ¸ì˜´lr = LogisticRegression(maxIter=5, regParam=0.01)# 2. Starting pipeplinepipeline = Pipeline(stages=[tokenizer, hashingTF, lr])# 3. Model Trainingmodel = pipeline.fit(training)# 4. Prepare test documents, which are unlabeled (id, text) tuples.test = spark.createDataFrame([ (4, &quot;spark i j k&quot;), (5, &quot;l m n&quot;), (6, &quot;spark hadoop spark&quot;), (7, &quot;apache hadoop&quot;)], [&quot;id&quot;, &quot;text&quot;])# 5. Predictionprediction = model.transform(test)selected = prediction.select(&quot;id&quot;, &quot;text&quot;, &quot;probability&quot;, &quot;prediction&quot;)for row in selected.collect(): row_id, text, prob, prediction = row #íŠœí”Œ í˜•íƒœë¡œ ë°˜í™˜ print( # ë¬¸ìì—´ í¬ë§·íŒ… &quot;(%d, %s) -------&gt; probability=%s, prediction=%f&quot; % (row_id, text, str(prob), prediction) )# training.show()# ì„¸ì…˜ ì¢…ë£Œspark.stop() 04.randomforest.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960from cProfile import labelfrom pyspark.sql import SparkSession# ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬from pyspark.ml import Pipelinefrom pyspark.ml.classification import RandomForestClassifierfrom pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexerfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° spark = SparkSession.builder.appName(&quot;RandomForest&quot;).getOrCreate()data = spark.read.format(&quot;libsvm&quot;).load(&quot;data/mllib/sample_libsvm_data.txt&quot;)print(type(data))# Feature Engineering# label column labelIndexer = StringIndexer(inputCol=&#x27;label&#x27;, outputCol=&#x27;indexedLabel&#x27;).fit(data)# ë²”ì£¼í˜• ë°ì´í„° ì²´í¬, ì¸ë±ìŠ¤í™”featureIndexer = VectorIndexer(inputCol=&#x27;features&#x27;, outputCol=&#x27;IndexedFeatures&#x27;, maxCategories=4).fit(data)# ë°ì´í„° ë¶„ë¦¬(trainingData, testData) = data.randomSplit([0.7, 0.3])# ëª¨ë¸ rf = RandomForestClassifier(labelCol=&#x27;indexedLabel&#x27;, # ì¢…ì†ë³€ìˆ˜ featuresCol=&#x27;IndexedFeatures&#x27;, # ë…ë¦½ë³€ìˆ˜ numTrees=10)# outputCol=&#x27;indexedLabel&#x27; --&gt; original labelë¡œ ë³€í™˜labelConvereter = IndexToString(inputCol=&#x27;prediction&#x27;, outputCol=&#x27;predictedLabel&#x27;, labels=labelIndexer.labels)# íŒŒì´í”„ë¼ì¸ êµ¬ì¶•pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConvereter])# ëª¨ë¸ í•™ìŠµmodel = pipeline.fit(trainingData)# ëª¨ë¸ ì˜ˆì¸¡predictions = model.transform(testData)# í–‰ì— í‘œì‹œí•  ê²ƒ ì¶”ì¶œ predictions.select(&quot;predictedLabel&quot;, &#x27;label&#x27;, &#x27;features&#x27;).show(5)# ëª¨í˜• í‰ê°€evaluator = MulticlassClassificationEvaluator( labelCol=&quot;indexedLabel&quot;, predictionCol=&quot;prediction&quot;, metricName=&quot;accuracy&quot;)accuracy = evaluator.evaluate(predictions)print(&quot;Test Error = %f &quot; % (1.0 - accuracy))spark.stop()","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Pyspark","slug":"Python/Pyspark","permalink":"https://jmj3047.github.io/categories/Python/Pyspark/"}],"tags":[{"name":"pyspark","slug":"pyspark","permalink":"https://jmj3047.github.io/tags/pyspark/"}]},{"title":"Unsupervised representation learning with deep convolutional generative adversarial networks","slug":"DCGAN","date":"2022-04-20T15:00:00.000Z","updated":"2022-07-09T04:05:29.393Z","comments":true,"path":"2022/04/21/DCGAN/","link":"","permalink":"https://jmj3047.github.io/2022/04/21/DCGAN/","excerpt":"","text":"Journal&#x2F;Conference: ICLRYear(published year): 2016Author: Alec Radford, Luke Metz, Soumith ChintalaSubject: DCGAN, Generative Model Unsupervised representation learning with deep convolutional generative adversarial networks Summary CNN ê³¼ GAN framework ë¥¼ ê²°í•©í•œ DCGAN ëª¨ë¸ì„ ì œì‹œ Introductionë…¼ë¬¸ ë‹¹ì‹œ GAN ì€ ë¶ˆì•ˆì •í•œ í•™ìŠµê³¼ Generator ì˜ ì˜¤ì‘ë™ìœ¼ë¡œ ì¸í•´ ì œí•œì ìœ¼ë¡œë§Œ ì“°ì˜€ìŠµë‹ˆë‹¤. ì´ì— ë…¼ë¬¸ì€ ì´ë¯¸ì§€ ìƒì„± ëª¨ë¸ì„ ë§Œë“¤ê¸° ìœ„í•œ CNN ê¸°ë°˜ GAN frameworkì¸ DCGAN(Deep Convolutional GANs) ì„ ì œì‹œí•©ë‹ˆë‹¤. ëª¨ë¸ êµ¬ì¡°ì— ì œì•½ì„ ê°€í•˜ì—¬ ëŒ€ë¶€ë¶„ì˜ ìƒí™©ì—ì„œ ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆê²Œ í•¨ Discriminator ë¡œ image classification ì„ ìˆ˜í–‰í•˜ì˜€ì„ ë•Œ ê¸°íƒ€ ë¹„ì§€ë„ í•™ìŠµ ëª¨ë¸ê³¼ ë¹„ìŠ·í•œ ì„±ëŠ¥ì„ ë³´ì„ íŠ¹ì • í•„í„°ê°€ íŠ¹ì • objectë¥¼ ê·¸ë ¤ë‚¸ë‹¤ëŠ” ê²ƒì„ ì‹œê°í™”í•˜ì—¬ ì œì‹œí•¨ Generator ì— ì…ë ¥í•˜ëŠ” noise ë¥¼ ì œì–´í•˜ì—¬ ìƒì„±ë˜ëŠ” ìƒ˜í”Œì˜ ë‹¤ì–‘í•œ ì†ì„±ì´ ë³€í™”í•˜ëŠ” ê²ƒì„ íƒêµ¬í•¨ Approach and Model Architectureë…¼ë¬¸ ì´ì „ì—ë„ GANì— CNNì„ ì¨ì„œ ì´ë¯¸ì§€ í’ˆì§ˆì„ ë†’ì´ë ¤ëŠ” ì‹œë„ê°€ ìˆì—ˆìœ¼ë‚˜ ì¢‹ì€ ì„±ê³¼ë¥¼ ê±°ë‘ì§€ ëª»í•˜ì˜€ë‹¤ê³  ì„¤ëª…í•©ë‹ˆë‹¤. ì´í›„, ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì—ì„œ ì•ˆì •ì ì´ê³  ë†’ì€ í•´ìƒë„ì˜ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•œ DCGAN ëª¨ë¸ ì„¤ê³„ ê°€ì´ë“œë¼ì¸ì„ ì œì‹œí•©ë‹ˆë‹¤. Details of Adversarial Training3ê°€ì§€ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. Large-scale Scene Understanding(LSUN) Imagenet-1k Faces ê·¸ì™¸ì— í•™ìŠµ ë””í…Œì¼ì„ ì•„ë˜ì™€ ê°™ì´ ì œì‹œí•©ë‹ˆë‹¤. pre-processing ì œì™¸ batch size 128 ê°€ì¤‘ì¹˜ëŠ” N(0, 0.02) ë¡œ ì´ˆê¸°í™” Leaky ReLUì˜ ê¸°ìš¸ê¸°ëŠ” 0.2ë¡œ ì„¤ì •í•¨ AdamOptimizer, $\\beta_1 &#x3D;0.0002, \\beta_2&#x3D;0.9$ Generator êµ¬ì¡°ì˜ ëª¨ì‹ë„ëŠ” ìœ„ì™€ ê°™ìŠµë‹ˆë‹¤. LSUN ë°ì´í„°ì…‹ìœ¼ë¡œ 1 epoch ë¥¼ í•™ìŠµì‹œí‚¨ í›„ ì¹¨ì‹¤ì„ ìƒì„±í–ˆì„ ë•Œì˜ ê²°ê³¼ì…ë‹ˆë‹¤. ì´ë¡ ì ìœ¼ë¡œ ëª¨ë¸ì´ í›ˆë ¨ ì˜ˆì‹œë¥¼ ê¸°ì–µí–ˆì„ ìˆ˜ë„ ìˆìœ¼ë‚˜, ì‘ì€ í•™ìŠµë¥ ê³¼ ë¯¸ë‹ˆë°°ì¹˜ë¥¼ ì‚¬ìš©í–ˆìŒì„ ê°ì•ˆí•  ë•Œ ê°€ëŠ¥ì„±ì´ ë‚®ë‹¤ê³  ì„¤ëª…í•©ë‹ˆë‹¤. LSUN ë°ì´í„°ì…‹ìœ¼ë¡œ 5 epoch í•™ìŠµ í›„ ì¹¨ì‹¤ì„ ìƒì„±í•œ ê²°ê³¼ì…ë‹ˆë‹¤. ì¹¨ëŒ€ ë“±ì˜ ê·¼ì²˜ì—ì„œ ì˜¤íˆë ¤ underfitting ì´ ë°œìƒí–ˆìŒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Empirical Validation of DCGANs CapabilitiesUnsupervised representation learning ì•Œê³ ë¦¬ì¦˜ì„ í‰ê°€í•˜ëŠ” ì¼ë°˜ì ì¸ ë°©ë²•ì€ supervised ë°ì´í„°ì…‹ìœ¼ë¡œ íŠ¹ì§•ì„ ì¶”ì¶œí•œ ë’¤ performanceë¥¼ ì¸¡ì •í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. CIFAR-10 ë°ì´í„°ì…‹ì— ëŒ€í•´ ê²€ì¦í•œ ê²°ê³¼, ë‹¤ë¥¸ ë°©ë²•ë“¤(K-means, Exemplar CNN ë“±)ê³¼ ë¹„êµí•˜ì—¬ ê²°ê³¼ì— í° ì°¨ì´ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. StreetView House Numbers dataset(SVHN) ë°ì´í„°ì…‹ì—ì„œëŠ” state-of-the-art ê²°ê³¼ë¥¼ ì–»ì—ˆìŒì„ ì œì‹œí•©ë‹ˆë‹¤. Investigating and Visualizing the Internals of the Networksê°€ì¥ ê°€ê¹Œìš´ í•™ìŠµ ë°ì´í„° ì´ë¯¸ì§€ë¥¼ ì°¾ê±°ë‚˜, ìµœê·¼ì ‘ í”½ì…€&#x2F;íŠ¹ì§•ì„ í™•ì¸í•˜ê±°ë‚˜ log-likelihood metric ìœ¼ë¡œ í‰ê°€ë¥¼ í•˜ëŠ” ë°©ë²•ì€ ëª¨ë‘ ì„±ëŠ¥ì´ ë–¨ì–´ì§€ëŠ” metric ì´ê¸°ì— ì‚¬ìš©í•˜ì§€ ì•Šì•˜ìŒì„ ì–¸ê¸‰í•©ë‹ˆë‹¤. ë…¼ë¬¸ì€ ëŒ€ì‹ , 2ê°œì˜ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•  ë•Œ ì‚¬ìš©í•œ noise 2ê°œë¥¼ interpolation í•˜ê³ , interpolated z ë¡œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•œ ê²°ê³¼ë¥¼ ì œì‹œí•©ë‹ˆë‹¤. í•œ ì´ë¯¸ì§€ì—ì„œ ë‹¤ë¥¸ ì´ë¯¸ì§€ë¡œ ì ì§„ì ìœ¼ë¡œ ë³€í•´ê°€ëŠ” ëª¨ìŠµì„ ê´€ì¸¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ ë…¸ì´ì¦ˆ ë²¡í„° z ì˜ ì‚°ìˆ  ì—°ì‚°ì„ í†µí•´, vec(ì›ƒëŠ” ì—¬ì) âˆ’âˆ’ vec(ë¬´í‘œì • ì—¬ì) ++ vec(ë¬´í‘œì • ë‚¨ì) &#x3D;&#x3D; vec(ì›ƒëŠ” ë‚¨ì) ê°™ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆì—ˆìŒì„ ì œì‹œí•©ë‹ˆë‹¤. ë˜í•œ, ëœë¤í•˜ê²Œ ìƒì„±í•œ í•„í„°ì™€ í•™ìŠµëœ í•„í„°ì˜ activation ì„ ì•„ë˜ì™€ ê°™ì´ ì‹œê°í™” í•˜ì˜€ìŠµë‹ˆë‹¤. ì´í•´í•  ìˆ˜ ì—†ëŠ” feature ê°€ ì•„ë‹Œ íŠ¹ì • objectë‚˜ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ê³  ìˆìŒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Conclusions and future workë…¼ë¬¸ì€ CNN ê¸°ë°˜ì˜ ì•ˆì •ì ì¸ ì´ë¯¸ì§€ ìƒì„±ëª¨ë¸ì¸ DCGANì„ ì œì•ˆí•˜ì˜€ìœ¼ë©°, image representationì— ì í•©í•œ ì„±ëŠ¥ì„ ë³´ì„ì„ ì œì‹œí•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì—¬ì „íˆ, í•™ìŠµì´ ê¸¸ì–´ì§€ëŠ” ê²½ìš° í•„í„° ì¼ë¶€ê°€ ìš”ë™ì¹˜ëŠ” ë“±ì˜ í˜„ìƒì„ ê´€ì¸¡í•˜ê¸°ë„ í•˜ì˜€ìŒì„ ì–¸ê¸‰í•©ë‹ˆë‹¤. Link: Unsupervised representation learning with deep convolutional generative adversarial networks","categories":[{"name":"Generative Model","slug":"Generative-Model","permalink":"https://jmj3047.github.io/categories/Generative-Model/"}],"tags":[{"name":"DCGAN","slug":"DCGAN","permalink":"https://jmj3047.github.io/tags/DCGAN/"},{"name":"Convolutional Neural Networks","slug":"Convolutional-Neural-Networks","permalink":"https://jmj3047.github.io/tags/Convolutional-Neural-Networks/"},{"name":"Image Classification","slug":"Image-Classification","permalink":"https://jmj3047.github.io/tags/Image-Classification/"},{"name":"GAN","slug":"GAN","permalink":"https://jmj3047.github.io/tags/GAN/"},{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"}]},{"title":"K-Nearest Neighbor","slug":"KNN","date":"2022-04-20T15:00:00.000Z","updated":"2022-07-09T05:59:18.488Z","comments":true,"path":"2022/04/21/KNN/","link":"","permalink":"https://jmj3047.github.io/2022/04/21/KNN/","excerpt":"","text":"1. Classification ë¶„ë¥˜ë‚˜ ì˜ˆì¸¡ì„ ì§„í–‰í• ë•Œ ë‚˜ë‘ ê°€ì¥ ê°€ê¹Œìš´ ì´ì›ƒ kê°œë¥¼ ê³ ë ¤í•˜ê² ë‹¤. ë‚˜ë‘ ê°€ê¹Œìš´ ì´ì›ƒ í•œëª…ì´ ê²€ì •ìƒ‰ì´ë©´ ê²€ì •ìƒ‰ìœ¼ë¡œ íŒë‹¨ íŒŒë€ìƒ‰ì˜ ê°€ì¥ ê°€ê¹Œìš´ ì´ì›ƒì„ í™•ì¸í•´ë³¸ ê²°ê³¼ ê²€ì •ìƒ‰ ì´ë¯€ë¡œ íŒŒë€ìƒ‰ë„ ê²€ì •ìƒ‰ìœ¼ë¡œ ë¶„ë¥˜ë˜ì—ˆë‹¤ K&#x3D;3 ì¼ ê²½ìš° í˜•ê´‘ìƒ‰ ì¹œêµ¬ë¥¼ ë¶„ë¥˜í•œë‹¤ê³  í•˜ì˜€ì„ë•Œ ì´ì›ƒì¤‘ íŒŒë€ìƒ‰ì´ 2ê°œ ê²€ì •ìƒ‰ì´ í•œê°œì´ê¸° ë•Œë¬¸ì— íŒŒë€ìƒ‰ìœ¼ë¡œ ë¶„ë¥˜ëœë‹¤. ë¶„ë¥˜ë¥¼ ì›í•˜ëŠ” ê´€ì¸¡ì¹˜ì˜ ì£¼ë³€ Nê°œì˜ ë°ì´í„°(ê·¼ì ‘ ì´ì›ƒ)ì„ ê³¨ë¼ì„œ, ì£¼ë³€ëŒ€ì„¸ë¥¼ í™•ì¸ (ë‹¤ìˆ˜ê²°ì˜ ì›ì¹™ìœ¼ë¡œ) 2. Prediction ì¸ì ‘ Kê°œì˜ ë°ì´í„°ì˜ ìˆ˜ì¹˜ë¥¼ í™•ì¸í•´ì¤˜ì„œ ê·¸ ë°ì´í„°ì˜ í‰ê· ì„ ê²€ì€ì ì˜ ì˜ˆì¸¡ì¹˜ë¡œ ì„¤ì •í•´ì¤€ë‹¤. 3. How to find optimal k?kì˜ ê²°ì • kê°€ ë„ˆë¬´ í° ê²½ìš°, KNNëª¨ë¸ì´ ì§€ë‚˜ì¹˜ê²Œ ì¼ë°˜í™”ë¨ Kê°€ ë„ˆë¬´ ì‘ì€ ê²½ìš°,KNN ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ì˜ ë¶„ì‚°ì´ í¼ ì£¼ë¡œ ì´ê²ƒì €ê²ƒ í•´ë³´ê³  errorì´ ê°€ì¥ ì‘ì€ kë¥¼ ì„¤ì •í•˜ì—¬ì¤€ë‹¤. ê±°ë¦¬ ì²™ë„ì˜ ê²°ì • ìƒí™©ì— ë§ëŠ” ê±°ë¦¬ì²™ë„ë¥¼ ì‚¬ìš©í•˜ì—¬ì•¼ í•œë‹¤. ê±°ë¦¬ì²™ë„ì˜ ì¢…ë¥˜:Minkowski distance , Euclidean distance, Citi block distance, Mahalanobis distance, Correlation distance ë“± Reference: í•œêµ­ê³µí•™ëŒ€í•™êµ ê°•ì§€í›ˆêµìˆ˜ë‹˜ ê°•ì˜","categories":[{"name":"Basic ML","slug":"Basic-ML","permalink":"https://jmj3047.github.io/categories/Basic-ML/"}],"tags":[{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"}]},{"title":"How to install PySpark","slug":"install_PySpark","date":"2022-04-18T15:00:00.000Z","updated":"2022-04-22T02:53:55.345Z","comments":true,"path":"2022/04/19/install_PySpark/","link":"","permalink":"https://jmj3047.github.io/2022/04/19/install_PySpark/","excerpt":"","text":"Preparation installing spark need python3 if you are first using python, install anaconda Installing JAVA Installing file: Java SE 8 Archive Downloads (JDK 8u211 and later) Need to login Oracle Run the download file as admin â†’ Click Next button â†’ Changing the path on file (Space between words like Program Files can be problem during installation) Changing Path Same changes to folders in the JAVA runtime environment folder (Click â€˜Changeâ€™ and modify) Create and save jre folder in the path right after the C dirve Installing Spark Installing site: https://spark.apache.org/downloads.html Download installation file After clicking Download Spark: [spark-3.2.0-bin-hadoop3.2.tgz](https://www.apache.org/dyn/closer.lua/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz), you can download it by clicking the HTTP í•˜ë‹¨ page like picture below Installation URL: https://www.apache.org/dyn/closer.lua/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz (2022.01) Download WinRAR Program You need to install WinRAR, to unzip .tgz file. Installation file: https://www.rarlab.com/download.htm Install what fits your computer Create Spark folder and move files Moving files Copy all the file in spark-3.2.0-bin-hadoop3.2 folder After that, create spark folder below C drive and move all of them to it. Modify log4j.properties file â€¢ Open the fileconf - [log4j.properties](http://log4j.properties) Open the log file as notebook and change INFO â†’ ERROR just like example below. During the process, all the output values can be removed. 1234567# Set everything to be logged to the console# log4j.rootCategory=INFO, consolelog4j.rootCategory=ERROR, consolelog4j.appender.console=org.apache.log4j.ConsoleAppenderlog4j.appender.console.target=System.errlog4j.appender.console.layout=org.apache.log4j.PatternLayoutlog4j.appender.console.layout.ConversionPattern=%d&#123;yy/MM/dd HH:mm:ss&#125; %p %c&#123;1&#125;: %m%n Installing winutils This time, we need program that makes local computer mistakes Sparks for Hadoop. Installing file: https://github.com/cdarlint/winutils Download winutils programs that fit installation version. I downloaded version 3.2.0 Create winutils&#x2F;bin folder on C drive and save the downloaded file. Ensure this file is authorized to be used so that it can be executed without errors whne running Spark This time, open CMD as admin and run the file If ChangeFileModeByMask error (3) occurs, create tmp\\hive folder below C drive. 12C:\\Windows\\system32&gt;cd c:\\winutils\\binc:\\winutils\\bin&gt; winutils.exe chmod 777 \\tmp\\hive Setting environment variables Set the system environment variable Click the ì‚¬ìš©ì ë³€ìˆ˜ - ìƒˆë¡œ ë§Œë“¤ê¸° button on each user account Set SPARK_HOME variable Set JAVA_HOME variable Set HADOOP_HOME variable Edit PATH variable. Add the code below. Add code below %SPARK_HOME%\\bin %JAVA_HOME%\\bin Testing Spark Open CMD file, set the path as c:\\spark folder if the logo appears when input â€˜sparkâ€™, success Check whether the code below works 1234&gt;&gt;&gt; rd = sc.textFile(&quot;README.md&quot;)&gt;&gt;&gt; rd.count()109&gt;&gt;&gt;","categories":[{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Pyspark","slug":"Python/Pyspark","permalink":"https://jmj3047.github.io/categories/Python/Pyspark/"}],"tags":[{"name":"pyspark","slug":"pyspark","permalink":"https://jmj3047.github.io/tags/pyspark/"}]},{"title":"Adversarial Speaker Verification","slug":"Adversaria 2d6a8","date":"2022-04-16T15:00:00.000Z","updated":"2022-07-09T04:05:09.579Z","comments":true,"path":"2022/04/17/Adversaria 2d6a8/","link":"","permalink":"https://jmj3047.github.io/2022/04/17/Adversaria%202d6a8/","excerpt":"","text":"Journal&#x2F;Conference: ICASSP IEEEYear(published year): 2019Author: Zhong Meng, Yong Zhao, Jinyu Li, Yifan GongSubject: Speaker Verification Adversarial Speaker Verification GoalWith ASV, our goal is to learn a condition-invariant and speaker-discriminative deep hidden feature in the background DNN through adversarial multi-task learning such that a noise-robust deep embedding can be obtained from these deep features for an enrolled speaker or a test utterance. Dataâ€œHey Cortanaâ€ from the Windows 10 desktop Cortana service logs.CHiME-3: buses (BUS), in cafes (CAF), in pedestrian areas (PED), at street junctions (STR))From the clean Cortana data, we select 6 utterances from each of the 3k speakers as the enrollment data (called â€œEnroll Aâ€). We select 60k utterances from 3k target speakers and 3k impostors in Cortana dataset and mix them with CHiME-3 real noise to generate the noisy evaluation set. Result Why? In ASV, a speaker classification network and a condition identification network are jointly trained to minimize the speaker classification loss and to mini-maximize the condition loss through adversarial multitask learning.The target labels of the condition network can be categorical (environment types) and continuous (SNR values). With ASV, speaker-discriminative and condition-invariant deep embeddings can be extracted for both enrollment and test speech. ì ëŒ€ì  í•™ìŠµì€ [22] ë…¼ë¬¸ì—ì„œ ë¨¼ì € ì ìš©ë˜ì—ˆëŠ”ë° ì´ ë…¼ë¬¸ê³¼ì˜ ì°¨ì´ì ì€, ë‘ê°€ì§€ ì†ŒìŒ ì»¨ë””ì…˜ì„ ì„œë¡œ ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ ë§‰ì€ ê²ƒ(22 ë…¼ë¬¸ì—ì„œëŠ” í™˜ê²½ ê°œì„  ë³´ë‹¤ëŠ” unlabeled íƒ€ê²Ÿ ë„ë©”ì¸ ë°ì´í„°ë¥¼ í›ˆë ¨í•˜ì—¬ ì ì‘ ì‹œí‚¤ëŠ” ê±¸ ëª©í‘œë¡œ í•¨), ê·¸ë¦¬ê³  ë³¸ ë…¼ë¬¸ì€ ë„¤íŠ¸ì›Œí¬ì— ì§ì ‘ì ìœ¼ë¡œ ìŒì„± í”¼ì²˜ë¥¼ ì¸í’‹ìœ¼ë¡œ ë„£ì–´ í›ˆë ¨í•˜ëŠ” ë°˜ë©´, 22 ë…¼ë¬¸ì€ i-ë²¡í„°ë¥¼ ì¸í’‹ìœ¼ë¡œ ë„£ì—ˆê³  ì´ëŠ” computationalí•œ ì‹œê°„ê³¼ ìì›ì´ ë” ë“¤ì–´ê°. ** Train ASV model to adapt to noise ** Experiment Embeddings : ì¸ê³µì‹ ê²½ë§ì—ì„œ ì›ë˜ ì°¨ì›ë³´ë‹¤ ì €ì°¨ì›ì˜ ë²¡í„°ë¡œ ë§Œë“œëŠ” ê²ƒì„ ì˜ë¯¸ì›ë˜ ì°¨ì›ì€ ë§¤ìš° ë§ì€ ë²”ì£¼í˜• ë³€ìˆ˜ë“¤ë¡œ êµ¬ì„±ë˜ì–´ ìˆê³  ì´ê²ƒë“¤ì´ í•™ìŠµë°©ì‹ì„ í†µí•´ ì €ì°¨ì›ìœ¼ë¡œ ëŒ€ì‘ë¨. ìˆ˜ì²œ ìˆ˜ë§Œê°œì˜ ê³ ì°¨ì› ë³€ìˆ˜ë“¤ì„ ëª‡ë°±ê°œì˜ ì €ì°¨ì› ë³€ìˆ˜ë¡œ ë§Œë“¤ì–´ ì£¼ê³ , ë˜í•œ ë³€í˜•ëœ ì €ì°¨ì› ê³µê°„ì—ì„œë„ ì¶©ë¶„íˆ ì¹´í…Œê³ ë¦¬í˜• ì˜ë¯¸ë¥¼ ë‚´ì¬í•¨.ì¶œì²˜: ì¸ê³µì‹ ê²½ë§(ë”¥ëŸ¬ë‹)ì˜ Embedding ì´ë€ ë¬´ì—‡ì¼ê¹Œ? - ì„ë² ë”©ì˜ ì˜ë¯¸(1&#x2F;3) í›ˆë ¨ë‹¨ê³„ì—ì„œ background DNNì„ í™”ìë“¤ì„ êµ¬ë³„í•˜ê¸° ìœ„í•´ í›ˆë ¨ ì‹œí‚´. F &#x3D; {f1 ,â€¦, fT }, ft âˆˆ Rrf : deep hidden featuresX &#x3D; {x1 ,â€¦, xT}, xt âˆˆ Rrx , t &#x3D; {1 ,â€¦, T} : input speech frames from training set to intermediate deep hidden featuresÎ˜f: parameters maps input speech framesMf: the hidden layers of the background DNN as a feature extractor network with parameters Î˜f P(a|ft;Î˜y), a âˆˆ A : Speaker posteriors, where A is the set of all speakers in the training setÎ˜y: maps the deep features F to the speaker posteriors.My: the upper layers of the background DNN as a speaker classifier network with parameters Î˜y Î˜f and Î˜y are optimized by Minimizing cross entropy loss of speaker classification. Y &#x3D; {y1 ,â€¦, yT }, yt âˆˆA : sequence of speaker labels aligned with X1[.]: indicator function equals to 1 if the condition in the bracket is satisfied and 0 other wise. Categorical Condition Classification Loss: to address the conditions that are characterized as a categorical variable additional condition classification network Mc: which predicts the condition posteriors p(b| ft;Î˜f ); b âˆˆ B given the deep features F from the training setB : the set of all conditions in the training set With a sequence of condition labels C &#x3D; {c1 ,â€¦, cT} that is aligned with X, compute the condition classification loss through cross-entropy Continuous Condition Regression Loss: an additional condition regression network Mc to predict the frame-level condition value (SNR value) compute the condition regression loss through mean-square error Deep feature F ë¥¼ condition invariant í•˜ê²Œ ë§Œë“¤ë ¤ë©´, ì†ŒìŒë“¤ ê°ê°ì˜ í™˜ê²½ì—ì„œ ë‚˜ì˜¤ëŠ” í”¼ì²˜ë“¤ì˜ ì°¨ì´ê°€ ìµœëŒ€í•œ ì ì–´ì•¼ í•¨.ë”°ë¼ì„œ Mf ì™€ Mc ëŠ” ê°™ì´ ì ëŒ€ì ìœ¼ë¡œ train í•˜ê²Œ ë˜ê³ , Î˜f ê°€ frame-level condition loss, Lcondition ì„ ìµœëŒ€í™” ì‹œí‚¤ê³  Î˜cê°€ Lconditionì„ ìµœì†Œí™” ì‹œí‚¤ëŠ” ë°©í–¥ìœ¼ë¡œ ê°.ì´ ë‘˜ì˜ ê²½ìŸì€ ì²˜ìŒì— Mcì— ëŒ€í•œ ì°¨ë³„ì„±ì„ ë†’ì—¬ì£¼ê³ , speaker invariance ì˜ deep feature ê°€ Mfì— ì˜í•´ ë§Œë“¤ì–´ì§.ê²°êµ­ Mfê°€ ê·¹ë‹¨ì ìœ¼ë¡œ Mcê°€ êµ¬ë³„í•˜ì§€ ëª»í•˜ëŠ” í”¼ì²˜ë¥¼ ë§Œë“œëŠ” ì§€ì ì— ìˆ˜ë ´.ê·¸ì™€ ë™ì‹œì— ë…¼ë¬¸ì—ì„œëŠ” í™”ì ì°¨ë³„ì ì¸ deep feature ë“¤ì„ Lspeaker(Eq3)ì˜ speaker classification ì†ì‹¤í•¨ìˆ˜ë¥¼ ìµœì†Œí™” í•˜ë©´ì„œ ë§Œë“¦. ìµœì ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ëŠ” ì‹: ì—¬ê¸°ì„œ Î»ê°€ speaker classification ì†ì‹¤í•¨ìˆ˜ì™€ condition í•¨ìˆ˜ ì‚¬ì´ì˜ ê· í˜•ì„ í†µì œ.GRLì€ forward propagation ì—ì„œ identity transform ì—­í• ì„ í•˜ë©° back propagation ì—ì„œ ê²½ì‚¬ë„ë¥¼ â€“ Î»ë¡œ ê³±í•¨. Link: ADVERSARIAL SPEAKER VERIFICATION","categories":[{"name":"Speaker Verification","slug":"Speaker-Verification","permalink":"https://jmj3047.github.io/categories/Speaker-Verification/"}],"tags":[{"name":"Adversarial Speaker Verification","slug":"Adversarial-Speaker-Verification","permalink":"https://jmj3047.github.io/tags/Adversarial-Speaker-Verification/"},{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"}]},{"title":"Definition of Distance","slug":"Definition_of_Distance","date":"2022-04-16T15:00:00.000Z","updated":"2022-07-09T05:53:29.637Z","comments":true,"path":"2022/04/17/Definition_of_Distance/","link":"","permalink":"https://jmj3047.github.io/2022/04/17/Definition_of_Distance/","excerpt":"","text":"1. Euclidean distance ê°€ì¥ í”íˆ ì‚¬ìš©í•˜ëŠ” ê±°ë¦¬ì¸¡ë„ ëŒ€ì‘ë˜ëŠ” x,yê°’ ê°„ ì°¨ì´ ì œê³±í•©ì˜ ì œê³±ê·¼ìœ¼ë¡œì¨, ë‘ ê´€ì¸¡ì¹˜ ì‚¬ì´ì˜ ì§ì„  ê±°ë¦¬ë¥¼ ì˜ë¯¸í•¨. ë‹¤ì°¨ì› ë°ì´í„°ì—ì„œë„ ë§ˆì°¬ê°€ì§€ ì´ë‹¤. 2. Manhattan Distance ë§¨í•˜íƒ„ì€ ë¸”ëŸ­ì´ ë‚˜ëˆ„ì–´ì ¸ ìˆì–´ ì§ì„ ìœ¼ë¡œ ê°ˆ ìˆ˜ê°€ ì—†ë‹¤. ì§ì„ ê±°ë¦¬ê°€ ì•„ë‹Œ ê²©ìê±°ë¦¬. ê²©ì:ë°”ë‘‘íŒì²˜ëŸ¼ ê°€ë¡œì„¸ë¡œë¥¼ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ì§ê°ì´ ë˜ê²Œ ì§  êµ¬ì¡°ë‚˜ ë¬¼ê±´. ê° ì¢Œí‘œì˜ ì°¨ì´ì˜ ì ˆëŒ“ê°’ì˜ í•© 3. Mahalanobis Distance ë³€ìˆ˜ ë‚´ ë¶„ì‚°,ë³€ìˆ˜ ê°„ ê³µë¶„ì‚°ì„ ëª¨ë‘ ë°˜ì˜í•˜ì—¬ x,y,ê°„ ê±°ë¦¬ë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ì‹â‡’ë³€ìˆ˜ê°„ ìƒê´€ê´€ê³„ë¥¼ ê³ ë ¤í•œ ê±°ë¦¬ì§€í‘œì´ë‹¤. ë°ì´í„°ì˜ ê³µë¶„ì‚° í–‰ë ¬ì´ ë‹¨ìœ„í–‰ë ¬ì¸ ê²½ìš°ëŠ” ìœ í´ë¦¬ë””ì•ˆ ê±°ë¦¬ì™€ ë™ì¼í•¨ ê³µë¶„ì‚° í–‰ë ¬ì˜ ì—­í–‰ë ¬ì„ ì·¨í–ˆë‹¤ëŠ” ê²ƒ â†’ ë¶„ì‚°ì´ ë¶„ëª¨ì— ë“¤ì–´ê°„ë‹¤ëŠ” ëœ» â†’ ë¶„ì‚°ì´ ì»¤ì§€ë©´ ê±°ë¦¬ê°€ ì‘ì•„ì§€ê³  , ë¶„ì‚°ì´ ì‘ì•„ì§€ë©´ ê±°ë¦¬ê°€ ê¸¸ì–´ì§ ë§ˆí• ë¼ë…¸ë¹„ìŠ¤ê±°ë¦¬ê°€ ì œê³±ê·¼ì´ ì·¨í•´ì ¸ ìˆê¸° ë•Œë¬¸ì— ì œê³±ê·¼ì„ ì—†ì•´ë‹¤. 2ì°¨ì› í–‰ë ¬ë¡œ ë¹„ìœ ë¥¼ í–ˆì„ì‹œ , ì­ˆìš± ëŒ€ì…í•˜ë©´ ì•„ë˜ì˜ ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚œë‹¤ yê°’ì— 0,0 ì„ì£¼ê³  ëŒ€ì…í•˜ë©´ íƒ€ì›ì˜ ë°©ì •ì‹ì´ ë‚˜ì˜¨ë‹¤. ìœ í´ë¦¬ë””ì•ˆ ê´€ì ì—ì„œëŠ” ì¤‘ì•™ì ê³¼ ë¹„êµí–ˆì„ë•Œ, Aê°€ ë” ë©€ë‹¤. ìƒê´€ê´€ê³„ë¥¼ ê³ ë ¤í•œ ë§ˆí• ë¼ë…¸ë¹„ìŠ¤ ê±°ë¦¬ë¡œ ë³´ë©´ Bê°€ ë” ë©€ë‹¤ Reference: í•œêµ­ê³µí•™ëŒ€í•™êµ ê°•ì§€í›ˆêµìˆ˜ë‹˜ ê°•ì˜","categories":[{"name":"Basic ML","slug":"Basic-ML","permalink":"https://jmj3047.github.io/categories/Basic-ML/"}],"tags":[{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"}]}],"categories":[{"name":"Setting","slug":"Setting","permalink":"https://jmj3047.github.io/categories/Setting/"},{"name":"Basic ML","slug":"Basic-ML","permalink":"https://jmj3047.github.io/categories/Basic-ML/"},{"name":"Speaker Verification","slug":"Speaker-Verification","permalink":"https://jmj3047.github.io/categories/Speaker-Verification/"},{"name":"NLP","slug":"NLP","permalink":"https://jmj3047.github.io/categories/NLP/"},{"name":"Python","slug":"Python","permalink":"https://jmj3047.github.io/categories/Python/"},{"name":"Django","slug":"Python/Django","permalink":"https://jmj3047.github.io/categories/Python/Django/"},{"name":"HTML","slug":"Python/HTML","permalink":"https://jmj3047.github.io/categories/Python/HTML/"},{"name":"Data Base","slug":"Data-Base","permalink":"https://jmj3047.github.io/categories/Data-Base/"},{"name":"Generative Model","slug":"Generative-Model","permalink":"https://jmj3047.github.io/categories/Generative-Model/"},{"name":"Computer Vision","slug":"Computer-Vision","permalink":"https://jmj3047.github.io/categories/Computer-Vision/"},{"name":"Pyspark","slug":"Python/Pyspark","permalink":"https://jmj3047.github.io/categories/Python/Pyspark/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://jmj3047.github.io/tags/Hexo/"},{"name":"WP_edu","slug":"WP-edu","permalink":"https://jmj3047.github.io/tags/WP-edu/"},{"name":"ML Analysis","slug":"ML-Analysis","permalink":"https://jmj3047.github.io/tags/ML-Analysis/"},{"name":"English","slug":"English","permalink":"https://jmj3047.github.io/tags/English/"},{"name":"Clustering","slug":"Clustering","permalink":"https://jmj3047.github.io/tags/Clustering/"},{"name":"Probabilistic sampling","slug":"Probabilistic-sampling","permalink":"https://jmj3047.github.io/tags/Probabilistic-sampling/"},{"name":"Nonprobability sampling","slug":"Nonprobability-sampling","permalink":"https://jmj3047.github.io/tags/Nonprobability-sampling/"},{"name":"TD-SV","slug":"TD-SV","permalink":"https://jmj3047.github.io/tags/TD-SV/"},{"name":"Deep/Machine Learning Paper Study","slug":"Deep-Machine-Learning-Paper-Study","permalink":"https://jmj3047.github.io/tags/Deep-Machine-Learning-Paper-Study/"},{"name":"Transformer","slug":"Transformer","permalink":"https://jmj3047.github.io/tags/Transformer/"},{"name":"Attention","slug":"Attention","permalink":"https://jmj3047.github.io/tags/Attention/"},{"name":"Git","slug":"Git","permalink":"https://jmj3047.github.io/tags/Git/"},{"name":"Virtualenv","slug":"Virtualenv","permalink":"https://jmj3047.github.io/tags/Virtualenv/"},{"name":"Chatbot","slug":"Chatbot","permalink":"https://jmj3047.github.io/tags/Chatbot/"},{"name":"NLP","slug":"NLP","permalink":"https://jmj3047.github.io/tags/NLP/"},{"name":"Doc2vec","slug":"Doc2vec","permalink":"https://jmj3047.github.io/tags/Doc2vec/"},{"name":"CGI","slug":"CGI","permalink":"https://jmj3047.github.io/tags/CGI/"},{"name":"Web Server","slug":"Web-Server","permalink":"https://jmj3047.github.io/tags/Web-Server/"},{"name":"Flask","slug":"Flask","permalink":"https://jmj3047.github.io/tags/Flask/"},{"name":"Brython","slug":"Brython","permalink":"https://jmj3047.github.io/tags/Brython/"},{"name":"MongoDB","slug":"MongoDB","permalink":"https://jmj3047.github.io/tags/MongoDB/"},{"name":"HTML","slug":"HTML","permalink":"https://jmj3047.github.io/tags/HTML/"},{"name":"Web Crawling","slug":"Web-Crawling","permalink":"https://jmj3047.github.io/tags/Web-Crawling/"},{"name":"BeautifulSoup","slug":"BeautifulSoup","permalink":"https://jmj3047.github.io/tags/BeautifulSoup/"},{"name":"Image Classification","slug":"Image-Classification","permalink":"https://jmj3047.github.io/tags/Image-Classification/"},{"name":"WGAN-GP","slug":"WGAN-GP","permalink":"https://jmj3047.github.io/tags/WGAN-GP/"},{"name":"WGAN","slug":"WGAN","permalink":"https://jmj3047.github.io/tags/WGAN/"},{"name":"GAN","slug":"GAN","permalink":"https://jmj3047.github.io/tags/GAN/"},{"name":"Generative Model","slug":"Generative-Model","permalink":"https://jmj3047.github.io/tags/Generative-Model/"},{"name":"Hueman","slug":"Hueman","permalink":"https://jmj3047.github.io/tags/Hueman/"},{"name":"Convolutional Neural Networks","slug":"Convolutional-Neural-Networks","permalink":"https://jmj3047.github.io/tags/Convolutional-Neural-Networks/"},{"name":"pyspark","slug":"pyspark","permalink":"https://jmj3047.github.io/tags/pyspark/"},{"name":"DCGAN","slug":"DCGAN","permalink":"https://jmj3047.github.io/tags/DCGAN/"},{"name":"Adversarial Speaker Verification","slug":"Adversarial-Speaker-Verification","permalink":"https://jmj3047.github.io/tags/Adversarial-Speaker-Verification/"}]}